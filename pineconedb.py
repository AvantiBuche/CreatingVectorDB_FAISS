# -*- coding: utf-8 -*-
"""PineconeDB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MknotMMB4KiY5_v8_vTTQXAScHl2rtN7
"""

!pip install pinecone sentence-transformers

from pinecone import Pinecone, ServerlessSpec
from sentence_transformers import SentenceTransformer

# Initialize Pinecone
pc = Pinecone(api_key="Your_API_KEY")
#Copy Key from your pinecone account

# Create index (only once)
index_name = "demo-index"

if index_name not in pc.list_indexes().names():
    pc.create_index(
        name=index_name,
        dimension=3977,  # depends on embedding model
        metric="cosine",
        spec=ServerlessSpec(
            cloud="aws",
            region="us-east-1"
        )
    )

index = pc.Index(index_name)

!pip install pypdf

import pandas as pd
import numpy as np
from pypdf import PdfReader

# Define the path to your PDF file
path = "/content/sample_data/Foundations_of_Machine_Learning.pdf"

#extract text from pdf
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PdfReader(file)
        for page in reader.pages:
            text += page.extract_text() or "" # Use or "" to handle pages with no extractable text
    return text

docs = extract_text_from_pdf(path)

# Simple chunking
chunk_size = 200
chunks = [docs[i:i+chunk_size]
          for i in range(0, len(docs), chunk_size)]

print("Total chunks:", len(chunks))
print(chunks)

model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = model.encode(chunks)

len(embeddings)

vectors = []

for i, doc in enumerate(chunks):
    vectors.append({
        "id": f"doc_{i}",
        "values": embeddings[i].tolist(),
        "metadata": {"text": doc}
    })

#If you store full PDFs / long text inside metadata, payload size explodes.
#If still error â†’ reduce batch_size to 50 or 20.

def batch_upsert(index, vectors, batch_size=200):
    for i in range(0, len(vectors), batch_size):
        batch = vectors[i:i+batch_size]
        index.upsert(vectors=batch)

batch_upsert(index, vectors, batch_size=200)

query = "What is a machine learning?"
query_embedding = model.encode([query])[0]

results = index.query(
    vector=query_embedding.tolist(),
    top_k=2,
    include_metadata=True
)

for match in results["matches"]:
    print(match["metadata"]["text"], "Score:", match["score"])