{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "SN0NlIiqEVRO",
    "outputId": "494f8c0a-9db7-4ac9-d2d2-465247615af3"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting pinecone\n",
      "  Downloading pinecone-8.1.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.3)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2026.1.4)\n",
      "Requirement already satisfied: orjson>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from pinecone) (3.11.7)\n",
      "Collecting pinecone-plugin-assistant<4.0.0,>=3.0.1 (from pinecone)\n",
      "  Downloading pinecone_plugin_assistant-3.0.2-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting pinecone-plugin-interface<0.1.0,>=0.0.7 (from pinecone)\n",
      "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.12/dist-packages (from pinecone) (4.15.0)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2.5.0)\n",
      "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (5.0.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.4.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.10.0+cpu)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.0.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.24.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (0.24.0)\n",
      "Collecting packaging>=20.9 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.12/dist-packages (from pinecone-plugin-assistant<4.0.0,>=3.0.1->pinecone) (2.32.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (4.12.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<4.0.0,>=3.0.1->pinecone) (3.4.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: typer>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (0.24.1)\n",
      "Requirement already satisfied: click>=8.2.1 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (8.3.1)\n",
      "Requirement already satisfied: rich>=12.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (13.9.4)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (0.0.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (0.1.2)\n",
      "Downloading pinecone-8.1.0-py3-none-any.whl (742 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m742.7/742.7 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pinecone_plugin_assistant-3.0.2-py3-none-any.whl (280 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m280.9/280.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
      "Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pinecone-plugin-interface, packaging, pinecone-plugin-assistant, pinecone\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 26.0\n",
      "    Uninstalling packaging-26.0:\n",
      "      Successfully uninstalled packaging-26.0\n",
      "Successfully installed packaging-24.2 pinecone-8.1.0 pinecone-plugin-assistant-3.0.2 pinecone-plugin-interface-0.0.7\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "packaging"
        ]
       },
       "id": "b87caf0ea73244c1adea96c265d12910"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "!pip install pinecone sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=\"Your_API_KEY\")\n",
    "#Copy Key from your pinecone account\n",
    "\n",
    "# Create index (only once)\n",
    "index_name = \"demo-index\"\n",
    "\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=3977,  # depends on embedding model\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "index = pc.Index(index_name)"
   ],
   "metadata": {
    "id": "yH4OSqhoiOZA"
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install pypdf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pypdf import PdfReader"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "FaUmDU6dizzX",
    "outputId": "cb4821de-0df7-493a-9c10-8adce4978f4e"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-6.7.4-py3-none-any.whl.metadata (7.1 kB)\n",
      "Downloading pypdf-6.7.4-py3-none-any.whl (331 kB)\n",
      "\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/331.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m184.3/331.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m331.5/331.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdf\n",
      "Successfully installed pypdf-6.7.4\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the path to your PDF file\n",
    "path = \"/content/sample_data/Foundations_of_Machine_Learning.pdf\"\n",
    "\n",
    "#extract text from pdf\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() or \"\" # Use or \"\" to handle pages with no extractable text\n",
    "    return text\n",
    "\n",
    "docs = extract_text_from_pdf(path)"
   ],
   "metadata": {
    "id": "lJeRwovBjCFP"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Simple chunking\n",
    "chunk_size = 200\n",
    "chunks = [docs[i:i+chunk_size]\n",
    "          for i in range(0, len(docs), chunk_size)]\n",
    "\n",
    "print(\"Total chunks:\", len(chunks))\n",
    "print(chunks)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E2fj4TBljk0x",
    "outputId": "1839c903-f56c-48b2-d033-8ce22770a4a4"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total chunks: 3977\n",
      "['Foundations of Machine LearningAdaptive Computation and Machine Learning\\nThomas Dietterich, Editor\\nChristopher Bishop, David Heckerman, Michael Jordan, and Michael Kearns,\\nAssociate Editors\\nA complete', ' list of books published in The Adaptive Computations and Machine\\nLearning series appears at the back of this book.Foundations of Machine Learning\\nMehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalk', 'ar\\nThe MIT Press\\nCambridge, Massachusetts\\nLondon, Englandc\u20dd 2012 Massachusetts Institute of Technology\\nAll rights reserved. No part of this book may be reproduced in any form by any\\nelectronic or mech', 'anical means (including photocopying, recording, or information\\nstorage and retrieval) without permission in writing from the publisher.\\nMIT Press books may be purchased at special quantity discounts ', 'for business or\\nsales promotional use. For information, please email special\\nsales@mitpress.mit.edu\\nor write to Special Sales Department, The MIT Press, 55 Hayward Street, Cam-\\nbridge, MA 02142.\\nThis ', 'book was set in LATEX by the authors. Printed and bound in the United States\\nof America.\\nLibrary of Congress Cataloging-in-Publication Data\\nMohri, Mehryar.\\nFoundations of machine learning / Mehryar Mo', 'hri, Afshin Rostamizadeh, and\\nAmeet Talwalkar.\\np. cm. - (Adaptive computation and machine learning series)\\nIncludes bibliographical references and index.\\nISBN 978-0-262-01825-8 (hardcover : alk. paper', ') 1. Machine learning. 2. Computer\\nalgorithms. I. Rostamizadeh, Afshin. II. Talwalkar, Ameet. III. Title.\\nQ325.5.M64 2012\\n006.3\u20191-dc23\\n2012007249\\n1 0987654321Contents\\nPreface xi\\n1 Introduction 1\\n1 . 1', ' A p p l i c a t i o n s a n d p r o b l e m s ........................ 1\\n1 . 2 D e \ufb01 n i t i o n s a n d t e r m i n o l o g y....................... 3\\n1 . 3 C r o s s - v a l i d a t i o n .........', '..................... 5\\n1 . 4 L e a r n i n g s c e n a r i o s ............................ 7\\n1 . 5 O u t l i n e .................................. 8\\n2 The PAC Learning Framework 11\\n2 . 1 T h e P A ', 'C l e a r n i n g m o d e l ......................... 1 1\\n2 . 2 G u a r a n t e e s f o r \ufb01 n i t e h y p o t h e s i s s e t s \u2014 c o n s i s t e n t c a s e........ 1 7\\n2 . 3 G u a r a n t e e s f o ', 'r \ufb01 n i t e h y p o t h e s i s s e t s \u2014 i n c o n s i s t e n t c a s e....... 2 1\\n2 . 4 G e n e r a l i t i e s................................ 2 4\\n2 . 4 . 1 D e t e r m i n i s t i c v e r s u s s', ' t o c h a s t i c s c e n a r i o s............ 2 4\\n2 . 4 . 2 B a y e s e r r o r a n d n o i s e ...................... 2 5\\n2 . 4 . 3 E s t i m a t i o n a n d a p p r o x i m a t i o n e r r o r s ', '.............. 2 6\\n2 . 4 . 4 M o d e l s e l e c t i o n.......................... 2 7\\n2 . 5 C h a p t e r n o t e s............................... 2 8\\n2 . 6 E x e r c i s e s ........................', '......... 2 9\\n3 Rademacher Complexity and VC-Dimension 33\\n3 . 1 R a d e m a c h e r c o m p l e x i t y ......................... 3 4\\n3 . 2 G r o w t h f u n c t i o n ............................. 3 ', '8\\n3 . 3 V C - d i m e n s i o n............................... 4 1\\n3 . 4 L o w e r b o u n d s............................... 4 8\\n3 . 5 C h a p t e r n o t e s............................... 5 4\\n3 . 6', ' E x e r c i s e s ................................. 5 5\\n4 Support Vector Machines 63\\n4 . 1 L i n e a r c l a s s i \ufb01 c a t i o n ............................ 6 3\\n4 . 2 S V M s \u2014 s e p a r a b l e c a', ' s e ......................... 6 4vi\\n4 . 2 . 1 P r i m a l o p t i m i z a t i o n p r o b l e m .................. 6 4\\n4 . 2 . 2 S u p p o r t v e c t o r s .......................... 6 6\\n4 . 2 . 3 D', ' u a l o p t i m i z a t i o n p r o b l e m ................... 6 7\\n4 . 2 . 4 L e a v e - o n e - o u t a n a l y s i s...................... 6 9\\n4 . 3 S V M s \u2014 n o n - s e p a r a b l e c a s e....', '................... 7 1\\n4 . 3 . 1 P r i m a l o p t i m i z a t i o n p r o b l e m .................. 7 2\\n4 . 3 . 2 S u p p o r t v e c t o r s .......................... 7 3\\n4 . 3 . 3 D u a l o p t ', 'i m i z a t i o n p r o b l e m ................... 7 4\\n4 . 4 M a r g i n t h e o r y ............................... 7 5\\n4 . 5 C h a p t e r n o t e s............................... 8 3\\n4 . 6 E x e r', ' c i s e s ................................. 8 4\\n5 Kernel Methods 89\\n5 . 1 I n t r o d u c t i o n ................................ 8 9\\n5 . 2 P o s i t i v e d e \ufb01 n i t e s y m m e t r i c k e r n e ', 'l s ................... 9 2\\n5 . 2 . 1 D e \ufb01 n i t i o n s ............................ 9 2\\n5 . 2 . 2 R e p r o d u c i n g k e r n e l H i l b e r t s p a c e................ 9 4\\n5 . 2 . 3 P r o p e r', ' t i e s............................. 9 6\\n5 . 3 K e r n e l - b a s e d a l g o r i t h m s......................... 1 0 0\\n5 . 3 . 1 S V M s w i t h P D S k e r n e l s..................... 1 0 0\\n5 . ', '3 . 2 R e p r e s e n t e r t h e o r e m....................... 1 0 1\\n5 . 3 . 3 L e a r n i n g g u a r a n t e e s ....................... 1 0 2\\n5 . 4 N e g a t i v e d e \ufb01 n i t e s y m m e t r i c', ' k e r n e l s ................... 1 0 3\\n5 . 5 S e q u e n c e k e r n e l s ............................. 1 0 6\\n5 . 5 . 1 W e i g h t e d t r a n s d u c e r s ...................... 1 0 6\\n5 . 5 . 2 ', 'R a t i o n a l k e r n e l s ......................... 1 1 1\\n5 . 6 C h a p t e r n o t e s............................... 1 1 5\\n5 . 7 E x e r c i s e s ................................. 1 1 6\\n6 Boost', 'ing 121\\n6 . 1 I n t r o d u c t i o n ................................ 1 2 1\\n6 . 2 A d a B o o s t ................................. 1 2 2\\n6 . 2 . 1 B o u n d o n t h e e m p i r i c a l e r r o r ...', '............... 1 2 4\\n6 . 2 . 2 R e l a t i o n s h i p w i t h c o o r d i n a t e d e s c e n t.............. 1 2 6\\n6 . 2 . 3 R e l a t i o n s h i p w i t h l o g i s t i c r e g r e s s i o n ....', '.......... 1 2 9\\n6 . 2 . 4 S t a n d a r d u s e i n p r a c t i c e..................... 1 2 9\\n6 . 3 T h e o r e t i c a l r e s u l t s ............................ 1 3 0\\n6 . 3 . 1 V C - d i m e n s', ' i o n - b a s e d a n a l y s i s .................. 1 3 1\\n6 . 3 . 2 M a r g i n - b a s e d a n a l y s i s ...................... 1 3 1\\n6 . 3 . 3 M a r g i n m a x i m i z a t i o n ...............', '....... 1 3 6\\n6 . 3 . 4 G a m e - t h e o r e t i c i n t e r p r e t a t i o n.................. 1 3 7vii\\n6 . 4 D i s c u s s i o n................................. 1 4 0\\n6 . 5 C h a p t e r n o t e ', 's............................... 1 4 1\\n6 . 6 E x e r c i s e s ................................. 1 4 2\\n7 On-Line Learning 147\\n7 . 1 I n t r o d u c t i o n ................................ 1 4 7\\n7 . 2', ' P r e d i c t i o n w i t h e x p e r t a d v i c e...................... 1 4 8\\n7 . 2 . 1 M i s t a k e b o u n d s a n d H a l v i n g a l g o r i t h m ............ 1 4 8\\n7 . 2 . 2 W e i g h t e d ', 'm a j o r i t y a l g o r i t h m .................. 1 5 0\\n7 . 2 . 3 R a n d o m i z e d w e i g h t e d m a j o r i t y a l g o r i t h m ........... 1 5 2\\n7 . 2 . 4 E x p o n e n t i a l w e i g h t', ' e d a v e r a g e a l g o r i t h m............ 1 5 6\\n7 . 3 L i n e a r c l a s s i \ufb01 c a t i o n ............................ 1 5 9\\n7 . 3 . 1 P e r c e p t r o n a l g o r i t h m ..................', '..... 1 6 0\\n7 . 3 . 2 W i n n o w a l g o r i t h m........................ 1 6 8\\n7 . 4 O n - l i n e t o b a t c h c o n v e r s i o n ....................... 1 7 1\\n7 . 5 G a m e - t h e o r e t i c ', 'c o n n e c t i o n ........................ 1 7 4\\n7 . 6 C h a p t e r n o t e s............................... 1 7 5\\n7 . 7 E x e r c i s e s ................................. 1 7 6\\n8 Multi-Class Clas', 'si\ufb01cation 183\\n8 . 1 M u l t i - c l a s s c l a s s i \ufb01 c a t i o n p r o b l e m.................... 1 8 3\\n8 . 2 G e n e r a l i z a t i o n b o u n d s.......................... 1 8 5\\n8 . 3 U n c o ', 'm b i n e d m u l t i - c l a s s a l g o r i t h m s................... 1 9 1\\n8 . 3 . 1 M u l t i - c l a s s S V M s ......................... 1 9 1\\n8 . 3 . 2 M u l t i - c l a s s b o o s t i n g a', ' l g o r i t h m s................. 1 9 2\\n8 . 3 . 3 D e c i s i o n t r e e s........................... 1 9 4\\n8.4 Aggregated multi-class algorithms ................... 1 9 8\\n8 . 4 . 1 O n e - v e r s', ' u s - a l l ........................... 1 9 8\\n8 . 4 . 2 O n e - v e r s u s - o n e.......................... 1 9 9\\n8 . 4 . 3 E r r o r - c o r r e c t i o n c o d e s...................... 2 0 1\\n8 .', ' 5 S t r u c t u r e d p r e d i c t i o n a l g o r i t h m s .................... 2 0 3\\n8 . 6 C h a p t e r n o t e s............................... 2 0 6\\n8 . 7 E x e r c i s e s ...................', '.............. 2 0 7\\n9 Ranking 209\\n9 . 1 T h e p r o b l e m o f r a n k i n g ......................... 2 0 9\\n9 . 2 G e n e r a l i z a t i o n b o u n d .......................... 2 1 1\\n9 . 3 R a n ', 'k i n g w i t h S V M s ........................... 2 1 3\\n9 . 4 R a n k B o o s t ................................ 2 1 4\\n9 . 4 . 1 B o u n d o n t h e e m p i r i c a l e r r o r .................. 2 ', '1 6\\n9 . 4 . 2 R e l a t i o n s h i p w i t h c o o r d i n a t e d e s c e n t.............. 2 1 8viii\\n9 . 4 . 3 M a r g i n b o u n d f o r e n s e m b l e m e t h o d s i n r a n k i n g ....... 2 ', '2 0\\n9 . 5 B i p a r t i t e r a n k i n g............................. 2 2 1\\n9 . 5 . 1 B o o s t i n g i n b i p a r t i t e r a n k i n g .................. 2 2 2\\n9 . 5 . 2 A r e a u n d e r t h e R ', 'O C c u r v e ................... 2 2 4\\n9 . 6 P r e f e r e n c e - b a s e d s e t t i n g......................... 2 2 6\\n9 . 6 . 1 S e c o n d - s t a g e r a n k i n g p r o b l e m ...............', '... 2 2 7\\n9 . 6 . 2 D e t e r m i n i s t i c a l g o r i t h m ..................... 2 2 9\\n9 . 6 . 3 R a n d o m i z e d a l g o r i t h m...................... 2 3 0\\n9 . 6 . 4 E x t e n s i o n t o ', 'o t h e r l o s s f u n c t i o n s................ 2 3 1\\n9 . 7 D i s c u s s i o n................................. 2 3 2\\n9 . 8 C h a p t e r n o t e s............................... 2 3 3\\n9 . 9 E x ', 'e r c i s e s ................................. 2 3 4\\n10 Regression 237\\n1 0 . 1T h e p r o b l e m o f r e g r e s s i o n........................ 2 3 7\\n1 0 . 2G e n e r a l i z a t i o n b o u n d s.', '......................... 2 3 8\\n1 0 . 2 . 1F i n i t e h y p o t h e s i s s e t s ...................... 2 3 8\\n1 0 . 2 . 2R a d e m a c h e r c o m p l e x i t y b o u n d s ................. 2 3 9\\n1', ' 0 . 2 . 3P s e u d o - d i m e n s i o n b o u n d s.................... 2 4 1\\n1 0 . 3R e g r e s s i o n a l g o r i t h m s .......................... 2 4 5\\n1 0 . 3 . 1L i n e a r r e g r e s s i o', ' n......................... 2 4 5\\n1 0 . 3 . 2K e r n e l r i d g e r e g r e s s i o n ...................... 2 4 7\\n1 0 . 3 . 3S u p p o r t v e c t o r r e g r e s s i o n .................... 2 5 2\\n', '1 0 . 3 . 4L a s s o ............................... 2 5 7\\n1 0 . 3 . 5G r o u p n o r m r e g r e s s i o n a l g o r i t h m s ............... 2 6 0\\n1 0 . 3 . 6O n - l i n e r e g r e s s i o n a l g', ' o r i t h m s .................. 2 6 1\\n1 0 . 4C h a p t e r n o t e s............................... 2 6 2\\n1 0 . 5E x e r c i s e s ................................. 2 6 3\\n11 Algorithmic Stability 26', '7\\n1 1 . 1D e \ufb01 n i t i o n s ................................. 2 6 7\\n1 1 . 2S t a b i l i t y - b a s e d g e n e r a l i z a t i o n g u a r a n t e e ................ 2 6 8\\n1 1 . 3S t a b i l i t y ', 'o f k e r n e l - b a s e d r e g u l a r i z a t i o n a l g o r i t h m s .......... 2 7 0\\n11.3.1 Application to regression algorithms: SVR and KRR . . . . . 274\\n1 1 . 3 . 2A p p l i c a t i o n t o', ' c l a s s i \ufb01 c a t i o n a l g o r i t h m s : S V M s ........ 2 7 6\\n1 1 . 3 . 3D i s c u s s i o n............................. 2 7 6\\n1 1 . 4C h a p t e r n o t e s............................... ', '2 7 7\\n1 1 . 5E x e r c i s e s ................................. 2 7 7\\n12 Dimensionality Reduction 281\\n1 2 . 1P r i n c i p a l C o m p o n e n t A n a l y s i s ..................... 2 8 2ix\\n1 2 . 2K', ' e r n e l P r i n c i p a l C o m p o n e n t A n a l y s i s ( K P C A ) ............ 2 8 3\\n1 2 . 3 K P C A a n d m a n i f o l d l e a r n i n g...................... 2 8 5\\n1 2 . 3 . 1I s o m a p .', '............................. 2 8 5\\n1 2 . 3 . 2L a p l a c i a n e i g e n m a p s....................... 2 8 6\\n1 2 . 3 . 3L o c a l l y l i n e a r e m b e d d i n g ( L L E ) ................ 2 8 7\\n', '1 2 . 4J o h n s o n - L i n d e n s t r a u s s l e m m a...................... 2 8 8\\n1 2 . 5C h a p t e r n o t e s............................... 2 9 0\\n1 2 . 6E x e r c i s e s ....................', '............. 2 9 0\\n13 Learning Automata and Languages 293\\n1 3 . 1I n t r o d u c t i o n ................................ 2 9 3\\n1 3 . 2F i n i t e a u t o m a t a ............................. 2 9 4\\n', '1 3 . 3E \ufb03 c i e n t e x a c t l e a r n i n g.......................... 2 9 5\\n1 3 . 3 . 1P a s s i v e l e a r n i n g ......................... 2 9 6\\n1 3 . 3 . 2L e a r n i n g w i t h q u e r i e s', ' ...................... 2 9 7\\n1 3 . 3 . 3L e a r n i n g a u t o m a t a w i t h q u e r i e s ................ 2 9 8\\n1 3 . 4I d e n t i \ufb01 c a t i o n i n t h e l i m i t ........................ 3 0 ', '3\\n1 3 . 4 . 1L e a r n i n g r e v e r s i b l e a u t o m a t a .................. 3 0 4\\n1 3 . 5C h a p t e r n o t e s............................... 3 0 9\\n1 3 . 6E x e r c i s e s .................', '................ 3 1 0\\n14 Reinforcement Learning 313\\n1 4 . 1L e a r n i n g s c e n a r i o............................. 3 1 3\\n1 4 . 2M a r k o v d e c i s i o n p r o c e s s m o d e l ..............', '....... 3 1 4\\n1 4 . 3P o l i c y ................................... 3 1 5\\n1 4 . 3 . 1D e \ufb01 n i t i o n............................. 3 1 5\\n1 4 . 3 . 2P o l i c y v a l u e............................ ', '3 1 6\\n1 4 . 3 . 3P o l i c y e v a l u a t i o n......................... 3 1 6\\n1 4 . 3 . 4O p t i m a l p o l i c y .......................... 3 1 8\\n1 4 . 4P l a n n i n g a l g o r i t h m s .......', '.................... 3 1 9\\n1 4 . 4 . 1V a l u e i t e r a t i o n .......................... 3 1 9\\n1 4 . 4 . 2P o l i c y i t e r a t i o n.......................... 3 2 2\\n1 4 . 4 . 3L i n e a r p r o', ' g r a m m i n g....................... 3 2 4\\n1 4 . 5L e a r n i n g a l g o r i t h m s ........................... 3 2 5\\n1 4 . 5 . 1S t o c h a s t i c a p p r o x i m a t i o n ....................', ' 3 2 6\\n1 4 . 5 . 2T D ( 0 ) a l g o r i t h m ......................... 3 3 0\\n1 4 . 5 . 3Q - l e a r n i n g a l g o r i t h m....................... 3 3 1\\n1 4 . 5 . 4S A R S A .......................', '....... 3 3 4\\n14.5.5 TD( \u03bb) a l g o r i t h m......................... 3 3 5\\n1 4 . 5 . 6L a r g e s t a t e s p a c e......................... 3 3 6\\n1 4 . 6C h a p t e r n o t e s.....................', '.......... 3 3 7x\\nConclusion 339\\nA Linear Algebra Review 341\\nA . 1 V e c t o r s a n d n o r m s ............................ 3 4 1\\nA . 1 . 1 N o r m s............................... 3 4 1\\nA . 1 . 2 D', ' u a l n o r m s............................ 3 4 2\\nA . 2 M a t r i c e s.................................. 3 4 4\\nA . 2 . 1 M a t r i x n o r m s........................... 3 4 4\\nA . 2 . 2 S i n g u l ', 'a r v a l u e d e c o m p o s i t i o n .................. 3 4 5\\nA . 2 . 3 S y m m e t r i c p o s i t i v e s e m i d e \ufb01 n i t e ( S P S D ) m a t r i c e s....... 3 4 6\\nB Convex Optimization 349\\nB ', '. 1 D i \ufb00 e r e n t i a t i o n a n d u n c o n s t r a i n e d o p t i m i z a t i o n ............ 3 4 9\\nB . 2 C o n v e x i t y................................. 3 5 0\\nB . 3 C o n s t r a i n e d o ', 'p t i m i z a t i o n ........................ 3 5 3\\nB . 4 C h a p t e r n o t e s............................... 3 5 7\\nC Probability Review 359\\nC.1 Probability ................................ 3 5 9\\n', 'C . 2 R a n d o m v a r i a b l e s ............................. 3 5 9\\nC . 3 C o n d i t i o n a l p r o b a b i l i t y a n d i n d e p e n d e n c e............... 3 6 1\\nC.4 Expectation, Markov\u2019s i', 'nequality, and moment-generating function\\nC . 5 V a r i a n c e a n d C h e b y s h e v \u2019 s i n e q u a l i t y.................. 3 6 5\\nD Concentration inequalities 369\\nD . 1 H o e \ufb00 d i n g \u2019 s i n e', ' q u a l i t y .......................... 3 6 9\\nD . 2 M c D i a r m i d \u2019 s i n e q u a l i t y ......................... 3 7 1\\nD . 3 O t h e r i n e q u a l i t i e s ............................. 3 ', '7 3\\nD . 3 . 1 B i n o m i a l d i s t r i b u t i o n : S l u d \u2019 s i n e q u a l i t y ............ 3 7 4\\nD . 3 . 2 N o r m a l d i s t r i b u t i o n : t a i l b o u n d................. 3 7 4\\nD . ', '3 . 3 K h i n t c h i n e - K a h a n e i n e q u a l i t y.................. 3 7 4\\nD . 4 C h a p t e r n o t e s............................... 3 7 6\\nD . 5 E x e r c i s e s .........................', '........ 3 7 7\\nE Notation 379\\nReferences 381\\nIndex 397\\n363.Preface\\nThis book is a general introduction to machine learning that can serve as a textbook\\nfor students and researchers in the \ufb01eld. It cov', 'ers fundamental modern topics in\\nmachine learning while providing the theoretical basis and conceptual tools needed\\nfor the discussion and justi\ufb01cation of algorithms. It also describes several key asp', 'ects\\nof the application of these algorithms.\\nWe have aimed to present the most novel theoretical tools and concepts while\\ngiving concise proofs, even for relatively advanced results. In general, whene', 'ver\\npossible, we have chosen to favor succinctness. Nevertheless, we discuss some crucial\\ncomplex topics arising in machine learning and highlight several open research\\nquestions. Certain topics often', ' merged with others or treated with insu\ufb03cient\\nattention are discussed separately here and with more emphasis: for example, a\\ndi\ufb00erent chapter is reserved for multi-class classi\ufb01cation, ranking, and r', 'egression.\\nAlthough we cover a very wide variety of important topics in machine learning, we\\nhave chosen to omit a few important ones, including graphical models and neural\\nnetworks, both for the sake', ' of brevity and because of the current lack of solid\\ntheoretical guarantees for some methods.\\nThe book is intended for students and researchers in machine learning, statistics\\nand other related areas.', ' It can be used as a textbook for both graduate and advanced\\nundergraduate classes in machine learning or as a reference text for a research\\nseminar. The \ufb01rst three chapters of the book lay the theore', 'tical foundation for the\\nsubsequent material. Other chapters are mostly self-contained, with the exception\\nof chapter 5 which introduces some concepts that are extensively used in later\\nones. Each cha', 'pter concludes with a series of exercises, with full solutions presented\\nseparately.\\nThe reader is assumed to be familiar with basic concepts in linear algebra,\\nprobability, and analysis of algorithms', '. However, to further help him, we present\\nin the appendix a concise linear algebra and a probability review, and a short\\nintroduction to convex optimization. We have also collected in the appendix a\\n', 'number of useful tools for concentration bounds used in this book.\\nTo our knowledge, there is no single textbook covering all of the material\\npresented here. The need for a uni\ufb01ed presentation has bee', 'n pointed out to usxii Preface\\nevery year by our machine learning students. There are several good books for\\nvarious specialized areas, but these books do not include a discussion of other\\nfundamental', ' topics in a general manner. For example, books about kernel methods\\ndo not include a discussion of other fundamental topics such as boosting, ranking,\\nreinforcement learning, learning automata or onl', 'ine learning. There also exist more\\ngeneral machine learning books, but the theoretical foundation of our book and our\\nemphasis on proofs make our presentation quite distinct.\\nM o s to ft h em a t e r', ' i a lp r e s e n t e dh e r et a k e si t so r i g i n si nam a c h i n el e a r n i n g\\ngraduate course ( Foundations of Machine Learning ) taught by the \ufb01rst author\\nat the Courant Institute of Math', 'ematical Sciences in New York University over\\nthe last seven years. This book has considerably bene\ufb01ted from the comments\\nand suggestions from students in these classes, along with those of many frien', 'ds,\\ncolleagues and researchers to whom we are deeply indebted.\\nWe are particularly grateful to Corinna Cortes and Yishay Mansour who have\\nboth made a number of key suggestions for the design and organ', 'ization of the\\nm a t e r i a lp r e s e n t e dw i t hd e t a i l e dc o m m e n t st h a tw eh a v ef u l l yt a k e ni n t oa c c o u n t\\nand that have greatly improved the presentation. We are also', ' grateful to Yishay\\nMansour for using a preliminary version of the book for teaching and for reporting\\nhis feedback to us.\\nWe also thank for discussions, suggested improvement, and contributions of ma', 'ny\\nkinds the following colleagues and friends from academic and corporate research lab-\\noratories: Cyril Allauzen, Stephen Boyd, Spencer Greenberg, Lisa Hellerstein, Sanjiv\\nKumar, Ryan McDonald, Andre', 's Mu\u02dcnoz Medina, Tyler Neylon, Peter Norvig, Fer-\\nnando Pereira, Maria Pershina, Ashish Rastogi, Michael Riley, Umar Syed, Csaba\\nSzepesv\u00b4ari, Eugene Weinstein, and Jason Weston.\\nFinally, we thank the ', 'MIT Press publication team for their help and support in\\nthe development of this text.1 Introduction\\nMachine learning can be broadly de\ufb01ned as computational methods using experience\\nto improve perform', 'ance or to make accurate predictions. Here,experience refers to\\nthe past information available to the learner, which typically takes the form of\\nelectronic data collected and made available for analys', 'is. This data could be in the\\nform of digitized human-labeled training sets, or other types of information obtained\\nvia interaction with the environment. In all cases, its quality and size are crucial', ' to\\nthe success of the predictions made by the learner.\\nMachine learning consists of designing e\ufb03cient and accurate prediction algo-\\nrithms. As in other areas of computer science, some critical measur', 'es of the quality\\nof these algorithms are their time and space complexity. But, in machine learning,\\nwe will need additionally a notion of sample complexity to evaluate the sample size\\nrequired for th', 'e algorithm to learn a family of concepts. More generally, theoreti-\\ncal learning guarantees for an algorithm depend on the complexity of the concept\\nclasses considered and the size of the training sa', 'mple.\\nSince the success of a learning algorithm depends on the data used, machine\\nlearning is inherently related to data analysis and statistics. More generally, learning\\ntechniques are data-driven me', 'thods combining fundamental concepts in computer\\nscience with ideas from statistics, probability and optimization.\\n1.1 Applications and problems\\nLearning algorithms have been successfully deployed in ', 'a variety of applications,\\nincluding\\nText or document classi\ufb01cation, e.g., spam detection;\\nNatural language processing, e.g., morphological analysis, part-of-speech tagging,\\nstatistical parsing, named', '-entity recognition;\\nSpeech recognition, speech synthesis, speaker veri\ufb01cation;\\nOptical character recognition (OCR);\\nComputational biology applications, e.g., protein function or structured predic-2 I', 'ntroduction\\ntion;\\nComputer vision tasks, e.g., image recognition, face detection;\\nFraud detection (credit card, telephone) and network intrusion;\\nGames, e.g., chess, backgammon;\\nUnassisted vehicle con', 'trol (robots, navigation);\\nMedical diagnosis;\\nRecommendation systems, search engines, information extraction systems.\\nThis list is by no means comprehensive, and learning algorithms are applied to new', '\\napplications every day. Moreover, such applications correspond to a wide variety of\\nlearning problems. Some major classes of learning problems are:\\nClassi\ufb01cation : Assign a category to each item. For', ' example, document classi\ufb01ca-\\ntion may assign items with categories such as politics, business, sports,o r weather\\nwhile image classi\ufb01cation may assign items with categories such as landscape, por-\\ntr', 'ait,o r animal. The number of categories in such tasks is often relatively small,\\nbut can be large in some di\ufb03cult tasks and even unbounded as in OCR, text clas-\\nsi\ufb01cation, or speech recognition.\\nRegr', 'ession: Predict a real value for each item. Examples of regression include\\nprediction of stock values or variations of economic variables. In this problem, the\\npenalty for an incorrect prediction depe', 'nds on the magnitude of the di\ufb00erence\\nbetween the true and predicted values, in contrast with the classi\ufb01cation problem,\\nwhere there is typically no notion of closeness between various categories.\\nRan', 'king: Order items according to some criterion. Web search, e.g., returning\\nweb pages relevant to a search query, is the canonical ranking example. Many other\\nsimilar ranking problems arise in the cont', 'ext of the design of information extraction\\nor natural language processing systems.\\nClustering: Partition items into homogeneous regions. Clustering is often per-\\nformed to analyze very large data set', 's. For example, in the context of social net-\\nwork analysis, clustering algorithms attempt to identify \u201ccommunities\u201d within large\\ngroups of people.\\nDimensionality reduction or manifold learning: Trans', 'form an initial representa-\\ntion of items into a lower-dimensional representation of these items while preserving\\nsome properties of the initial representation. A common example involves prepro-\\ncessi', 'ng digital images in computer vision tasks.\\nThe main practical objectives of machine learning consist of generating accurate\\npredictions for unseen items and of designing e\ufb03cient and robust algorithms', ' to\\nproduce these predictions, even for large-scale problems. To do so, a number of\\nalgorithmic and theoretical questions arise. Some fundamental questions include:1.2 De\ufb01nitions and terminology 3\\nFig', 'ure 1.1 The zig-zag line on the left panel is consistent over the blue and red\\ntraining sample, but it is a complex separation surface that is not likely to generalize\\nwell to unseen data. In contrast', ', the decision surface on the right panel is simpler\\nand might generalize better in spite of its misclassi\ufb01cation of a few points of the\\ntraining sample.\\nWhich concept families can actually be learned', ', and under what conditions? How\\nwell can these concepts be learned computationally?\\n1.2 De\ufb01nitions and terminology\\nWe will use the canonical problem of spam detection as a running example to\\nillustra', 'te some basic de\ufb01nitions and to describe the use and evaluation of machine\\nlearning algorithms in practice. Spam detection is the problem of learning to\\nautomatically classify email messages as either', ' spam or non-spam.\\nExamples: Items or instances of data used for learning or evaluation. In our spam\\nproblem, these examples correspond to the collection of email messages we will use\\nfor learning and', ' testing.\\nFeatures: The set of attributes, often represented as a vector, associated to an\\nexample. In the case of email messages, some relevant features may include the\\nlength of the message, the nam', 'e of the sender, various characteristics of the header,\\nthe presence of certain keywords in the body of the message, and so on.\\nLabels: Values or categories assigned to examples. In classi\ufb01cation prob', 'lems,\\nexamples are assigned speci\ufb01c categories, for instance, the spam and non- spam\\ncategories in our binary classi\ufb01cation problem. In regression, items are assigned\\nreal-valued labels.\\nTraining samp', 'le: Examples used to train a learning algorithm. In our spam\\nproblem, the training sample consists of a set of email examples along with their\\nassociated labels. The training sample varies for di\ufb00eren', 't learning scenarios, as\\ndescribed in section 1.4.\\nValidation sample: Examples used to tune the parameters of a learning algorithm4 Introduction\\nwhen working with labeled data. Learning algorithms typ', 'ically have one or more\\nfree parameters, and the validation sample is used to select appropriate values for\\nthese model parameters.\\nTest sample: Examples used to evaluate the performance of a learning', ' algorithm.\\nThe test sample is separate from the training and validation data and is not made\\navailable in the learning stage. In the spam problem, the test sample consists of a\\ncollection of email ex', 'amples for which the learning algorithm must predict labels\\nbased on features. These predictions are then compared with the labels of the test\\nsample to measure the performance of the algorithm.\\nLoss ', 'function: A function that measures the di\ufb00erence, or loss, between a pre-\\ndicted label and a true label. Denoting the set of all labels as Y and the set of\\npossible predictions as Y\u2032, a loss function ', 'L is a mapping L: Y\u00d7Y \u2032 \u2192 R+. In most\\ncases, Y\u2032 = Y and the loss function is bounded, but these conditions do not always\\nhold. Common examples of loss functions include the zero-one (or misclassi\ufb01cati', 'on)\\nloss de\ufb01ned over {\u22121, +1}\u00d7{ \u2212 1, +1} by L(y,y\\n\u2032)=1 y\u2032 \u0338=y and the squared loss\\nde\ufb01ned over I \u00d7 I by L(y,y \u2032)=( y\u2032 \u2212 y)2,w h e r eI \u2286 R is typically a bounded\\ninterval.\\nHypothesis set: A set of fun', 'ctions mapping features (feature vectors) to the set of\\nlabels Y. In our example, these may be a set of functions mapping email features\\nto Y = {spam, non-spam}. More generally, hypotheses may be func', 'tions mapping\\nfeatures to a di\ufb00erent setY\u2032. They could be linear functions mapping email feature\\nvectors to real numbers interpreted as scores (Y\u2032 = R), with higher score values\\nmore indicative of spa', 'm than lower ones.\\nWe now de\ufb01ne the learning stages of our spam problem. We start with a given\\ncollection of labeled examples. We \ufb01rst randomly partition the data into a training\\nsample, a validation ', 'sample, and a test sample. The size of each of these samples\\ndepends on a number of di\ufb00erent considerations. For example, the amount of data\\nreserved for validation depends on the number of free param', 'eters of the algorithm.\\nAlso, when the labeled sample is relatively small, the amount of training data is\\noften chosen to be larger than that of test data since the learning performance\\ndirectly depen', 'ds on the training sample.\\nNext, we associate relevant features to the examples. This is a critical step in\\nthe design of machine learning solutions. Useful features can e\ufb00ectively guide the\\nlearning ', 'algorithm, while poor or uninformative ones can be misleading. Although\\nit is critical, to a large extent, the choice of the features is left to the user. This\\nchoice re\ufb02ects the user\u2019sprior knowledge', ' about the learning task which in practice\\ncan have a dramatic e\ufb00ect on the performance results.\\nNow, we use the features selected to train our learning algorithm by \ufb01xing di\ufb00erent\\nvalues of its free ', 'parameters. For each value of these parameters, the algorithm1.3 Cross-validation 5\\nselects a di\ufb00erent hypothesis out of the hypothesis set. We choose among them\\nthe hypothesis resulting in the best p', 'erformance on the validation sample. Finally,\\nusing that hypothesis, we predict the labels of the examples in the test sample. The\\nperformance of the algorithm is evaluated by using the loss function ', 'associated to\\nthe task, e.g., the zero-one loss in our spam detection task, to compare the predicted\\nand true labels.\\nThus, the performance of an algorithm is of course evaluated based on its test err', 'or\\nand not its error on the training sample. A learning algorithm may be consistent,\\nthat is it may commit no error on the examples of the training data, and yet\\nhave a poor performance on the test da', 'ta. This occurs for consistent learners\\nde\ufb01ned by very complex decision surfaces, as illustrated in \ufb01gure 1.1, which tend\\nto memorize a relatively small training sample instead of seeking to generaliz', 'e well.\\nThis highlights the key distinction between memorization and generalization, which\\nis the fundamental property sought for an accurate learning algorithm. Theoretical\\nguarantees for consistent ', 'learners will be discussed with great detail in chapter 2.\\n1.3 Cross-validation\\nIn practice, the amount of labeled data available is often too small to set aside\\na validation sample since that would l', 'eave an insu\ufb03cient amount of training data.\\nInstead, a widely adopted method known asn-fold cross-validation is used to exploit\\nthe labeled data both for model selection (selection of the free paramet', 'ers of the\\nalgorithm) and for training.\\nLet \u03b8 denote the vector of free parameters of the algorithm. For a \ufb01xed value\\nof \u03b8, the method consists of \ufb01rst randomly partitioning a given sample S of\\nm labe', 'led examples into n subsamples, or folds. The ith fold is thus a labeled\\nsample ((x\\ni1,y i1),..., (ximi ,y imi )) of size mi. Then, for any i \u2208 [1,n ], the learning\\nalgorithm is trained on all but the', ' ith fold to generate a hypothesis hi,a n dt h e\\nperformance of hi is tested on the ith fold, as illustrated in \ufb01gure 1.2a. The\\nparameter value \u03b8 is evaluated based on the average error of the hypothe', 'ses hi,\\nwhich is called the cross-validation error.T h i sq u a n t i t yi sd e n o t e db y\u02c6RCV(\u03b8)a n d\\nde\ufb01ned by\\n\u02c6RCV(\u03b8)= 1\\nn\\nn\u2211\\ni=1\\n1\\nmi\\nmi\u2211\\nj=1\\nL(hi(xij),y ij)\\n\\ued19 \\ued18\\ued17 \\ued1a\\nerror of hi on the ith fold\\n.', '\\nThe folds are generally chosen to have equal size, that ismi = m/n for all i \u2208 [1,n ].\\nHow should n be chosen? The appropriate choice is subject to a trade-o\ufb00 and the\\ntopic of much learning theory re', 'search that we cannot address in this introductory6 Introduction\\ntest train train train train\\ntesttrain train train train\\n.\\n.\\n.\\ntesttrain train traintrain\\nerror\\nm\\n(a) (b)\\nFigure 1.2 n-fold cross valid', 'ation. (a) Illustration of the partitioning of the\\ntraining data into 5 folds. (b) Typical plot of a classi\ufb01er\u2019s prediction error as a\\nfunction of the size of the training sample: the error decreases ', 'as a function of the\\nnumber of training points.\\nchapter. For a largen, each training sample used in n-fold cross-validation has size\\nm\u2212m/n = m(1\u22121/n) (illustrated by the right vertical red line in \ufb01gu', 're 1.2b), which\\nis close to m, the size of the full sample, but the training samples are quite similar.\\nThus, the method tends to have a small bias but a large variance. In contrast,\\nsmaller values of', ' n lead to more diverse training samples but their size (shown by\\nthe left vertical red line in \ufb01gure 1.2b) is signi\ufb01cantly less than m,t h u st h em e t h o d\\ntends to have a smaller variance but a l', 'arger bias.\\nIn machine learning applications, n is typically chosen to be 5 or 10. n-fold cross\\nvalidation is used as follows in model selection. The full labeled data is \ufb01rst split\\ninto a training an', 'd a test sample. The training sample of size m is then used to\\ncompute the n-fold cross-validation error \u02c6R\\nCV(\u03b8) for a small number of possible\\nvalues of \u03b8. \u03b8 is next set to the value \u03b80 for which \u02c6R', 'CV(\u03b8) is smallest and the\\nalgorithm is trained with the parameter setting \u03b80 over the full training sample of\\nsize m.I t sp e r f o r m a n c ei se v a l u a t e do nt h et e s ts a m p l ea sa l r e ', 'a d yd e s c r i b e di nt h e\\nprevious section.\\nThe special case of n-fold cross validation where n = m is called leave-one-out\\ncross-validation, since at each iteration exactly one instance is left ', 'out of the training\\nsample. As shown in chapter 4, the average leave-one-out error is an approximately\\nunbiased estimate of the average error of an algorithm and can be used to derive\\nsimple guarantee', 's for some algorithms. In general, the leave-one-out error is very\\ncostly to compute, since it requires training n times on samples of size m \u2212 1, but\\nfor some algorithms it admits a very e\ufb03cient comp', 'utation (see exercise 10.9).\\nIn addition to model selection, n-fold cross validation is also commonly used for\\nperformance evaluation. In that case, for a \ufb01xed parameter setting\u03b8, the full labeled\\nsam', 'ple is divided inton random folds with no distinction between training and test\\nsamples. The performance reported is the n-fold cross-validation on the full sample\\nas well as the standard deviation of', ' the errors measured on each fold.1.4 Learning scenarios 7\\n1.4 Learning scenarios\\nWe next brie\ufb02y describe common machine learning scenarios. These scenarios di\ufb00er\\nin the types of training data availab', 'le to the learner, the order and method by which\\ntraining data is received and the test data used to evaluate the learning algorithm.\\nSupervised learning: The learner receives a set of labeled example', 's as training\\ndata and makes predictions for all unseen points. This is the most common scenario\\nassociated with classi\ufb01cation, regression, and ranking problems. The spam detection\\nproblem discussed i', 'n the previous section is an instance of supervised learning.\\nUnsupervised learning: The learner exclusively receives unlabeled training data,\\nand makes predictions for all unseen points. Since in gen', 'eral no labeled exam-\\nple is available in that setting, it can be di\ufb03cult to quantitatively evaluate the\\nperformance of a learner. Clustering and dimensionality reduction are example of\\nunsupervised l', 'earning problems.\\nSemi-supervised learning: The learner receives a training sample consisting of\\nboth labeled and unlabeled data, and makes predictions for all unseen points. Semi-\\nsupervised learning', ' is common in settings where unlabeled data is easily accessible\\nbut labels are expensive to obtain. Various types of problems arising in applications,\\nincluding classi\ufb01cation, regression, or ranking ', 'tasks, can be framed as instances\\nof semi-supervised learning. The hope is that the distribution of unlabeled data\\naccessible to the learner can help him achieve a better performance than in the\\nsuper', 'vised setting. The analysis of the conditions under which this can indeed\\nbe realized is the topic of much modern theoretical and applied machine learning\\nresearch.\\nTransductive inference: As in the s', 'emi-supervised scenario, the learner receives\\na labeled training sample along with a set of unlabeled test points. However, the\\nobjective of transductive inference is to predict labels only for these ', 'particular test\\npoints. Transductive inference appears to be an easier task and matches the scenario\\nencountered in a variety of modern applications. However, as in the semi-supervised\\nsetting, the as', 'sumptions under which a better performance can be achieved in this\\nsetting are research questions that have not been fully resolved.\\nOn-line learning : In contrast with the previous scenarios, the onl', 'ine scenario\\ninvolves multiple rounds and training and testing phases are intermixed. At each\\nround, the learner receives an unlabeled training point, makes a prediction, receives\\nthe true label, and ', 'incurs a loss. The objective in the on-line setting is to minimize\\nthe cumulative loss over all rounds. Unlike the previous settings just discussed, no\\ndistributional assumption is made in on-line lea', 'rning. In fact, instances and their\\nlabels may be chosen adversarially within this scenario.8 Introduction\\nReinforcement learning: The training and testing phases are also intermixed in\\nreinforcement ', 'learning. To collect information, the learner actively interacts with the\\nenvironment and in some cases a\ufb00ects the environment, and receives an immediate\\nreward for each action. The object of the lear', 'ner is to maximize his reward over\\na course of actions and iterations with the environment. However, no long-term\\nreward feedback is provided by the environment, and the learner is faced with the\\nexpl', 'oration versus exploitation dilemma, since he must choose between exploring\\nunknown actions to gain more information versus exploiting the information already\\ncollected.\\nActive learning: The learner a', 'daptively or interactively collects training examples,\\ntypically by querying an oracle to request labels for new points. The goal in\\nactive learning is to achieve a performance comparable to the stand', 'ard supervised\\nlearning scenario, but with fewer labeled examples. Active learning is often used\\nin applications where labels are expensive to obtain, for example computational\\nbiology applications.\\nI', 'n practice, many other intermediate and somewhat more complex learning scenarios\\nmay be encountered.\\n1.5 Outline\\nThis book presents several fundamental and mathematically well-studied algo-\\nrithms. It', ' discusses in depth their theoretical foundations as well as their practical\\napplications. The topics covered include:\\nProbably approximately correct (PAC) learning framework; learning guarantees\\nfor ', '\ufb01nite hypothesis sets;\\nLearning guarantees for in\ufb01nite hypothesis sets, Rademacher complexity, VC-\\ndimension;\\nSupport vector machines (SVMs), margin theory;\\nKernel methods, positive de\ufb01nite symmetric ', 'kernels, representer theorem, rational\\nkernels;\\nBoosting, analysis of empirical error, generalization error, margin bounds;\\nOnline learning, mistake bounds, the weighted majority algorithm, the expone', 'n-\\ntial weighted average algorithm, the Perceptron and Winnow algorithms;\\nMulti-class classi\ufb01cation, multi-class SVMs, multi-class boosting, one-versus-all,\\none-versus-one, error-correction methods;\\nR', 'anking, ranking with SVMs, RankBoost, bipartite ranking, preference-based1.5 Outline 9\\nranking;\\nRegression, linear regression, kernel ridge regression, support vector regression,\\nLasso;\\nStability-base', 'd analysis, applications to classi\ufb01cation and regression;\\nDimensionality reduction, principal component analysis (PCA), kernel PCA,\\nJohnson-Lindenstrauss lemma;\\nLearning automata and languages;\\nReinfo', 'rcement learning, Markov decision processes, planning and learning prob-\\nlems.\\nThe analyses in this book are self-contained, with relevant mathematical concepts\\nrelated to linear algebra, convex optim', 'ization, probability and statistics included in\\nthe appendix.2 The PAC Learning Framework\\nSeveral fundamental questions arise when designing and analyzing algorithms that\\nlearn from examples: What can', ' be learned e\ufb03ciently? What is inherently hard to\\nlearn? How many examples are needed to learn successfully? Is there a general model\\nof learning? In this chapter, we begin to formalize and address th', 'ese questions by\\nintroducing the Probably Approximately Correct (PAC) learning framework. The\\nPAC framework helps de\ufb01ne the class of learnable concepts in terms of the number\\nof sample points needed t', 'o achieve an approximate solution,sample complexity,a n d\\nthe time and space complexity of the learning algorithm, which depends on the cost\\nof the computational representation of the concepts.\\nWe \ufb01rs', 't describe the PAC framework and illustrate it, then present some general\\nlearning guarantees within this framework when the hypothesis set used is \ufb01nite,\\nboth for the consistent case where the hypoth', 'esis set used contains the concept to\\nlearn and for the opposite inconsistent case.\\n2.1 The PAC learning model\\nWe \ufb01rst introduce several de\ufb01nitions and the notation needed to present the PAC\\nmodel, wh', 'ich will also be used throughout much of this book.\\nWe denote byX the set of all possibleexamples or instances. X is also sometimes\\nreferred to as theinput space. The set of all possiblelabels or targ', 'et values is denoted\\nby Y. For the purpose of this introductory chapter, we will limit ourselves to the\\ncase where Y is reduced to two labels, Y = {0, 1}, so-called binary classi\ufb01cation .\\nLater chapte', 'rs will extend these results to more general settings.\\nA concept c: X\u2192 Y is a mapping from X to Y.S i n c eY = {0,1},w ec a ni d e n t i f y\\nc with the subset of X over which it takes the value 1. Thu', 's, in the following, we\\ne q u i v a l e n t l yr e f e rt oac o n c e p tt ol e a r na sam a p p i n gf r o mX to {0, 1},o rt oa\\nsubset of X. As an example, a concept may be the set of points inside a', ' triangle or\\nthe indicator function of these points. In such cases, we will say in short that the\\nconcept to learn is a triangle. A concept class is a set of concepts we may wish to\\nlearn and is denot', 'ed byC. This could, for example, be the set of all triangles in the12 The PAC Learning Framework\\nplane.\\nWe assume that examples are independently and identically distributed (i.i.d.)\\naccording to some', ' \ufb01xed but unknown distribution D. The learning problem is then\\nformulated as follows. The learner considers a \ufb01xed set of possible concepts H,\\ncalled a hypothesis set ,w h i c hm a yn o tc o i n c i d', ' ew i t hC. He receives a sample\\nS =( x1,...,x m) drawn i.i.d. according toD as well as the labels (c(x1),...,c (xm)),\\nw h i c ha r eb a s e do nas p e c i \ufb01 ct a r g e tc o n c e p tc \u2208 C to learn. H', 'is task is to use the\\nlabeled sample S to select a hypothesis hS \u2208 H that has a small generalization\\nerror with respect to the conceptc. The generalization error of a hypothesish \u2208 H,\\nalso referred to', ' as the true error or just error of h is denoted by R(h) and de\ufb01ned\\nas follows.1\\nDe\ufb01nition 2.1 Generalization error\\nGiven a hypothesis h \u2208 H, a target concept c \u2208 C, and an underlying distribution\\nD,t', ' h egeneralization error or risk of h is de\ufb01ned by\\nR(h)= P r\\nx\u223cD\\n[h(x) \u0338= c(x)] = E\\nx\u223cD\\n[\\n1h(x)\u0338=c(x)\\n]\\n, (2.1)\\nwhere 1\u03c9 is the indicator function of the event \u03c9.2\\nThe generalization error of a hypoth', 'esis is not directly accessible to the learner\\nsince both the distribution D and the target concept c are unknown. However, the\\nlearner can measure the empirical error of a hypothesis on the labeled s', 'ample S.\\nDe\ufb01nition 2.2 Empirical error\\nGiven a hypothesis h \u2208 H, a target concept c \u2208 C, and a sample S =( x1,...,x m),\\nthe empirical error or empirical risk of h is de\ufb01ned by\\n\u02c6R(h)= 1\\nm\\nm\u2211\\ni=1\\n1h(xi)', '\u0338=c(xi). (2.2)\\nThus, the empirical error of h \u2208 H is its average error over the sampleS, while the\\ngeneralization error is its expected error based on the distributionD. We will see in\\nthis chapter an', 'd the following chapters a number of guarantees relating to these two\\nquantities with high probability, under some general assumptions. We can already\\nnote that for a \ufb01xed h \u2208 H, the expectation of th', 'e empirical error based on an i.i.d.\\n1 .T h ec h o i c eo fR instead of E to denote an error avoids possible confusions with the\\nnotation for expectations and is further justi\ufb01ed by the fact that the ', 'termrisk is also used\\nin machine learning and statistics to refer to an error.\\n2. For this and other related de\ufb01nitions, the family of functionsH and the target concept\\nc must be measurable. The funct', 'ion classes we consider in this book all have this property.2.1 The PAC learning model 13\\nsample S is equal to the generalization error:\\nE[ \u02c6R(h)] = R(h). (2.3)\\nIndeed, by the linearity of the expecta', 'tion and the fact that the sample is drawn\\ni.i.d., we can write\\nE\\nS\u223cDm\\n[ \u02c6R(h)] = 1\\nm\\nm\u2211\\ni=1\\nE\\nS\u223cDm\\n[1h(xi)\u0338=c(xi)]= 1\\nm\\nm\u2211\\ni=1\\nE\\nS\u223cDm\\n[1h(x)\u0338=c(x)],\\nfor any x in sample S.T h u s ,\\nE\\nS\u223cDm\\n[ \u02c6R(h)] = ', 'E\\nS\u223cDm\\n[1{h(x)\u0338=c(x)}]= E\\nx\u223cD\\n[1{h(x)\u0338=c(x)}]= R(h).\\nThe following introduces the Probably Approximately Correct (PAC) learning\\nframework. We denote by O(n) an upper bound on the cost of the computati', 'onal\\nrepresentation of any element x \u2208X and by size( c) the maximal cost of the\\ncomputational representation of c \u2208 C. For example, x may be a vector in Rn,\\nfor which the cost of an array-based repres', 'entation would be in O(n).\\nDe\ufb01nition 2.3 PAC-learning\\nA concept class C is said to be PAC-learnable if there exists an algorithm A and\\na polynomial function poly(\u00b7, \u00b7, \u00b7, \u00b7) such that for any \u03f5> 0 and', ' \u03b4> 0, for all\\ndistributions D on X and for any target concept c \u2208 C, the following holds for any\\nsample size m \u2265 poly(1/\u03f5,1/\u03b4, n,size(c)):\\nPr\\nS\u223cDm\\n[R(hS) \u2264 \u03f5] \u2265 1 \u2212 \u03b4. (2.4)\\nIf A further runs in poly', '(1/\u03f5,1/\u03b4, n,size(c)),t h e nC is said to be e\ufb03ciently PAC-\\nlearnable. When such an algorithm A exists, it is called a PAC-learning algorithm\\nfor C.\\nA concept classC is thus PAC-learnable if the hypoth', 'esis returned by the algorithm\\nafter observing a number of points polynomial in 1 /\u03f5 and 1/\u03b4 is approximately\\ncorrect (error at most \u03f5) with high probability (at least 1 \u2212 \u03b4), which justi\ufb01es the\\nPAC t', 'erminology.\u03b4> 0i su s e dt od e \ufb01 n et h econ\ufb01dence 1\u2212\u03b4and \u03f5> 0t h eaccuracy\\n1 \u2212 \u03f5. Note that if the running time of the algorithm is polynomial in 1/\u03f5 and 1/\u03b4,\\nthen the sample size m must also be pol', 'ynomial if the full sample is received by the\\nalgorithm.\\nSeveral key points of the PAC de\ufb01nition are worth emphasizing. First, the PAC\\nframework is adistribution-free model : no particular assumption ', 'is made about the\\ndistribution D from which examples are drawn. Second, the training sample and the\\ntest examples used to de\ufb01ne the error are drawn according to the same distribution\\nD. This is a nece', 'ssary assumption for generalization to be possible in most cases.14 The PAC Learning Framework\\nR\\nR\u2019\\nFigure 2.1 Target concept R and possible hypothesis R\u2032. Circles represent training\\ninstances. A blue', ' circle is a point labeled with 1, since it falls within the rectangle\\nR.O t h e r sa r er e da n dl a b e l e dw i t h0.\\nFinally, the PAC framework deals with the question of learnability for a conce', 'pt\\nclass C and not a particular concept. Note that the concept classC is known to the\\nalgorithm, but of course target concept c \u2208 C is unknown.\\nIn many cases, in particular when the computational repr', 'esentation of the con-\\ncepts is not explicitly discussed or is straightforward, we may omit the polynomial\\ndependency on n and size(c) in the PAC de\ufb01nition and focus only on the sample\\ncomplexity.\\nWe ', 'now illustrate PAC-learning with a speci\ufb01c learning problem.\\nExample 2.1 Learning axis-aligned rectangles\\nC o n s i d e rt h ec a s ew h e r et h es e to fi n s t a n c e sa r ep o i n t si nt h ep l ', 'a n e ,X = R\\n2,a n d\\nthe concept class C is the set of all axis-aligned rectangles lying in R2.T h u s ,e a c h\\nconcept c is the set of points inside a particular axis-aligned rectangle. The learning\\n', 'problem consists of determining with small error a target axis-aligned rectangle\\nusing the labeled training sample. We will show that the concept class of axis-\\naligned rectangles is PAC-learnable.\\nFi', 'gure 2.1 illustrates the problem. R represents a target axis-aligned rectangle\\nand R\u2032 a hypothesis. As can be seen from the \ufb01gure, the error regions of R\u2032 are\\nformed by the area within the rectangleR ', 'but outside the rectangle R\u2032 and the area\\nwithin R\u2032 but outside the rectangle R. The \ufb01rst area corresponds to false negatives,\\nthat is, points that are labeled as 0 or negatively by R\u2032, which are in f', 'act positive\\nor labeled with 1. The second area corresponds to false positives,t h a ti s ,p o i n t s\\nlabeled positively by R\u2032 which are in fact negatively labeled.\\nTo show that the concept class is ', 'PAC-learnable, we describe a simple PAC-\\nlearning algorithm A. Given a labeled sampleS, the algorithm consists of returning\\nthe tightest axis-aligned rectangle R\u2032 = RS containing the points labeled wi', 'th 1.\\nFigure 2.2 illustrates the hypothesis returned by the algorithm. By de\ufb01nition, RS\\ndoes not produce any false positive, since its points must be included in the target\\nconcept R. Thus, the error ', 'region of RS is included in R.2.1 The PAC learning model 15\\nR\\nR\u2019\\nFigure 2.2 Illustration of the hypothesis R\u2032 = RS returned by the algorithm.\\nLet R \u2208 C be a target concept. Fix \u03f5> 0. Let Pr[RS] denote', ' the probability mass\\no ft h er e g i o nd e \ufb01 n e db yRS, that is the probability that a point randomly drawn\\naccording to D falls within RS. Since errors made by our algorithm can be due only\\nto poi', 'nts falling inside RS, we can assume that Pr[ RS] >\u03f5 ; otherwise, the error of\\nRS is less than or equal to \u03f5 regardless of the training sample S received.\\nNow, since Pr[ RS] >\u03f5 , we can de\ufb01ne four rec', 'tangular regions r1,r2,r3, and r4\\nalong the sides of RS, each with probability at least \u03f5/4. These regions can be\\nconstructed by starting with the empty rectangle along a side and increasing its\\nsize ', 'until its distribution mass is at least\u03f5/4. Figure 2.3 illustrates the de\ufb01nition of\\nthese regions.\\nObserve that if R\\nS meets all of these four regions, then, because it is a rectangle,\\nit will have on', 'e side in each of these four regions (geometric argument). Its error\\narea, which is the part of R that it does not cover, is thus included in these regions\\nand cannot have probability mass more than \u03f5', '. By contraposition, if R(RS) >\u03f5 ,\\nthen RS must miss at least one of the regionsri, i \u2208 [1,4]. As a result, we can write\\nPr\\nS\u223cDm\\n[R(RS) >\u03f5 ] \u2264 Pr\\nS\u223cDm\\n[\u222a4\\ni=1{RS \u2229 ri = \u2205}] (2.5)\\n\u2264\\n4\u2211\\ni=1\\nPr\\nS\u223cDm\\n[{RS', ' \u2229 ri = \u2205}] (by the union bound)\\n\u2264 4(1 \u2212 \u03f5/4)m (since Pr[ ri] >\u03f5 /4)\\n\u2264 4e x p (\u2212m\u03f5/4),\\nwhere for the last step we used the general identity 1\u2212 x \u2264 e\u2212x valid for all x \u2208 R.\\nFor any \u03b4> 0, to ensure that', ' PrS\u223cDm [R(RS) >\u03f5 ] \u2264 \u03b4,w ec a ni m p o s e\\n4e x p (\u2212\u03f5m/4) \u2264 \u03b4\u21d4 m \u2265 4\\n\u03f5 log 4\\n\u03b4. (2.6)\\nThus, for any \u03f5> 0a n d \u03b4> 0, if the sample size m is greater than 4\\n\u03f5 log 4\\n\u03b4,\\nthen PrS\u223cDm [R(RS) >\u03f5 ] \u2264 1 \u2212 \u03b4. ', 'Furthermore, the computational cost of the16 The PAC Learning Framework\\nR\\nR\u2019\\nr1\\nr2\\nr3\\nr4\\nFigure 2.3 Illustration of the regions r1,...,r 4.\\nrepresentation of points in R2 and axis-aligned rectangles, ', 'which can be de\ufb01ned by\\ntheir four corners, is constant. This proves that the concept class of axis-aligned\\nrectangles is PAC-learnable and that the sample complexity of PAC-learning axis-\\naligned rect', 'angles is in O(\\n1\\n\u03f5 log 1\\n\u03b4).\\nAn equivalent way to present sample complexity results like (2.6), which we will\\noften see throughout this book, is to give ageneralization bound. It states that with\\npro', 'bability at least 1 \u2212 \u03b4, R(RS) is upper bounded by some quantity that depends\\non the sample size m and \u03b4. To obtain this, if su\ufb03ces to set \u03b4 to be equal to the\\nupper bound derived in (2.5), that is \u03b4=', '4e x p (\u2212m\u03f5/4) and solve for \u03f5.T h i sy i e l d s\\nthat with probability at least 1 \u2212 \u03b4, the error of the algorithm is bounded as:\\nR(RS) \u2264 4\\nm log 4\\n\u03b4. (2.7)\\nOther PAC-learning algorithms could be cons', 'idered for this example. One alterna-\\ntive is to return the largest axis-aligned rectangle not containing the negative points,\\nfor example. The proof of PAC-learning just presented for the tightest ax', 'is-aligned\\nrectangle can be easily adapted to the analysis of other such algorithms.\\nNote that the hypothesis set H we considered in this example coincided with the\\nconcept class C and that its cardin', 'ality was in\ufb01nite. Nevertheless, the problem\\nadmitted a simple proof of PAC-learning. We may then ask if a similar proof\\ncan readily apply to other similar concept classes. This is not as straightforw', 'ard\\nbecause the speci\ufb01c geometric argument used in the proof is key. It is non-trivial\\nto extend the proof to other concept classes such as that of non-concentric circles\\n(see exercise 2.4). Thus, we ', 'need a more general proof technique and more general\\nresults. The next two sections provide us with such tools in the case of a \ufb01nite\\nhypothesis set.2.2 Guarantees for \ufb01nite hypothesis sets \u2014 consiste', 'nt case 17\\n2.2 Guarantees for \ufb01nite hypothesis sets \u2014 consistent case\\nIn the example of axis-aligned rectangles that we examined, the hypothesis hS\\nreturned by the algorithm was always consistent, tha', 't is, it admitted no error on\\nthe training sample S. In this section, we present a general sample complexity\\nbound, or equivalently, a generalization bound, for consistent hypotheses, in the\\ncase wher', 'e the cardinality |H| of the hypothesis set is \ufb01nite. Since we consider\\nconsistent hypotheses, we will assume that the target concept c is in H.\\nTheorem 2.1 Learning bounds \u2014 \ufb01nite H, consistent case\\n', 'Let H be a \ufb01nite set of functions mapping from X to Y.L e tA be an algorithm that\\nfor any target conceptc \u2208 H and i.i.d. sample S returns a consistent hypothesish\\nS:\\n\u02c6R(hS)=0 .T h e n ,f o ra n y\u03f5, \u03b4 ', '>0,t h ei n e q u a l i t yPrS\u223cDm [R(hS) \u2264 \u03f5] \u2265 1 \u2212 \u03b4holds\\nif\\nm \u2265 1\\n\u03f5\\n(\\nlog |H| +l o g1\\n\u03b4\\n\u23a1\\n. (2.8)\\nThis sample complexity result admits the following equivalent statement as a gener-\\nalization bound:', ' for any \u03f5, \u03b4 >0, with probability at least 1 \u2212 \u03b4,\\nR(hS) \u2264 1\\nm\\n(\\nlog |H| +l o g1\\n\u03b4\\n\u23a1\\n. (2.9)\\nProof Fix \u03f5> 0. We do not know which consistent hypothesishS \u2208 H is selected\\nby the algorithm A. This hypot', 'hesis further depends on the training sample S.\\nTherefore, we need to give a uniform convergence bound, that is, a bound that\\nholds for the set of all consistent hypotheses, which a fortiori includes ', 'hS.T h u s ,\\nwe will bound the probability that some h \u2208 H would be consistent and have error\\nmore than \u03f5:\\nPr[\u2203h \u2208 H : \u02c6R(h)=0 \u2227 R(h) >\u03f5 ]\\n=P r [ (h1 \u2208 H, \u02c6R(h1)=0 \u2227 R(h1) >\u03f5 ) \u2228 (h2 \u2208 H, \u02c6R(h2)=0 \u2227 R', '(h2) >\u03f5 ) \u2228\u00b7\u00b7\u00b7 ]\\n\u2264\\n\u2211\\nh\u2208H\\nPr[ \u02c6R(h)=0 \u2227 R(h) >\u03f5 ] (union bound)\\n\u2264\\n\u2211\\nh\u2208H\\nPr[ \u02c6R(h)=0 | R(h) >\u03f5 ]. (de\ufb01nition of conditional probability)\\nNow, consider any hypothesis h \u2208 H with R(h) >\u03f5 . Then, the proba', 'bility that h\\nwould be consistent on a training sample S drawn i.i.d., that is, that it would have\\nno error on any point in S, can be bounded as:\\nPr[ \u02c6R(h)=0 | R(h) >\u03f5 ] \u2264 (1 \u2212 \u03f5)m.18 The PAC Learning', ' Framework\\nThe previous inequality implies\\nPr[\u2203h \u2208 H : \u02c6R(h)=0 \u2227 R(h) >\u03f5 ] \u2264| H|(1 \u2212 \u03f5)m.\\nSetting the right-hand side to be equal to\u03b4and solving for\u03f5 concludes the proof.\\nThe theorem shows that when t', 'he hypothesis setH is \ufb01nite, a consistent algorithm\\nA is a PAC-learning algorithm, since the sample complexity given by (2.8) is\\ndominated by a polynomial in 1 /\u03f5 and 1/\u03b4. As shown by (2.9), the gener', 'alization\\nerror of consistent hypotheses is upper bounded by a term that decreases as\\na function of the sample size m. This is a general fact: as expected, learning\\nalgorithms bene\ufb01t from larger label', 'ed training samples. The decrease rate ofO(1/m)\\nguaranteed by this theorem, however, is particularly favorable.\\nThe price to pay for coming up with a consistent algorithm is the use of a\\nlarger hypoth', 'esis set H containing target concepts. Of course, the upper bound\\n(2.9) increases with |H|. However, that dependency is only logarithmic. Note that\\nthe term log |H|,o rt h er e l a t e dt e r ml o g2 ', '|H| from which it di\ufb00ers by a constant\\nfactor, can be interpreted as the number of bits needed to represent H.T h u s ,t h e\\ngeneralization guarantee of the theorem is controlled by the ratio of this ', 'number of\\nbits, log2 |H|,a n dt h es a m p l es i z em.\\nWe now use theorem 2.1 to analyze PAC-learning with various concept classes.\\nExample 2.2 Conjunction of Boolean literals\\nConsider learning the c', 'oncept classCn of conjunctions of at mostn Boolean literals\\nx1,...,x n. A Boolean literal is either a variablexi, i \u2208 [1,n ], or its negation xi.F o r\\nn = 4, an example is the conjunction: x1 \u2227 x2 \u2227 x', '4,w h e r ex2 denotes the negation\\nof the Boolean literal x2.( 1, 0, 0, 1) is a positive example for this concept while\\n(1,0,0, 0) is a negative example.\\nObserve that for n = 4, a positive example (1 ', ', 0, 1, 0) implies that the target\\nconcept cannot contain the literalsx1 and x3 and that it cannot contain the literals\\nx2 and x4. In contrast, a negative example is not as informative since it is not', '\\nknown which of its n bits are incorrect. A simple algorithm for \ufb01nding a consistent\\nhypothesis is thus based on positive examples and consists of the following: for each\\npositive example (b1,...,b n)', 'a n di \u2208 [1,n ], if bi =1t h e nxi is ruled out as a possible\\nliteral in the concept class and if bi =0t h e nxi is ruled out. The conjunction of all\\nthe literals not ruled out is thus a hypothesis co', 'nsistent with the target. Figure 2.4\\nshows an example training sample as well as a consistent hypothesis for the case\\nn =6 .\\nWe have |H| = |Cn| =3 n, since each literal can be included positively, wit', 'h\\nnegation, or not included. Plugging this into the sample complexity bound for\\nconsistent hypotheses yields the following sample complexity bound for any \u03f5> 02.2 Guarantees for \ufb01nite hypothesis sets ', '\u2014 consistent case 19\\n011011 +\\n011111 +\\n001101 -\\n011111 +\\n100110 -\\n010011 +\\n01? ?11\\nFigure 2.4 Each of the \ufb01rst six rows of the table represents a training example with\\nits label, + or \u2212, indicated in ', 'the last column. The last row contains0 (respectively\\n1) in columni \u2208 [1, 6] if theith entry is0 (respectively 1) for all the positive examples.\\nIt contains \u201c?\u201d if both 0 and 1 appear as an ith entry ', 'for some positive example.\\nThus, for this training sample, the hypothesis returned by the consistent algorithm\\nd e s c r i b e di nt h et e x ti s\\nx1 \u2227 x2 \u2227 x5 \u2227 x6.\\nand \u03b4> 0:\\nm \u2265 1\\n\u03f5\\n(\\n(log 3)n +l o ', 'g1\\n\u03b4\\n\u23a1\\n. (2.10)\\nThus, the class of conjunctions of at mostn Boolean literals is PAC-learnable. Note\\nthat the computational complexity is also polynomial, since the training cost per\\ne x a m p l ei si ', 'nO(n). For\u03b4=0 .02, \u03f5 =0 .1, and n = 10, the bound becomesm \u2265 149.\\nThus, for a labeled sample of at least 149 examples, the bound guarantees 99%\\naccuracy with a con\ufb01dence of at least 98%.\\nExample 2.3 U', 'niversal concept class\\nConsider the set X = {0, 1}\\nn of all Boolean vectors with n components, and let Un\\nbe the concept class formed by all subsets ofX. Is this concept class PAC-learnable?\\nTo guaran', 'tee a consistent hypothesis the hypothesis class must include the concept\\nclass, thus |H|\u2265| Un| =2 (2n). Theorem 2.1 gives the following sample complexity\\nbound:\\nm \u2265 1\\n\u03f5\\n(\\n(log 2)2n +l o g1\\n\u03b4\\n\u23a1\\n. (2.1', '1)\\nHere, the number of training samples required is exponential inn, which is the cost\\nof the representation of a point in X. Thus, PAC-learning is not guaranteed by\\nthe theorem. In fact, it is not ha', 'rd to show that this universal concept class is not\\nPAC-learnable.20 The PAC Learning Framework\\nExample 2.4 k-term DNF formulae\\nA disjunctive normal form (DNF) formula is a formula written as the disj', 'unction of\\nseveral terms, each term being a conjunction of Boolean literals. Ak-term DNF is a\\nDNF formula de\ufb01ned by the disjunction of k terms, each term being a conjunction\\nof at most n Boolean liter', 'als. Thus, for k = 2 and n = 3, an example of a k-term\\nDNF is (x\\n1 \u2227 x2 \u2227 x3) \u2228 (x1 \u2227 x3).\\nIs the class C of k-term DNF formulae is PAC-learnable? The cardinality of the\\nc l a s si s3nk, since each te', 'rm is a conjunction of at most n v a r i a b l e sa n dt h e r ea r e\\n3n such conjunctions, as seen previously. The hypothesis set H must contain C for\\nconsistency to be possible, thus |H|\u2265 3nk. Theor', 'em 2.1 gives the following sample\\ncomplexity bound:\\nm \u2265 1\\n\u03f5\\n(\\n(log 3)nk +l o g1\\n\u03b4\\n\u23a1\\n, (2.12)\\nwhich is polynomial. However, it can be shown that the problem of learning k-\\nterm DNF is in RP, the comple', 'xity class of problems that admit a randomized\\npolynomial-time decision solution. The problem is therefore computationally in-\\ntractable unless RP = NP, which is commonly conjectured not to be the cas', 'e. Thus,\\nwhile the sample size needed for learningk-term DNF formulae is only polynomial,\\ne\ufb03cient PAC-learning of this class is not possible unless RP = NP.\\nExample 2.5 k-CNF formulae\\nA conjunctive no', 'rmal form (CNF) formula is a conjunction of disjunctions. A k-\\nCNF formula is an expression of the form T\\n1 \u2227... \u2227Tj with arbitrary length j \u2208 N\\nand with each term Ti being a disjunction of at most k ', 'Boolean attributes.\\nThe problem of learning k-CNF formulae can be reduced to that of learning\\nconjunctions of Boolean literals, which, as seen previously, is a PAC-learnable\\nconcept class. To do so, i', 't su\ufb03ces to associate to each term T\\ni a new variable.\\nThen, this can be done with the following bijection:\\nai(x1) \u2228\u00b7\u00b7\u00b7\u2228 ai(xn) \u2192 Yai(x1),...,ai(xn), (2.13)\\nwhere ai(xj) denotes the assignment to xj i', 'n term Ti. This reduction to PAC-\\nlearning of conjunctions of Boolean literals may a\ufb00ect the original distribution, but\\nthis is not an issue since in the PAC framework no assumption is made about the\\n', 'distribution. Thus, the PAC-learnability of conjunctions of Boolean literals implies\\nthat of k-CNF formulae.\\nThis is a surprising result, however, since anyk-term DNF formula can be written\\nas a k-CNF', ' formula. Indeed, using associativity, ak-term DNF can be rewritten as2.3 Guarantees for \ufb01nite hypothesis sets \u2014 inconsistent case 21\\na k-CNF formula via\\nk\u22c1\\ni=1\\nai(x1) \u2227\u00b7\u00b7\u00b7\u2227 ai(xn)=\\nn\u22c0\\ni1,...,ik=1\\na1(', 'xi1 ) \u2228\u00b7\u00b7\u00b7\u2228 ak(xik ).\\nTo illustrate this rewriting in a speci\ufb01c case, observe, for example, that\\n(u1 \u2227 u2 \u2227 u3) \u2228 (v1 \u2227 v2 \u2227 v3)=\\n3\u22c0\\ni,j=1\\n(ui \u2227 vj).\\nBut, as we previously saw,k-term DNF formulae are ', 'not e\ufb03ciently PAC-learnable!\\nWhat can explain this apparent inconsistency? Observe that the number of new\\nvariables needed to write ak-term DNF as ak-CNF formula via the transformation\\njust described ', 'is exponential ink,i ti si nO(n\\nk). The discrepancy comes from the size\\nof the representation of a concept. A k-term DNF formula can be an exponentially\\nmore compact representation, and e\ufb03cient PAC-le', 'arning is intractable if a time-\\ncomplexity polynomial in that size is required. Thus, this apparent paradox deals\\nwith key aspects of PAC-learning, which include the cost of the representation of a\\nc', 'oncept and the choice of the hypothesis set.\\n2.3 Guarantees for \ufb01nite hypothesis sets \u2014 inconsistent case\\nIn the most general case, there may be no hypothesis in H consistent with the\\nlabeled training', ' sample. This, in fact, is the typical case in practice, where the\\nlearning problems may be somewhat di\ufb03cult or the concept classes more complex\\nthan the hypothesis set used by the learning algorithm.', ' However, inconsistent\\nhypotheses with a small number of errors on the training sample can be useful and,\\nas we shall see, can bene\ufb01t from favorable guarantees under some assumptions. This\\nsection pre', 'sents learning guarantees precisely for this inconsistent case and \ufb01nite\\nhypothesis sets.\\nTo derive learning guarantees in this more general setting, we will use Hoe\ufb00ding\u2019s\\ninequality (theorem D.1) or', ' the following corollary, which relates the generalization\\nerror and empirical error of a single hypothesis.22 The PAC Learning Framework\\nCorollary 2.1\\nFix \u03f5> 0 and let S denote an i.i.d. sample of si', 'ze m. Then, for any hypothesis\\nh: X \u2192{ 0, 1}, the following inequalities hold:\\nPr\\nS\u223cDm\\n[ \u02c6R(h) \u2212 R(h) \u2265 \u03f5] \u2264 exp(\u22122m\u03f52) (2.14)\\nPr\\nS\u223cDm\\n[ \u02c6R(h) \u2212 R(h) \u2264\u2212 \u03f5] \u2264 exp(\u22122m\u03f52). (2.15)\\nBy the union bound, thi', 's implies the following two-sided inequality:\\nPr\\nS\u223cDm\\n[\\n| \u02c6R(h) \u2212 R(h)|\u2265 \u03f5\\n]\\n\u2264 2e x p (\u22122m\u03f52). (2.16)\\nProof The result follows immediately theorem D.1.\\nSetting the right-hand side of (2.16) to be equa', 'l to \u03b4 and solving for \u03f5 yields\\nimmediately the following bound for a single hypothesis.\\nCorollary 2.2 Generalization bound \u2014 single hypothesis\\nFix a hypothesis h: X\u2192 { 0, 1}. Then, for any \u03b4> 0, the ', 'following inequality holds\\nwith probability at least 1 \u2212 \u03b4:\\nR(h) \u2264 \u02c6R(h)+\\n\u221a\\nlog 2\\n\u03b4\\n2m . (2.17)\\nThe following example illustrates this corollary in a simple case.\\nExample 2.6 Tossing a coin\\nImagine to', 'ssing a biased coin that lands heads with probability p,a n dl e to u r\\nhypothesis be the one that always guesses heads. Then the true error rate isR(h)= p\\nand the empirical error rate \u02c6R(h)= \u02c6p,w h e', ' r e\u02c6p is the empirical probability of\\nheads based on the training sample drawn i.i.d. Thus, corollary 2.2 guarantees with\\nprobability at least 1 \u2212 \u03b4that\\n|p \u2212 \u02c6p|\u2264\\n\u221a\\nlog 2\\n\u03b4\\n2m . (2.18)\\nTherefore, if ', 'we choose \u03b4=0 .02 and use a sample of size 500, with probability at\\nleast 98%, the following approximation quality is guaranteed for \u02c6p:\\n|p \u2212 \u02c6p|\u2264\\n\u221a\\nlog(10)\\n1000 \u2248 0.048. (2.19)\\nCan we readily apply c', 'orollary 2.2 to bound the generalization error of the\\nhypothesis hS returned by a learning algorithm when training on a sample S? No,\\nsince hS is not a \ufb01xed hypothesis, but a random variable depending', ' on the training\\nsample S drawn. Note also that unlike the case of a \ufb01xed hypothesis for which2.3 Guarantees for \ufb01nite hypothesis sets \u2014 inconsistent case 23\\nthe expectation of the empirical error is ', 'the generalization error (equation 2.3), the\\ngeneralization error R(hS) is a random variable and in general distinct from the\\nexpectation E[ \u02c6R(hS)], which is a constant.\\nThus, as in the proof for the', ' consistent case, we need to derive a uniform con-\\nvergence bound, that is a bound that holds with high probability for all hypotheses\\nh \u2208 H.\\nTheorem 2.2 Learning bound \u2014 \ufb01nite H, inconsistent case\\nLe', 't H be a \ufb01nite hypothesis set. Then, for any \u03b4> 0, with probability at least1 \u2212 \u03b4,\\nthe following inequality holds:\\n\u2200h \u2208 H, R (h) \u2264 \u02c6R(h)+\\n\u221a\\nlog |H| +l o g2\\n\u03b4\\n2m . (2.20)\\nProof Let h1,...,h |H| be the ', 'elements ofH. Using the union bound and applying\\ncorollary 2.2 to each hypothesis yield:\\nPr\\n[\\n\u2203h \u2208 H\\n\u23d0\u23d0 \u02c6R(h) \u2212 R(h)\\n\u23d0\u23d0 >\u03f5\\n]\\n=P r\\n[(\u23d0\u23d0 \u02c6R(h1) \u2212 R(h1)\\n\u23d0\u23d0 >\u03f5\\n\u23a1\\n\u2228 ... \u2228\\n(\u23d0\u23d0 \u02c6R(h|H|) \u2212 R(h|H|)\\n\u23d0\u23d0 >\u03f5\\n\u23a1]\\n\u2264\\n', '\u2211\\nh\u2208H\\nPr\\n[\u23d0\u23d0 \u02c6R(h) \u2212 R(h)\\n\u23d0\u23d0 >\u03f5\\n]\\n\u2264 2|H| exp(\u22122m\u03f52).\\nSetting the right-hand side to be equal to \u03b4completes the proof.\\nThus, for a \ufb01nite hypothesis set H,\\nR(h) \u2264 \u02c6R(h)+ O\\n(\u221a\\nlog2 |H|\\nm\\n\u23a1\\n.\\nAs already p', 'ointed out, log 2 |H| can be interpreted as the number of bits needed\\nto represent H. Several other remarks similar to those made on the generalization\\nbound in the consistent case can be made here: a', ' larger sample size m guarantees\\nbetter generalization, and the bound increases with |H|, but only logarithmically.\\nBut, here, the bound is a less favorable function of log2 |H|\\nm ;i tv a r i e sa st ', 'h es q u a r e\\nroot of this term. This is not a minor price to pay: for a \ufb01xed |H|, to attain the\\nsame guarantee as in the consistent case, a quadratically larger labeled sample is\\nneeded.\\nNote that t', 'he bound suggests seeking a trade-o\ufb00 between reducing the empirical\\nerror versus controlling the size of the hypothesis set: a larger hypothesis set is\\npenalized by the second term but could help redu', 'ce the empirical error, that is the\\n\ufb01rst term. But, for a similar empirical error, it suggests using a smaller hypothesis24 The PAC Learning Framework\\ns e t .T h i sc a nb ev i e w e da sa ni n s t a ', 'n c eo ft h es o - c a l l e dOccam\u2019s Razor principle\\nnamed after the theologian William of Occam:Plurality should not be posited without\\nnecessity, also rephrased as,the simplest explanation is best.', ' In this context, it could\\nbe expressed as follows: All other things being equal, a simpler (smaller) hypothesis\\nset is better.\\n2.4 Generalities\\nIn this section we will consider several important ques', 'tions related to the learning\\nscenario, which we left out of the discussion of the earlier sections for simplicity.\\n2.4.1 Deterministic versus stochastic scenarios\\nIn the most general scenario of supe', 'rvised learning, the distribution D is de\ufb01ned\\nover X\u00d7 Y , and the training data is a labeled sample S drawn i.i.d. according to\\nD:\\nS =( (x\\n1,y1),..., (xm,y m)).\\nThe learning problem is to \ufb01nd a hypoth', 'esis h \u2208 H with small generalization error\\nR(h)= P r\\n(x,y)\u223cD\\n[h(x) \u0338= y]= E\\n(x,y)\u223cD\\n[1h(x)\u0338=y].\\nThis more general scenario is referred to as the stochastic scenario. Within this\\nsetting, the output la', 'bel is a probabilistic function of the input. The stochastic\\nscenario captures many real-world problems where the label of an input point is not\\nunique. For example, if we seek to predict gender based', ' on input pairs formed by\\nthe height and weight of a person, then the label will typically not be unique. For\\nmost pairs, both male and female are possible genders. For each \ufb01xed pair, there\\nwould be ', 'a probability distribution of the label being male.\\nThe natural extension of the PAC-learning framework to this setting is known as\\nthe agnostic PAC-learning.\\nDe\ufb01nition 2.4 Agnostic PAC-learning\\nLet H', ' be a hypothesis set. A is an agnostic PAC-learning algorithm if there\\nexists a polynomial function poly(\u00b7, \u00b7, \u00b7, \u00b7) such that for any \u03f5> 0 and \u03b4> 0,\\nfor all distributions D over X\u00d7 Y , the following ', 'holds for any sample size m \u2265\\npoly(1/\u03f5,1/\u03b4, n,size(c)):\\nPr\\nS\u223cDm\\n[R(hS) \u2212 min\\nh\u2208H\\nR(h) \u2264 \u03f5] \u2265 1 \u2212 \u03b4. (2.21)2.4 Generalities 25\\nIf A further runs inpoly(1/\u03f5,1/\u03b4, n,size(c)),t h e ni ti ss a i dt ob ea n', 'e\ufb03cient agnostic\\nPAC-learning algorithm.\\nWhen the label of a point can be uniquely determined by some measurable func-\\ntion f : X\u2192 Y (with probability one), then the scenario is said to bedeterministi', 'c.\\nIn that case, it su\ufb03ces to consider a distribution D over the input space. The\\ntraining sample is obtained by drawing (x1,...,x m)a c c o r d i n gt oD and the labels\\nare obtained via f: yi = f(xi)', ' for all i \u2208 [1,m]. Many learning problems can be\\nformulated within this deterministic scenario.\\nIn the previous sections, as well as in most of the material presented in this book,\\nwe have restricted', ' our presentation to the deterministic scenario in the interest of\\nsimplicity. However, for all of this material, the extension to the stochastic scenario\\nshould be straightforward for the reader.\\n2.4', '.2 Bayes error and noise\\nIn the deterministic case, by de\ufb01nition, there exists a target function f with no\\ngeneralization error: R(h) = 0. In the stochastic case, there is a minimal non-zero\\nerror for', ' any hypothesis.\\nDe\ufb01nition 2.5 Bayes error\\nGiven a distribution D over X\u00d7 Y ,t h eBayes error R\\n\u2217 is de\ufb01ned as the in\ufb01mum\\nof the errors achieved by measurable functionsh: X\u2192 Y :\\nR\u22c6 =i n f\\nh\\nh measurab', 'le\\nR(h). (2.22)\\nA hypothesis h with R(h)= R\u2217 is called a Bayes hypothesis or Bayes classi\ufb01er.\\nBy de\ufb01nition, in the deterministic case, we haveR\u2217 = 0, but, in the stochastic case,\\nR\u2217 \u0338= 0. Clearly, the', ' Bayes classi\ufb01erhBayes can be de\ufb01ned in terms of the conditional\\nprobabilities as:\\n\u2200x \u2208X ,h Bayes(x) = argmax\\ny\u2208{0,1}\\nPr[y|x]. (2.23)\\nThe average error made by hBayes on x \u2208X is thus min{Pr[0|x], Pr[1', '|x]},a n dt h i s\\nis the minimum possible error. This leads to the following de\ufb01nition of noise.\\nDe\ufb01nition 2.6 Noise\\nGiven a distribution D over X\u00d7 Y ,t h enoise at point x \u2208X is de\ufb01ned by\\nnoise(x)=m ', 'i n{Pr[1|x], Pr[0|x]}. (2.24)\\nThe average noise or the noise associated to D is E[noise(x)].26 The PAC Learning Framework\\nThus, the average noise is precisely the Bayes error: noise = E[noise(x)] = R\u2217', '.T h e\\nnoise is a characteristic of the learning task indicative of its level of di\ufb03culty. A\\npoint x \u2208X , for which noise(x)i sc l o s et o1/2, is sometimes referred to as noisy\\nand is of course a cha', 'llenge for accurate prediction.\\n2.4.3 Estimation and approximation errors\\nThe di\ufb00erence between the error of a hypothesish \u2208 H and the Bayes error can be\\ndecomposed as:\\nR(h) \u2212 R\\n\u2217 =( R(h) \u2212 R(h\u2217))\\ued19 \\ued18\\ued17', ' \\ued1a\\nestimation\\n+(R(h\u2217) \u2212 R\u2217)\\ued19 \\ued18\\ued17 \\ued1a\\napproximation\\n, (2.25)\\nwhere h\u2217 is a hypothesis in H w i t hm i n i m a le r r o r ,o rabest-in-class hypothesis.3\\nThe second term is referred to as theapproximation ', 'error, since it measures how\\nwell the Bayes error can be approximated usingH. It is a property of the hypothesis\\nset H, a measure of its richness. The approximation error is not accessible, since\\nin g', 'eneral the underlying distribution D is not known. Even with various noise\\nassumptions, estimating the approximation error is di\ufb03cult.\\nThe \ufb01rst term is the estimation error , and it depends on the hyp', 'othesis h\\nselected. It measures the quality of the hypothesish with respect to the best-in-class\\nhypothesis. The de\ufb01nition of agnostic PAC-learning is also based on the estimation\\nerror. The estimatio', 'n error of an algorithm A, that is, the estimation error of the\\nhypothesis hS returned after training on a sample S,c a ns o m e t i m e sb eb o u n d e di n\\nterms of the generalization error.\\nFor exa', 'mple, let hERM\\nS denote the hypothesis returned by the empirical risk\\nminimization algorithm, that is the algorithm that returns a hypothesishERM\\nS with\\nthe smallest empirical error. Then, the general', 'ization bound given by theorem 2.2,\\nor any other bound on sup\\nh\u2208H |R(h) \u2212 \u02c6R(h)|, can be used to bound the estimation\\nerror of the empirical risk minimization algorithm. Indeed, rewriting the estimati', 'on\\nerror to make \u02c6R(h\\nERM\\nS ) appear and using \u02c6R(hERM\\nS ) \u2264 \u02c6R(h\u2217), which holds by the\\nde\ufb01nition of the algorithm, we can write\\nR(hERM\\nS ) \u2212 R(h\u2217)= R(hERM\\nS ) \u2212 \u02c6R(hERM\\nS )+ \u02c6R(hERM\\nS ) \u2212 R(h\u2217)\\n\u2264 R(h', 'ERM\\nS ) \u2212 \u02c6R(hERM\\nS )+ \u02c6R(h\u2217) \u2212 R(h\u2217)\\n\u2264 2s u p\\nh\u2208H\\n|R(h) \u2212 \u02c6R(h)|. (2.26)\\n3. When H is a \ufb01nite hypothesis set, h\u2217 necessarily exists; otherwise, in this discussion\\nR(h\u2217) can be replaced by infh\u2208H R(h)', '.2.4 Generalities 27\\nmeasure of capacity\\ntraining error\\ncomplexity term\\nbound on generalization errorerror\\nFigure 2.5 Illustration of structural risk minimization. The plots of three errors\\nare shown ', 'as a function of a measure of capacity. Clearly, as the size or capacity of\\nthe hypothesis set increases, the training error decreases, while the complexity term\\nincreases. SRM selects the hypothesis ', 'minimizing a bound on the generalization\\nerror, which is a sum of the empirical error, and the complexity term is shown in\\nred.\\nThe right-hand side of (2.26) can be bounded by theorem 2.2 and increase', 's with\\nthe size of the hypothesis set, while R(h\\n\u2217)d e c r e a s e sw i t h|H|.\\n2.4.4 Model selection\\nHere, we discuss some broad model selection and algorithmic ideas based on the\\ntheoretical results', ' presented in the previous sections. We assume an i.i.d. labeled\\ntraining sample S of size m and denote the error of a hypothesis h on S by \u02c6R\\nS(h)\\nto explicitly indicate its dependency on S.\\nWhile th', 'e guarantee of theorem 2.2 holds only for \ufb01nite hypothesis sets, it already\\nprovides us with some useful insights for the design of algorithms and, as we will see\\nin the next chapters, similar guarant', 'ees hold in the case of in\ufb01nite hypothesis sets.\\nSuch results invite us to consider two terms: the empirical error and a complexity\\nterm, which here is a function of |H| and the sample size m.\\nIn view', ' of that, the ERM algorithm , which only seeks to minimize the error on\\nthe training sample\\nh\\nERM\\nS =a r g m i n\\nh\u2208H\\n\u02c6RS(h), (2.27)\\nmight not be successful, since it disregards the complexity term. In', ' fact, the\\nperformance of the ERM algorithm is typically very poor in practice. Additionally,\\nin many cases, determining the ERM solution is computationally intractable. For\\nexample, \ufb01nding a linear h', 'ypothesis with the smallest error on the training sample\\ni sN P - h a r d( a saf u n c t i o no ft h ed i m e n s i o no ft h es p a c e ) .\\nAnother method known as structural risk minimization (SRM) ', 'consists of con-28 The PAC Learning Framework\\nsidering instead an in\ufb01nite sequence of hypothesis sets with increasing sizes\\nH0 \u2282 H1 \u2282 \u00b7\u00b7\u00b7 \u2282 Hn \u00b7\u00b7\u00b7 (2.28)\\nand to \ufb01nd the ERM solution hERM\\nn for each Hn', '. The hypothesis selected is the\\none among the hERM\\nn solutions with the smallest sum of the empirical error and\\na complexity term complexity(Hn,m) that depends on the size (or more generally\\nthe capa', 'city, that is, another measure of the richness of H)o f Hn, and the sample\\nsize m:\\nhSRM\\nS =a r g m i n\\nh\u2208Hn\\nn\u2208N\\n\u02c6RS(h)+c o m p l e x i t y (Hn,m). (2.29)\\nFigure 2.5 illustrates the SRM method. While S', 'RM bene\ufb01ts from strong theoretical\\nguarantees, it is typically computationally very expensive, since it requires deter-\\nmining the solution of multiple ERM problems. Note that the number of ERM\\nproble', 'ms is not in\ufb01nite if for some n the minimum empirical error is zero: The\\nobjective function can only be larger for n\u2032 \u2265 n.\\nAn alternative family of algorithms is based on a more straightforward optimi', 'za-\\ntion that consists of minimizing the sum of the empirical error and aregularization\\nterm that penalizes more complex hypotheses. The regularization term is typically\\nde\ufb01ned as \u2225h\u22252 for some norm \u2225', '\u00b7\u2225 when H is a vector space:\\nhREG\\nS =a r g m i n\\nh\u2208H\\n\u02c6RS(h)+ \u03bb\u2225h\u22252. (2.30)\\n\u03bb \u2265 0i sa regularization parameter, which can be used to determine the trade-o\ufb00\\nbetween empirical error minimization and cont', 'rol of the complexity. In practice, \u03bb\\nis typically selected using n-fold cross-validation. In the next chapters, we will see\\na number of di\ufb00erent instances of such regularization-based algorithms.\\n2.5', ' Chapter notes\\nThe PAC learning framework was introduced by Valiant [1984]. The book of Kearns\\nand Vazirani [1994] is an excellent reference dealing with most aspects of PAC-\\nlearning and several othe', 'r foundational questions in machine learning. Our example\\nof learning axis-aligned rectangles is based on that reference.\\nThe PAC learning framework is a computational framework since it takes into\\nac', 'count the cost of the computational representations and the time complexity of\\nthe learning algorithm. If we omit the computational aspects, it is similar to the\\nlearning framework considered earlier ', 'by Vapnik and Chervonenkis [see Vapnik,\\n2000].2.6 Exercises 29\\nOccam\u2019s razor principle is invoked in a variety of contexts, such as in linguistics to\\njustify the superiority of a set of rules or synta', 'x. The Kolmogorov complexity can be\\nviewed as the corresponding framework in information theory. In the context of the\\nlearning guarantees presented in this chapter, the principle suggests selecting t', 'he\\nmost parsimonious explanation (the hypothesis set with the smallest cardinality).\\nWe will see in the next sections other applications of this principle with di\ufb00erent\\nnotions of simplicity or comple', 'xity. The idea of structural risk minimization (SRM)\\nis due to Vapnik [1998].\\n2.6 Exercises\\n2.1 Two-oracle variant of the PAC model. Assume that positive and negative\\nexamples are now drawn from two s', 'eparate distributions D\\n+ and D\u2212 . For an\\naccuracy (1 \u2212 \u03f5), the learning algorithm must \ufb01nd a hypothesis h such that:\\nPr\\nx\u223cD+\\n[h(x)=0 ] \u2264 \u03f5 and Pr\\nx\u223cD\u2212\\n[h(x)=1 ] \u2264 \u03f5. (2.31)\\nThus, the hypothesis must ', 'have a small error on both distributions. Let C be any\\nconcept class andH be any hypothesis space. Leth0 and h1 represent the identically\\n0 and identically 1 functions, respectively. Prove thatC is e\ufb03', 'ciently PAC-learnable\\nusing H in the standard (one-oracle) PAC model if and only if it is e\ufb03ciently PAC-\\nlearnable using H \u222a{ h0,h1} in this two-oracle PAC model.\\n2.2 PAC learning of hyper-rectangles.', ' An axis-aligned hyper-rectangle in Rn is a\\nset of the form [a1,b1] \u00d7 ... \u00d7 [an,b n]. Show that axis-aligned hyper-rectangles are\\nPAC-learnable by extending the proof given in Example 2.1 for the case', ' n =2 .\\n2.3 Concentric circles. Let X = R2 and consider the set of concepts of the form\\nc = {(x, y): x2 + y2 \u2264 r2} for some real number r. Show that this class can be\\n(\u03f5, \u03b4)-PAC-learned from training ', 'data of size m \u2265 (1/\u03f5)l o g( 1/\u03b4).\\n2.4 Non-concentric circles. Let X = R2 and consider the set of concepts of the form\\nc = {x \u2208 R2 : ||x\u2212x0|| \u2264 r} for some pointx0 \u2208 R2 and real numberr. Gertrude, an\\n', 'aspiring machine learning researcher, attempts to show that this class of concepts\\nmay be (\u03f5, \u03b4)-PAC-learned with sample complexity m \u2265 (3/\u03f5)l o g( 3/\u03b4), but she is\\nhaving trouble with her proof. Her ', 'idea is that the learning algorithm would select\\nthe smallest circle consistent with the training data. She has drawn three regions\\nr\\n1,r2,r3 around the edge of concept c, with each region having prob', 'ability\u03f5/3( s e e\\n\ufb01gure 2.6). She wants to argue that if the generalization error is greater than or\\nequal to \u03f5, then one of these regions must have been missed by the training data,30 The PAC Learnin', 'g Framework\\nr1\\nr2\\nr3\\nFigure 2.6 Gertrude\u2019s regions r1,r 2,r 3.\\nand hence this event will occur with probability at most \u03b4.C a ny o ut e l lG e r t r u d e\\nif her approach works?\\n2.5 Triangles. Let X =', ' R2 with orthonormal basis (e1,e2), and consider the set of\\nconcepts de\ufb01ned by the area inside a right triangle ABC with two sides parallel to\\nthe axes, with \u2212\u2212\u2192AB/\u2225\u2212\u2212\u2192AB\u2225 = e1 and \u2212\u2192AC/\u2225\u2212\u2192AC\u2225 = e2,a ', 'n d\u2225\u2212\u2212\u2192AB\u2225/\u2225\u2212\u2192AC\u2225 = \u03b1 for some\\npositive real \u03b1 \u2208 R+. Show, using similar methods to those used in the chapter for\\nthe axis-aligned rectangles, that this class can be ( \u03f5, \u03b4)-PAC-learned from training\\n', 'data of size m \u2265 (3/\u03f5)l o g( 3/\u03b4).\\n2.6 Learning in the presence of noise \u2014 rectangles. In example 2.1, we showed that\\nthe concept class of axis-aligned rectangles is PAC-learnable. Consider now the ca', 'se\\nwhere the training points received by the learner are subject to the following noise:\\npoints negatively labeled are una\ufb00ected by noise but the label of a positive training\\npoint is randomly \ufb02ipped ', 'to negative with probability\u03b7 \u2208 (0,\\n1\\n2 ). The exact value of\\nthe noise rate\u03b7is not known to the learner but an upper bound\u03b7\u2032 is supplied to him\\nwith \u03b7 \u2264 \u03b7\u2032 < 1/2. Show that the algorithm described in', ' class returning the tightest\\nrectangle containing positive points can still PAC-learn axis-aligned rectangles in\\nthe presence of this noise. To do so, you can proceed using the following steps:\\n(a) U', 'sing the same notation as in example 2.1, assume that Pr[R] >\u03f5 .S u p p o s e\\nthat R(R\u2032) >\u03f5 .G i v ea nu p p e rb o u n do nt h ep r o b a b i l i t yt h a tR\u2032 misses a region\\nrj, j \u2208 [1, 4] in terms ', 'of \u03f5 and \u03b7\u2032?\\n(b) Use that to give an upper bound on Pr[R(R\u2032) >\u03f5 ]i nt e r m so f\u03f5 and \u03b7\u2032 and\\nconclude by giving a sample complexity bound.\\n2.7 Learning in the presence of noise \u2014 general case. In this', ' question, we will seek\\na result that is more general than in the previous question. We consider a \ufb01nite\\nhypothesis set H, assume that the target concept is in H, and adopt the following2.6 Exercises ', '31\\nnoise model: the label of a training point received by the learner is randomly changed\\nwith probability \u03b7 \u2208 (0, 1\\n2 ). The exact value of the noise rate \u03b7 is not known to the\\nlearner but an upper b', 'ound \u03b7\u2032 is supplied to him with \u03b7 \u2264 \u03b7\u2032 < 1/2.\\n(a) For any h \u2208 H,l e td(h) denote the probability that the label of a training\\npoint received by the learner disagrees with the one given by h.L e th\u2217 be', ' the\\ntarget hypothesis, show that d(h\u2217)= \u03b7.\\n(b) More generally, show that for any h \u2208 H, d(h)= \u03b7+( 1\u2212 2\u03b7) R(h), where\\nR(h) denotes the generalization error of h.\\n(c) Fix \u03f5> 0 for this and all the foll', 'owing questions. Use the previous questions\\nto show that if R(h) >\u03f5 ,t h e nd(h) \u2212 d(h\u2217) \u2265 \u03f5\u2032,w h e r e\u03f5\u2032 = \u03f5(1 \u2212 2\u03b7\u2032).\\n(d) For any hypothesis h \u2208 H and sample S of size m,l e t \u02c6d(h)d e n o t et h e\\n', 'fraction of the points inS whose labels disagree with those given byh.W ew i l l\\nconsider the algorithm L which, after receiving S, returns the hypothesis hS\\nwith the smallest number of disagreements ', '(thus \u02c6d(hS)i sm i n i m a l ) .T os h o w\\nPAC-learning for L, we will show that for any h,i f R(h) >\u03f5 , then with high\\nprobability \u02c6d(h) \u2265 \u02c6d(h\u2217). First, show that for any \u03b4> 0, with probability at\\nl', 'east 1 \u2212 \u03b4/2, for m \u2265 2\\n\u03f5\u20322 log 2\\n\u03b4, the following holds:\\n\u02c6d(h\u2217) \u2212 d(h\u2217) \u2264 \u03f5\u2032/2\\n(e) Second, show that for any \u03b4> 0, with probability at least 1 \u2212 \u03b4/2, for\\nm \u2265 2\\n\u03f5\u20322 (log |H| +l o g2\\n\u03b4), the following ', 'holds for all h \u2208 H:\\nd(h) \u2212 \u02c6d(h) \u2264 \u03f5\u2032/2\\n(f) Finally, show that for any \u03b4> 0, with probability at least 1 \u2212 \u03b4,f o r\\nm \u2265 2\\n\u03f52(1\u22122\u03b7\u2032)2 (log |H|+log 2\\n\u03b4), the following holds for allh \u2208 H with R(h) >\u03f5 :\\n', '\u02c6d(h) \u2212 \u02c6d(h\u2217) \u2265 0.\\n(Hint:u s e \u02c6d(h) \u2212 \u02c6d(h\u2217)=[ \u02c6d(h) \u2212 d(h) ]+[d(h) \u2212 d(h\u2217) ]+[d(h\u2217) \u2212 \u02c6d(h\u2217)] and\\nuse previous questions to lower bound each of these three terms).\\n2.8 Learning union of intervals. ', 'Let [a, b]a n d[c, d] be two intervals of the real line\\nwith a \u2264 b \u2264 c \u2264 d.L e t\u03f5> 0, and assume that Pr D((b, c)) >\u03f5 ,w h e r eD is the\\ndistribution according to which points are drawn.\\n(a) Show that', ' the probability that m points are drawn i.i.d. without any of\\nthem falling in the interval (b, c)i sa tm o s te\u2212m\u03f5.\\n(b) Show that the concept class formed by the union of two closed intervals32 The P', 'AC Learning Framework\\nin R,e . g . ,[a, b] \u222a [c, d], is PAC-learnable by giving a proof similar to the one\\ngiven in Example 2.1 for axis-aligned rectangles. (Hint: your algorithm might\\nnot return a hy', 'pothesis consistent with future negative points in this case.)\\n2.9 Consistent hypotheses. In this chapter, we showed that for a \ufb01nite hypothesis\\nset H, a consistent learning algorithm A is a PAC-learn', 'ing algorithm. Here, we\\nconsider a converse question. LetZ be a \ufb01nite set of m labeled points. Suppose that\\nyou are given a PAC-learning algorithm A. Show that you can use A and a \ufb01nite\\ntraining sampl', 'e S to \ufb01nd in polynomial time a hypothesis h \u2208 H that is consistent\\nwith Z, with high probability. (Hint: you can select an appropriate distribution D\\nover Z and give a condition on R(h)f o rh to be c', 'onsistent.)\\n2.10 Senate laws. For important questions, President Mouth relies on expert advice.\\nHe selects an appropriate advisor from a collection of H =2 ,800 experts.\\n(a) Assume that laws are propo', 'sed in a random fashion independently and\\nidentically according to some distributionD determined by an unknown group\\nof senators. Assume that President Mouth can \ufb01nd and select an expert senator\\nout o', 'f H who has consistently voted with the majority for the last m = 200\\nlaws. Give a bound on the probability that such a senator incorrectly predicts\\nthe global vote for a future law. What is the value', ' of the bound with 95%\\ncon\ufb01dence?\\n(b) Assume now that President Mouth can \ufb01nd and select an expert senator\\nout of H who has consistently voted with the majority for all but m\\n\u2032 =2 0o f\\nthe last m = 20', '0 laws. What is the value of the new bound?3 Rademacher Complexity and VC-\\nDimension\\nThe hypothesis sets typically used in machine learning are in\ufb01nite. But the sample\\ncomplexity bounds of the previou', 's chapter are uninformative when dealing with\\nin\ufb01nite hypothesis sets. One could ask whether e\ufb03cient learning from a \ufb01nite sample\\nis even possible when the hypothesis set H is in\ufb01nite. Our analysis of', ' the family of\\naxis-aligned rectangles (Example 2.1) indicates that this is indeed possible at least\\nin some cases, since we proved that that in\ufb01nite concept class was PAC-learnable.\\nOur goal in this ', 'chapter will be to generalize that result and derive general learning\\nguarantees for in\ufb01nite hypothesis sets.\\nA general idea for doing so consists of reducing the in\ufb01nite case to the analysis\\nof \ufb01nite', ' sets of hypotheses and then proceed as in the previous chapter. There\\nare di\ufb00erent techniques for that reduction, each relying on a di\ufb00erent notion of\\ncomplexity for the family of hypotheses. The \ufb01rs', 't complexity notion we will use\\nis that of Rademacher complexity. This will help us derive learning guarantees\\nusing relatively simple proofs based on McDiarmid\u2019s inequality, while obtaining\\nhigh-qual', 'ity bounds, including data-dependent ones, which we will frequently make\\nuse of in future chapters. However, the computation of the empirical Rademacher\\ncomplexity is NP-hard for some hypothesis sets.', ' Thus, we subsequently introduce\\ntwo other purely combinatorial notions, thegrowth function and the VC-dimension.\\nWe \ufb01rst relate the Rademacher complexity to the growth function and then bound\\nthe gro', 'wth function in terms of the VC-dimension. The VC-dimension is often easier\\nto bound or estimate. We will review a series of examples showing how to compute\\nor bound it, then relate the growth functio', 'n and the VC-dimensions. This leads to\\ngeneralization bounds based on the VC-dimension. Finally, we present lower bounds\\nbased on the VC-dimension both in the realizable and non-realizable cases, whic', 'h\\nwill demonstrate the critical role of this notion in learning.34 Rademacher Complexity and VC-Dimension\\n3.1 Rademacher complexity\\nWe will continue to use H to denote a hypothesis set as in the previ', 'ous chapters,\\nand h an element of H. Many of the results of this section are general and hold for\\nan arbitrary loss function L: Y\u00d7Y \u2192 R. To each h: X\u2192 Y , we can associate a\\nfunction g that maps (x, y', ') \u2208X\u00d7Y to L(h(x),y ) without explicitly describing the\\nspeci\ufb01c loss L used. In what follows G will generally be interpreted as the family of\\nloss functions associated to H.\\nThe Rademacher complexity c', 'aptures the richness of a family of functions by\\nmeasuring the degree to which a hypothesis set can \ufb01t random noise. The following\\nstates the formal de\ufb01nitions of the empirical and average Rademacher ', 'complexity.\\nDe\ufb01nition 3.1 Empirical Rademacher complexity\\nLet G be a family of functions mapping fromZ to [a, b] and S =( z1,...,z m) a\ufb01 x e d\\nsample of size m with elements in Z. Then, the empirical ', 'Rademacher complexity\\nof G with respect to the sample S is de\ufb01ned as:\\n\u02c6RS(G)=E\\n\u03c3\\n[\\nsup\\ng\u2208G\\n1\\nm\\nm\u2211\\ni=1\\n\u03c3ig(zi)\\n]\\n, (3.1)\\nwhere \u03c3 =( \u03c31,...,\u03c3 m)\u22a4,w i t h\u03c3is independent uniform random variables taking\\nv', 'alues in {\u22121, +1}.1 The random variables \u03c3i are called Rademacher variables.\\nLet gS denote the vector of values taken by function g over the sample S: gS =\\n(g(z1),...,g (zm))\u22a4. Then, the empirical Rad', 'emacher complexity can be rewritten\\nas\\n\u02c6RS(G)=E\\n\u03c3\\n[\\nsup\\ng\u2208G\\n\u03c3 \u00b7 gS\\nm\\n]\\n.\\nThe inner product \u03c3 \u00b7gS measures the correlation of gS with the vector of random\\nnoise \u03c3. The supremum supg\u2208G\\n\u03c3 \u00b7gS\\nm is a meas', 'ure of how well the function class G\\ncorrelates with \u03c3 over the sample S. Thus, the empirical Rademacher complexity\\nmeasures on average how well the function class G correlates with random noise\\non S.', ' This describes the richness of the family G: richer or more complex families\\nG can generate more vectors gS and thus better correlate with random noise, on\\naverage.\\n1. We assume implicitly that the s', 'upremum over the family G in this de\ufb01nition is\\nmeasurable and in general will adopt the same assumption throughout this book for other\\nsuprema over a class of functions. This assumption does not hold ', 'for arbitrary function\\nclasses but it is valid for the hypotheses sets typically considered in practice in machine\\nlearning, and the instances discussed in this book.3.1 Rademacher complexity 35\\nDe\ufb01ni', 'tion 3.2 Rademacher complexity\\nLet D denote the distribution according to which samples are drawn. For any\\ninteger m \u2265 1,t h eRademacher complexity of G is the expectation of the empirical\\nRademacher ', 'complexity over all samples of size m drawn according toD:\\nRm(G)= E\\nS\u223cDm\\n[ \u02c6RS(G)]. (3.2)\\nWe are now ready to present our \ufb01rst generalization bounds based on Rademacher\\ncomplexity.\\nTheorem 3.1\\nLet G b', 'e a family of functions mapping from Z to [0,1].T h e n ,f o ra n y\u03b4> 0,w i t h\\nprobability at least 1 \u2212 \u03b4, each of the following holds for all g \u2208 G:\\nE[g(z)] \u2264 1\\nm\\nm\u2211\\ni=1\\ng(zi)+2 Rm(G)+\\n\u221a\\nlog 1\\n\u03b4\\n2m ', '(3.3)\\nand E[g(z)] \u2264 1\\nm\\nm\u2211\\ni=1\\ng(zi)+2 \u02c6RS(G)+3\\n\u221a\\nlog 2\\n\u03b4\\n2m . (3.4)\\nProof For any sample S =( z1,...,z m) and any g \u2208 G,w ed e n o t eb y\u02c6ES[g]t h e\\nempirical average ofg over S: \u02c6ES[g]= 1\\nm\\n\u2211m\\ni=1 g', '(zi). The proof consists of applying\\nMcDiarmid\u2019s inequality to function \u03a6 de\ufb01ned for any sample S by\\n\u03a6(S)=s u p\\ng\u2208G\\nE[g] \u2212 \u02c6ES[g]. (3.5)\\nLet S and S\u2032 be two samples di\ufb00ering by exactly one point, say ', 'zm in S and z\u2032\\nm\\nin S\u2032. Then, since the di\ufb00erence of suprema does not exceed the supremum of the\\ndi\ufb00erence, we have\\n\u03a6(S\u2032) \u2212 \u03a6(S) \u2264 sup\\ng\u2208G\\n\u02c6ES[g] \u2212 \u02c6ES\u2032 [g]=s u p\\ng\u2208G\\ng(zm) \u2212 g(z\u2032\\nm)\\nm \u2264 1\\nm. (3.6)\\nS ', 'i m i l a r l y ,w ec a no b t a i n\u03a6 (S) \u2212 \u03a6(S\u2032) \u2264 1/m,t h u s|\u03a6(S) \u2212 \u03a6(S\u2032)|\u2264 1/m. Then,\\nby McDiarmid\u2019s inequality, for any \u03b4> 0, with probability at least 1 \u2212 \u03b4/2, the\\nfollowing holds:\\n\u03a6(S) \u2264 E\\nS\\n[\u03a6', '(S)] +\\n\u221a\\nlog 2\\n\u03b4\\n2m . (3.7)36 Rademacher Complexity and VC-Dimension\\nWe next bound the expectation of the right-hand side as follows:\\nE\\nS\\n[\u03a6(S)] = E\\nS\\n[\\nsup\\ng\u2208H\\nE[g] \u2212 \u02c6ES(g)\\n]\\n=E\\nS\\n[\\nsup\\ng\u2208H\\nE\\nS\u2032\\n[\u02c6E', 'S\u2032 (g) \u2212 \u02c6ES(g)\\n]]\\n(3.8)\\n\u2264 E\\nS,S\u2032\\n[\\nsup\\ng\u2208H\\n\u02c6ES\u2032 (g) \u2212 \u02c6ES(g)\\n]\\n(3.9)\\n=E\\nS,S\u2032\\n[\\nsup\\ng\u2208H\\n1\\nm\\nm\u2211\\ni=1\\n(g(z\u2032\\ni) \u2212 g(zi))\\n]\\n(3.10)\\n=E\\n\u03c3,S,S\u2032\\n[\\nsup\\ng\u2208H\\n1\\nm\\nm\u2211\\ni=1\\n\u03c3i(g(z\u2032\\ni) \u2212 g(zi))\\n]\\n(3.11)\\n\u2264 E\\n\u03c3,S\u2032\\n[\\nsup', '\\ng\u2208H\\n1\\nm\\nm\u2211\\ni=1\\n\u03c3ig(z\u2032\\ni)\\n]\\n+E\\n\u03c3,S\\n[\\nsup\\ng\u2208H\\n1\\nm\\nm\u2211\\ni=1\\n\u2212\u03c3ig(zi)\\n]\\n(3.12)\\n=2 E\\n\u03c3,S\\n[\\nsup\\ng\u2208H\\n1\\nm\\nm\u2211\\ni=1\\n\u03c3ig(zi)\\n]\\n=2 Rm(G). (3.13)\\nEquation 3.8 uses the fact that points inS\u2032 are sampled in an i.i.d. ', 'fashion and thus\\nE[g]=E S\u2032 [\u02c6ES\u2032 (g)], as in (2.3). Inequality 3.9 holds by Jensen\u2019s inequality and the\\nconvexity of the supremum function. In equation 3.11, we introduce Rademacher\\nvariables \u03c3\\nis, th', 'at is uniformly distributed independent random variables taking\\nvalues in {\u22121, +1} as in de\ufb01nition 3.2. This does not change the expectation\\nappearing in (3.10): when \u03c3i = 1, the associated summand re', 'mains unchanged;\\nwhen \u03c3i = \u22121, the associated summand \ufb02ips signs, which is equivalent to swapping\\nzi and z\u2032\\ni between S and S\u2032. Since we are taking the expectation over all possibleS\\nand S\u2032, this swap', ' does not a\ufb00ect the overall expectation. We are simply changing the\\norder of the summands within the expectation. (3.12) holds by the sub-additivity of\\nthe supremum function, that is the identity sup(', 'U +V ) \u2264 sup(U)+sup( V ). Finally,\\n(3.13) stems from the de\ufb01nition of Rademacher complexity and the fact that the\\nvariables \u03c3\\ni and \u2212\u03c3i are distributed in the same way.\\nThe reduction to Rm(G) in equat', 'ion 3.13 yields the bound in equation 3.3,\\nusing \u03b4 instead of \u03b4/2. To derive a bound in terms of \u02c6RS(G), we observe that,\\nby de\ufb01nition 3.2, changing one point in S changes \u02c6RS(G)b ya tm o s t1/m. Then', ',\\nusing again McDiarmid\u2019s inequality, with probability 1 \u2212 \u03b4/2 the following holds:\\nRm(G) \u2264 \u02c6RS(G)+\\n\u221a\\nlog 2\\n\u03b4\\n2m . (3.14)\\nFinally, we use the union bound to combine inequalities 3.7 and 3.14, which yi', 'elds3.1 Rademacher complexity 37\\nwith probability at least 1 \u2212 \u03b4:\\n\u03a6(S) \u2264 2 \u02c6RS(G)+3\\n\u221a\\nlog 2\\n\u03b4\\n2m , (3.15)\\nwhich matches (3.4).\\nThe following result relates the empirical Rademacher complexities of a h', 'ypothe-\\nsis set H and to the family of loss functionsG associated to H in the case of binary\\nloss (zero-one loss).\\nLemma 3.1\\nLet H be a family of functions taking values in {\u22121, +1} and let G be the f', 'amily of\\nloss functions associated toH for the zero-one loss:G = {(x, y) \u21a6\u2192 1h(x)\u0338=y : h \u2208 H\\n}\\n.\\nFor any sample S =( (x1,y1),..., (xm,y m)) of elements in X\u00d7 { \u2212 1, +1},l e tSX\\ndenote its projection o', 'ver X: SX =( x1,...,x m). Then, the following relation holds\\nbetween the empirical Rademacher complexities ofG and H:\\n\u02c6RS(G)= 1\\n2\\n\u02c6RSX (H). (3.16)\\nProof For any sample S =( (x1,y1),..., (xm,y m)) of e', 'lements in X\u00d7 { \u2212 1, +1},\\nby de\ufb01nition, the empirical Rademacher complexity of G c a nb ew r i t t e na s :\\n\u02c6RS(G)=E\\n\u03c3\\n[\\nsup\\nh\u2208H\\n1\\nm\\nm\u2211\\ni=1\\n\u03c3i1h(xi)\u0338=yi\\n]\\n=E\\n\u03c3\\n[\\nsup\\nh\u2208H\\n1\\nm\\nm\u2211\\ni=1\\n\u03c3i\\n1\u2212yih(xi)\\n2\\n]\\n= ', '1\\n2 E\\n\u03c3\\n[\\nsup\\nh\u2208H\\n1\\nm\\nm\u2211\\ni=1\\n\u2212\u03c3iyih(xi)\\n]\\n= 1\\n2 E\\n\u03c3\\n[\\nsup\\nh\u2208H\\n1\\nm\\nm\u2211\\ni=1\\n\u03c3ih(xi)\\n]\\n= 1\\n2 RSX (H),\\nw h e r ew eu s e dt h ef a c tt h a t1h(xi)\u0338=yi =( 1\u2212 yih(xi))/2 and the fact that for a \ufb01xed\\nyi \u2208{ \u2212', '1, +1}, \u03c3i and \u2212yi\u03c3i are distributed in the same way.\\nNote that the lemma implies, by taking expectations, that for anym \u2265 1, Rm(G)=\\n1\\n2 Rm(H). These connections between the empirical and average Rade', 'macher com-\\nplexities can be used to derive generalization bounds for binary classi\ufb01cation in\\nterms of the Rademacher complexity of the hypothesis set H.\\nTheorem 3.2 Rademacher complexity bounds \u2013 bin', 'ary classi\ufb01cation\\nLet H be a family of functions taking values in{\u22121, +1} and let D be the distribution\\nover the input space X. Then, for any \u03b4> 0, with probability at least 1 \u2212 \u03b4 over38 Rademacher Co', 'mplexity and VC-Dimension\\na sample S of size m drawn according to D, each of the following holds for any\\nh \u2208 H:\\nR(h) \u2264 \u02c6R(h)+ Rm(H)+\\n\u221a\\nlog 1\\n\u03b4\\n2m (3.17)\\nand R(h) \u2264 \u02c6R(h)+ \u02c6RS(H)+3\\n\u221a\\nlog 2\\n\u03b4\\n2m . (3.18', ')\\nProof The result follows immediately by theorem 3.1 and lemma 3.1.\\nThe theorem provides two generalization bounds for binary classi\ufb01cation based on\\nthe Rademacher complexity. Note that the second bo', 'und, (3.18), is data-dependent:\\nthe empirical Rademacher complexity \u02c6RS(H) is a function of the speci\ufb01c sample\\nS drawn. Thus, this bound could be particularly informative if we could compute\\n\u02c6RS(H). B', 'ut, how can we compute the empirical Rademacher complexity? Using\\nagain the fact that \u03c3i and \u2212\u03c3i are distributed in the same way, we can write\\n\u02c6RS(H)=E\\n\u03c3\\n[\\nsup\\nh\u2208H\\n1\\nm\\nm\u2211\\ni=1\\n\u2212\u03c3ih(xi)\\n]\\n= \u2212 E\\n\u03c3\\n[\\ninf\\n', 'h\u2208H\\n1\\nm\\nm\u2211\\ni=1\\n\u03c3ih(xi)\\n]\\n.\\nNow, for a \ufb01xed value of \u03c3, computing inf h\u2208H 1\\nm\\n\u2211m\\ni=1 \u03c3ih(xi) is equivalent to\\nan empirical risk minimization problem, which is known to be computationally\\nhard for some ', 'hypothesis sets. Thus, in some cases, computing \u02c6RS(H)c o u l d\\nbe computationally hard. In the next sections, we will relate the Rademacher\\ncomplexity to combinatorial measures that are easier to com', 'pute.\\n3.2 Growth function\\nHere we will show how the Rademacher complexity can be bounded in terms of the\\ngrowth function.\\nDe\ufb01nition 3.3 Growth function\\nThe growth function \u03a0\\nH : N \u2192 N for a hypothesis', ' set H is de\ufb01ned by:\\n\u2200m \u2208 N, \u03a0H(m)= m a x\\n{x1,...,xm}\u2286X\\n\u23d0\u23d0\\n\u23d0\\n{(\\nh(x\\n1),...,h (xm)\\n\u23a1\\n: h \u2208 H\\n}\u23d0\u23d0\\n\u23d0. (3.19)\\nThus, \u03a0\\nH(m) is the maximum number of distinct ways in which m points can be\\nclassi\ufb01ed using h', 'ypotheses in H. This provides another measure of the richness of\\nt h eh y p o t h e s i ss e tH. However, unlike the Rademacher complexity, this measure\\ndoes not depend on the distribution, it is pure', 'ly combinatorial.3.2 Growth function 39\\nTo relate the Rademacher complexity to the growth function, we will use Mas-\\nsart\u2019s lemma.\\nTheorem 3.3 Massart\u2019s lemma\\nLet A \u2286 Rm be a \ufb01nite set, with r =m a xx', '\u2208A \u2225x\u22252, then the following holds:\\nE\\n\u03c3\\n[ 1\\nm sup\\nx\u2208A\\nm\u2211\\ni=1\\n\u03c3ixi\\n]\\n\u2264 r\\n\u221a\\n2l o g|A|\\nm , (3.20)\\nwhere \u03c3is are independent uniform random variables taking values in{\u22121, +1} and\\nx1,...,x m are the compone', 'nts of vectorx.\\nProof For any t> 0, using Jensen\u2019s inequality, rearranging terms, and bounding\\nthe supremum by a sum, we obtain:\\nexp\\n(\\nt E\\n\u03c3\\n[\\nsup\\nx\u2208A\\nm\u2211\\ni=1\\n\u03c3ixi\\n]\u23a1\\n\u2264 E\\n\u03c3\\n(\\nexp\\n[\\nt sup\\nx\u2208A\\nm\u2211\\ni=1\\n\u03c3ix', 'i\\n]\u23a1\\n=E\\n\u03c3\\n(\\nsup\\nx\u2208A\\nexp\\n[\\nt\\nm\u2211\\ni=1\\n\u03c3ixi\\n]\u23a1\\n\u2264\\n\u2211\\nx\u2208A\\nE\\n\u03c3\\n(\\nexp\\n[\\nt\\nm\u2211\\ni=1\\n\u03c3ixi\\n]\u23a1\\n.\\nWe next use the independence of the\u03c3is, then apply Hoe\ufb00ding\u2019s lemma (lemma D.1),\\nand use the de\ufb01nition of r to write:\\n', 'exp\\n(\\nt E\\n\u03c3\\n[\\nsup\\nx\u2208A\\nm\u2211\\ni=1\\n\u03c3ixi\\n]\u23a1\\n\u2264\\n\u2211\\nx\u2208A\\n\u03a0m\\ni=1 E\\n\u03c3i\\n(exp [t\u03c3ixi])\\n\u2264\\n\u2211\\nx\u2208A\\n\u03a0m\\ni=1 exp\\n[t2(2xi)2\\n8\\n]\\n=\\n\u2211\\nx\u2208A\\nexp\\n[\\nt2\\n2\\nm\u2211\\ni=1\\nx2\\ni\\n]\\n\u2264\\n\u2211\\nx\u2208A\\nexp\\n[t2r2\\n2\\n]\\n= |A|e\\nt2R2\\n2 .\\nTaking the log of both si', 'des and dividing by t gives us:\\nE\\n\u03c3\\n[\\nsup\\nx\u2208A\\nm\u2211\\ni=1\\n\u03c3ixi\\n]\\n\u2264 log |A|\\nt + tr2\\n2 . (3.21)\\nIf we choose t =\\n\u221a\\n2 log|A|\\nr , which minimizes this upper bound, we get:\\nE\\n\u03c3\\n[\\nsup\\nx\u2208A\\nm\u2211\\ni=1\\n\u03c3ixi\\n]\\n\u2264 r\\n\u221a\\n2l ', 'o g|A|. (3.22)\\nDividing both sides by m leads to the statement of the lemma.40 Rademacher Complexity and VC-Dimension\\nUsing this result, we can now bound the Rademacher complexity in terms of the\\ngrow', 'th function.\\nCorollary 3.1\\nLet G be a family of functions taking values in {\u22121, +1}. Then the following holds:\\nRm(G) \u2264\\n\u221a\\n2 log \u03a0G(m)\\nm . (3.23)\\nProof For a \ufb01xed sample S =( x1,...,x m), we denote by G', '|S the set of vectors\\nof function values ( g(x1),...,g (xm))\u22a4 where g is in G.S i n c eg \u2208 G takes values\\nin {\u22121, +1}, the norm of these vectors is bounded by \u221am. We can then apply\\nMassart\u2019s lemma as ', 'follows:\\nRm(G)=E\\nS\\n[\\nE\\n\u03c3\\n[\\nsup\\nu\u2208G|S\\n1\\nm\\nm\u2211\\ni=1\\n\u03c3iui\\n]]\\n\u2264 E\\nS\\n[ \u221am\\n\u221a\\n2l o g|G|S |\\nm\\n]\\n.\\nBy de\ufb01nition, |G|S | is bounded by the growth function, thus,\\nRm(G) \u2264 E\\nS\\n[ \u221am\\n\u221a\\n2 log \u03a0G(m)\\nm\\n]\\n=\\n\u221a\\n2 log \u03a0G(m)', '\\nm ,\\nwhich concludes the proof.\\nCombining the generalization bound (3.17) of theorem 3.2 with corollary 3.1 yields\\nimmediately the following generalization bound in terms of the growth function.\\nCorol', 'lary 3.2 Growth function generalization bound\\nLet H be a family of functions taking values in{\u22121, +1}.T h e n ,f o ra n y\u03b4> 0,w i t h\\nprobability at least 1 \u2212 \u03b4, for any h \u2208 H,\\nR(h) \u2264 \u02c6R(h)+\\n\u221a\\n2 log \u03a0', 'H(m)\\nm +\\n\u221a\\nlog 1\\n\u03b4\\n2m . (3.24)\\nGrowth function bounds can be also derived directly (without using Rademacher\\ncomplexity bounds \ufb01rst). The resulting bound is then the following:\\nPr\\n[\u23d0\u23d0\u23d0R(h) \u2212 \u02c6R(h)\\n\u23d0\u23d0\\n', '\u23d0 >\u03f5\\n]\\n\u2264 4\u03a0\\nH(2m)e x p\\n(\\n\u2212 m\u03f52\\n8\\n\u23a1\\n, (3.25)\\nwhich only di\ufb00ers from (3.24) by constants.\\nThe computation of the growth function may not be always convenient since, by\\nde\ufb01nition, it requires computing \u03a0', ' H(m) for all m \u2265 1. The next section introduces\\nan alternative measure of the complexity of a hypothesis setH that is based instead\\non a single scalar, which will turn out to be in fact deeply relate', 'd to the behavior\\nof the growth function.3.3 VC-dimension 41\\n- - + -\\n+ +\\n- +\\n+ - +\\n(a) (b)\\nFigure 3.1 VC-dimension of intervals on the real line. (a) Any two points can be\\nshattered. (b) No sample of ', 'three points can be shattered as the (+, \u2212, +) labeling\\ncannot be realized.\\n3.3 VC-dimension\\nHere, we introduce the notion of VC-dimension (Vapnik-Chervonenkis dimension).\\nThe VC-dimension is also a p', 'urely combinatorial notion but it is often easier to\\ncompute than the growth function (or the Rademacher Complexity). As we shall\\nsee, the VC-dimension is a key quantity in learning and is directly re', 'lated to the\\ngrowth function.\\nTo de\ufb01ne the VC-dimension of a hypothesis setH, we \ufb01rst introduce the concepts\\nof dichotomy and that of shattering. Given a hypothesis set H, a dichotomy of a\\nset S is on', 'e of the possible ways of labeling the points of S using a hypothesis in\\nH.As e t S of m \u2265 1p o i n t si ss a i dt ob es h a t t e r e db yah y p o t h e s i ss e tH when H\\nrealizes all possible dicho', 'tomies of S,t h a ti sw h e n\u03a0H(m)=2 m.\\nDe\ufb01nition 3.4 VC-dimension\\nThe VC-dimension of a hypothesis set H is the size of the largest set that can be\\nfully shattered by H:\\nVCdim(H)=m a x{m:\u03a0 H(m)=2 m}.', ' (3.26)\\nNote that, by de\ufb01nition, if VCdim( H)= d, there exists a set of size d that can\\nbe fully shattered. But, this does not imply that all sets of size d or less are fully\\nshattered, in fact, this ', 'is typically not the case.\\nTo further illustrate this notion, we will examine a series of examples of hypothesis\\nsets and will determine the VC-dimension in each case. To compute the VC-\\ndimension we ', 'will typically show a lower bound for its value and then a matching\\nupper bound. To give a lower boundd for VCdim(H), it su\ufb03ces to show that a set\\nS of cardinality d can be shattered byH. To give an u', 'pper bound, we need to prove\\nthat no set S of cardinality d + 1 can be shattered by H, which is typically more\\ndi\ufb03cult.\\nExample 3.1 Intervals on the real line\\nOur \ufb01rst example involves the hypothesis ', 'class of intervals on the real line.\\nIt is clear that the VC-dimension is at least two, since all four dichotomies42 Rademacher Complexity and VC-Dimension\\n+\\n+\\n--\\n+\\n+\\n-\\n+\\n(a) (b)\\nFigure 3.2 Unrealizab', 'le dichotomies for four points using hyperplanes in R2.( a )\\nAll four points lie on the convex hull. (b) Three points lie on the convex hull while\\nthe remaining point is interior.\\n(+,+), (\u2212, \u2212), (+, \u2212', '), (\u2212,+) can be realized, as illustrated in \ufb01gure 3.1(a). In con-\\ntrast, by the de\ufb01nition of intervals, no set of three points can be shattered since the\\n(+, \u2212,+) labeling cannot be realized. Hence, V', 'Cdim(intervals in R)=2 .\\nExample 3.2 Hyperplanes\\nConsider the set of hyperplanes inR2. We \ufb01rst observe that any three non-collinear\\npoints in R2 can be shattered. To obtain the \ufb01rst three dichotomies,', ' we choose a\\nhyperplane that has two points on one side and the third point on the opposite\\nside. To obtain the fourth dichotomy we have all three points on the same side of\\nthe hyperplane. The remain', 'ing four dichotomies are realized by simply switching\\nsigns. Next, we show that four points cannot be shattered by considering two cases:\\n(i) the four points lie on the convex hull de\ufb01ned by the four ', 'points, and (ii) three\\nof the four points lie on the convex hull and the remaining point is internal. In\\nthe \ufb01rst case, a positive labeling for one diagonal pair and a negative labeling for\\nthe other ', 'diagonal pair cannot be realized, as illustrated in \ufb01gure 3.2(a). In the\\nsecond case, a labeling which is positive for the points on the convex hull and\\nnegative for the interior point cannot be reali', 'zed, as illustrated in \ufb01gure 3.2(b).\\nHence, VCdim(hyperplanes in R\\n2)=3 .\\nMore generally in Rd, we derive a lower bound by starting with a set of d +1\\npoints in Rd,s e t t i n gx0 to be the origin and', ' de\ufb01ning xi,f o ri \u2208{ 1,...,d },a st h e\\npoint whose ith coordinate is 1 and all others are 0. Lety0,y1,...,y d \u2208{ \u22121, +1} be\\nan arbitrary set of labels forx0,x1,...,x d.L e tw be the vector whoseith ', 'coordinate\\nis yi. Then the classi\ufb01er de\ufb01ned by the hyperplane of equationw\u00b7x+ y0\\n2 = 0 shatters\\nx0,x1,...,x d since for any i \u2208 [0,d ],\\nsgn\\n(\\nw \u00b7 xi + y0\\n2\\n\u23a1\\n=s g n\\n(\\nyi + y0\\n2\\n\u23a1\\n= yi. (3.27)\\nTo obtai', 'n an upper bound, it su\ufb03ces to show that no set of d +2p o i n t sc a nb e\\nshattered by halfspaces. To prove this, we will use the following general theorem.3.3 VC-dimension 43\\n+\\n-\\n--\\n+\\n+\\n--\\n+\\n+\\n-+\\n+\\n', '-\\n+-\\n(a)\\n+\\n-\\n+\\n+\\n+\\n(b)\\nFigure 3.3 VC-dimension of axis-aligned rectangles. (a) Examples of realizable\\ndichotomies for four points in a diamond pattern. (b) No sample of \ufb01ve points can\\nbe realized if t', 'he interior point and the remaining points have opposite labels.\\nTheorem 3.4 Radon\u2019s theorem\\nAny set X of d+2 points in R\\nd can be partitioned into two subsetsX1 and X2 such\\nthat the convex hulls of X', '1 and X2 intersect.\\nProof Let X = {x1,..., xd+2}\u2282 Rd. The following is a system of d +1l i n e a r\\nequations in \u03b11,...,\u03b1 d+2:\\nd+2\u2211\\ni=1\\n\u03b1ixi = 0 and\\nd+2\u2211\\ni=1\\n\u03b1i =0 , (3.28)\\nsince the \ufb01rst equality lead', 's to d equations, one for each component. The number\\nof unknowns, d + 2, is larger than the number of equations, d + 1, therefore\\nthe system admits a non-zero solution \u03b21,...,\u03b2 d+2.S i n c e\u2211d+2\\ni=1 \u03b2', 'i = 0, both\\nI1 = {i \u2208 [1,d +2 ] :\u03b2i > 0} and I2 = {i \u2208 [1,d +2 ] :\u03b2i < 0} are non-empty\\nsets and X1 = {xi : i \u2208 I1} and X2 = {xi : i \u2208 I2} form a partition of X.B yt h e\\nlast equation of (3.28), \u2211\\ni\u2208I', '1 \u03b2i = \u2212 \u2211\\ni\u2208I2 \u03b2i.L e t\u03b2 = \u2211\\ni\u2208I1 \u03b2i. Then, the \ufb01rst\\npart of (3.28) implies\\n\u2211\\ni\u2208I1\\n\u03b2i\\n\u03b2 xi =\\n\u2211\\ni\u2208I2\\n\u2212\u03b2i\\n\u03b2 xi,\\nwith \u2211\\ni\u2208I1\\n\u03b2i\\n\u03b2 = \u2211\\ni\u2208I2\\n\u2212\u03b2i\\n\u03b2 =1 ,a n d\u03b2i\\n\u03b2 \u2265 0f o ri \u2208 I1 and \u2212\u03b2i\\n\u03b2 \u2265 0f o ri \u2208 I2.B y', '\\nde\ufb01nition of the convex hulls (B.4), this implies that \u2211\\ni\u2208I1\\n\u03b2i\\n\u03b2 xi belongs both to44 Rademacher Complexity and VC-Dimension\\n+\\n++\\n- -\\n-\\n--\\n--\\n|positive points| < |negative points|\\n+\\n+\\n+ +\\n+\\n+\\n+\\n-\\n-', '\\n-\\n-\\n|positive points| > |negative points|\\n(a) (b)\\nFigure 3.4 Convex d-gons in the plane can shatter 2d +1 points. (a) d-gon\\nconstruction when there are more negative labels. (b) d-gon construction wh', 'en\\nthere are more positive labels.\\nt h ec o n v e xh u l lo fX1 and to that of X2.\\nNow, let X be a set of d + 2 points. By Radon\u2019s theorem, it can be partitioned\\ninto two sets X1 and X2 such that thei', 'r convex hulls intersect. Observe that when\\ntwo sets of points X1 and X2 are separated by a hyperplane, their convex hulls\\nare also separated by that hyperplane. Thus, X1 and X2 cannot be separated by', '\\nah y p e r p l a n ea n dX is not shattered. Combining our lower and upper bounds, we\\nhave proven that VCdim(hyperplanes in Rd)= d +1 .\\nExample 3.3 Axis-aligned Rectangles\\nWe \ufb01rst show that the VC-di', 'mension is at least four, by considering four points\\nin a diamond pattern. Then, it is clear that all 16 dichotomies can be realized,\\nsome of which are illustrated in \ufb01gure 3.2(a). In contrast, for an', 'y set of \ufb01ve distinct\\npoints, if we construct the minimal axis-aligned rectangle containing these points,\\none of the \ufb01ve points is in the interior of this rectangle. Imagine that we assign a\\nnegative ', 'label to this interior point and a positive label to each of the remaining\\nfour points, as illustrated in \ufb01gure 3.2(b). There is no axis-aligned rectangle that\\ncan realize this labeling. Hence, no set', ' of \ufb01ve distinct points can be shattered and\\nVCdim(axis-aligned rectangles) = 4.\\nExample 3.4 Convex Polygons\\nWe focus on the class of convex d-gons in the plane. To get a lower bound, we\\nshow that any', ' set of 2 d + 1 points can be fully shattered. To do this, we select\\n2d + 1 points that lie on a circle, and for a particular labeling, if there are more\\nnegative than positive labels, then the points', ' with the positive labels are used as\\nthe polygon\u2019s vertices, as in \ufb01gure 3.4(a). Otherwise, the tangents of the negative\\npoints serve as the edges of the polygon, as shown in (3.4)(b). To derive an u', 'pper3.3 VC-dimension 45\\nsin(50x)\\nx\\n1\\n-1\\n10\\nFigure 3.5 An example of a sine function (with \u03c9 =5 0) used for classi\ufb01cation.\\nbound, it can be shown that choosing points on the circle maximizes the number', '\\nof possible dichotomies, and thus VCdim(convex d-gons) = 2d + 1. Note also that\\nVCdim(convex polygons) = +\u221e .\\nExample 3.5 Sine Functions\\nThe previous examples could suggest that the VC-dimension of H', ' coincides with\\nthe number of free parameters de\ufb01ning H. For example, the number of parameters\\nde\ufb01ning hyperplanes matches their VC-dimension. However, this does not hold in\\ngeneral. Several of the ex', 'ercises in this chapter illustrate this fact. The following\\nprovides a striking example from this point of view. Consider the following family\\nof sine functions: {t \u21a6\u2192 sin(\u03c9t): \u03c9 \u2208 R}. One instance of', ' this function class is shown\\nin \ufb01gure 3.5. These sine functions can be used to classify the points on the real line:\\na point is labeled positively if it is above the curve, negatively otherwise. Alth', 'ough\\nthis family of sine function is de\ufb01ned via a single parameter, \u03c9, it can be shown\\nthat VCdim(sine functions) = +\u221e (exercise 3.12).\\nThe VC-dimension of many other hypothesis sets can be determined', ' or upper-\\nbounded in a similar way (see this chapter\u2019s exercises). In particular, the VC-\\ndimension of any vector space of dimension r< \u221e can be shown to be at most\\nr (exercise 3.11). The next result', ' known as Sauer\u2019s lemma clari\ufb01es the connection\\nbetween the notions of growth function and VC-dimension.\\nTheorem 3.5 Sauer\u2019s lemma\\nLet H be a hypothesis set with VCdim(H)= d.T h e n ,f o ra l lm \u2208 N, ', 'the following\\ninequality holds:\\n\u03a0\\nH(m) \u2264\\nd\u2211\\ni=0\\n(m\\ni\\n\u23a1\\n. (3.29)46 Rademacher Complexity and VC-Dimension\\nx1 x2 \u00b7\u00b7\u00b7 xm\u22121 xm\\n\u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7\\n11010\\n11011\\n01111\\n10010\\n10001\\nG1 =G|S\u2032 G2 = {g\u2032 \u2286 S\u2032 :( g', '\u2032 \u2208 G) \u2227 (g\u2032 \u222a{ xm}\u2208 G)}.\\nFigure 3.6 Illustration of how G1 and G2 are constructed in the proof of Sauer\u2019s\\nlemma.\\nProof The proof is by induction onm+d. The statement clearly holds form =1\\nand d =0o r', ' d = 1. Now, assume that it holds for ( m \u2212 1,d \u2212 1) and (m \u2212 1,d ).\\nFix a set S = {x1,...,x m} with \u03a0H(m)d i c h o t o m i e sa n dl e tG = H|S be the set of\\nconcepts H induces by restriction to S.\\nN', 'ow consider the following families over S\u2032 = {x1,...,x m\u22121}.W ed e \ufb01 n eG1 =\\nG|S\u2032 as the set of conceptsH includes by restriction toS\u2032. Next, by identifying each\\nconcept as the set of points (in S\u2032 or', ' S) for which it is non-zero, we can de\ufb01ne G2\\nas\\nG2 = {g\u2032 \u2286 S\u2032 :( g\u2032 \u2208 G) \u2227 (g\u2032 \u222a{ xm}\u2208 G)}.\\nSince g\u2032 \u2286 S\u2032, g\u2032 \u2208 G means that without adding xm it is a concept of G. Further,\\nthe constraint g\u2032 \u222a{ xm}\u2208', ' G means that adding xm to g\u2032 also makes it a concept\\nof G. The construction of G1 and G2 is illustrated pictorially in \ufb01gure 3.6. Given\\nour de\ufb01nitions of G1 and G2,o b s e r v et h a t|G1| + |G2| = |', 'G|.\\nSince VCdim(G1) \u2264 VCdim(G) \u2264 d, then by de\ufb01nition of the growth function\\nand using the induction hypothesis,\\n|G1|\u2264 \u03a0G1 (m \u2212 1) \u2264\\nd\u2211\\ni=0\\n(m \u2212 1\\ni\\n\u23a1\\n.\\nFurther, by de\ufb01nition ofG2,i fas e tZ \u2286 S\u2032 is s', 'hattered byG2, then the setZ \u222a{xm}\\nis shattered by G. Hence,\\nVCdim(G2) \u2264 VCdim(G) \u2212 1= d \u2212 1,3.3 VC-dimension 47\\nand by de\ufb01nition of the growth function and using the induction hypothesis,\\n|G2|\u2264 \u03a0G2 (', 'm \u2212 1) \u2264\\nd\u22121\u2211\\ni=0\\n(m \u2212 1\\ni\\n\u23a1\\n.\\nThus,\\n|G| = |G1| + |G2|\u2264\\nd\u2211\\ni=0\\n(m\u22121\\ni\\n\u23a1\\n+\\nd\u22121\u2211\\ni=0\\n(m\u22121\\ni\\n\u23a1\\n=\\nd\u2211\\ni=0\\n(m\u22121\\ni\\n\u23a1\\n+\\n(m\u22121\\ni\u22121\\n\u23a1\\n=\\nd\u2211\\ni=0\\n(m\\ni\\n\u23a1\\n,\\nwhich completes the inductive proof.\\nThe signi\ufb01cance of Sau', 'er\u2019s lemma can be seen by corollary 3.3, which remarkably\\nshows that growth function only exhibits two types of behavior: either VCdim(H)=\\nd< +\u221e ,i nw h i c hc a s e\u03a0H(m)= O(md), or VCdim( H)=+ \u221e ,i n', 'w h i c hc a s e\\n\u03a0H(m)=2 m.\\nCorollary 3.3\\nLet H be a hypothesis set with VCdim(H)= d. Then for all m \u2265 d,\\n\u03a0H(m) \u2264\\n(em\\nd\\n\u23a1d\\n= O(md). (3.30)\\nProof The proof begins by using Sauer\u2019s lemma. The \ufb01rst inequ', 'ality multiplies\\neach summand by a factor that is greater than or equal to one since m \u2265 d,w h i l e\\nthe second inequality adds non-negative summands to the summation.\\n\u03a0H(m) \u2264\\nd\u2211\\ni=0\\n(m\\ni\\n\u23a1\\n\u2264\\nd\u2211\\ni=0\\n(', 'm\\ni\\n\u23a1 (m\\nd\\n\u23a1d\u2212i\\n\u2264\\nm\u2211\\ni=0\\n(m\\ni\\n\u23a1 (m\\nd\\n\u23a1d\u2212i\\n=\\n(m\\nd\\n\u23a1d m\u2211\\ni=0\\n(m\\ni\\n\u23a1( d\\nm\\n\u23a1i\\n=\\n(m\\nd\\n\u23a1d (\\n1+ d\\nm\\n\u23a1m\\n\u2264\\n(m\\nd\\n\u23a1d\\ned.\\nAfter simplifying the expression using the binomial theorem, the \ufb01nal inequality\\nfollows u', 'sing the general identity (1 \u2212 x) \u2264 e\u2212x.\\nThe explicit relationship just formulated between VC-dimension and the growth\\nfunction combined with corollary 3.2 leads immediately to the following generaliz', 'a-48 Rademacher Complexity and VC-Dimension\\ntion bounds based on the VC-dimension.\\nCorollary 3.4 VC-dimension generalization bounds\\nLet H be a family of functions taking values in {\u22121, +1} with VC-dim', 'ension d.\\nThen, for any \u03b4> 0, with probability at least 1 \u2212 \u03b4, the following holds for all\\nh \u2208 H:\\nR(h) \u2264 \u02c6R(h)+\\n\u221a\\n2d log em\\nd\\nm +\\n\u221a\\nlog 1\\n\u03b4\\n2m . (3.31)\\nThus, the form of this generalization bound is\\nR', '(h) \u2264 \u02c6R(h)+ O\\n(\u221a\\nlog(m/d)\\n(m/d)\\n\u23a1\\n, (3.32)\\nwhich emphasizes the importance of the ratio m/d for generalization. The theorem\\nprovides another instance of Occam\u2019s razor principle where simplicity is me', 'asured\\nin terms of smaller VC-dimension.\\nVC-dimension bounds can be derived directly without using an intermediate\\nRademacher complexity bound, as for (3.25): combining Sauer\u2019s lemma with (3.25)\\nleads', ' to the following high-probability bound\\nR(h) \u2264 \u02c6R(h)+\\n\u221a\\n8d log 2em\\nd +8l o g4\\n\u03b4\\nm ,\\nwhich has the general form of (3.32). The log factor plays only a minor role in these\\nbounds. A \ufb01ner analysis can b', 'e used in fact to eliminate that factor.\\n3.4 Lower bounds\\nIn the previous section, we presented several upper bounds on the generalization\\nerror. In contrast, this section provides lower bounds on the', ' generalization error of\\nany learning algorithm in terms of the VC-dimension of the hypothesis set used.\\nThese lower bounds are shown by \ufb01nding for any algorithm a \u2018bad\u2019 distribution.\\nSince the learni', 'ng algorithm is arbitrary, it will be di\ufb03cult to specify that particular\\ndistribution. Instead, it su\ufb03ces to prove its existence non-constructively. At a high\\nlevel, the proof technique used to achiev', 'e this is the probabilistic method of Paul\\nErd\u00a8os. In the context of the following proofs, \ufb01rst a lower bound is given on the\\nexpected error over the parameters de\ufb01ning the distributions. From that, t', 'he lower\\nbound is shown to hold for at least one set of parameters, that is one distribution.3.4 Lower bounds 49\\nTheorem 3.6 Lower bound, realizable case\\nLet H be a hypothesis set with VC-dimension d>', ' 1. Then, for any learning\\nalgorithm A, there exist a distribution D over X and a target function f \u2208 H\\nsuch that\\nPr\\nS\u223cDm\\n[\\nRD(hS,f ) > d \u2212 1\\n32m\\n]\\n\u2265 1/100. (3.33)\\nProof Let X = {x0,x1,...,x d\u22121}\u2286X be', ' a set that is fully shattered by H.F o r\\nany \u03f5> 0, we choose D such that its support is reduced to X and so that one\\npoint (x0) has very high probability (1 \u2212 \u03f5), with the rest of the probability mas', 's\\ndistributed uniformly among the other points:\\nPr\\nD\\n[x0]=1 \u2212 8\u03f5 and \u2200i \u2208 [1,d \u2212 1],Pr\\nD\\n[xi]= 8\u03f5\\nd \u2212 1. (3.34)\\nWith this de\ufb01nition, most samples would containx0 and, since X is fully shattered,\\nA can', ' essentially do no better than tossing a coin when determining the label of a\\npoint xi not falling in the training set.\\nWe assume without loss of generality that A makes no error on x0. For a sample\\nS', ',w el e tS d e n o t et h es e to fi t se l e m e n t sf a l l i n gi n{x1,...,x d\u22121},a n dl e tS be the\\nset of samples S of size m such that |S|\u2264 (d \u2212 1)/2. Now, \ufb01x a sample S \u2208S ,a n d\\nconsider the ', 'uniform distribution U over all labelings f : X \u2192{ 0, 1},w h i c ha r ea l l\\nin H since the set is shattered. Then, the following lower bound holds:\\nE\\nf \u223cU\\n[RD(hS,f )] =\\n\u2211\\nf\\n\u2211\\nx\u2208X\\n1h(x)\u0338=f(x) Pr[x]P r', ' [f]\\n\u2265\\n\u2211\\nf\\n\u2211\\nx\u0338\u2208S\\n1h(x)\u0338=f(x) Pr[x]P r [f]\\n=\\n\u2211\\nx\u0338\u2208S\\n(\u2211\\nf\\n1h(x)\u0338=f(x) Pr[f]\\n\u23a1\\nPr[x]\\n= 1\\n2\\n\u2211\\nx\u0338\u2208S\\nPr[x] \u2265 1\\n2\\nd \u2212 1\\n2\\n8\u03f5\\nd \u2212 1 =2 \u03f5. (3.35)\\nThe \ufb01rst lower bound holds because we remove non-negative term', 's from the\\nsummation when we only consider x \u0338\u2208 S i n s t e a do fa l lx in X. After rearranging\\nterms, the subsequent equality holds since we are taking an expectation overf \u2208 H\\nwith uniform weight o', 'n eachf and H shatters X. The \ufb01nal lower bound holds due\\nto the de\ufb01nitions of D and S, the latter which implies that |X \u2212 S|\u2265 (d \u2212 1)/2.\\nSince (3.35) holds for all S \u2208S , it also holds in expectation ', 'over all S \u2208S :\\nES\u2208S\\n[\\nEf \u223cU [RD(hS,f )]\\n]\\n\u2265 2\u03f5. By Fubini\u2019s theorem, the expectations can be50 Rademacher Complexity and VC-Dimension\\npermuted, thus,\\nE\\nf \u223cU\\n[\\nE\\nS\u2208S\\n[RD(hS,f )]\\n]\\n\u2265 2\u03f5. (3.36)\\nThis im', 'plies that ES\u2208S[RD(hS,f0)] \u2265 2\u03f5 for at least one labeling f0 \u2208 H. Decom-\\nposing this expectation into two parts and using RD(hS,f0) \u2264 PrD[X \u2212{ x0}], we\\nobtain:\\nE\\nS\u2208S\\n[RD(hS,f0)] =\\n\u2211\\nS:RD(hS,f0)\u2265\u03f5\\nRD(h', 'S,f0)P r [RD(hS,f0)] +\\n\u2211\\nS:RD(hS,f0)<\u03f5\\nRD(hS,f0)P r [RD(hS,f0)]\\n\u2264 Pr\\nD\\n[X \u2212{ x0}]P r\\nS\u2208S\\n[RD(hS,f0) \u2265 \u03f5]+ \u03f5 Pr\\nS\u2208S\\n[RD(hS,f0) <\u03f5 ]\\n\u2264 8\u03f5 Pr\\nS\u2208S\\n[RD(hS,f0) \u2265 \u03f5]+ \u03f5\\n(\\n1 \u2212 Pr\\nS\u2208S\\n[RD(hS,f0) \u2265 \u03f5]\\n\u23a1\\n.\\nColle', 'cting terms in PrS\u2208S[RD(hS,f0) \u2265 \u03f5] yields\\nPr\\nS\u2208S\\n[RD(hS,f0) \u2265 \u03f5] \u2265 1\\n7\u03f5(2\u03f5 \u2212 \u03f5)= 1\\n7. (3.37)\\nThus, the probability over all samplesS (not necessarily in S) can be lower bounded\\nas\\nPr\\nS\\n[RD(hS,f0) \u2265 \u03f5', '] \u2265 Pr\\nS\u2208S\\n[RD(hS,f0) \u2265 \u03f5]P r [S] \u2265 1\\n7 Pr[S]. (3.38)\\nThis leads us to \ufb01nd a lower bound for Pr[ S]. The probability that more than\\n(d \u2212 1)/2 points are drawn in a sample of sizem veri\ufb01es the Cherno\ufb00 ', 'bound for any\\n\u03b3> 0:\\n1 \u2212 Pr[S]=P r [Sm \u2265 8\u03f5m(1 +\u03b3)] \u2264 e\u22128\u03f5m \u03b3 2\\n3 . (3.39)\\nTherefore, for \u03f5 =( d \u2212 1)/(32m)a n d\u03b3 =1 ,\\nPr[Sm \u2265 d\u22121\\n2 ] \u2264 e\u2212(d\u22121)/12 \u2264 e\u22121/12 \u2264 1 \u2212 7\u03b4, (3.40)\\nfor \u03b4\u2264 .01. Thus Pr[S] \u2265 7\u03b4', 'and PrS[RD(hS,f0) \u2265 \u03f5] \u2265 \u03b4.\\nThe theorem shows that for any algorithm A, there exists a \u2018bad\u2019 distribution over\\nX and a target function f for which the error of the hypothesis returned by A is\\n\u03a9( d\\nm )', ' with some constant probability. This further demonstrates the key role played\\nby the VC-dimension in learning. The result implies in particular that PAC-learning\\nin the non-realizable case is not pos', 'sible when the VC-dimension is in\ufb01nite.\\nNote that the proof shows a stronger result than the statement of the theorem:\\nthe distribution D is selected independently of the algorithm A. We now present a', '\\ntheorem giving a lower bound in the non-realizable case. The following two lemmas\\nwill be needed for the proof.3.4 Lower bounds 51\\nLemma 3.2\\nLet \u03b1 be a uniformly distributed random variable taking va', 'lues in {\u03b1\u2212 ,\u03b1+},w h e r e\\n\u03b1\u2212 = 1\\n2 \u2212 \u03f5\\n2 and \u03b1+ = 1\\n2 + \u03f5\\n2,a n dl e tS be a sample of m \u2265 1 random variables\\nX1,...,X m taking values in {0, 1} and drawn i.i.d. according to the distributionD\u03b1\\nde\ufb01ne', 'd by PrD\u03b1 [X =1 ]= \u03b1.L e th be a function from Xm to {\u03b1\u2212 ,\u03b1+}, then the\\nfollowing holds:\\nE\\n\u03b1\\n[\\nPr\\nS\u223cDm\\n\u03b1\\n[h(S) \u0338= \u03b1]\\n]\\n\u2265 \u03a6(2\u2308m/2\u2309,\u03f5), (3.41)\\nwhere \u03a6(m, \u03f5)= 1\\n4\\n(\\n1 \u2212\\n\u221a\\n1 \u2212 exp\\n(\\n\u2212 m\u03f52\\n1\u2212\u03f52\\n\u23a1\u23a1\\nfor all ', 'm and \u03f5.\\nProof The lemma can be interpreted in terms of an experiment with two coins\\nwith biases \u03b1\u2212 and \u03b1+. It implies that for a discriminant rule h(S) based on a\\nsample S drawn from D\u03b1\u2212 or D\u03b1+ , to ', 'determine which coin was tossed, the sample\\nsize m must be at least \u03a9(1/\u03f52). The proof is left as an exercise (exercise 3.19).\\nWe will make use of the fact that for any \ufb01xed \u03f5 the function m \u21a6\u2192 \u03a6(m, x', ')i s\\nconvex, which is not hard to establish.\\nLemma 3.3\\nLet Z be a random variable taking values in [0, 1].T h e n ,f o ra n y\u03b3 \u2208 [0,1),\\nPr[z>\u03b3 ] \u2265 E[Z] \u2212 \u03b3\\n1 \u2212 \u03b3 > E[Z] \u2212 \u03b3. (3.42)\\nProof Since the val', 'ues taken by Z are in [0, 1],\\nE[Z]=\\n\u2211\\nz\u2264\u03b3\\nPr[Z = z]z +\\n\u2211\\nz>\u03b3\\nPr[Z = z]z\\n\u2264\\n\u2211\\nz\u2264\u03b3\\nPr[Z = z]\u03b3+\\n\u2211\\nz>\u03b3\\nPr[Z = z]\\n= \u03b3Pr[Z \u2264 \u03b3]+P r [Z>\u03b3 ]\\n= \u03b3(1 \u2212 Pr[Z>\u03b3 ]) + Pr[Z>\u03b3 ]\\n=( 1 \u2212 \u03b3)P r [Z>\u03b3 ]+ \u03b3,\\nwhich concludes', ' the proof.\\nTheorem 3.7 Lower bound, non-realizable case\\nLet H be a hypothesis set with VC-dimension d> 1. Then, for any learning\\nalgorithm A, there exists a distribution D over X\u00d7 { 0, 1} such that:\\n', 'Pr\\nS\u223cDm\\n[\\nRD(hS) \u2212 inf\\nh\u2208H\\nRD(h) >\\n\u221a\\nd\\n320m\\n]\\n\u2265 1/64. (3.43)52 Rademacher Complexity and VC-Dimension\\nEquivalently, for any learning algorithm, the sample complexity veri\ufb01es\\nm \u2265 d\\n320\u03f52 . (3.44)\\nProof', ' Let X = {x1,x1,...,x d}\u2286X be a set fully shattered by H. For any\\n\u03b1 \u2208 [0, 1] and any vector \u03c3 =( \u03c31,...,\u03c3 d)\u22a4 \u2208{ \u2212 1, +1}d, we de\ufb01ne a distribution\\nD\u03c3 with support X \u00d7{ 0, 1} as follows:\\n\u2200i \u2208 [1,d ], ', 'Pr\\nD\u03c3\\n[(xi, 1)] = 1\\nd\\n(1\\n2 + \u03c3i\u03b1\\n2\\n\u23a1\\n. (3.45)\\nT h u s ,t h el a b e lo fe a c hp o i n txi, i \u2208 [1,d ], follows the distribution PrD\u03c3 [\u00b7|xi], that\\nof a biased coin where the bias is determined by the ', 'sign of \u03c3i and the magnitude\\nof \u03b1. To determine the most likely label of each point xi, the learning algorithm\\nwill therefore need to estimate PrD\u03c3 [1|xi]w i t ha na c c u r a c yb e t t e rt h a n\u03b1. ', 'To make\\nthis further di\ufb03cult, \u03b1 and \u03c3 will be selected based on the algorithm, requiring, as\\nin lemma 3.2, \u03a9(1/\u03b12) instances of each point xi in the training sample.\\nClearly, the Bayes classi\ufb01erh\u2217\\nD\u03c3 ', 'is de\ufb01ned by h\u2217\\nD\u03c3 (xi) = argmaxy\u2208{0,1} Pr[y|xi]=\\n1\u03c3i>0 for all i \u2208 [1,d ]. h\u2217\\nD\u03c3 is in H since X is fully shattered. For all h \u2208 H,\\nRD\u03c3 (h) \u2212 RD\u03c3 (h\u2217\\nD\u03c3 )= 1\\nd\\n\u2211\\nx\u2208X\\n(\u03b1\\n2 + \u03b1\\n2\\n\u23a1\\n1h(x)\u0338=h\u2217\\nD\u03c3 (x) = \u03b1', '\\nd\\n\u2211\\nx\u2208X\\n1h(x)\u0338=h\u2217\\nD\u03c3 (x). (3.46)\\nLet hS denote the hypothesis returned by the learning algorithm A after receiving\\na labeled sample S drawn according to D\u03c3 . We will denote by |S|x the number of\\noccu', 'rrences of a pointx in S.L e tU denote the uniform distribution over{\u22121, +1}d.3.4 Lower bounds 53\\nThen, in view of (3.46), the following holds:\\nE\\n\u03c3 \u223cU\\nS\u223cDm\\n\u03c3\\n[ 1\\n\u03b1\\n[\\nRD\u03c3 (hS) \u2212 RD\u03c3 (h\u2217\\nD\u03c3 )\\n]]\\n= 1\\nd\\n\u2211', '\\nx\u2208X\\nE\\n\u03c3 \u223cU\\nS\u223cDm\\n\u03c3\\n[\\n1hS(x)\u0338=h\u2217\\nD\u03c3 (x)\\n]\\n= 1\\nd\\n\u2211\\nx\u2208X\\nE\\n\u03c3 \u223cU\\n[\\nPr\\nS\u223cDm\\n\u03c3\\n[\\nhS(x) \u0338= h\u2217\\nD\u03c3 (x)\\n]]\\n= 1\\nd\\n\u2211\\nx\u2208X\\nm\u2211\\nn=0\\nE\\n\u03c3 \u223cU\\n[\\nPr\\nS\u223cDm\\n\u03c3\\n[\\nhS(x) \u0338= h\u2217\\nD\u03c3 (x)\\n\u23d0\u23d0 |S|x = n\\n]\\nPr[|S|x = n]\\n]\\n\u2265 1\\nd\\n\u2211\\nx\u2208X\\nm\u2211\\nn', '=0\\n\u03a6(n +1 ,\u03b1)P r [|S|x = n] (lemma 3.2)\\n\u2265 1\\nd\\n\u2211\\nx\u2208X\\n\u03a6(m/d +1 ,\u03b1) (convexity of \u03a6( \u00b7,\u03b1) and Jensen\u2019s ineq.)\\n=\u03a6 (m/d +1 ,\u03b1).\\nSince the expectation over \u03c3 is lower-bounded by \u03a6(m/d +1 ,\u03b1), there must exi', 'st\\nsome \u03c3 \u2208{ \u22121, +1}d for which\\nE\\nS\u223cDm\\n\u03c3\\n[ 1\\n\u03b1\\n[\\nRD\u03c3 (hS) \u2212 RD\u03c3 (h\u2217\\nD\u03c3 )\\n]]\\n> \u03a6(m/d +1 ,\u03b1). (3.47)\\nThen, by lemma 3.3, for that \u03c3, for any \u03b3 \u2208 [0, 1],\\nPr\\nS\u223cDm\\n\u03c3\\n[ 1\\n\u03b1\\n[\\nRD\u03c3 (hS) \u2212 RD\u03c3 (h\u2217\\nD\u03c3 )\\n]\\n>\u03b3 u\\n', ']\\n> (1 \u2212 \u03b3)u, (3.48)\\nwhere u =\u03a6 (m/d +1 ,\u03b1). Selecting \u03b4 and \u03f5 such that \u03b4 \u2264 (1 \u2212 \u03b3)u and \u03f5 \u2264 \u03b3\u03b1u\\ngives\\nPr\\nS\u223cDm\u03c3\\n[\\nRD\u03c3 (hS) \u2212 RD\u03c3 (h\u2217\\nD\u03c3 ) >\u03f5\\n]\\n>\u03b4 . (3.49)54 Rademacher Complexity and VC-Dimension\\nTo ', 'satisfy the inequalities de\ufb01ning \u03f5 and \u03b4,l e t\u03b3 =1 \u2212 8\u03b4. Then,\\n\u03b4\u2264 (1 \u2212 \u03b3)u \u21d0\u21d2 u \u2265 1\\n8 (3.50)\\n\u21d0\u21d2 1\\n4\\n(\\n1 \u2212\\n\u221a\\n1 \u2212 exp\\n(\\n\u2212 (m/d +1 )\u03b12\\n1 \u2212 \u03b12\\n\u23a1\u23a1\\n\u2265 1\\n8 (3.51)\\n\u21d0\u21d2 (m/d +1 )\u03b12\\n1 \u2212 \u03b12 \u2264 log 4\\n3 (3.52)\\n\u21d0\u21d2 m\\nd', ' \u2264 ( 1\\n\u03b12 \u2212 1) log4\\n3 \u2212 1. (3.53)\\nSelecting \u03b1 =8 \u03f5/(1 \u2212 8\u03b4)g i v e s\u03f5 = \u03b3\u03b1/8 and the condition\\nm\\nd \u2264\\n((1 \u2212 8\u03b4)2\\n64\u03f52 \u2212 1\\n\u23a1\\nlog 4\\n3 \u2212 1. (3.54)\\nLet f(1/\u03f52) denote the right-hand side. We are seeking a ', 'su\ufb03cient condition of the\\nform m/d \u2264 \u03c9/\u03f52.S i n c e\u03f5 \u2264 1/64, to ensure that \u03c9/\u03f52 \u2264 f(1/\u03f52), it su\ufb03ces to\\nimpose \u03c9/(1/64)2 = f(1/(1/64)2). This condition gives\\n\u03c9 =( 7/64)2 log(4/3) \u2212 (1/64)2(log(4/3) +', ' 1)\u2248 .003127 \u2265 1/320 = .003125.\\nThus, \u03f52 \u2264 1\\n320(m/d) is su\ufb03cient to ensure the inequalities.\\nThe theorem shows that for any algorithmA, in the non-realizable case, there exists\\na \u2018bad\u2019 distribution o', 'verX\u00d7 { 0, 1} such that the error of the hypothesis returned\\nby A is \u03a9\\n(\u221a\\nd\\nm\\n\u23a1\\nwith some constant probability. The VC-dimension appears as a\\ncritical quantity in learning in this general setting as w', 'ell. In particular, with an\\nin\ufb01nite VC-dimension, agnostic PAC-learning is not possible.\\n3.5 Chapter notes\\nThe use of Rademacher complexity for deriving generalization bounds in learning\\nwas \ufb01rst advo', 'cated by Koltchinskii [2001], Koltchinskii and Panchenko [2000], and\\nBartlett, Boucheron, and Lugosi [2002a], see also [Koltchinskii and Panchenko,\\n2002, Bartlett and Mendelson, 2002]. Bartlett, Bousq', 'uet, and Mendelson [2002b]\\nintroduced the notion of local Rademacher complexity , that is the Rademacher\\ncomplexity restricted to a subset of the hypothesis set limited by a bound on\\nthe variance. Thi', 's can be used to derive better guarantees under some regularity\\nassumptions about the noise.\\nTheorem 3.3 is due to Massart [2000]. The notion of VC-dimension was introduced\\nby Vapnik and Chervonenkis ', '[1971] and has been since extensively studied [Vapnik,3.6 Exercises 55\\n2006, Vapnik and Chervonenkis, 1974, Blumer et al., 1989, Assouad, 1983, Dudley,\\n1999]. In addition to the key role it plays in m', 'achine learning, the VC-dimension is\\nalso widely used in a variety of other areas of computer science and mathematics\\n(e.g., see Shelah [1972], Chazelle [2000]). Theorem 3.5 is known as Sauer\u2019s lemma\\n', 'in the learning community, however the result was \ufb01rst given by Vapnik and\\nChervonenkis [1971] (in a somewhat di\ufb00erent version) and later independently by\\nSauer [1972] and Shelah [1972].\\nI nt h er e a', ' l i z a b l ec a s e ,l o w e rb o u n d sf o rt h ee x p e c t e de r r o ri nt e r m so ft h eV C -\\ndimension were given by Vapnik and Chervonenkis [1974] and Haussler et al. [1988].\\nLater, a lower', ' bound for the probability of error such as that of theorem 3.6 was\\ngiven by Blumer et al. [1989]. Theorem 3.6 and its proof, which improves upon\\nthis previous result, are due to Ehrenfeucht, Haussler', ', Kearns, and Valiant [1988].\\nDevroye and Lugosi [1995] gave slightly tighter bounds for the same problem with\\na more complex expression. Theorem 3.7 giving a lower bound in the non-realizable\\ncase an', 'd the proof presented are due to Anthony and Bartlett [1999]. For other\\nexamples of application of the probabilistic method demonstrating its full power,\\nconsult the reference book of Alon and Spencer', ' [1992].\\nThere are several other measures of the complexity of a family of functions used\\nin machine learning, including covering numbers, packing numbers, and some other\\ncomplexity measures discussed', ' in chapter 10. A covering number N\\np(G, \u03f5)i st h e\\nminimal number ofLp balls of radius\u03f5> 0 needed to cover a family of loss functions\\nG.Ap a c k i n gn u m b e rMp(G, \u03f5) is the maximum number of non-', 'overlapping Lp\\nballs of radius \u03f5 centered in G. The two notions are closely related, in particular\\nit can be shown straightfowardly that Mp(G, 2\u03f5) \u2264N p(G, \u03f5) \u2264M p(G, \u03f5)f o r G\\nand \u03f5> 0. Each complexit', 'y measure naturally induces a di\ufb00erent reduction of\\nin\ufb01nite hypothesis sets to \ufb01nite ones, thereby resulting in generalization bounds\\nfor in\ufb01nite hypothesis sets. Exercise 3.22 illustrates the use of ', 'covering numbers\\nfor deriving generalization bounds using a very simple proof. There are also close\\nrelationships between these complexity measures: for example, by Dudley\u2019s theorem,\\nthe empirical Rad', 'emacher complexity can be bounded in terms ofN\\n2(G, \u03f5)[ D u d l e y ,\\n1967, 1987] and the covering and packing numbers can be bounded in terms of the\\nVC-dimension [Haussler, 1995]. See also [Ledoux an', 'd Talagrand, 1991, Alon et al.,\\n1997, Anthony and Bartlett, 1999, Cucker and Smale, 2001, Vidyasagar, 1997] for\\na number of upper bounds on the covering number in terms of other complexity\\nmeasures.\\n3', '.6 Exercises\\n3 . 1 G r o w t hf u n c t i o no fi n t e r v a l si nR.L e tH be the set of intervals in R.T h eV C -\\ndimension of H is 2. Compute its shattering coe\ufb03cient \u03a0 H(m), m \u2265 0. Compare56 Rade', 'macher Complexity and VC-Dimension\\nyour result with the general bound for growth functions.\\n3.2 Lower bound on growth function. Prove that Sauer\u2019s lemma (theorem 3.5) is\\ntight, i.e., for any set X of ', 'm>d elements, show that there exists a hypothesis\\nclass H of VC-dimension d such that \u03a0H(m)= \u2211d\\ni=0\\n(m\\ni\\n\u23a1\\n.\\n3.3 Singleton hypothesis class. Consider the trivial hypothesis set H = {h0}.\\n(a) Show that', ' Rm(H) = 0 for any m> 0.\\n(b) Use a similar construction to show that Massart\u2019s lemma (theorem 3.3) is\\ntight.\\n3.4 Rademacher identities. Fix m \u2265 1. Prove the following identities for any\u03b1 \u2208 R\\nand any t', 'wo hypothesis sets H and H\u2032 of functions mapping from X to R:\\n(a) Rm(\u03b1H)= |\u03b1|Rm(H).\\n(b) Rm(H + H\u2032)= Rm(H)+ Rm(H\u2032).\\n(c) Rm({max(h, h\u2032): h \u2208 H,h \u2032 \u2208 H\u2032}),\\nwhere max(h, h\u2032) denotes the function x \u21a6\u2192 maxx', '\u2208X (h(x),h \u2032(x)) (Hint:y o u\\ncould use the identity max(a, b)= 1\\n2 [a + b + |a \u2212 b|] valid for all a, b \u2208 R and\\nTalagrand\u2019s contraction lemma (see lemma 4.2)).\\n3.5 Rademacher complexity. Professor Jes', 'etoo claims to have found a better bound\\non the Rademacher complexity of any hypothesis set H of functions taking values\\nin {\u22121, +1}, in terms of its VC-dimension VCdim( H). His bound is of the form\\nR', 'm(H) \u2264 O\\n(VCdim(H)\\nm\\n\u23a1\\n. Can you show that Professor Jesetoo\u2019s claim cannot be\\ncorrect? (Hint: consider a hypothesis set H reduced to just two simple functions.)\\n3.6 VC-dimension of union of k interva', 'ls. What is the VC-dimension of subsets of\\nthe real line formed by the union of k intervals?\\n3.7 VC-dimension of \ufb01nite hypothesis sets. Show that the VC-dimension of a \ufb01nite\\nhypothesis set H is at mos', 't log2 |H|.\\n3.8 VC-dimension of subsets. What is the VC-dimension of the set of subsets I\u03b1 of\\nthe real line parameterized by a single parameter \u03b1: I\u03b1 =[ \u03b1, \u03b1+1 ]\u222a [\u03b1 +2 , +\u221e )?\\n3.9 VC-dimension of clo', 'sed balls in Rn. Show that the VC-dimension of the set\\nof all closed balls in Rn, i.e., sets of the form {x \u2208 Rn : \u2225x \u2212 x0\u22252 \u2264 r} for some\\nx0 \u2208 Rn and r \u2265 0, is less than or equal to n +2 .3.6 Exercis', 'es 57\\n3.10 VC-dimension of ellipsoids. What is the VC-dimension of the set of all ellipsoids\\nin Rn?\\n3.11 VC-dimension of a vector space of real functions. LetF be a \ufb01nite-dimensional\\nvector space of r', 'eal functions on Rn,d i m (F)= r< \u221e .L e t H be the set of\\nhypotheses:\\nH = {{x: f(x) \u2265 0}: f \u2208 F }.\\nShow that d, the VC-dimension of H, is \ufb01nite and that d \u2264 r.( Hint: select an\\narbitrary set of m = r', ' + 1 points and consider linear mapping u: F \u2192 Rm de\ufb01ned\\nby: u(f)=( f(x1),...,f (xm)).)\\n3.12 VC-dimension of sine functions. Consider the hypothesis family of sine func-\\ntions (Example 3.5): {x \u2192 sin(', '\u03c9x): \u03c9 \u2208 R} .\\n(a) Show that for any x \u2208 R the points x,2x,3x and 4x cannot be shattered\\nby this family of sine functions.\\n(b) Show that the VC-dimension of the family of sine functions is in\ufb01nite.\\n(Hi', 'nt: show that {2\u2212m : m \u2208 N} can be fully shattered for any m> 0.)\\n3.13 VC-dimension of union of halfspaces. Determine the VC-dimension of the\\nsubsets of the real line formed by the union of k interval', 's.\\n3.14 VC-dimension of intersection of halfspaces. Consider the class Ck of convex\\nintersections ofk halfspaces. Give lower and upper bound estimates for VCdim(Ck).\\n3.15 VC-dimension of intersection ', 'concepts.\\n(a) Let C1 and C2 be two concept classes. Show that for any concept class\\nC = {c1 \u2229 c2 : c1 \u2208 C1,c2 \u2208 C2},\\n\u03a0C(m) \u2264 \u03a0C1 (m)\u03a0 C2 (m). (3.55)\\n(b) Let C be a concept class with VC-dimension d an', 'd let Cs be the concept\\nclass formed by all intersections of s concepts from C, s \u2265 1. Show that the\\nVC-dimension of Cs is bounded by 2ds log2(3s). (Hint: show that log2(3x) <\\n9x/(2e) for any x \u2265 2.)\\n', '3.16 VC-dimension of union of concepts. Let A and B be two sets of functions\\nmapping from X into {0, 1}, and assume that both A and B have \ufb01nite VC-\\ndimension, with VCdim( A)= dA and VCdim(B)= dB.L e ', 't C = A \u222a B be the58 Rademacher Complexity and VC-Dimension\\nunion of A and B.\\n(a) Prove that for all m,\u03a0 C(m) \u2264 \u03a0A(m)+\u03a0 B(m).\\n(b) Use Sauer\u2019s lemma to show that for m \u2265 dA + dB +2 ,\u03a0 C(m) < 2m, and\\ngi', 've a bound on the VC-dimension of C.\\n3.17 VC-dimension of symmetric di\ufb00erence of concepts. For two sets A and B,l e t\\nA\u0394 B denote the symmetric di\ufb00erence ofA and B,i . e . ,A\u0394 B =( A \u222aB) \u2212 (A \u2229B).\\nLet', ' H be a non-empty family of subsets of X with \ufb01nite VC-dimension. Let A be\\nan element of H and de\ufb01ne H\u0394 A = {X\u0394 A: X \u2208 H}. Show that\\nVCdim(H\u0394 A)=V C d i m (H).\\n3.18 Symmetric functions. A function h: ', '{0,1}n \u2192{ 0, 1} is symmetric if its value\\nis uniquely determined by the number of 1\u2019s in the input. Let C denote the set of\\nall symmetric functions.\\n(a) Determine the VC-dimension of C.\\n(b) Give lower', ' and upper bounds on the sample complexity of any consistent\\nPAC learning algorithm for C.\\n(c) Note that any hypothesish \u2208 C can be represented by a vector (y0,y1, ..., yn) \u2208\\n{0, 1}n+1,w h e r eyi is ', 'the value of h on examples having precisely i 1\u2019s. Devise\\na consistent learning algorithm for C based on this representation.\\n3.19 Biased coins. Professor Moent has two coins in his pocket, coin xA an', 'd coin\\nxB. Both coins are slightly biased, i.e., Pr[ xA =0 ]=1 /2 \u2212 \u03f5/2 and Pr[xB =0 ]=\\n1/2+ \u03f5/2, where 0 <\u03f5< 1 is a small positive number, 0 denotes heads and 1\\ndenotes tails. He likes to play the fo', 'llowing game with his students. He picks a coin\\nx \u2208{ xA,xB } from his pocket uniformly at random, tosses it m times, reveals the\\nsequence of 0s and 1s he obtained and asks which coin was tossed. Deter', 'mine how\\nlarge m needs to be for a student\u2019s coin prediction error to be at most \u03b4> 0.\\n(a) Let S be a sample of size m. Professor Moent\u2019s best student, Oskar, plays\\naccording to the decision rule f\\no ', ': {0, 1}m \u2192{ xA,xB } de\ufb01ned by fo(S)= xA\\ni\ufb00 N(S) <m /2, where N(S) is the number of 0\u2019s in sample S.\\nSuppose m is even, then show that\\nerror(fo) \u2265 1\\n2 Pr\\n[\\nN(S) \u2265 m\\n2\\n\u23d0\u23d0\\n\u23d0x = x\\nA\\n]\\n. (3.56)\\n(b) Assumi', 'ng m even, use the inequalities given in the appendix (section D.3)3.6 Exercises 59\\nto show that\\nerror(fo) > 1\\n4\\n[\\n1 \u2212\\n[\\n1 \u2212 e\u2212 m\u03f52\\n1\u2212 \u03f52\\n]1\\n2\\n]\\n. (3.57)\\n(c) Argue that if m is odd, the probability ca', 'n be lower bounded by using\\nm + 1 in the bound in (a) and conclude that for both odd and even m,\\nerror(fo) > 1\\n4\\n[\\n1 \u2212\\n[\\n1 \u2212 e\u2212 2\u2308m/2\u2309\u03f52\\n1\u2212 \u03f52\\n]1\\n2\\n]\\n. (3.58)\\n(d) Using this bound, how large must m be', ' if Oskar\u2019s error is at most\u03b4,w h e r e\\n0 <\u03b4< 1/4. What is the asymptotic behavior of this lower bound as a function\\nof \u03f5?\\n(e) Show that no decision rule f : {0, 1}m \u2192{ xa,xB } can do better than\\nOska', 'r\u2019s rulefo. Conclude that the lower bound of the previous question applies\\nto all rules.\\n3.20 In\ufb01nite VC-dimension.\\n(a) Show that if a concept class C has in\ufb01nite VC-dimension, then it is not\\nPAC-lear', 'nable.\\n(b) In the standard PAC-learning scenario, the learning algorithm receives all\\nexamples \ufb01rst and then computes its hypothesis. Within that setting, PAC-\\nlearning of concept classes with in\ufb01nite', ' VC-dimension is not possible as seen\\nin the previous question.\\nImagine now a di\ufb00erent scenario where the learning algorithm can alternate\\nbetween drawing more examples and computation. The objective ', 'of this prob-\\nlem is to prove that PAC-learning can then be possible for some concept classes\\nwith in\ufb01nite VC-dimension.\\nConsider for example the special case of the concept class C of all subsets of\\n', 'natural numbers. Professor Vitres has an idea for the \ufb01rst stage of a learning\\nalgorithm L PAC-learning C. In the \ufb01rst stage, L draws a su\ufb03cient number of\\npoints m such that the probability of drawing', ' a point beyond the maximum\\nvalue M observed be small with high con\ufb01dence. Can you complete Professor\\nVitres\u2019 idea by describing the second stage of the algorithm so that it PAC-\\nlearns C? The descrip', 'tion should be augmented with the proof that L can\\nPAC-learn C.\\n3.21 VC-dimension generalization bound \u2013 realizable case. In this exercise we show\\nthat the bound given in corollary 3.4 can be improved', ' to O(\\nd log(m/d)\\nm )i nt h e\\nrealizable setting. Assume we are in the realizable scenario, i.e. the target concept is\\nincluded in our hypothesis classH. We will show that if a hypothesish is consiste', 'nt60 Rademacher Complexity and VC-Dimension\\nwith a sample S \u223c Dm then for any \u03f5> 0 such that m\u03f5 \u2265 8\\nPr[R(h) >\u03f5 ] \u2264 2\\n[2em\\nd\\n]d\\n2\u2212m\u03f5/2 . (3.59)\\n(a) Let HS \u2286 H be the subset of hypotheses consistent wit', 'h the sample S,\\nlet \u02c6RS(h) denote the empirical error with respect to the sample S and de\ufb01ne\\nS\u2032 as a another independent sample drawn from Dm. Show that the following\\ninequality holds for any h0 \u2208 HS:', '\\nPr\\n[\\nsup\\nh\u2208HS\\n| \u02c6RS(h) \u2212 \u02c6RS\u2032 (h)| > \u03f5\\n2\\n]\\n\u2265 Pr\\n[\\nB[m, \u03f5] > m\u03f5\\n2\\n]\\nPr[R(h0) >\u03f5 ] ,\\nwhere B[m, \u03f5] is a binomial random variable with parameters [ m, \u03f5]. (Hint:\\nprove and use the fact that Pr[ \u02c6R(h) \u2265 ', '\u03f5\\n2 ] \u2265 Pr[ \u02c6R(h) > \u03f5\\n2 \u2227 R(h) >\u03f5 ].)\\n(b) Prove that Pr\\n[\\nB(m, \u03f5) > m\u03f5\\n2\\n]\\n\u2265 1\\n2 . Use this inequality along with the\\nresult from (a) to show that for any h0 \u2208 HS\\nPr\\n[\\nR(h0) >\u03f5\\n]\\n\u2264 2P r\\n[\\nsup\\nh\u2208HS\\n| \u02c6', 'RS(h) \u2212 \u02c6RS\u2032 (h)| > \u03f5\\n2\\n]\\n.\\n(c) Instead of drawing two samples, we can draw one sampleT of size 2m then\\nuniformly at random split it intoS and S\u2032. The right hand side of part (b) can\\nthen be rewritten', ' as:\\nPr\\n[\\nsup\\nh\u2208HS\\n| \u02c6RS(h)\u2212 \u02c6RS\u2032 (h)| > \u03f5\\n2\\n]\\n=P r\\nT \u223cD2m:\\nT \u2192 [S,S\u2032]\\n[\\n\u2203h \u2208H : \u02c6RS(h)=0 \u2227 \u02c6RS\u2032 (h) > \u03f5\\n2\\n]\\n.\\nLet h0 be a hypothesis such that \u02c6RT (h0) > \u03f5\\n2 and let l> m\u03f5\\n2 be the total\\nnumber of er', 'rors h0 makes on T. Show that the probability of all l errors\\nfalling into S\u2032 is upper bounded by 2\u2212l.\\n(d) Part (b) implies that for any h \u2208 H\\nPr\\nT \u223cD2m:\\nT \u2192 (S,S\u2032)\\n[\\n\u02c6RS(h)=0 \u2227 \u02c6RS\u2032 (h) > \u03f5\\n2\\n\u23d0\u23d0\u23d0 \u02c6RT', ' (h0) > \u03f5\\n2\\n]\\n\u2264 2\u2212l .\\nUse this bound to show that for any h \u2208 H\\nPr\\nT \u223cD2m:\\nT \u2192 (S,S\u2032)\\n[\\n\u02c6RS(h)=0 \u2227 \u02c6RS\u2032 (h) > \u03f5\\n2\\n]\\n\u2264 2\u2212 \u03f5m\\n2 .\\n(e) Complete the proof of inequality (3.59) by using the union bound to ', 'upper\\nbound PrT \u223cD2m:\\nT \u2192 (S,S\u2032)\\n[\\n\u2203h \u2208H : \u02c6RS(h)=0 \u2227 \u02c6RS\u2032 (h) > \u03f5\\n2\\n]\\n. Show that we can achieve\\na high probability generalization bound that is of the order O(d log(m/d)\\nm ).3.6 Exercises 61\\n3.22 Ge', 'neralization bound based on covering numbers. Let H be a family of\\nfunctions mapping X to a subset of real numbers Y\u2286 R. For any \u03f5> 0, the\\ncovering number N(H,\u03f5 )o f H for the L\u221e norm is the minimal k', ' \u2208 N such that H\\ncan be covered with k balls of radius \u03f5, that is, there exists {h1,...,h k}\u2286 H such\\nthat, for all h \u2208 H, there exists i \u2264 k with \u2225h \u2212 hi\u2225\u221e =m a xx\u2208X |h(x) \u2212 hi(x)|\u2264 \u03f5.\\nIn particular, ', 'when H is a compact set, a \ufb01nite covering can be extracted from a\\ncovering of H with balls of radius \u03f5 and thus N(H,\u03f5 )i s\ufb01 n i t e .\\nCovering numbers provide a measure of the complexity of a class of', ' functions: the\\nlarger the covering number, the richer is the family of functions. The objective of\\nthis problem is to illustrate this by proving a learning bound in the case of the\\nsquared loss. Let ', 'D denote a distribution over X\u00d7 Y according to which labeled\\nexamples are drawn. Then, the generalization error ofh \u2208 H for the squared loss is\\nde\ufb01ned by R(h)=E (x,y)\u223cD[(h(x) \u2212 y)2] and its empirical ', 'error for a labeled sample\\nS =( (x1,y1),..., (xm,y m)) by \u02c6R(h)= 1\\nm\\n\u2211m\\ni=1(h(xi)\u2212yi)2.W ew i l la s s u m et h a tH\\nis bounded, that is there existsM> 0 such that|h(x)\u2212y|\u2264 M for all (x, y) \u2208X \u00d7 Y.\\nTh', 'e following is the generalization bound proven in this problem:\\nPr\\nS\u223cDm\\n[\\nsup\\nh\u2208H\\n|R(h) \u2212 \u02c6R(h)|\u2265 \u03f5\\n]\\n\u2264N\\n(\\nH, \u03f5\\n8M\\n\u23a1\\n2e x p\\n(\u2212m\u03f52\\n2M4\\n\u23a1\\n. (3.60)\\nThe proof is based on the following steps.\\n(a) Let LS =', ' R(h) \u2212 \u02c6R(h), then show that for all h1,h2 \u2208 H and any labeled\\nsample S, the following inequality holds:\\n|LS(h1) \u2212 LS(h2)|\u2264 4M \u2225h1 \u2212 h2\u2225\u221e .\\n(b) Assume that H can be covered by k subsets B1,...,B k,t ', 'h a ti sH =\\nB1 \u222a... \u222aBk. Then, show that, for any\u03f5> 0, the following upper bound holds:\\nPr\\nS\u223cDm\\n[\\nsup\\nh\u2208H\\n|LS(h)|\u2265 \u03f5\\n]\\n\u2264\\nk\u2211\\ni=1\\nPr\\nS\u223cDm\\n[\\nsup\\nh\u2208Bi\\n|LS(h)|\u2265 \u03f5\\n]\\n.\\n(c) Finally, let k = N(H, \u03f5\\n8M )a n dl', ' e tB1,...,B k be balls of radius \u03f5/(8M)\\ncentered at h1,...,h k covering H. Use part (a) to show that for all i \u2208 [1,k ],\\nPr\\nS\u223cDm\\n[\\nsup\\nh\u2208Bi\\n|LS(h)|\u2265 \u03f5\\n]\\n\u2264 Pr\\nS\u223cDm\\n[\\n|LS(hi)|\u2265 \u03f5\\n2\\n]\\n,\\nand apply Hoe\ufb00di', 'ng\u2019s inequality (theorem D.1) to prove (3.60).4 Support Vector Machines\\nThis chapter presents one of the most theoretically well motivated and practically\\nmost e\ufb00ective classi\ufb01cation algorithms in mod', 'ern machine learning: Support Vector\\nMachines (SVMs). We \ufb01rst introduce the algorithm for separable datasets, then\\npresent its general version designed for non-separable datasets, and \ufb01nally provide\\na', ' theoretical foundation for SVMs based on the notion of margin. We start with\\nthe description of the problem of linear classi\ufb01cation.\\n4.1 Linear classi\ufb01cation\\nConsider an input space X that is a subse', 't of RN with N \u2265 1, and the output\\nor target space Y = {\u22121,+1},a n dl e tf : X\u2192 Y be the target function. Given\\na hypothesis set H of functions mapping X to Y, the binary classi\ufb01cation task is\\nformula', 'ted as follows. The learner receives a training sampleS of size m drawn i.i.d.\\nfrom X according to some unknown distribution D, S =( (x1,y1),..., (xm,y m)) \u2208\\n(X\u00d7 Y )m,w i t hyi = f(xi) for all i \u2208 [1,', 'm]. The problem consists of determining a\\nhypothesis h \u2208 H,a binary classi\ufb01er , with small generalization error:\\nRD(h)= P r\\nx\u223cD\\n[h(x) \u0338= f(x)]. (4.1)\\nDi\ufb00erent hypothesis sets H can be selected for thi', 's task. In view of the results\\npresented in the previous section, which formalized Occam\u2019s razor principle, hy-\\npothesis sets with smaller complexity \u2014 e.g., smaller VC-dimension or Rademacher\\ncomplex', 'ity \u2014 provide better learning guarantees, everything else being equal. A\\nnatural hypothesis set with relatively small complexity is that of linear classi\ufb01ers ,\\nor hyperplanes, which can be de\ufb01ned as f', 'ollows:\\nH = {x \u21a6\u2192 sign(w \u00b7 x + b): w \u2208 RN ,b \u2208 R}. (4.2)\\nA hypothesis of the form x \u21a6\u2192 sign(w \u00b7x+b) thus labels positively all points falling\\non one side of the hyperplane w \u00b7 x + b = 0 and negatively', ' all others. The problem\\nis referred to as a linear classi\ufb01cation problem.64 Support Vector Machines\\nw\u00b7x+b=0\\nw\u00b7x+b=0\\nFigure 4.1 Two possible separating hyperplanes. The right-hand side \ufb01gure shows\\na h', 'yperplane that maximizes the margin.\\n4.2 SVMs \u2014 separable case\\nIn this section, we assume that the training sample S can be linearly separated,\\nthat is, we assume the existence of a hyperplane that pe', 'rfectly separates the\\ntraining sample into two populations of positively and negatively labeled points,\\nas illustrated by the left panel of \ufb01gure 4.1. But there are then in\ufb01nitely many\\nsuch separating', ' hyperplanes. Which hyperplane should a learning algorithm select?\\nThe solution returned by the SVM algorithm is the hyperplane with the maximum\\nmargin, or distance to the closest points, and is thus ', 'known as themaximum-margin\\nhyperplane. The right panel of \ufb01gure 4.1 illustrates that choice.\\nWe will present later in this chapter a margin theory that provides a strong\\njusti\ufb01cation for this solution', '. We can observe already, however, that the SVM\\nsolution can also be viewed as the \u201csafest\u201d choice in the following sense: a test\\npoint is classi\ufb01ed correctly by a separating hyperplane with margin \u03c1 ', 'even when\\nit falls within a distance \u03c1 of the training samples sharing the same label; for the\\nSVM solution, \u03c1 is the maximum margin and thus the \u201csafest\u201d value.\\n4.2.1 Primal optimization problem\\nWe n', 'ow derive the equations and optimization problem that de\ufb01ne the SVM\\nsolution. The general equation of a hyperplane in R\\nN is\\nw \u00b7 x + b =0 , (4.3)\\nwhere w \u2208 RN is a non-zero vector normal to the hyperp', 'lane and b \u2208 R a\\nscalar. Note that this de\ufb01nition of a hyperplane is invariant to non-zero scalar\\nmultiplication. Hence, for a hyperplane that does not pass through any sample\\npoint, we can scale w an', 'd b appropriately such that min (x,y)\u2208S |w \u00b7 x + b| =1 .4.2 SVMs \u2014 separable case 65\\nmargin\\nw\u00b7x+b=+1\\nw\u00b7x+b= \u22121\\nw\u00b7x+b=0\\nFigure 4.2 Margin and equations of the hyperplanes for a canonical maximum-\\nmargi', 'n hyperplane. The marginal hyperplanes are represented by dashed lines on\\nthe \ufb01gure.\\nWe de\ufb01ne this representation of the hyperplane, i.e., the corresponding pair (w, b),\\nas the canonical hyperplane. T', 'he distance of any point x0 \u2208 RN to a hyperplane\\nde\ufb01ned by (4.3) is given by\\n|w \u00b7 x0 + b|\\n\u2225w\u2225 . (4.4)\\nThus, for a canonical hyperplane, the margin \u03c1 is given by\\n\u03c1=m i n\\n(x,y)\u2208S\\n|w \u00b7 x + b|\\n\u2225w\u2225 = 1\\n\u2225w\u2225', ' . (4.5)\\nFigure 4.2 illustrates the margin for a maximum-margin hyperplane with a canon-\\nical representation (w, b). It also shows the marginal hyperplanes, which are the\\nhyperplanes parallel to the s', 'eparating hyperplane and passing through the closest\\npoints on the negative or positive sides. Since they are parallel to the separating\\nhyperplane, they admit the same normal vector w.F u r t h e r m', ' o r e ,b yd e \ufb01 n i t i o no fa\\ncanonical representation, for a point x on a marginal hyperplane, |w \u00b7 x + b| =1 ,\\nand thus the equations of the marginal hyperplanes are w \u00b7 x + b = \u00b11.\\nA hyperplane ', 'de\ufb01ned by ( w, b) correctly classi\ufb01es a training point x\\ni, i \u2208 [1,m]\\nwhen w \u00b7 xi + b has the same sign as yi. For a canonical hyperplane, by de\ufb01nition,\\nwe have |w \u00b7 xi + b|\u2265 1 for all i \u2208 [1,m]; thus', ', xi is correctly classi\ufb01ed when\\nyi(w \u00b7xi +b) \u2265 1. In view of (4.5), maximizing the margin of a canonical hyperplane\\nis equivalent to minimizing \u2225w\u2225 or 1\\n2 \u2225w\u22252. Thus, in the separable case, the SVM\\ns', 'olution, which is a hyperplane maximizing the margin while correctly classifying all\\ntraining points, can be expressed as the solution to the following convex optimization\\nproblem:66 Support Vector Ma', 'chines\\nmin\\nw,b\\n1\\n2 \u2225w\u22252 (4.6)\\nsubject to: yi(w \u00b7 xi + b) \u2265 1, \u2200i \u2208 [1,m] .\\nThe objective function F : w \u21a6\u2192 1\\n2 \u2225w\u22252 is in\ufb01nitely di\ufb00erentiable. Its gradient is\\n\u2207w(F)= w and its Hessian the identity ma', 'trix\u22072F(w)= I, whose eigenvalues are\\nstrictly positive. Therefore, \u22072F(w) \u227b 0 and F is strictly convex. The constraints\\nare all de\ufb01ned by a\ufb03ne functions gi :( w,b ) \u21a6\u2192 1\u2212yi(w\u00b7xi+b) and are thus quali\ufb01', 'ed.\\nThus, in view of the results known for convex optimization (see appendix B for\\ndetails), the optimization problem of (4.6) admits a unique solution, an important\\nand favorable property that does n', 'ot hold for all learning algorithms.\\nMoreover, since the objective function is quadratic and the constraints a\ufb03ne, the\\noptimization problem of (4.6) is in fact a speci\ufb01c instance of quadratic program-', '\\nming (QP), a family of problems extensively studied in optimization. A variety of\\ncommercial and open-source solvers are available for solving convex QP problems.\\nAdditionally, motivated by the empir', 'ical success of SVMs along with its rich theo-\\nretical underpinnings, specialized methods have been developed to more e\ufb03ciently\\nsolve this particular convex QP problem, notably the block coordinate de', 'scent al-\\ngorithms with blocks of just two coordinates.\\n4.2.2 Support vectors\\nThe constraints are a\ufb03ne and thus quali\ufb01ed. The objective function as well as the\\na\ufb03ne constraints are convex and di\ufb00erent', 'iable. Thus, the hypotheses of theorem B.8\\nhold and the KKT conditions apply at the optimum. We shall use these conditions\\nto both analyze the algorithm and demonstrate several of its crucial properti', 'es,\\nand subsequently derive the dual optimization problem associated to SVMs in\\nsection 4.2.3.\\nWe introduce Lagrange variables \u03b1\\ni \u2265 0, i \u2208 [1,m ], associated to the m\\nconstraints and denote by\u03b1 the v', 'ector (\u03b11,...,\u03b1 m)\u22a4. The Lagrangian can then be\\nde\ufb01ned for all w \u2208 RN , b \u2208 R,a n d\u03b1 \u2208 Rm\\n+ ,b y\\nL(w,b ,\u03b1)= 1\\n2 \u2225w\u22252 \u2212\\nm\u2211\\ni=1\\n\u03b1i[yi(w \u00b7 xi + b) \u2212 1]. (4.7)\\nThe KKT conditions are obtained by setting t', 'he gradient of the Lagrangian with\\nrespect to the primal variablesw and b to zero and by writing the complementarity4.2 SVMs \u2014 separable case 67\\nconditions:\\n\u2207wL = w \u2212\\nm\u2211\\ni=1\\n\u03b1iyixi =0 = \u21d2 w =\\nm\u2211\\ni=1\\n\u03b1', 'iyixi (4.8)\\n\u2207bL = \u2212\\nm\u2211\\ni=1\\n\u03b1iyi =0 = \u21d2\\nm\u2211\\ni=1\\n\u03b1iyi =0 ( 4 . 9 )\\n\u2200i, \u03b1i[yi(w \u00b7 xi + b) \u2212 1] = 0 = \u21d2 \u03b1i =0 \u2228 yi(w \u00b7 xi + b)=1 . (4.10)\\nBy equation 4.8, the weight vectorw solution of the SVM problem is ', 'a linear combi-\\nnation of the training set vectorsx1,..., xm.Av e c t o rxi appears in that expansion\\ni\ufb00 \u03b1i \u0338= 0. Such vectors are called support vectors.B yt h ec o m p l e m e n t a r i t yc o n d i', ' -\\ntions (4.10), if \u03b1i \u0338=0 ,t h e nyi(w \u00b7 xi + b) = 1. Thus, support vectors lie on the\\nmarginal hyperplanes w \u00b7 xi + b = \u00b11.\\nSupport vectors fully de\ufb01ne the maximum-margin hyperplane or SVM solution,', '\\nwhich justi\ufb01es the name of the algorithm. By de\ufb01nition, vectors not lying on the\\nmarginal hyperplanes do not a\ufb00ect the de\ufb01nition of these hyperplanes \u2014 in their\\nabsence, the solution to the SVM probl', 'em remains unchanged. Note that while the\\nsolution w of the SVM problem is unique, the support vectors are not. In dimension\\nN, N + 1 points are su\ufb03cient to de\ufb01ne a hyperplane. Thus, when more than N ', '+1\\npoints lie on a marginal hyperplane, di\ufb00erent choices are possible for the N +1\\nsupport vectors.\\n4.2.3 Dual optimization problem\\nTo derive the dual form of the constrained optimization problem (4.6', '), we plug\\ninto the Lagrangian the de\ufb01nition of w i nt e r m so ft h ed u a lv a r i a b l e sa se x p r e s s e d\\nin (4.8) and apply the constraint (4.9). This yields\\nL = 1\\n2 \u2225\\nm\u2211\\ni=1\\n\u03b1iyixi\u22252 \u2212\\nm\u2211\\ni', ',j=1\\n\u03b1i\u03b1jyiyj(xi \u00b7 xj)\\n\\ued19 \\ued18\\ued17 \\ued1a\\n\u2212 1\\n2\\nPm\\ni,j=1 \u03b1i\u03b1jyiyj(xi\u00b7xj)\\n\u2212\\nm\u2211\\ni=1\\n\u03b1iyib\\n\\ued19 \\ued18\\ued17 \\ued1a\\n0\\n+\\nm\u2211\\ni=1\\n\u03b1i , (4.11)\\nwhich simpli\ufb01es to\\nL =\\nm\u2211\\ni=1\\n\u03b1i \u2212 1\\n2\\nm\u2211\\ni,j=1\\n\u03b1i\u03b1jyiyj(xi \u00b7 xj) . (4.12)68 Support Vector Ma', 'chines\\nThis leads to the following dual optimization problem for SVMs in the separable\\ncase:\\nmax\\n\u03b1\\nm\u2211\\ni=1\\n\u03b1i \u2212 1\\n2\\nm\u2211\\ni,j=1\\n\u03b1i\u03b1jyiyj(xi \u00b7 xj) (4.13)\\nsubject to: \u03b1i \u2265 0 \u2227\\nm\u2211\\ni=1\\n\u03b1iyi =0 , \u2200i \u2208 [1,m] .\\n', 'The objective function G: \u03b1 \u21a6\u2192 \u2211m\\ni=1 \u03b1i \u2212 1\\n2\\n\u2211m\\ni,j=1 \u03b1i\u03b1jyiyj(xi \u00b7 xj) is in\ufb01nitely\\ndi\ufb00erentiable. Its Hessian is given by \u22072G = \u2212A,w i t hA =\\n(\\nyixi \u00b7 yjxj\\n\u23a1\\nij. A is\\nthe Gram matrix associated to', ' the vectorsy1x1,...,y mxm and is therefore positive\\nsemide\ufb01nite, which shows that \u22072G \u2aaf 0 and that G is a concave function. Since\\nthe constraints are a\ufb03ne and convex, the maximization problem (4.13) ', 'is equivalent\\nto a convex optimization problem. Since G is a quadratic function of \u03b1,t h i sd u a l\\noptimization problem is also a QP problem, as in the case of the primal optimization\\nand once again ', 'both general-purpose and specialized QP solvers can be used to\\nobtain the solution (see exercise 4.4 for details on the SMO algorithm, which is\\noften used to solve the dual form of the SVM problem in ', 'the more general non-\\nseparable setting).\\nMoreover, since the constraints are a\ufb03ne, they are quali\ufb01ed and strong duality\\nholds (see appendix B). Thus, the primal and dual problems are equivalent, i.e.', ',\\nthe solution \u03b1 of the dual problem (4.13) can be used directly to determine the\\nhypothesis returned by SVMs, using equation (4.8):\\nh(x)=s g n (w \u00b7 x + b)=s g n\\n(\\nm\u2211\\ni=1\\n\u03b1iyi(xi \u00b7 x)+ b\\n\u23a1\\n. (4.14)\\nSi', 'nce support vectors lie on the marginal hyperplanes, for any support vector xi,\\nw \u00b7 xi + b = yi, and thus b c a nb eo b t a i n e dv i a\\nb = yi \u2212\\nm\u2211\\nj=1\\n\u03b1jyj(xj \u00b7 xi) . (4.15)\\nThe dual optimization pr', 'oblem (4.13) and the expressions (4.14) and (4.15) reveal\\nan important property of SVMs: the hypothesis solution depends only on inner\\nproducts between vectors and not directly on the vectors themselv', 'es.\\nEquation (4.15) can now be used to derive a simple expression of the margin\u03c1in\\nterms of \u03b1. Since (4.15) holds for all i with \u03b1i \u0338= 0, multiplying both sides by \u03b1iyi\\nand taking the sum leads to\\nm\u2211\\n', 'i=1\\n\u03b1iyib =\\nm\u2211\\ni=1\\n\u03b1iy2\\ni \u2212\\nm\u2211\\ni,j=1\\n\u03b1i\u03b1jyiyj(xi \u00b7 xj) . (4.16)4.2 SVMs \u2014 separable case 69\\nUsing the fact that y2\\ni = 1 along with equation 4.8 then yields\\n0=\\nm\u2211\\ni=1\\n\u03b1i \u2212\u2225 w\u22252. (4.17)\\nNoting that \u03b1i ', '\u2265 0, we obtain the following expression of the margin \u03c1 in terms of\\nthe L1 norm of \u03b1:\\n\u03c12 = 1\\n\u2225w\u22252\\n2\\n= 1\u2211m\\ni=1 \u03b1i\\n= 1\\n\u2225\u03b1\u22251\\n. (4.18)\\n4.2.4 Leave-one-out analysis\\nWe now use the notion of leave-one-out e', 'rror to derive a \ufb01rst learning guarantee\\nfor SVMs based on the fraction of support vectors in the training set.\\nDe\ufb01nition 4.1 Leave-one-out error\\nLet hS denote the hypothesis returned by a learning al', 'gorithm A, when trained on\\na\ufb01 x e ds a m p l eS. Then, the leave-one-out error of A on a sample S of size m is\\nde\ufb01ned by\\n\u02c6RLOO(A)= 1\\nm\\nm\u2211\\ni=1\\n1hS\u2212{ xi}(xi)\u0338=yi .\\nThus, for each i \u2208 [1,m], A is trained', ' on all the points in S except for xi,i . e . ,\\nS \u2212{ xi},a n di t se r r o ri st h e nc o m p u t e du s i n gxi. The leave-one-out error is the\\naverage of these errors. We will use an important prope', 'rty of the leave-one-out error\\nstated in the following lemma.\\nLemma 4.1\\nThe average leave-one-out error for samples of size m \u2265 2 is an unbiased estimate\\nof the average generalization error for sample', 's of size m \u2212 1:\\nE\\nS\u223cDm\\n[ \u02c6RLOO(A)] = E\\nS\u2032 \u223cDm\u2212 1\\n[R(hS\u2032 )], (4.19)\\nwhere D denotes the distribution according to which points are drawn.70 Support Vector Machines\\nProof By the linearity of expectatio', 'n, we can write\\nE\\nS\u223cDm\\n[ \u02c6RLOO(A)] = 1\\nm\\nm\u2211\\ni=1\\nE\\nS\u223cDm\\n[1hS\u2212{ xi}(xi)\u0338=yi ]\\n=E\\nS\u223cDm\\n[1hS\u2212{ x1}(x1)\u0338=y1 ]\\n=E\\nS\u2032 \u223cDm\u2212 1,x1\u223cD\\n[1hS\u2032 (x1)\u0338=y1 ]\\n=E\\nS\u2032 \u223cDm\u2212 1\\n[E\\nx1\u223cD\\n[1hS\u2032 (x1)\u0338=y1 ]]\\n=E\\nS\u2032 \u223cDm\u2212 1\\n[R(hS\u2032 )', '].\\nFor the second equality, we used the fact that, since the points ofS are drawn in an\\ni.i.d. fashion, the expectation ES\u223cDm [1hS\u2212{ xi}(xi)\u0338=yi ] does not depend on the choice\\nof i \u2208 [1,m] and is thu', 's equal to ES\u223cDm [1hS\u2212{ x1}(x1)\u0338=y1 ].\\nIn general, computing the leave-one-out error may be costly since it requires training\\nm times on samples of sizem \u2212 1. In some situations however, it is possibl', 'e to derive\\nthe expression of \u02c6Rloo(A) much more e\ufb03ciently (see exercise 10.9).\\nTheorem 4.1\\nLet hS be the hypothesis returned by SVMs for a sample S,a n dl e tNSV (S) be the\\nnumber of support vectors ', 'that de\ufb01ne hS.T h e n ,\\nE\\nS\u223cDm\\n[R(hS)] \u2264 E\\nS\u223cDm+1\\n[NSV(S)\\nm +1\\n]\\n.\\nProof Let S be a linearly separable sample of m +1 .I f x is not a support vector\\nfor hS, removing it does not change the SVM solutio', 'n. Thus, hS\u2212{x} = hS and\\nhS\u2212{x} correctly classi\ufb01es x. By contraposition, if hS\u2212{x} misclassi\ufb01es x, x must be\\na support vector, which implies\\n\u02c6Rloo(SVM) \u2264 NSV(S)\\nm +1 . (4.20)\\nTaking the expectation o', 'f both sides and using lemma 4.1 yields the result.\\nTheorem 4.1 gives a sparsity argument in favor of SVMs: the average error of\\nthe algorithm is upper bounded by the average fraction of support vecto', 'rs. One\\nmay hope that for many distributions seen in practice, a relatively small number\\nof the training points will lie on the marginal hyperplanes. The solution will then\\nbe sparse in the sense that', ' a small fraction of the dual variables \u03b1\\ni will be non-\\nzero. Note, however, that this bound is relatively weak since it applies only to the\\naverage generalization error of the algorithm over all sam', 'ples of sizem. It provides\\nno information about the variance of the generalization error. In section 4.4, we\\npresent stronger high-probability bounds using a di\ufb00erent argument based on the4.3 SVMs \u2014 n', 'on-separable case 71\\n\u03bei\\n\u03bej\\nw\u00b7x+b=+1\\nw\u00b7x+b= \u22121\\nw\u00b7x+b=0\\nFigure 4.3 A separating hyperplane with point xi classi\ufb01ed incorrectly and point\\nxj correctly classi\ufb01ed, but with margin less than 1.\\nnotion of ma', 'rgin.\\n4.3 SVMs \u2014 non-separable case\\nIn most practical settings, the training data is not linearly separable, i.e., for any\\nhyperplane w \u00b7 x + b = 0, there exists xi \u2208 S such that\\nyi [w \u00b7 xi + b] \u0338\u2265 1 ', '. (4.21)\\nThus, the constraints imposed in the linearly separable case discussed in section 4.2\\ncannot all hold simultaneously. However, a relaxed version of these constraints can\\nindeed hold, that is,', ' for each i \u2208 [1,m], there exist \u03be\\ni \u2265 0 such that\\nyi [w \u00b7 xi + b] \u2265 1 \u2212 \u03bei . (4.22)\\nThe variables\u03bei are known asslack variables and are commonly used in optimization\\nto de\ufb01ne relaxed versions of some', ' constraints. Here, a slack variable \u03bei measures\\nthe distance by which vector xi violates the desired inequality, yi(w \u00b7 xi + b) \u2265 1.\\nFigure 4.3 illustrates the situation. For a hyperplane w \u00b7 x + b =', '0 ,av e c t o rxi\\nwith \u03bei > 0 can be viewed as an outlier. Each xi must be positioned on the correct\\nside of the appropriate marginal hyperplane to not be considered an outlier. As a\\nconsequence, a ve', 'ctor x\\ni with 0 <y i(w \u00b7 xi + b) < 1 is correctly classi\ufb01ed by the\\nhyperplane w\u00b7x+b = 0 but is nonetheless considered to be an outlier, that is,\u03bei > 0.\\nIf we omit the outliers, the training data is co', 'rrectly separated by w \u00b7 x + b =0\\nwith a margin \u03c1 =1 /\u2225w\u2225 that we refer to as the soft margin, as opposed to the\\nhard margin in the separable case.\\nHow should we select the hyperplane in the non-separ', 'able case? One idea consists\\nof selecting the hyperplane that minimizes the empirical error. But, that solution72 Support Vector Machines\\n0/1 loss function\\nHinge loss\\nQuadratic hinge loss\\n\u03be1\\n\u03be2\\n01\\nlos', 's\\n1\\n0\\nx\\nFigure 4.4 Both the hinge loss and the quadratic hinge loss provide convex upper\\nbounds on the binary zero-one loss.\\nwill not bene\ufb01t from the large-margin guarantees we will present in section', ' 4.4.\\nFurthermore, the problem of determining a hyperplane with the smallest zero-one\\nloss, that is the smallest number of misclassi\ufb01cations, is NP-hard as a function of\\nthe dimension N of the space.\\n', 'Here, there are two con\ufb02icting objectives: on one hand, we wish to limit the\\ntotal amount of slack due to outliers, which can be measured by \u2211\\nm\\ni=1 \u03bei, or, more\\ngenerally by \u2211m\\ni=1 \u03bep\\ni for some p \u2265 ', '1; on the other hand, we seek a hyperplane with\\na large margin, though a larger margin can lead to more outliers and thus larger\\namounts of slack.\\n4.3.1 Primal optimization problem\\nThis leads to the f', 'ollowing general optimization problem de\ufb01ning SVMs in the\\nnon-separable case where the parameter C \u2265 0 determines the trade-o\ufb00 between\\nmargin-maximization (or minimization of \u2225w\u2225\\n2) and the minimizati', 'on of the slack\\npenalty \u2211m\\ni=1 \u03bep\\ni :\\nmin\\nw,b,\u03be\\n1\\n2 \u2225w\u22252 + C\\nm\u2211\\ni=1\\n\u03bep\\ni (4.23)\\nsubject to yi(w \u00b7 xi + b) \u2265 1 \u2212 \u03bei \u2227 \u03bei \u2265 0,i \u2208 [1,m ],\\nwhere \u03be=( \u03be1,...,\u03be m)\u22a4. The parameter C is typically determined ', 'via n-fold cross-\\nvalidation (see section 1.3).\\nAs in the separable case, (4.23) is a convex optimization problem since the\\nconstraints are a\ufb03ne and thus convex and since the objective function is con', 'vex\\nfor any p \u2265 1. In particular, \u03be \u21a6\u2192 \u2211\\nm\\ni=1 \u03bep\\ni = \u2225\u03be\u2225p\\np is convex in view of the convexity\\nof the norm \u2225\u00b7\u2225 p.4.3 SVMs \u2014 non-separable case 73\\nThere are many possible choices for p leading to more', ' or less aggressive penal-\\nizations of the slack terms (see exercise 4.1). The choices p = 1 and p =2l e a dt o\\nthe most straightforward solutions and analyses. The loss functions associated with\\np = ', '1 and p = 2 are called the hinge loss and the quadratic hinge loss, respectively.\\nFigure 4.4 shows the plots of these loss functions as well as that of the standard\\nzero-one loss function. Both hinge ', 'losses are convex upper bounds on the zero-one\\nloss, thus making them well suited for optimization. In what follows, the analysis is\\npresented in the case of the hinge loss ( p = 1), which is the most', ' widely used loss\\nfunction for SVMs.\\n4.3.2 Support vectors\\nAs in the separable case, the constraints are a\ufb03ne and thus quali\ufb01ed. The objective\\nfunction as well as the a\ufb03ne constraints are convex and d', 'i\ufb00erentiable. Thus, the\\nhypotheses of theorem B.8 hold and the KKT conditions apply at the optimum.\\nWe use these conditions to both analyze the algorithm and demonstrate several\\nof its crucial propert', 'ies, and subsequently derive the dual optimization problem\\nassociated to SVMs in section 4.3.3.\\nWe introduce Lagrange variables \u03b1\\ni \u2265 0, i \u2208 [1,m], associated to the \ufb01rst m\\nconstraints and \u03b2i \u2265 0, i \u2208', ' [1,m] associated to the non-negativity constraints of\\nthe slack variables. We denote by \u03b1 the vector (\u03b11,...,\u03b1 m)\u22a4 and by \u03b2 the vector\\n(\u03b21,...,\u03b2 m)\u22a4. The Lagrangian can then be de\ufb01ned for all w \u2208 RN ', ', b \u2208 R,a n d\\n\u03b1 \u2208 Rm\\n+ ,b y\\nL(w,b ,\u03be, \u03b1, \u03b2)= 1\\n2 \u2225w\u22252 +C\\nm\u2211\\ni=1\\n\u03bei \u2212\\nm\u2211\\ni=1\\n\u03b1i[yi(w \u00b7xi +b) \u2212 1+ \u03bei] \u2212\\nm\u2211\\ni=1\\n\u03b2i\u03bei . (4.24)\\nThe KKT conditions are obtained by setting the gradient of the Lagrangian\\nwi', 'th respect to the primal variables w, b,a n d \u03beist oz e r oa n db yw r i t i n gt h e\\ncomplementarity conditions:\\n\u2207wL = w \u2212\\nm\u2211\\ni=1\\n\u03b1iyixi =0 = \u21d2 w =\\nm\u2211\\ni=1\\n\u03b1iyixi (4.25)\\n\u2207bL = \u2212\\nm\u2211\\ni=1\\n\u03b1iyi =0 = \u21d2\\nm\u2211\\n', 'i=1\\n\u03b1iyi = 0 (4.26)\\n\u2207\u03bei L = C \u2212 \u03b1i \u2212 \u03b2i =0 = \u21d2 \u03b1i + \u03b2i = C (4.27)\\n\u2200i, \u03b1i[yi(w \u00b7 xi + b) \u2212 1+ \u03bei]=0 = \u21d2 \u03b1i =0 \u2228 yi(w \u00b7 xi + b)=1 \u2212 \u03bei (4.28)\\n\u2200i, \u03b2i\u03bei =0 = \u21d2 \u03b2i =0 \u2228 \u03bei =0 . (4.29)\\nBy equation 4.25, as ', 'in the separable case, the weight vector w solution of the\\nSVMproblem is a linear combination of the training set vectorsx1,..., xm.Av e c t o r74 Support Vector Machines\\nxi appears in that expansion ', 'i\ufb00\u03b1i \u0338= 0. Such vectors are calledsupport vectors. Here,\\nthere are two types of support vectors. By the complementarity condition (4.28), if\\n\u03b1\\ni \u0338=0 ,t h e nyi(w \u00b7 xi + b)=1 \u2212 \u03bei.I f \u03bei =0 ,t h e nyi(', 'w \u00b7 xi + b)=1a n d xi lies\\non a marginal hyperplane, as in the separable case. Otherwise, \u03bei \u0338= 0 and xi is an\\noutlier. In this case, (4.29) implies \u03b2i = 0 and (4.27) then requires \u03b1i = C.T h u s ,\\nsu', 'pport vectors xi are either outliers, in which case \u03b1i = C, or vectors lying on the\\nmarginal hyperplanes. As in the separable case, note that while the weight vector\\nw solution is unique, the support ', 'vectors are not.\\n4.3.3 Dual optimization problem\\nTo derive the dual form of the constrained optimization problem (4.23), we plug\\ninto the Lagrangian the de\ufb01nition of w in terms of the dual variables (', '4.25) and\\napply the constraint (4.26). This yields\\nL = 1\\n2 \u2225\\nm\u2211\\ni=1\\n\u03b1iyixi\u22252 \u2212\\nm\u2211\\ni,j=1\\n\u03b1i\u03b1jyiyj(xi \u00b7 xj)\\n\\ued19 \\ued18\\ued17 \\ued1a\\n\u2212 1\\n2\\nPm\\ni,j=1 \u03b1i\u03b1jyiyj(xi\u00b7xj)\\n\u2212\\nm\u2211\\ni=1\\n\u03b1iyib\\n\\ued19 \\ued18\\ued17 \\ued1a\\n0\\n+\\nm\u2211\\ni=1\\n\u03b1i . (4.30)\\nRemarkably,', ' we \ufb01nd that the objective function is no di\ufb00erent than in the separable\\ncase:\\nL =\\nm\u2211\\ni=1\\n\u03b1i \u2212 1\\n2\\nm\u2211\\ni,j=1\\n\u03b1i\u03b1jyiyj(xi \u00b7 xj) . (4.31)\\nHowever, here, in addition to\u03b1i \u2265 0, we must impose the constrain', 't on the Lagrange\\nvariables \u03b2i \u2265 0. In view of (4.27), this is equivalent to \u03b1i \u2264 C. This leads to the\\nfollowing dual optimization problem for SVMs in the non-separable case, which only\\ndi\ufb00ers from th', 'at of the separable case (4.13) by the constraints \u03b1i \u2264 C:\\nmax\\n\u03b1\\nm\u2211\\ni=1\\n\u03b1i \u2212 1\\n2\\nm\u2211\\ni,j=1\\n\u03b1i\u03b1jyiyj(xi \u00b7 xj) (4.32)\\nsubject to: 0 \u2264 \u03b1i \u2264 C \u2227\\nm\u2211\\ni=1\\n\u03b1iyi =0 ,i \u2208 [1,m ].\\nThus, our previous comments abou', 't the optimization problem (4.13) apply to (4.32)\\nas well. In particular, the objective function is concave and in\ufb01nitely di\ufb00erentiable\\nand (4.32) is equivalent to a convex QP. The problem is equivale', 'nt to the primal\\nproblem (4.23).\\nThe solution \u03b1 of the dual problem (4.32) can be used directly to determine the4.4 Margin theory 75\\nhypothesis returned by SVMs, using equation (4.25):\\nh(x)=s g n (w \u00b7', ' x + b)=s g n\\n( m\u2211\\ni=1\\n\u03b1iyi(xi \u00b7 x)+ b\\n\u23a1\\n. (4.33)\\nMoreover, b can be obtained from any support vector xi lying on a marginal\\nhyperplane, that is any vector xi with 0 <\u03b1 i <C . For such support vectors', ',\\nw \u00b7 xi + b = yi and thus\\nb = yi \u2212\\nm\u2211\\nj=1\\n\u03b1jyj(xj \u00b7 xi) . (4.34)\\nAs in the separable case, the dual optimization problem (4.32) and the expressions\\n(4.33) and (4.34) show an important property of SVM', 's: the hypothesis solution\\ndepends only on inner products between vectors and not directly on the vectors\\nthemselves. This fact can be used to extend SVMs to de\ufb01ne non-linear decision\\nboundaries, as w', 'e shall see in chapter 5.\\n4.4 Margin theory\\nThis section presents generalization bounds based on the notion of margin, which\\nprovide a strong theoretical justi\ufb01cation for the SVM algorithm. We \ufb01rst gi', 've the\\nde\ufb01nitions of some basic margin concepts.\\nDe\ufb01nition 4.2 Margin\\nThe geometric margin \u03c1(x) of a pointx with labely with respect to a linear classi\ufb01er\\nh: x \u21a6\u2192 w \u00b7 x + b is its distance to the hype', 'rplane w \u00b7 x + b =0 :\\n\u03c1(x)= y (w \u00b7 x + b)\\n\u2225w\u2225 . (4.35)\\nThe margin of a linear classi\ufb01er h for a sample S =( x1,..., xm) is the minimum\\nmargin over the points in the sample:\\n\u03c1=m i n\\n1\u2264i\u2264m\\nyi (w \u00b7 xi + ', 'b)\\n\u2225w\u2225 . (4.36)\\nRecall that the VC-dimension of the family of hyperplanes or linear hypotheses in\\nRN is N+1. Thus, the application of the VC-dimension bound (3.31) of corollary 3.4\\nto this hypothesis ', 'set yields the following: for any \u03b4> 0, with probability at least76 Support Vector Machines\\n1 \u2212 \u03b4, for any h \u2208 H,\\nR(h) \u2264 \u02c6R(h)+\\n\u221a\\n2(N +1 )l o gem\\nN+1\\nm +\\n\u221a\\nlog 1\\n\u03b4\\n2m . (4.37)\\nWhen the dimension of th', 'e feature space N is large compared to the sample size,\\nthis bound is uninformative. The following theorem presents instead a bound on the\\nVC-dimension of canonical hyperplanes that does not depend on', ' the dimension of\\nfeature space N, but only on the margin and the radius r of the sphere containing\\nthe data.\\nTheorem 4.2\\nLet S \u2286{ x: \u2225x\u2225\u2264 r}. Then, the VC-dimensiond of the set of canonical hyperplan', 'es\\n{x \u21a6\u2192 sgn(w \u00b7 x): min\\nx\u2208S |w \u00b7 x| =1 \u2227\u2225w\u2225\u2264 \u039b} veri\ufb01es\\nd \u2264 r2\u039b2 .\\nProof Assume {x1,..., xd} is a set that can be fully shattered. Then, for all\\ny =( y1,...,y d) \u2208{ \u22121, +1}d,t h e r ee x i s t sw suc', 'h that,\\n\u2200i \u2208 [1,d ], 1 \u2264 yi(w \u00b7 xi) .\\nSumming up these inequalities yields\\nd \u2264 w \u00b7\\nd\u2211\\ni=1\\nyixi \u2264\u2225 w\u2225\u2225\\nd\u2211\\ni=1\\nyixi\u2225\u2264 \u039b\u2225\\nd\u2211\\ni=1\\nyixi\u2225 .\\nSince this inequality holds for all y \u2208{ \u22121,+1}d, it also holds on', ' expectation over\\ny1,...,y d drawn i.i.d. according to a uniform distribution over{\u22121, +1}. In view of\\nthe independence assumption, for i \u0338= j we have E[yiyj]=E [ yi]E [yj]. Thus, since\\nthe distributi', 'on is uniform, E[yiyj]=0i f i \u0338= j,E [yiyj] = 1 otherwise. This gives\\nd \u2264 \u039bE\\ny\\n[\u2225\\nd\u2211\\ni=1\\nyixi\u2225] (taking expectations)\\n\u2264 \u039b\\n[\\nE\\ny\\n[\u2225\\nd\u2211\\ni=1\\nyixi\u22252]\\n]1/2\\n(Jensen\u2019s inequality)\\n=\u039b\\n[ d\u2211\\ni,j=1\\nE\\ny\\n[yiyj](xi', ' \u00b7 xj)\\n]1/2\\n=\u039b\\n[ d\u2211\\ni=1\\n(xi \u00b7 xi)\\n]1/2\\n\u2264 \u039b\\n[\\ndr2]1/2\\n=\u039b r\\n\u221a\\nd.\\nThus,\\n\u221a\\nd \u2264 \u039br, which completes the proof.4.4 Margin theory 77\\nWhen the training data is linearly separable, by the results of section 4.', '2, the\\nmaximum-margin canonical hyperplane with \u2225w\u2225 =1 /\u03c1 c a nb ep l u g g e di n t o\\nt h e o r e m4 . 2 .I nt h i sc a s e ,\u039bc a nb es e tt o1/\u03c1, and the upper bound can be rewritten\\nas r2/\u03c12. Note ', 'that the choice of \u039b must be made before receiving the sample S.\\nIt is also possible to bound the Rademacher complexity of linear hypotheses with\\nbounded weight vector in a similar way, as shown by th', 'e following theorem.\\nTheorem 4.3\\nLet S \u2286{ x: \u2225x\u2225\u2264 R} be a sample of size m and let H = {x \u21a6\u2192 w \u00b7 x: \u2225w\u2225\u2264 \u039b}.\\nThen, the empirical Rademacher complexity of H can be bounded as follows:\\n\u02c6RS(H) \u2264\\n\u221a\\nr2\u039b2\\nm', ' .\\nProof The proof follows through a series of inequalities similar to those of theo-\\nrem 4.2:\\n\u02c6RS(H)= 1\\nm E\\n\u03c3\\n[ m\u2211\\ni=1\\n\u03c3iw \u00b7 xi\\n]\\n= 1\\nm E\\n\u03c3\\n[\\nw \u00b7\\nm\u2211\\ni=1\\n\u03c3ixi\\n]\\n\u2264 \u039b\\nm E\\n\u03c3\\n[\\ued79\\ued79\\n\\ued79\\nm\u2211\\ni=1\\n\u03c3ixi\\n\\ued79\\ued79\\n\\ued79\\n]\\n\u2264 \u039b\\n', 'm\\n[\\nE\\n\u03c3\\n[\\ued79\\ued79\\n\\ued79\\nm\u2211\\ni=1\\n\u03c3ixi\\n\\ued79\\ued79\\n\\ued79\\n2]]1/2\\n= \u039b\\nm\\n[\\nE\\n\u03c3\\n[ m\u2211\\ni,j=1\\n\u03c3i\u03c3j(xi \u00b7 xj)\\n]]1/2\\n\u2264 \u039b\\nm\\n[ m\u2211\\ni=1\\n\u2225xi\u22252\\n]1/2\\n\u2264 \u039b\\n\u221a\\nmr2\\nm =\\n\u221a\\nr2\u039b2\\nm ,\\nThe \ufb01rst inequality makes use of the Cauchy-Schwarz inequality and t', 'he bound on\\n\u2225w\u2225, the second follows by Jensen\u2019s inequality, the third by E[\u03c3i\u03c3j]=E [ \u03c3i]E [\u03c3j]=\\n0f o ri \u0338= j, and the last one by \u2225xi\u2225\u2264 R.\\nTo present the main margin-based generalization bounds of thi', 's section, we need\\nto introduce a margin loss function. Here, the training data is not assumed to be\\nseparable. The quantity \u03c1> 0s h o u l dt h u sb ei n t e r p r e t e da st h em a r g i nw ew i s h', 't o\\nachieve.\\nDe\ufb01nition 4.3 Margin loss function\\nFor any \u03c1> 0,t h e\u03c1-margin loss is the function L\u03c1: R \u00d7 R \u2192 R+ de\ufb01ned for all\\ny,y \u2032 \u2208 R by L\u03c1(y,y \u2032)=\u03a6 \u03c1(yy\u2032) with,\\n\u03a6\u03c1(x)=\\n\u23a7\\n\u23aa\u23aa\\n\u23a8\\n\u23aa\u23aa\\n\u23a9\\n0 if \u03c1 \u2264 x\\n1 \u2212 x/', '\u03c1 if 0 \u2264 x \u2264 \u03c1\\n1 if x \u2264 0 .\\nThis loss function is illustrated in \ufb01gure 4.5. The empirical margin loss is then\\nde\ufb01ned as the margin loss over the training sample.78 Support Vector Machines\\n1\\n0 \u03c11\\nFigur', 'e 4.5 The margin loss, de\ufb01ned with respect to margin parameter \u03c1.\\nDe\ufb01nition 4.4 Empirical margin loss\\nGiven a sample S =( x1,...,x m) and a hypothesis h, the empirical margin loss is\\nde\ufb01ned by\\n\u02c6R\u03c1(h)=', ' 1\\nm\\nm\u2211\\ni=1\\n\u03a6\u03c1(yih(xi)). (4.38)\\nNote that for any i \u2208 [1,m], \u03a6\u03c1(yih(xi)) \u2264 1yih(xi)\u2264\u03c1. Thus, the empirical margin\\nloss can be upper-bounded as follows:\\n\u02c6R\u03c1(h) \u2264 1\\nm\\nm\u2211\\ni=1\\n1yih(xi)\u2264\u03c1 . (4.39)\\nIn all t', 'he results that follow, the empirical margin loss can be replaced by this upper\\nbound, which admits a simple interpretation: it is the fraction of the points in the\\ntraining sample S that have been mi', 'sclassi\ufb01ed or classi\ufb01ed with con\ufb01dence less than\\n\u03c1.W h e nh is a linear function de\ufb01ned by a weight vector w with \u2225w\u2225 =1 , yih(xi)\\nis the margin of point xi. Thus, the upper bound is then the fraction', ' of the points\\nin the training data with margin less than \u03c1. This corresponds to the loss function\\nindicated by the blue dotted line in \ufb01gure 4.5.\\nThe slope of the function \u03a6\u03c1 de\ufb01ning the margin loss ', 'is at most 1 /\u03c1,t h u s\u03a6\u03c1 is\\n1/\u03c1-Lipschitz. The following lemma bounds the empirical Rademacher complexity\\nof a hypothesis set H after composition with such a Lipschitz function in terms of\\nthe empiri', 'cal Rademacher complexity of H. It will be needed for the proof of the\\nmargin-based generalization bound.\\nLemma 4.2 Talagrand\u2019s lemma\\nLet \u03a6: R \u2192 R be an l-Lipschitz. Then, for any hypothesis set H of ', 'real-valued\\nfunctions, the following inequality holds:\\n\u02c6RS(\u03a6 \u25e6H) \u2264 l \u02c6RS(H) .4.4 Margin theory 79\\nProof First we \ufb01x a sample S =( x1,...,x m), then, by de\ufb01nition,\\n\u02c6RS(\u03a6 \u25e6H)= 1\\nm E\\n\u03c3\\n[\\nsup\\nh\u2208H\\nm\u2211\\ni=1\\n\u03c3', 'i(\u03a6 \u25e6h)(xi)\\n]\\n= 1\\nm E\\n\u03c31,...,\u03c3m\u2212 1\\n[\\nE\\n\u03c3m\\n[\\nsup\\nh\u2208H\\num\u22121(h)+ \u03c3m(\u03a6 \u25e6h)(xm)\\n]]\\n,\\nwhere um\u22121(h)= \u2211m\u22121\\ni=1 \u03c3i(\u03a6 \u25e6h)(xi). By de\ufb01nition of the supremum, for any\u03f5> 0,\\nthere exist h1,h2 \u2208 H such that\\num\u22121(h1)', '+( \u03a6 \u25e6h1)(xm) \u2265 (1 \u2212 \u03f5)\\n[\\nsup\\nh\u2208H\\num\u22121(h)+( \u03a6 \u25e6h)(xm)\\n]\\nand um\u22121(h2) \u2212 (\u03a6 \u25e6h2)(xm) \u2265 (1 \u2212 \u03f5)\\n[\\nsup\\nh\u2208H\\num\u22121(h) \u2212 (\u03a6 \u25e6h)(xm)\\n]\\n.\\nThus, for any \u03f5> 0, by de\ufb01nition of E \u03c3m ,\\n(1 \u2212 \u03f5)E\\n\u03c3m\\n[\\nsup\\nh\u2208H\\num\u22121(h)', '+ \u03c3m(\u03a6 \u25e6h)(xm)\\n]\\n=( 1 \u2212 \u03f5)\\n[1\\n2 sup\\nh\u2208H\\num\u22121(h)+( \u03a6 \u25e6h)(xm)+ 1\\n2 sup\\nh\u2208H\\num\u22121(h) \u2212 (\u03a6 \u25e6h)(xm)\\n]\\n\u2264 1\\n2[um\u22121(h1)+( \u03a6 \u25e6h1)(xm)] + 1\\n2[um\u22121(h2) \u2212 (\u03a6 \u25e6h2)(xm)].\\nLet s = sgn(h1(xm) \u2212 h2(xm)). Then, the prev', 'ious inequality implies\\n(1 \u2212 \u03f5)E\\n\u03c3m\\n[\\nsup\\nh\u2208H\\num\u22121(h)+ \u03c3m(\u03a6 \u25e6h)(xm)\\n]\\n\u2264 1\\n2[um\u22121(h1)+ um\u22121(h2)+ sl(h1(xm) \u2212 h2(xm))] (Lipschitz property)\\n= 1\\n2[um\u22121(h1)+ slh1(xm)] + 1\\n2[um\u22121(h2) \u2212 slh2(xm)] (rearrang', 'ing)\\n\u2264 1\\n2 sup\\nh\u2208H\\n[um\u22121(h)+ slh(xm)] + 1\\n2 sup\\nh\u2208H\\n[um\u22121(h) \u2212 slh(xm)] (de\ufb01nition of sup)\\n=E\\n\u03c3m\\n[\\nsup\\nh\u2208H\\num\u22121(h)+ \u03c3mlh(xm)\\n]\\n. (de\ufb01nition of E\\n\u03c3m\\n)\\nSince the inequality holds for all \u03f5> 0, we have\\nE', '\\n\u03c3m\\n[\\nsup\\nh\u2208H\\num\u22121(h)+ \u03c3m(\u03a6 \u25e6h)(xm)\\n]\\n\u2264 E\\n\u03c3m\\n[\\nsup\\nh\u2208H\\num\u22121(h)+ \u03c3mlh(xm)\\n]\\n.\\nProceeding in the same way for all other \u03c3is( i \u0338= m)p r o v e st h el e m m a .\\nThe following is a general margin-based ge', 'neralization bound that will be used\\nin the analysis of several algorithms.80 Support Vector Machines\\nTheorem 4.4 Margin bound for binary classi\ufb01cation\\nLet H be a set of real-valued functions. Fix \u03c1> ', '0,t h e n ,f o ra n y\u03b4> 0,w i t h\\nprobability at least 1 \u2212 \u03b4, each of the following holds for all h \u2208 H:\\nR(h) \u2264 \u02c6R\u03c1(h)+ 2\\n\u03c1Rm(H)+\\n\u221a\\nlog 1\\n\u03b4\\n2m (4.40)\\nR(h) \u2264 \u02c6R\u03c1(h)+ 2\\n\u03c1\\n\u02c6RS(H)+3\\n\u221a\\nlog 2\\n\u03b4\\n2m . (4.41)\\n', 'Proof Let \u02dcH = {z =( x, y) \u21a6\u2192 yh(x): h \u2208 H}. Consider the family of functions\\ntaking values in [0, 1]:\\n\u02dcH = {\u03a6\u03c1 \u25e6f : f \u2208 \u02dcH} .\\nBy theorem 3.1, with probability at least 1 \u2212 \u03b4, for all g \u2208 \u02dcH,\\nE[g(z)] ', '\u2264 1\\nm\\nm\u2211\\ni=1\\ng(zi)+2 Rm( \u02dcH)+\\n\u221a\\nlog 1\\n\u03b4\\n2m ,\\nand thus, for all h \u2208 H,\\nE[\u03a6\u03c1(yh(x))] \u2264 \u02c6R\u03c1(h)+2 Rm\\n(\\n\u03a6\u03c1 \u25e6 \u02dcH\\n\u23a1\\n+\\n\u221a\\nlog 1\\n\u03b4\\n2m .\\nSince 1u\u22640 \u2264 \u03a6\u03c1(u) for all u \u2208 R,w eh a v eR(h)=E [ 1yh(x)\u22640] \u2264 E[\u03a6\u03c1(yh(x)', ')], thus\\nR(h) \u2264 \u02c6R\u03c1(h)+2 Rm\\n(\\n\u03a6\u03c1 \u25e6 \u02dcH\\n\u23a1\\n+\\n\u221a\\nlog 1\\n\u03b4\\n2m .\\nRm is invariant to a constant shift, therefore we have\\nRm\\n(\\n\u03a6\u03c1 \u25e6 \u02dcH\\n\u23a1\\n= Rm\\n(\\n(\u03a6\u03c1 \u2212 1) \u25e6 \u02dcH\\n\u23a1\\n.\\nSince (\u03a6\u03c1 \u2212 1)(0) = 0 and since (\u03a6\u03c1 \u2212 1) is 1/\u03c1-', 'Lipschitz as with \u03a6\u03c1,b yl e m m a4 . 2 ,\\nwe have Rm\\n(\\n\u03a6\u03c1 \u25e6 \u02dcH\\n\u23a1\\n\u2264 1\\n\u03c1Rm( \u02dcH)a n dRm( \u02dcH) can be rewritten as follows:\\nRm( \u02dcH)= 1\\nm E\\nS,\u03c3\\n[\\nsup\\nh\u2208H\\nm\u2211\\ni=1\\n\u03c3iyih(xi)\\n]\\n= 1\\nm E\\nS,\u03c3\\n[\\nsup\\nh\u2208H\\nm\u2211\\ni=1\\n\u03c3ih(x', 'i)\\n]\\n= Rm\\n(\\nH\\n\u23a1\\n.\\nThis proves (4.40). The second inequality, (4.41), can be derived in the same way\\nby using the second inequality of theorem 3.1, (3.4), instead of (3.3).\\nThe generalization bounds of', ' theorem 4.4 shows the con\ufb02ict between two terms:\\nthe larger the desired margin \u03c1, the smaller the middle term; however, the \ufb01rst4.4 Margin theory 81\\nterm, the empirical margin loss \u02c6R\u03c1, increases as ', 'a function of \u03c1. The bounds of\\nthis theorem can be generalized to hold uniformly for all \u03c1> 0 at the cost of an\\nadditional term\\n\u221a\\nlog log2\\n2\\n\u03c1\\nm , as shown in the following theorem (a version of this\\n', 'theorem with better constants can be derived, see exercise 4.2).\\nTheorem 4.5\\nLet H be a set of real-valued functions. Then, for any\u03b4> 0, with probability at least\\n1 \u2212 \u03b4, each of the following holds fo', 'r all h \u2208 H and \u03c1 \u2208 (0, 1):\\nR(h) \u2264 \u02c6R\u03c1(h)+ 4\\n\u03c1Rm(H)+\\n\u221a\\nlog log2\\n2\\n\u03c1\\nm +\\n\u221a\\nlog 2\\n\u03b4\\n2m (4.42)\\nR(h) \u2264 \u02c6R\u03c1(h)+ 4\\n\u03c1\\n\u02c6RS(H)+\\n\u221a\\nlog log2\\n2\\n\u03c1\\nm +3\\n\u221a\\nlog 4\\n\u03b4\\n2m . (4.43)\\nProof Consider two sequences ( \u03c1k)k\u22651 a', 'nd (\u03f5k)k\u22651,w i t h\u03f5k \u2208 (0, 1 ) .B yt h e o -\\nrem 4.4, for any \ufb01xed k \u2265 1,\\nPr\\n[\\nR(h) \u2212 \u02c6R\u03c1k (h) > 2\\n\u03c1k\\nRm(H)+ \u03f5k\\n]\\n\u2264 exp(\u22122m\u03f52\\nk). (4.44)\\nChoose \u03f5k = \u03f5 +\\n\u221a\\nlog k\\nm , then, by the union bound,\\nPr\\n[\\n\u2203k: ', 'R(h) \u2212 \u02c6R\u03c1k (h) > 2\\n\u03c1k\\nRm(H)+ \u03f5k\\n]\\n\u2264\\n\u2211\\nk\u22651\\nexp(\u22122m\u03f52\\nk)\\n=\\n\u2211\\nk\u22651\\nexp\\n[\\n\u2212 2m(\u03f5 +\\n\u221a\\n(log k)/m)2]\\n\u2264\\n\u2211\\nk\u22651\\nexp(\u22122m\u03f52)e x p (\u22122l o gk)\\n=\\n(\u2211\\nk\u22651\\n1/k2\u23a1\\nexp(\u22122m\u03f52)\\n= \u03c02\\n6 exp(\u22122m\u03f52) \u2264 2e x p (\u22122m\u03f52).\\nWe can ch', 'oose \u03c1k =1 /2k. For any \u03c1 \u2208 (0, 1), there exists k \u2265 1 such that\\n\u03c1 \u2208 (\u03c1k,\u03c1k\u22121], with \u03c10 = 1. For that k, \u03c1 \u2264 \u03c1k\u22121 =2 \u03c1k,t h u s1/\u03c1k \u2264 2/\u03c1\\nand log k =\\n\u221a\\nlog log2(1/\u03c1k) \u2264\\n\u221a\\nlog log2(2/\u03c1). Furthermore, f', 'or any h \u2208 H,\\n\u02c6R\u03c1k (h) \u2264 \u02c6R\u03c1(h). Thus,\\nPr\\n[\\n\u2203k: R(h) \u2212 \u02c6R\u03c1(h) > 4\\n\u03c1Rm(H)+\\n\u221a\\nlog log2(2/\u03c1)\\nm + \u03f5\\n]\\n\u2264 2e x p (\u22122m\u03f52),\\nwhich proves the \ufb01rst statement. The second statement can be proven in a similar82 S', 'upport Vector Machines\\nway.\\nCombining theorem 4.3 and theorem 4.4 gives directly the following general\\nmargin bound for linear hypotheses with bounded weight vectors, presented in\\ncorollary 4.1.\\nCorol', 'lary 4.1\\nLet H = {x \u21a6\u2192 w \u00b7 x: \u2225w\u2225\u2264 \u039b} and assume that X \u2286{ x: \u2225x\u2225\u2264 r}.F i x\u03c1> 0,\\nthen, for any \u03b4> 0, with probability at least 1 \u2212 \u03b4, for any h \u2208 H,\\nR(h) \u2264 \u02c6R\\n\u03c1(h)+2\\n\u221a\\nr2\u039b2/\u03c12\\nm +\\n\u221a\\nlog 1\\n\u03b4\\n2m . (4.45', ')\\nAs with theorem 4.4, the bound of this corollary can be generalized to hold uniformly\\nfor all \u03c1> 0 at the cost of an additional term\\n\u221a\\nlog log2\\n2\\n\u03c1\\nm by combining theorems 4.3\\nand 4.5. This generali', 'zation bound for linear hypotheses is remarkable, since it does\\nnot depend directly on the dimension of the feature space, but only on the margin.\\nIt suggests that a small generalization error can be ', 'achieved when\u03c1/ris large (small\\nsecond term) while the empirical margin loss is relatively small (\ufb01rst term). The\\nlatter occurs when few points are either classi\ufb01ed incorrectly or correctly, but with\\n', 'margin less than \u03c1.\\nThe fact that the guarantee does not explicitly depend on the dimension of the\\nfeature space may seem surprising and appear to contradict the VC-dimension lower\\nbounds of theorems ', '3.6 and 3.7. Those lower bounds show that for any learning\\nalgorithm A there exists a bad distribution for which the error of the hypothesis\\nreturned by the algorithm is \u03a9(\\n\u221a\\nd/m) with a non-zero prob', 'ability. The bound of\\nthe corollary does not rule out such bad cases, however: for such bad distributions,\\nthe empirical margin loss would be large even for a relatively small margin \u03c1,a n d\\nthus the ', 'bound of the corollary would be loose in that case.\\nThus, in some sense, the learning guarantee of the corollary hinges upon the\\nhope of a good margin value \u03c1: if there exists a relatively large margi', 'n value\\n\u03c1> 0 for which the empirical margin loss is small, then a small generalization\\nerror is guaranteed by the corollary. This favorable margin situation depends on the\\ndistribution: while the lear', 'ning bound is distribution-independent, the existence of\\na good margin is in fact distribution-dependent. A favorable margin seems to appear\\nrelatively often in applications.\\nThe bound of the corollar', 'y gives a strong justi\ufb01cation for margin-maximization\\nalgorithms such as SVMs. First, note that for \u03c1= 1, the margin loss can be upper\\nbounded by the hinge loss:\\n\u2200x \u2208 R,\u03a6\\n1(x) \u2264 max(1 \u2212 x,0). (4.46)4.', '5 Chapter notes 83\\nUsing this fact, the bound of the corollary implies that with probability at least\\n1 \u2212 \u03b4, for all h \u2208 H = {x \u21a6\u2192 w \u00b7 x: \u2225w\u2225\u2264 \u039b},\\nR(h) \u2264 1\\nm\\nm\u2211\\ni=1\\n\u03bei +2\\n\u221a\\nr2\u039b2\\nm +\\n\u221a\\nlog 1\\n\u03b4\\n2m , (4.', '47)\\nwhere \u03bei =m a x ( 1\u2212 yi(w \u00b7 xi), 0). The objective function minimized by the SVM\\nalgorithm has precisely the form of this upper bound: the \ufb01rst term corresponds to\\nthe slack penalty over the train', 'ing set and the second to the minimization of the\\n\u2225w\u2225 which is equivalent to that of\u2225w\u2225\\n2. Note that an alternative objective function\\nwould be based on the empirical margin loss instead of the hinge ', 'loss. However, the\\nadvantage of the hinge loss is that it is convex, while the margin loss is not.\\nAs already pointed out, the bounds just discussed do not directly depend on the\\ndimension of the feat', 'ure space and guarantee good generalization with a favorable\\nmargin. Thus, they suggest seeking large-margin separating hyperplanes in a very\\nhigh-dimensional space. In view of the form of the dual op', 'timization problems for\\nSVMs, determining the solution of the optimization and using it for prediction both\\nrequire computing many inner products in that space. For very high-dimensional\\nspaces, the c', 'omputation of these inner products could become very costly. The\\nnext chapter provides a solution to this problem which further generalizes SVMs to\\nnon-linear separation.\\n4.5 Chapter notes\\nThe maximum', '-margin or optimal hyperplane solution described in section 4.2\\nwas introduced by Vapnik and Chervonenkis [1964]. The algorithm had limited\\napplications, since in most tasks in practice the data is no', 't linearly separable.\\nIn contrast, the SVM algorithm of section 4.3 for the general non-separable case,\\nintroduced by Cortes and Vapnik [1995] under the name support-vector networks,\\nhas been widely a', 'dopted and been shown to be e\ufb00ective in practice. The algorithm\\nand its theory have had a profound impact on theoretical and applied machine\\nlearning and inspired research on a variety of topics. Seve', 'ral specialized algorithms\\nhave been suggested for solving the speci\ufb01c QP that arises when solving the SVM\\nproblem, for example the SMO algorithm of Platt [1999] (see exercise 4.4) and a\\nvariety of ot', 'her decomposition methods such as those used in the LibLinear software\\nlibrary [Hsieh et al., 2008], and [Allauzen et al., 2010] for solving the problem when\\nusing rational kernels (see chapter 5).\\nMu', 'ch of the theory supporting the SVM algorithm ([Cortes and Vapnik, 1995,\\nVapnik, 1998]), in particular the margin theory presented in section 4.4, has been\\nadopted in the learning theory and statistic', 's communities and applied to a variety84 Support Vector Machines\\nof other problems. The margin bound on the VC-dimension of canonical hyper-\\nplanes (theorem 4.2) is by Vapnik [1998], the proof is very', ' similar to Noviko\ufb00\u2019s\\nmargin bound on the number of updates made by the Perceptron algorithm in the\\nseparable case. Our presentation of margin guarantees based on the Rademacher\\ncomplexity follows the', ' elegant analysis of Koltchinskii and Panchenko [2002] (see\\nalso Bartlett and Mendelson [2002], Shawe-Taylor et al. [1998]). Our proof of Ta-\\nlagrand\u2019s lemma 4.2 is a simpler and more concise version ', 'of a more general result\\ngiven by Ledoux and Talagrand [1991, pp. 112\u2013114]. See H\u00a8 o\ufb00gen et al. [1995] for\\nhardness results related to the problem of \ufb01nding a hyperplane with the minimal\\nnumber of err', 'ors on a training sample.\\n4.6 Exercises\\n4.1 Soft margin hyperplanes. The function of the slack variables used in the opti-\\nmization problem for soft margin hyperplanes has the form: \u03be \u21a6\u2192 \u2211\\nm\\ni=1 \u03bei. I', 'nstead,\\nwe could use \u03be\u21a6\u2192 \u2211m\\ni=1 \u03bep\\ni ,w i t hp> 1.\\n(a) Give the dual formulation of the problem in this general case.\\n(b) How does this more general formulation ( p> 1) compare to the standard\\nsetting', ' (p = 1)? In the case p = 2 is the optimization still convex?\\nSparse SVM. One can give two types of arguments in favor of the SVM algorithm:\\none based on the sparsity of the support vectors, another b', 'ased on the notion\\nof margin. Suppose that instead of maximizing the margin, we choose instead to\\nmaximize sparsity by minimizing the L\\np norm of the vector \u03b1 that de\ufb01nes the\\nweight vector w,f o rs o ', 'm ep \u2265 1. First, consider the case p =2 .T h i sg i v e st h e\\nfollowing optimization problem:\\nmin\\n\u03b1 ,b\\n1\\n2\\nm\u2211\\ni=1\\n\u03b12\\ni + C\\nm\u2211\\ni=1\\n\u03bei (4.48)\\nsubject to yi\\n( m\u2211\\nj=1\\n\u03b1jyjxi \u00b7 xj + b\\n\u23a1\\n\u2265 1 \u2212 \u03bei,i \u2208 [1,m ', ']\\n\u03bei,\u03b1i \u2265 0,i \u2208 [1,m].\\n(a) Show that modulo the non-negativity constraint on \u03b1, the problem coin-\\ncides with an instance of the primal optimization problem of SVM.\\n(b) Derive the dual optimization of ', 'problem of (4.48).\\n(c) Setting p = 1 will induce a more sparse\u03b1. Derive the dual optimization in4.6 Exercises 85\\nthis case.\\n4.2 Tighter Rademacher Bound. Derive the following tighter version of the bo', 'und\\nof theorem 4.5: for any \u03b4> 0, with probability at least 1 \u2212 \u03b4, for all h \u2208 H and\\n\u03c1 \u2208 (0, 1) the following holds:\\nR(h) \u2264 \u02c6R\u03c1(h)+ 2\u03b3\\n\u03c1 Rm(H)+\\n\u221a\\nlog log\u03b3\\n\u03b3\\n\u03c1\\nm +\\n\u221a\\nlog 2\\n\u03b4\\n2m (4.49)\\nfor any \u03b3> 1.\\n4.3', ' Importance weighted SVM. Suppose you wish to use SVMs to solve a learning\\nproblem where some training data points are more important than others. More\\nformally, assume that each training point consis', 'ts of a triplet ( xi,y i,p i), where\\n0 \u2264 pi \u2264 1 is the importance of the ith point. Rewrite the primal SVM constrained\\noptimization problem so that the penalty for mis-labeling a pointxi is scaled by ', 'the\\npriority pi. Then carry this modi\ufb01cation through the derivation of the dual solution.\\n4.4 Sequential minimal optimization (SMO). The SMO algorithm is an optimiza-\\ntion algorithm introduced to spee', 'd up the training of SVMs. SMO reduces a (po-\\ntentially) large quadratic programming (QP) optimization problem into a series of\\nsmall optimizations involving only two Lagrange multipliers. SMO reduces', ' memory\\nrequirements, bypasses the need for numerical QP optimization and is easy to im-\\nplement. In this question, we will derive the update rule for the SMO algorithm in\\nthe context of the dual form', 'ulation of the SVM problem.\\n(a) Assume that we want to optimize equation 4.32 only over\u03b1\\n1 and \u03b12.S h o w\\nthat the optimization problem reduces to\\nmax\\n\u03b11,\u03b12\\n\u03b11 + \u03b12 \u2212 1\\n2K11\u03b12\\n1 \u2212 1\\n2K22\u03b12\\n2 \u2212 sK12\u03b11\u03b1', '2 \u2212 y1\u03b11v1 \u2212 y2\u03b12v2\\n\\ued19 \\ued18\\ued17 \\ued1a\\n\u03a8 1(\u03b11,\u03b12)\\nsubject to: 0 \u2264 \u03b11,\u03b12 \u2264 C \u2227 \u03b11 + s\u03b12 = \u03b3,\\nwhere \u03b3 = y1\\n\u2211m\\ni=3 yi\u03b1i, s = y1y2 \u2208{ \u2212 1, +1}, Kij =( xi \u00b7 xj)a n d vi =\u2211m\\nj=3 \u03b1jyjKij for i =1 , 2.\\n(b) Substitute the', ' linear constraint \u03b11 = \u03b3 \u2212 s\u03b12 into \u03a8 1 to obtain a new\\nobjective function \u03a82 that depends only on\u03b12. Show that the\u03b12 that minimizes\\n\u03a82 (without the constraints 0 \u2264 \u03b11,\u03b12 \u2264 C) can be expressed as\\n\u03b12 ', '= s(K11 \u2212 K12)\u03b3+ y2(v1 \u2212 v2) \u2212 s +1\\n\u03b7 ,86 Support Vector Machines\\nwhere \u03b7= K11 + K22 \u2212 2K12.\\n(c) Show that\\nv1 \u2212 v2 = f(x1) \u2212 f(x2)+ \u03b1\u2217\\n2y2\u03b7\u2212 sy2\u03b3(K11 \u2212 K12)\\nwhere f(x)= \u2211m\\ni=1 \u03b1\u2217\\ni yi(xi \u00b7 x)+ b\u2217 and ', '\u03b1\u2217\\ni are values for the Lagrange\\nmultipliers prior to optimization over \u03b11 and \u03b12 (similarly, b\u2217 is the previous\\nvalue for the o\ufb00set).\\n(d) Show that\\n\u03b12 = \u03b1\u2217\\n2 + y2\\n(y2 \u2212 f(x2)) \u2212 (y1 \u2212 f(x1))\\n\u03b7 .\\n(e) ', 'For s = +1, de\ufb01ne L =m a x{0,\u03b3 \u2212 C} and H =m i n{C,\u03b3} as the lower\\nand upper bounds on \u03b12. Similarly, for s = \u22121, de\ufb01ne L =m a x{0, \u2212\u03b3} and\\nH =m i n{C,C \u2212 \u03b3}. The update rule for SMO involves \u201cclippin', 'g\u201d the value of\\n\u03b12, i.e.,\\n\u03b1clip\\n2 =\\n\u23a7\\n\u23aa\u23aa\\n\u23a8\\n\u23aa\u23aa\\n\u23a9\\n\u03b1\\n2 if L<\u03b1 2 <H\\nL if \u03b12 \u2264 L\\nH if \u03b12 \u2265 H\\n.\\nWe subsequently solve for \u03b11 such that we satisfy the equality constraint,\\nresulting in \u03b11 = \u03b1\u2217\\n1 + s(\u03b1\u2217\\n2 \u2212 \u03b1', 'clip\\n2 ). Why is \u201cclipping\u201d is required? How are L\\nand H derived for the case s =+ 1 ?\\n4.5 SVMs hands-on.\\n(a) Download and install the libsvm software library from:\\nhttp://www.csie.ntu.edu.tw/~cjlin/l', 'ibsvm/.\\n(b) Download the satimage data set found at:\\nhttp://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/.\\nMerge the training and validation sets into one. We will refer to the resulting set\\nas the', ' training set from now on. Normalize both the training and test vectors.\\n(c) Consider the binary classi\ufb01cation that consists of distinguishing class 6\\nfrom the rest of the data points. Use SVMs combin', 'ed with polynomial kernels\\n(see chapter 5) to solve this classi\ufb01cation problem. To do so, randomly split the\\ntraining data into ten equal-sized disjoint sets. For each value of the polynomial\\ndegree, ', 'd =1 , 2, 3, 4, plot the average cross-validation error plus or minus one\\nstandard deviation as a function of C (let the other parameters of polynomial\\nkernels in libsvm, \u03b3 and c, be equal to their de', 'fault values 1). Report the best4.6 Exercises 87\\nvalue of the trade-o\ufb00 constant C measured on the validation set.\\n(d) Let ( C\u2217,d \u2217) be the best pair found previously. Fix C to be C\u2217.P l o tt h e\\nten-f', 'old cross-validation training and test errors for the hypotheses obtained\\nas a function of d. Plot the average number of support vectors obtained as a\\nfunction of d.\\n(e) How many of the support vector', 's lie on the margin hyperplanes?\\n(f) In the standard two-group classi\ufb01cation, errors on positive or negative\\npoints are treated in the same manner. Suppose, however, that we wish to\\npenalize an error ', 'on a negative point (false positive error)k> 0t i m e sm o r et h a n\\nan error on a positive point. Give the dual optimization problem corresponding\\nto SVMs modi\ufb01ed in this way.\\n(g) Assume that k is a', 'n integer. Show how you can uselibsvm without writing\\nany additional code to \ufb01nd the solution of the modi\ufb01ed SVMs just described.\\n(h) Apply the modi\ufb01ed SVMs to the classi\ufb01cation task previously examin', 'ed\\nand compare with your previous SVMs results for k =2 ,4,8, 16.5 Kernel Methods\\nKernel methods are widely used in machine learning. They are \ufb02exible techniques\\nthat can be used to extend algorithms ', 'such as SVMs to de\ufb01ne non-linear decision\\nboundaries. Other algorithms that only depend on inner products between sample\\npoints can be extended similarly, many of which will be studied in future chapt', 'ers.\\nThe main idea behind these methods is based on so-calledkernels or kernel func-\\ntions, which, under some technical conditions of symmetry andpositive-de\ufb01niteness ,\\nimplicitly de\ufb01ne an inner produ', 'ct in a high-dimensional space. Replacing the orig-\\ninal inner product in the input space with positive de\ufb01nite kernels immediately\\nextends algorithms such as SVMs to a linear separation in that high-', 'dimensional\\nspace, or, equivalently, to a non-linear separation in the input space.\\nIn this chapter, we present the main de\ufb01nitions and key properties of positive\\nde\ufb01nite symmetric kernels, including ', 'the proof of the fact that they de\ufb01ne an inner\\nproduct in a Hilbert space, as well as their closure properties. We then extend the\\nSVM algorithm using these kernels and present several theoretical res', 'ults including\\ngeneral margin-based learning guarantees for hypothesis sets based on kernels. We\\nalso introduce negative de\ufb01nite symmetric kernels and point out their relevance to\\nthe construction of ', 'positive de\ufb01nite kernels, in particular from distances or metrics.\\nFinally, we illustrate the design of kernels for non-vectorial discrete structures by\\nintroducing a general family of kernels for seq', 'uences, rational kernels. We describe\\nan e\ufb03cient algorithm for the computation of these kernels and illustrate them with\\nseveral examples.\\n5.1 Introduction\\nIn the previous chapter, we presented an alg', 'orithm for linear classi\ufb01cation, SVMs,\\nwhich is both e\ufb00ective in applications and bene\ufb01ts from a strong theoretical justi-\\n\ufb01cation. In practice, linear separation is often not possible. Figure 5.1a sh', 'ows an\\nexample where any hyperplane crosses both populations. However, one can use more\\ncomplex functions to separate the two sets as in \ufb01gure 5.1b. One way to de\ufb01ne such\\na non-linear decision boundar', 'y is to use a non-linear mapping \u03a6 from the input90 Kernel Methods\\n(a) (b)\\nFigure 5.1 Non-linearly separable case. The classi\ufb01cation task consists of discrim-\\ninating between solid squares and solid c', 'ircles. (a) No hyperplane can separate the\\ntwo populations. (b) A non-linear mapping can be used instead.\\nspace X to a higher-dimensional space H, where linear separation is possible.\\nThe dimension of', ' H can truly be very large in practice. For example, in the\\ncase of document classi\ufb01cation, one may wish to use as features sequences of three\\nconsecutive words, i.e., trigrams. Thus, with a vocabular', 'y of just 100 ,000 words,\\nthe dimension of the feature space H reaches 1015. On the positive side, the margin\\nbounds presented in section 4.4 show that, remarkably, the generalization ability of\\nlarge', '-margin classi\ufb01cation algorithms such as SVMs do not depend on the dimension\\nof the feature space, but only on the margin\u03c1and the number of training examples\\nm. Thus, with a favorable margin\u03c1, such al', 'gorithms could succeed even in very high-\\ndimensional space. However, determining the hyperplane solution requires multiple\\ninner product computations in high-dimensional spaces, which can become be v', 'ery\\ncostly.\\nAs o l u t i o nt ot h i sp r o b l e mi st ou s ekernel methods, which are based on kernels\\nor kernel functions.\\nDe\ufb01nition 5.1 Kernels\\nA function K : X\u00d7 X\u2192 R is called a kernel over X.\\nT ', 'h ei d e ai st od e \ufb01 n eak e r n e lK such that for any two pointsx, x\u2032 \u2208X , K(x, x\u2032)b e5.1 Introduction 91\\nequal to an inner product of vectors \u03a6(x)a n d\u03a6 (y):1\\n\u2200x, x\u2032 \u2208X ,K (x, x\u2032)= \u27e8\u03a6(x), \u03a6(x\u2032)\u27e9 ,', ' (5.1)\\nfor some mapping \u03a6: X\u2192 H to a Hilbert space H called a feature space.S i n c ea n\\ninner product is a measure of the similarity of two vectors, K is often interpreted\\nas a similarity measure bet', 'ween elements of the input space X.\\nAn important advantage of such a kernel K is e\ufb03ciency: K is often signi\ufb01cantly\\nmore e\ufb03cient to compute than \u03a6 and an inner product in H.W ew i l ls e es e v e r a l', '\\ncommon examples where the computation of K(x, x\u2032) can be achieved in O(N)\\nwhile that of \u27e8\u03a6(x), \u03a6(x\u2032)\u27e9 typically requires O(dim(H)) work, with dim(H) \u226b N.\\nFurthermore, in some cases, the dimension of ', 'H is in\ufb01nite.\\nPerhaps an even more crucial bene\ufb01t of such a kernel function K is \ufb02exibility:\\nthere is no need to explicitly de\ufb01ne or compute a mapping \u03a6. The kernel K can\\nbe arbitrarily chosen so long', ' as the existence of \u03a6 is guaranteed, i.e. K satis\ufb01es\\nMercer\u2019s condition (see theorem 5.1).\\nTheorem 5.1 Mercer\u2019s condition\\nLet X\u2282 RN be a compact set and letK : X\u00d7 X \u2192 R be a continuous and symmetric\\n', 'function. Then, K admits a uniformly convergent expansion of the form\\nK(x, x\u2032)=\\n\u221e\u2211\\nn=0\\nan\u03c6n(x)\u03c6n(x\u2032),\\nwith an > 0 i\ufb00 for any square integrable function c (c \u2208 L2(X)), the following\\ncondition holds:\\n\u222b\u222b', '\\nX\u00d7 X\\nc(x)c(x\u2032)K(x, x\u2032)dxdx\u2032 \u2265 0.\\nThis condition is important to guarantee the convexity of the optimization problem\\nfor algorithms such as SVMs and thus convergence guarantees. A condition that\\nis eq', 'uivalent to Mercer\u2019s condition under the assumptions of the theorem is that\\nthe kernel K be positive de\ufb01nite symmetric (PDS). This property is in fact more\\ngeneral since in particular it does not requ', 'ire any assumption aboutX. In the next\\nsection, we give the de\ufb01nition of this property and present several commonly used\\nexamples of PDS kernels, then show that PDS kernels induce an inner product in\\n', 'a Hilbert space, and prove several general closure properties for PDS kernels.\\n1. To di\ufb00erentiate that inner product from the one of the input space, we will typically\\ndenote it by \u27e8\u00b7, \u00b7\u27e9.92 Kernel Me', 'thods\\n5.2 Positive de\ufb01nite symmetric kernels\\n5.2.1 De\ufb01nitions\\nDe\ufb01nition 5.2 Positive de\ufb01nite symmetric kernels\\nA kernel K : X\u00d7 X \u2192 R is said to be positive de\ufb01nite symmetric (PDS) if for\\nany {x1,...,x', ' m}\u2286X ,t h em a t r i xK =[ K(xi,xj)]ij \u2208 Rm\u00d7m is symmetric positive\\nsemide\ufb01nite (SPSD).\\nK is SPSD if it is symmetric and one of the following two equivalent conditions\\nholds:\\nthe eigenvalues of K are', ' non-negative;\\nfor any column vector c =( c1,...,c m)\u22a4 \u2208 Rm\u00d71,\\nc\u22a4Kc =\\nn\u2211\\ni,j=1\\ncicjK(xi,xj) \u2265 0. (5.2)\\nFor a sample S =( x1,...,x m), K =[ K(xi,xj)]ij \u2208 Rm\u00d7m is called the kernel\\nmatrix or the Gram ma', 'trix associated to K and the sample S.\\nLet us insist on the terminology: the kernel matrix associated to apositive de\ufb01nite\\nkernel is positive semide\ufb01nite . This is the correct mathematical terminology', '.\\nNevertheless, the reader should be aware that in the context of machine learning,\\nsome authors have chosen to use instead the term positive de\ufb01nite kernel to imply\\na positive de\ufb01nite kernel matrix o', 'r used new terms such as positive semide\ufb01nite\\nkernel.\\nThe following are some standard examples of PDS kernels commonly used in\\napplications.\\nExample 5.1 Polynomial kernels\\nFor any constantc> 0, a poly', 'nomial kernel of degreed \u2208 N is the kernel K de\ufb01ned\\nover R\\nN by:\\n\u2200x,x\u2032 \u2208 RN ,K (x,x\u2032)=( x \u00b7 x\u2032 + c)d. (5.3)\\nPolynomial kernels map the input space to a higher-dimensional space of dimension(N+d\\nd\\n\u23a1\\n(s', 'ee exercise 5.9). As an example, for an input space of dimension N =2 ,\\na second-degree polynomial (d = 2) corresponds to the following inner product in5.2 Positive de\ufb01nite symmetric kernels 93\\n(\u22121,1)', ' (1, 1)\\n(1, \u22121)(\u22121, \u22121)\\nx2\\nx1\\n(1, 1, \u2212\\n\u221a\\n2, +\\n\u221a\\n2, \u2212\\n\u221a\\n2, 1)\\n\u221a\\n2 x1x2\\n\u221a\\n2 x1\\n(1, 1, \u2212\\n\u221a\\n2, \u2212\\n\u221a\\n2, +\\n\u221a\\n2, 1)\\n(1, 1, +\\n\u221a\\n2, \u2212\\n\u221a\\n2, \u2212\\n\u221a\\n2, 1) (1 , 1, +\\n\u221a\\n2, +\\n\u221a\\n2, +\\n\u221a\\n2, 1)\\n(a) (b)\\nFigure 5.2 Illustrati', 'on of the XOR classi\ufb01cation problem and the use of poly-\\nnomial kernels. (a) XOR problem linearly non-separable in the input space. (b)\\nLinearly separable using second-degree polynomial kernel.\\ndimens', 'ion 6:\\n\u2200x,x\\n\u2032 \u2208 R2,K (x,x\u2032)=( x1x\u2032\\n1 + x2x\u2032\\n2 + c)2 =\\n\u23a1\\n\u23a2\u23a2\u23a2\u23a2\\n\u23a2\\n\u23a2\\n\u23a2\u23a2\u23a2\u23a3\\nx\\n2\\n1\\nx2\\n2\\n\u221a\\n2 x1x2\\n\u221a\\n2cx1\\n\u221a\\n2cx2\\nc\\n\u23a4\\n\u23a5\u23a5\u23a5\\n\u23a5\\n\u23a5\\n\u23a5\\n\u23a5\u23a5\u23a5\u23a6\\n\u00b7\\n\u23a1\\n\u23a2\u23a2\u23a2\\n\u23a2\\n\u23a2\\n\u23a2\\n\u23a2\u23a2\u23a2\u23a3\\nx\\n\u20322\\n1\\nx\u20322\\n2\u221a\\n2 x\u2032\\n1x\u2032\\n2\\n\u221a\\n2cx \u2032\\n1\u221a\\n2cx \u2032\\n2\\nc\\n\u23a4\\n\u23a5\u23a5\u23a5\\n\u23a5\\n\u23a5\\n\u23a5\\n', '\u23a5\u23a5\u23a5\u23a6\\n. (5.4)\\nThus, the features corresponding to a second-degree polynomial are the original\\nfeatures (x\\n1 and x2), as well as products of these features, and the constant feature.\\nMore generally, the', ' features associated to a polynomial kernel of degree d are all\\nthe monomials of degree at most d based on the original features. The explicit\\nexpression of polynomial kernels as inner products, as in', ' (5.4), proves directly that\\nthey are PDS kernels.\\nTo illustrate the application of polynomial kernels, consider the example of \ufb01g-\\nure 5.2a which shows a simple data set in dimension two that is not ', 'linearly sepa-\\nrable. This is known as the XOR problem due to its interpretation in terms of the\\nexclusive OR (XOR) function: the label of a point is blue i\ufb00 exactly one of its coor-\\ndinates is 1. How', 'ever, if we map these points to the six-dimensional space de\ufb01ned\\nby a second-degree polynomial as described in (5.4), then the problem becomes\\nseparable by the hyperplane of equation x\\n1x2 = 0. Figure', ' 5.2b illustrates that by\\nshowing the projection of these points on the two-dimensional space de\ufb01ned by their\\nthird and fourth coordinates.\\nExample 5.2 Gaussian kernels94 Kernel Methods\\nFor any consta', 'nt \u03c3> 0, a Gaussian kernel or radial basis function (RBF) is the\\nkernel K de\ufb01ned over RN by:\\n\u2200x,x\u2032 \u2208 RN ,K (x,x\u2032)=e x p\\n(\\n\u2212 \u2225x\u2032 \u2212 x\u22252\\n2\u03c32\\n\u23a1\\n. (5.5)\\nGaussians kernels are among the most frequently used', ' kernels in applications. We\\nwill prove in section 5.2.3 that they are PDS kernels and that they can be derived\\nby normalization from the kernels K\\n\u2032 :( x,x\u2032) \u21a6\u2192 exp\\n(x\u00b7x\u2032\\n\u03c32\\n\u23a1\\n. Using the power serie', 's\\nexpansion of the function exponential, we can rewrite the expression ofK\u2032 as follows:\\n\u2200x,x\u2032 \u2208 RN ,K \u2032(x,x\u2032)=\\n+\u221e\u2211\\nn=0\\n(x \u00b7 x\u2032)n\\n\u03c3n n! ,\\nwhich shows that the kernels K\u2032, and thus Gaussian kernels, are', ' positive linear\\ncombinations of polynomial kernels of all degrees n \u2265 0.\\nExample 5.3 Sigmoid kernels\\nFor any real constants a, b \u2265 0, a sigmoid kernel is the kernel K de\ufb01ned over RN\\nby:\\n\u2200x,x\u2032 \u2208 RN ,K', ' (x,x\u2032)=t a n h\\n(\\na(x \u00b7 x\u2032)+ b\\n\u23a1\\n. (5.6)\\nUsing sigmoid kernels with SVMs leads to an algorithm that is closely related to\\nlearning algorithms based on simple neural networks, which are also often de\ufb01n', 'ed\\nvia a sigmoid function. When a< 0o r b< 0, the kernel is not PDS and the\\ncorresponding neural network does not bene\ufb01t from the convergence guarantees of\\nconvex optimization (see exercise 5.15).\\n5.2', '.2 Reproducing kernel Hilbert space\\nHere, we prove the crucial property of PDS kernels, which is to induce an inner\\nproduct in a Hilbert space. The proof will make use of the following lemma.\\nLemma 5.', '1 Cauchy-Schwarz inequality for PDS kernels\\nLet K be a PDS kernel. Then, for any x, x\\n\u2032 \u2208X ,\\nK(x, x\u2032)2 \u2264 K(x, x)K(x\u2032,x \u2032). (5.7)\\nProof Consider the matrix K =\\n(\\nK(x,x) K(x,x\u2032)\\nK(x\u2032,x) K(x\u2032,x\u2032)\\n\u23a1\\n.B yd', ' e \ufb01 n i t i o n ,i fK is PDS,\\nthen K is SPSD for all x, x\u2032 \u2208X . In particular, the product of the eigenvalues of\\nK,d e t (K), must be non-negative, thus, using K(x\u2032,x)= K(x, x\u2032), we have\\ndet(K)= K(x,', ' x)K(x\u2032,x \u2032) \u2212 K(x, x\u2032)2 \u2265 0,5.2 Positive de\ufb01nite symmetric kernels 95\\nwhich concludes the proof.\\nT h ef o l l o w i n gi st h em a i nr e s u l to ft h i ss e c t i o n .\\nTheorem 5.2 Reproducing kern', 'el Hilbert space (RKHS)\\nLet K : X\u00d7 X \u2192 R be a PDS kernel. Then, there exists a Hilbert space H and a\\nmapping \u03a6 from X to H such that:\\n\u2200x, x\u2032 \u2208X ,K (x, x\u2032)= \u27e8\u03a6(x),\u03a6(x\u2032)\u27e9 . (5.8)\\nFurthermore, H has the ', 'following property known as the reproducing property:\\n\u2200h \u2208 H, \u2200x \u2208X ,h (x)= \u27e8h, K(x, \u00b7)\u27e9 . (5.9)\\nH is called a reproducing kernel Hilbert space (RKHS) associated to K.\\nProof For any x \u2208X ,d e \ufb01 n e\u03a6 (', 'x): X\u2192 R as follows:\\n\u2200x\u2032 \u2208X , \u03a6(x)(x\u2032)= K(x, x\u2032).\\nWe de\ufb01ne H0 as the set of \ufb01nite linear combinations of such functions \u03a6( x):\\nH0 =\\n{ \u2211\\ni\u2208I\\nai\u03a6(xi): ai \u2208 R,xi \u2208X ,card(I) < \u221e\\n}\\n.\\nNow, we introduce an ', 'operation \u27e8\u00b7, \u00b7\u27e9 on H0 \u00d7 H0 de\ufb01ned for all f,g \u2208 H0 with\\nf = \u2211\\ni\u2208I ai\u03a6(xi)a n dg = \u2211\\nj\u2208J bj\u03a6(xj)b y\\n\u27e8f,g \u27e9 =\\n\u2211\\ni\u2208I,j \u2208J\\naibjK(xi,x \u2032\\nj)=\\n\u2211\\nj\u2208J\\nbjf(x\u2032\\nj)=\\n\u2211\\ni\u2208I\\naig(xi).\\nBy de\ufb01nition, \u27e8\u00b7, \u00b7\u27e9 is symmetr', 'ic. The last two equations show that \u27e8f,g \u27e9 does not\\ndepend on the particular representations of f and g, and also show that \u27e8\u00b7, \u00b7\u27e9 is\\nbilinear. Further, for any f = \u2211\\ni\u2208I ai\u03a6(xi) \u2208 H0,s i n c eK is P', 'DS, we have\\n\u27e8f,f \u27e9 =\\n\u2211\\ni,j\u2208I\\naiajK(xi,xj) \u2265 0.\\nThus, \u27e8\u00b7, \u00b7\u27e9 is positive semide\ufb01nite bilinear form. This inequality implies more\\ngenerally using the bilinearity of \u27e8\u00b7, \u00b7\u27e9 that for any f1,...,f m and c1', ',...,c m \u2208 R,\\nm\u2211\\ni,j=1\\ncicj \u27e8fi,fj \u27e9 =\\n\u28e8 m\u2211\\ni=1\\ncifi,\\nm\u2211\\nj=1\\ncjfj\\n\u27e9\\n\u2265 0.\\nHence, \u27e8\u00b7, \u00b7\u27e9 is a PDS kernel on H0. Thus, for any f \u2208 H0 and any x \u2208X ,b y96 Kernel Methods\\nlemma 5.1, we can write\\n\u27e8f, \u03a6(x)\u27e92', ' \u2264\u27e8 f,f \u27e9\u27e8\u03a6(x), \u03a6(x)\u27e9.\\nFurther, we observe the reproducing property of \u27e8\u00b7, \u00b7\u27e9: for any f = \u2211\\ni\u2208I ai\u03a6(xi) \u2208\\nH0,b yd e \ufb01 n i t i o no f\u27e8\u00b7, \u00b7\u27e9,\\n\u2200x \u2208X ,f (x)=\\n\u2211\\ni\u2208I\\naiK(xi,x)= \u27e8f, \u03a6(x)\u27e9 . (5.10)\\nThus, [f(', 'x)]2 \u2264\u27e8 f,f \u27e9K(x, x) for all x \u2208X , which shows the de\ufb01niteness of \u27e8\u00b7, \u00b7\u27e9.\\nThis implies that \u27e8\u00b7, \u00b7\u27e9 de\ufb01nes an inner product on H0, which thereby becomes a\\npre-Hilbert space. H0 can be completed to for', 'm a Hilbert space H in which it is\\ndense, following a standard construction. By the Cauchy-Schwarz inequality , for\\nany x \u2208X , f \u21a6\u2192\u27e8 f, \u03a6(x)\u27e9 is Lipschitz, therefore continuous. Thus, sinceH0 is dense', '\\nin H, the reproducing property (5.10) also holds over H.\\nThe Hilbert space H de\ufb01ned in the proof of the theorem for a PDS kernelK is called\\nthe reproducing kernel Hilbert space (RKHS) associated toK.', ' Any Hilbert space H\\nsuch that there exists \u03a6: X\u2192 H with K(x, x\u2032)= \u27e8\u03a6(x), \u03a6(x\u2032)\u27e9 for all x, x\u2032 \u2208X\\nis called a feature space associated to K and \u03a6 is called a feature mapping.W e\\nwill denote by \u2225\u00b7\u2225 H t', 'he norm induced by the inner product in feature space H:\\n\u2225w\u2225H =\\n\u221a\\n\u27e8w,w\u27e9 for all w \u2208 H. Note that the feature spaces associated toK are in\\ngeneral not unique and may have di\ufb00erent dimensions. In practi', 'ce, when referring to\\nthe dimension of the feature spaceassociated to K, we either refer to the dimension\\nof the feature space based on a feature mapping described explicitly, or to that of\\nthe RKHS a', 'ssociated to K.\\nTheorem 5.2 implies that PDS kernels can be used to implicitly de\ufb01ne a feature\\nspace or feature vectors. As already underlined in previous chapters, the role played\\nby the features in ', 'the success of learning algorithms is crucial: with poor features,\\nuncorrelated with the target labels, learning could become very challenging or\\neven impossible; in contrast, good features could prov', 'ide invaluable clues to the\\nalgorithm. Therefore, in the context of learning with PDS kernels and for a \ufb01xed\\ninput space, the problem of seeking useful features is replaced by that of \ufb01nding\\nuseful PD', 'S kernels. While features represented the user\u2019s prior knowledge about the\\ntask in the standard learning problems, here PDS kernels will play this role. Thus,\\nin practice, an appropriate choice of PDS', ' kernel for a task will be crucial.\\n5.2.3 Properties\\nThis section highlights several important properties of PDS kernels. We \ufb01rst show\\nthat PDS kernels can be normalized and that the resulting normali', 'zed kernels are\\nalso PDS. We also introduce the de\ufb01nition of empirical kernel maps and describe5.2 Positive de\ufb01nite symmetric kernels 97\\ntheir properties and extension. We then prove several important', ' closure properties\\nof PDS kernels, which can be used to construct complex PDS kernels from simpler\\nones.\\nTo any kernel K, we can associate a normalized kernel K\\n\u2032 de\ufb01ned by\\n\u2200x, x\u2032 \u2208X ,K \u2032(x, x\u2032)=\\n\u23a7\\n\u23a8', '\\n\u23a9\\n0i f ( K(x, x)=0 ) \u2227 (K(x\u2032,x \u2032)=0 )\\nK(x,x\u2032)\u221a\\nK(x,x)K(x\u2032,x\u2032)\\notherwise.\\n(5.11)\\nBy de\ufb01nition, for a normalized kernel K\u2032, K\u2032(x, x) = 1 for all x \u2208X such that\\nK(x, x) \u0338= 0. An example of normalized ke', 'rnel is the Gaussian kernel with parameter\\n\u03c3> 0, which is the normalized kernel associated to K\u2032 :( x,x\u2032) \u21a6\u2192 exp\\n(x\u00b7x\u2032\\n\u03c32\\n\u23a1\\n:\\n\u2200x,x\u2032 \u2208 RN , K\u2032(x,x\u2032)\u221a\\nK\u2032(x,x)K\u2032(x\u2032,x\u2032)\\n= e\\nx\u00b7x\u2032\\n\u03c3 2\\ne\\n\u2225x\u22252\\n2\u03c3 2 e\\n\u2225x\u2032 \u22252\\n', '2\u03c3 2\\n=e x p\\n(\\n\u2212 \u2225x\u2032 \u2212 x\u2032\u22252\\n2\u03c32\\n\u23a1\\n. (5.12)\\nLemma 5.2 Normalized PDS kernels\\nLet K be a PDS kernel. Then, the normalized kernel K\u2032 associated to K is PDS.\\nProof Let {x1,...,x m}\u2286X and let c be an arbitr', 'ary vector inRm. We will show\\nthat the sum \u2211m\\ni,j=1 cicjK\u2032(xi,xj) is non-negative. By lemma 5.1, if K(xi,xi)=0\\nthen K(xi,xj)=0a n dt h u sK\u2032(xi,xj) = 0 for all j \u2208 [1,m]. Thus, we can assume\\nthat K(xi', ',xi) > 0 for all i \u2208 [1,m]. Then, the sum can be rewritten as follows:\\nm\u2211\\ni,j=1\\ncicjK(xi,xj)\u221a\\nK(xi,xi)K(xj,xj)\\n=\\nm\u2211\\ni,j=1\\ncicj \u27e8\u03a6(xi), \u03a6(xj)\u27e9\\n\u2225\u03a6(xi)\u2225H \u2225\u03a6(xj)\u2225H\\n=\\n\\ued79\\ued79\\n\\ued79\\ued79\\ued79\\nm\u2211\\ni=1\\nci\u03a6(xi)\\n\u2225\u03a6(xi)\u2225H\\n\\ued79\\ued79\\n\\ued79\\ued79\\ued79\\n', '2\\nH\\n\u2265 0,\\nwhere \u03a6 is a feature mapping associated to K, which exists by theorem 5.2.\\nAs indicated earlier, PDS kernels can be interpreted as a similarity measure since\\nthey induce an inner product in s', 'ome Hilbert space H.T h i si sm o r ee v i d e n tf o ra\\nnormalized kernel K since K(x, x\u2032) is then exactly the cosine of the angle between\\nthe feature vectors \u03a6(x)a n d\u03a6 (x\u2032), provided that none of t', 'hem is zero: \u03a6( x)a n d\\n\u03a6(x\u2032) are then unit vectors since \u2225\u03a6(x)\u2225H = \u2225\u03a6(x\u2032)\u2225H =\\n\u221a\\nK(x, x)=1 .\\nWhile one of the advantages of PDS kernels is an implicit de\ufb01nition of a feature\\nmapping, in some instances', ', it may be desirable to de\ufb01ne an explicit feature\\nmapping based on a PDS kernel. This may be to work in the primal for various\\noptimization and computational reasons, to derive an approximation based', ' on an\\nexplicit mapping, or as part of a theoretical analysis where an explicit mapping\\nis more convenient. The empirical kernel map \u03a6 associated to a PDS kernel K is\\na feature mapping that can be use', 'd precisely in such contexts. Given a training98 Kernel Methods\\nsample containing points x1,...,x m \u2208X ,\u03a6 : X\u2192 Rm is de\ufb01ned for all x \u2208X by\\n\u03a6(x)=\\n\u23a1\\n\u23a2\u23a2\u23a3\\nK(x, x1)\\n..\\n.\\nK(x, x\\nm)\\n\u23a4\\n\u23a5\u23a5\u23a6.\\nThus, \u03a6(x) is the', ' vector of theK-similarity measures of x with each of the training\\npoints. Let K be the kernel matrix associated to K and ei the ith unit vector.\\nNote that for any i \u2208 [1,m], \u03a6(xi)i st h eith column o', 'f K,t h a ti s\u03a6 (xi)= Kei.I n\\nparticular, for all i, j \u2208 [1,m],\\n\u27e8\u03a6(xi),\u03a6(xj)\u27e9 =( Kei)\u22a4(Kej)= e\u22a4\\ni K2ej.\\nThus, the kernel matrix K\u2032 associated to \u03a6 is K2. It may desirable in some cases\\nto de\ufb01ne a feat', 'ure mapping whose kernel matrix coincides with K.L e tK\u2020\\n1\\n2 denote\\nthe SPSD matrix whose square isK\u2020 , the pseudo-inverse ofK. K\u2020\\n1\\n2 can be derived\\nfrom K\u2020 via singular value decomposition and if th', 'e matrix K is invertible, K\u2020\\n1\\n2\\ncoincides with K\u22121/2 (see appendix A for properties of the pseudo-inverse). Then,\\n\u03a8 can be de\ufb01ned as follows using the empirical kernel map \u03a6:\\n\u2200x \u2208X , \u03a8(x)= K\u2020\\n1\\n2\\n\u03a6(x', ').\\nUsing the identityKK\u2020 K = K valid for any symmetric matrixK, for alli, j \u2208 [1,m],\\nthe following holds:\\n\u27e8\u03a8(xi), \u03a8(xj)\u27e9 =( K\u2020\\n1\\n2\\nKei)\u22a4(K\u2020\\n1\\n2\\nKej)= e\u22a4\\ni KK\u2020 Kej = e\u22a4\\ni Kej.\\nThus, the kernel matrix a', 'ssociated to \u03a8 is K. Finally, note that for the feature\\nmapping \u03a9: X\u2192 Rm de\ufb01ned by\\n\u2200x \u2208X , \u03a9(x)= K\u2020 \u03a6(x),\\nfor all i, j \u2208 [1,m], we have\u27e8\u03a9(xi), \u03a9(xj)\u27e9 = e\u22a4\\ni KK\u2020 K\u2020 Kej = e\u22a4\\ni KK\u2020 ej,u s i n gt h e\\nide', 'ntity K\u2020 K\u2020 K = K\u2020 valid for any symmetric matrix K.T h u s ,t h ek e r n e lm a t r i x\\nassociated to \u03a9 is KK\u2020 , which reduces to the identity matrixI \u2208 Rm\u00d7m when K is\\ninvertible, since K\u2020 = K\u22121 in t', 'hat case.\\nAs pointed out in the previous section, kernels represent the user\u2019s prior knowl-\\nedge about a task. In some cases, a user may come up with appropriate similarity\\nmeasures or PDS kernels for', ' some subtasks \u2014 for example, for di\ufb00erent subcat-\\negories of proteins or text documents to classify. But how can he combine these\\nPDS kernels to form a PDS kernel for the entire class? Is the resulti', 'ng combined\\nkernel guaranteed to be PDS? In the following, we will show that PDS kernels are\\nclosed under several useful operations which can be used to design complex PDS5.2 Positive de\ufb01nite symmetri', 'c kernels 99\\nk e r n e l s .T h e s eo p e r a t i o n sa r et h es u ma n dt h ep r o d u c to fk e r n e l s ,a sw e l la st h e\\ntensor product of two kernels K and K\u2032, denoted by K \u2297 K\u2032 and de\ufb01ned ', 'by\\n\u2200x1,x2,x \u2032\\n1,x \u2032\\n2\\n\u2208X , (K \u2297 K\u2032)(x1,x \u2032\\n1,x2,x \u2032\\n2\\n)= K(x1,x2)K\u2032(x\u2032\\n1,x \u2032\\n2\\n).\\nThey also include the pointwise limit: given a sequence of kernels (Kn)n\u2208N such that\\nfor all x, x\u2032 \u2208X (Kn(x, x\u2032))n\u2208N a', 'dmits a limit, the pointwise limit of ( Kn)n\u2208N is\\nthe kernel K de\ufb01ned for all x, x\u2032 \u2208X by K(x, x\u2032) = limn\u2192 +\u221e (Kn)(x, x\u2032). Similarly,\\nif \u2211\u221e\\nn=0 anxn is a power series with radius of convergence \u03c1> 0a ', 'n dK ak e r n e l\\ntaking values in (\u2212\u03c1,+\u03c1), then \u2211\u221e\\nn=0 anKn is the kernel obtained by composition\\nof K with that power series. The following theorem provides closure guarantees for\\nall of these opera', 'tions.\\nTheorem 5.3 PDS kernels \u2014 closure properties\\nPDS kernels are closed under sum, product, tensor product, pointwise limit, and\\ncomposition with a power series \u2211\\n\u221e\\nn=0 anxn with an \u2265 0 for all n \u2208', ' N.\\nProof We start with two kernel matrices,K and K\u2032, generated from PDS kernels\\nK and K\u2032 for an arbitrary set of m points. By assumption, these kernel matrices\\nare SPSD. Observe that for any c \u2208 Rm\u00d71', ',\\n(c\u22a4Kc \u2265 0) \u2227 (c\u22a4K\u2032c \u2265 0) \u21d2 c\u22a4(K + K\u2032)c \u2265 0.\\nBy (5.2), this shows that K + K\u2032 is SPSD and thus that K + K\u2032 is PDS. To show\\nclosure under product, we will use the fact that for any SPSD matrixK there ', 'exists\\nM such that K = MM\u22a4. The existence of M is guaranteed as it can be generated\\nvia, for instance, singular value decomposition ofK, or by Cholesky decomposition.\\nT h ek e r n e lm a t r i xa s s ', 'o c i a t e dt oKK \u2032 is (KijK\u2032\\nij)ij. For any c \u2208 Rm\u00d71,e x p r e s s i n g\\nKij in terms of the entries of M, we can write\\nm\u2211\\ni,j=1\\ncicj(KijK\u2032\\nij)=\\nm\u2211\\ni,j=1\\ncicj\\n([ m\u2211\\nk=1\\nMikMjk\\n]\\nK\u2032\\nij\\n\u23a1\\n=\\nm\u2211\\nk=1\\n[ m', '\u2211\\ni,j=1\\ncicjMikMjkK\u2032\\nij\\n]\\n=\\nm\u2211\\nk=1\\nz\u22a4\\nk K\u2032zk \u2265 0,\\nwith zk =\\n[ c1M1k..\\n.\\ncmMmk\\n]\\n. This shows that PDS kernels are closed under product.\\nThe tensor product of K and K\u2032 is PDS as the product of the two ', 'PDS kernels\\n(x1,x \u2032\\n1,x2,x \u2032\\n2\\n) \u21a6\u2192 K(x1,x2)a n d( x1,x \u2032\\n1,x2,x \u2032\\n2\\n) \u21a6\u2192 K\u2032(y1,y2). Next, let ( Kn)n\u2208N\\nbe a sequence of PDS kernels with pointwise limit K.L e tK be the kernel matrix100 Kernel Method', 's\\nassociated to K and Kn the one associated to Kn for any n \u2208 N.O b s e r v et h a t\\n(\u2200n,c\u22a4Knc \u2265 0) \u21d2 lim\\nn\u2192\u221e\\nc\u22a4Knc = c\u22a4Kc \u2265 0.\\nThis shows the closure under pointwise limit. Finally, assume that K is ', 'a PDS\\nkernel with |K(x, x\u2032)| <\u03c1 for all x, x\u2032 \u2208X and let f : x \u21a6\u2192 \u2211\u221e\\nn=0 anxn,an \u2265 0b ea\\npower series with radius of convergence\u03c1. Then, for any n \u2208 N, Kn and thus anKn\\nare PDS by closure under produc', 't. For any N \u2208 N, \u2211N\\nn=0 anKn is PDS by closure\\nunder sum of anKnsa n df \u25e6K is PDS by closure under the limit of \u2211N\\nn=0 anKn\\nas N tends to in\ufb01nity.\\nThe theorem implies in particular that for any PDS k', 'ernel matrix K,e x p (K)i s\\nPDS, since the radius of convergence of exp is in\ufb01nite. In particular, the kernel\\nK\\n\u2032 :( x,x\u2032) \u21a6\u2192 exp\\n(x\u00b7x\u2032\\n\u03c32\\n\u23a1\\nis PDS since (x,x\u2032) \u21a6\u2192 x\u00b7x\u2032\\n\u03c32 is PDS. Thus, by lemma 5.2,\\n', 'this shows that a Gaussian kernel, which is the normalized kernel associated toK\u2032,\\nis PDS.\\n5.3 Kernel-based algorithms\\nIn this section we discuss how SVMs can be used with kernels and analyze the\\nimpa', 'ct that kernels have on generalization.\\n5.3.1 SVMs with PDS kernels\\nIn chapter 4, we noted that the dual optimization problem for SVMs as well as the\\nform of the solution did not directly depend on th', 'e input vectors but only on inner\\nproducts. Since a PDS kernel implicitly de\ufb01nes an inner product (theorem 5.2), we\\ncan extend SVMs and combine it with an arbitrary PDS kernelK by replacing each\\ninsta', 'nce of an inner product x \u00b7x\\n\u2032 with K(x, x\u2032). This leads to the following general\\nform of the SVM optimization problem and solution with PDS kernels extending\\n(4.32):\\nmax\\n\u03b1\\nm\u2211\\ni=1\\n\u03b1i \u2212 1\\n2\\nm\u2211\\ni,j=1\\n\u03b1i', '\u03b1jyiyjK(xi,xj) (5.13)\\nsubject to: 0 \u2264 \u03b1i \u2264 C \u2227\\nm\u2211\\ni=1\\n\u03b1iyi =0 ,i \u2208 [1,m ].\\nIn view of (4.33), the hypothesis h solution can be written as:\\nh(x)=s g n\\n( m\u2211\\ni=1\\n\u03b1iyiK(xi,x)+ b\\n\u23a1\\n, (5.14)5.3 Kernel-based', ' algorithms 101\\nwith b = yi \u2212 \u2211m\\nj=1 \u03b1jyjK(xj,xi) for any xi with 0 <\u03b1 i <C . We can rewrite\\nthe optimization problem (5.13) in a vector form, by using the kernel matrix K\\nassociated to K for the trai', 'ning sample (x1,...,x m) as follows:\\nmax\\n\u03b1\\n2 1\u22a4\u03b1 \u2212 (\u03b1 \u25e6y)\u22a4K(\u03b1 \u25e6y) (5.15)\\nsubject to: 0 \u2264 \u03b1 \u2264 C \u2227 \u03b1\u22a4y =0 .\\nIn this formulation, \u03b1 \u25e6y is the Hadamard product or entry-wise product of the\\nvectors \u03b1 and y', '. Thus, it is the column vector in Rm\u00d71 whose ith component\\nequals \u03b1iyi. The solution in vector form is the same as in (5.14), but with b =\\nyi \u2212 (\u03b1 \u25e6y)\u22a4Kei for any xi with 0 <\u03b1 i <C .\\nThis version of ', 'SVMs used with PDS kernels is the general form of SVMs we\\nwill consider in all that follows. The extension is important, since it enables an\\nimplicit non-linear mapping of the input points to a high-d', 'imensional space where\\nlarge-margin separation is sought.\\nMany other algorithms in areas including regression, ranking, dimensionality\\nreduction or clustering can be extended using PDS kernels followi', 'ng the same\\nscheme (see in particular chapters 8, 9, 10, 12).\\n5.3.2 Representer theorem\\nObserve that modulo the o\ufb00set b, the hypothesis solution of SVMs can be written\\nas a linear combination of the f', 'unctions K(x\\ni, \u00b7), where xi is a sample point. The\\nfollowing theorem known as the representer theorem s h o w st h a tt h i si si nf a c ta\\ngeneral property that holds for a broad class of optimizati', 'on problems, including\\nthat of SVMs with no o\ufb00set.\\nTheorem 5.4 Representer theorem\\nLet K : X\u00d7 X \u2192 R be a PDS kernel and H its corresponding RKHS. Then, for any\\nnon-decreasing function G: R \u2192 R and any', ' loss function L: R\\nm \u2192 R \u222a{+\u221e} ,t h e\\noptimization problem\\nargmin\\nh\u2208H\\nF(h) = argmin\\nh\u2208H\\nG(\u2225h\u2225H)+ L\\n(\\nh(x1),...,h (xm)\\n\u23a1\\nadmits a solution of the form h\u2217 = \u2211m\\ni=1 \u03b1iK(xi, \u00b7).I f G is further assumed t', 'o be\\nincreasing, then any solution has this form.\\nProof Let H1 =s p a n ({K(xi, \u00b7): i \u2208 [1,m]}). Anyh \u2208 H admits the decomposition\\nh = h1 + h\u22a5 according to H = H1 \u2295 H\u22a5\\n1 ,w h e r e\u2295 is the direct sum.', ' Since G is\\nnon-decreasing, G(\u2225h1\u2225H) \u2264 G(\\n\u221a\\n\u2225h1\u22252\\nH + \u2225h\u22a5 \u22252\\nH)= G(\u2225h\u2225H). By the reproducing\\nproperty, for all i \u2208 [1,m], h(xi)= \u27e8h, K(xi, \u00b7)\u27e9 = \u27e8h1,K (xi, \u00b7)\u27e9 = h1(xi). Thus,\\nL\\n(\\nh(x1),...,h (xm)\\n\u23a1\\n=', ' L\\n(\\nh1(x1),...,h 1(xm)\\n\u23a1\\nand F(h1) \u2264 F(h). This proves the102 Kernel Methods\\n\ufb01rst part of the theorem. If G is further increasing, then F(h1) <F (h)w h e n\\n\u2225h\u22a5 \u2225H > 0 and any solution of the optimiza', 'tion problem must be in H1.\\n5.3.3 Learning guarantees\\nHere, we present general learning guarantees for hypothesis sets based on PDS\\nkernels, which hold in particular for SVMs combined with PDS kernels', '.\\nThe following theorem gives a general bound on the empirical Rademacher\\ncomplexity of kernel-based hypotheses with bounded norm, that is a hypothesis\\nset of the form H = {h \u2208 H: \u2225h\u2225\\nH \u2264 \u039b}, for some', ' \u039b \u2265 0, where H is the\\nRKHS associated to a kernel K.B yt h er e p r o d u c i n gp r o p e r t y ,a n yh \u2208 H is of\\nthe form x \u21a6\u2192\u27e8 h, K(x, \u00b7)\u27e9 = \u27e8h, \u03a6(x)\u27e9 with \u2225h\u2225H \u2264 \u039b, where \u03a6 is a feature mapping\\na', 'ssociated to K, that is of the form x \u21a6\u2192\u27e8 w, \u03a6(x)\u27e9 with \u2225w\u2225H \u2264 \u039b.\\nTheorem 5.5 Rademacher complexity of kernel-based hypotheses\\nLet K : X\u00d7 X \u2192 R be a PDS kernel and let \u03a6: X\u2192 H be a feature mapping\\nass', 'ociated to K.L e t S \u2286{ x: K(x, x) \u2264 r2} be a sample of size m,a n dl e t\\nH = {x \u21a6\u2192 w \u00b7 \u03a6(x): \u2225w\u2225H \u2264 \u039b} for some \u039b \u2265 0.T h e n\\n\u02c6RS(H) \u2264 \u039b\\n\u221a\\nTr[K]\\nm \u2264\\n\u221a\\nr2\u039b2\\nm . (5.16)\\nProof The proof steps are as fol', 'lows:\\n\u02c6RS(H)= 1\\nm E\\n\u03c3\\n[\\nsup\\n\u2225w\u2225\u2264\u039b\\n\u28e8\\nw,\\nm\u2211\\ni=1\\n\u03c3i\u03a6(xi)\\n\u27e9]\\n= \u039b\\nm E\\n\u03c3\\n[\\ued79\\ued79\\ued79\\nm\u2211\\ni=1\\n\u03c3i\u03a6(xi)\\n\\ued79\\ued79\\ued79\\nH\\n]\\n(Cauchy-Schwarz , eq. case)\\n\u2264 \u039b\\nm\\n[\\nE\\n\u03c3\\n[\\ued79\\ued79\\n\\ued79\\nm\u2211\\ni=1\\n\u03c3i\u03a6(xi)\\n\\ued79\\ued79\\n\\ued79\\n2\\nH\\n]]1/2\\n(Jensen\u2019s ineq.)\\n= \u039b\\nm\\n[\\nE\\n\u03c3\\n', '[ m\u2211\\ni=1\\n\u2225\u03a6(xi)\u22252\\nH\\n]]1/2\\n(i \u0338= j \u21d2 E\\n\u03c3\\n[\u03c3i\u03c3j]=0 )\\n= \u039b\\nm\\n[\\nE\\n\u03c3\\n[ m\u2211\\ni=1\\nK(xi,xi)\\n]]1/2\\n= \u039b\\n\u221a\\nTr[K]\\nm \u2264\\n\u221a\\nr2\u039b2\\nm .\\nThe initial equality holds by de\ufb01nition of the empirical Rademacher complexity\\n(de\ufb01nit', 'ion 3.2). The \ufb01rst inequality is due to the Cauchy-Schwarz inequality and\\n\u2225w\u2225\\nH \u2264 \u039b. The following inequality results from Jensen\u2019s inequality (theorem B.4)\\napplied to the concave function \u221a \u00b7. The su', 'bsequent equality is a consequence of5.4 Negative de\ufb01nite symmetric kernels 103\\nE\u03c3 [\u03c3i\u03c3j]=E \u03c3 [\u03c3i]E\u03c3 [\u03c3j]=0f o r i \u0338= j, since the Rademacher variables \u03c3i and\\n\u03c3j are independent. The statement of the ', 'theorem then follows by noting that\\nTr[K] \u2264 mr2.\\nThe theorem indicates that the trace of the kernel matrix is an important quantity\\nfor controlling the complexity of hypothesis sets based on kernels. ', 'Observe that\\nby the Khintchine-Kahane inequality (D.22), the empirical Rademacher complexity\\n\u02c6R\\nS(H)= \u039b\\nm E\u03c3 [\u2225 \u2211m\\ni=1 \u03c3i\u03a6(xi)\u2225H] can also be lower bounded by 1\u221a\\n2\\n\u039b\\n\u221a\\nTr[K]\\nm ,w h i c h\\nonly di\ufb00ers f', 'rom the upper bound found by the constant 1\u221a\\n2 . Also, note that if\\nK(x, x) \u2264 r2 for all x \u2208X , then the inequalities 5.16 hold for all samples S.\\nThe bound of theorem 5.5 or the inequalities 5.16 can', ' be plugged into any of the\\nRademacher complexity generalization bounds presented in the previous chapters.\\nIn particular, in combination with theorem 4.4, they lead directly to the following\\nmargin b', 'ound similar to that of corollary 4.1.\\nCorollary 5.1 Margin bounds for kernel-based hypotheses\\nLet K : X\u00d7 X\u2192 R be a PDS kernel with r =s u px\u2208X K(x, x).L e t\u03a6: X\u2192 H be a\\nfeature mapping associated toK', ' and let H = {x \u21a6\u2192 w \u00b7 \u03a6(x): \u2225w\u2225H \u2264 \u039b} for some\\n\u039b \u2265 0.F i x\u03c1> 0.T h e n ,f o ra n y\u03b4> 0, each of the following statements holds with\\nprobability at least 1 \u2212 \u03b4for any h \u2208 H:\\nR(h) \u2264 \u02c6R\u03c1(h)+2\\n\u221a\\nr2\u039b2/\u03c12\\n', 'm +\\n\u221a\\nlog 1\\n\u03b4\\n2m (5.17)\\nR(h) \u2264 \u02c6R\u03c1(h)+2\\n\u221a\\nTr[K]\u039b2/\u03c12\\nm +3\\n\u221a\\nlog 2\\n\u03b4\\n2m . (5.18)\\n5.4 Negative de\ufb01nite symmetric kernels\\nOften in practice, a natural distance or metric is available for the learning tas', 'k\\nconsidered. This metric could be used to de\ufb01ne a similarity measure. As an example,\\nGaussian kernels have the form exp(\u2212d\\n2), where d i sam e t r i cf o rt h ei n p u tv e c t o r\\nspace. Several nat', 'ural questions arise such as: what other PDS kernels can we\\nconstruct from a metric in a Hilbert space? What technical condition should d\\nsatisfy to guarantee that exp(\u2212d\\n2) is PDS? A natural mathemat', 'ical de\ufb01nition that\\nhelps address these questions is that of negative de\ufb01nite symmetric (NDS) kernels .\\nDe\ufb01nition 5.3 Negative de\ufb01nite symmetric (NDS) kernels\\nA kernel K : X\u00d7 X \u2192 R is said to be negat', 'ive-de\ufb01nite symmetric (NDS) if it\\nis symmetric and if for all {x1,...,x m}\u2286X and c \u2208 Rm\u00d71 with 1\u22a4c =0 ,t h e104 Kernel Methods\\nfollowing holds:\\nc\u22a4Kc \u2264 0.\\nClearly, if K is PDS, then \u2212K is NDS, but the ', 'converse does not hold in general.\\nThe following gives a standard example of an NDS kernel.\\nExample 5.4 Squared distance \u2014 NDS kernel\\nThe squared distance (x, x\u2032) \u21a6\u2192\u2225 x\u2032 \u2212 x\u22252 in RN de\ufb01nes an NDS kern', 'el. Indeed, let\\nc \u2208 Rm\u00d71 with \u2211m\\ni=1 ci = 0. Then, for any {x1,...,x m}\u2286X , we can write\\nm\u2211\\ni,j=1\\ncicj ||xi \u2212 xj ||2 =\\nm\u2211\\ni,j=1\\ncicj(\u2225xi\u22252 + \u2225xj \u22252 \u2212 2xi \u00b7 xj)\\n=\\nm\u2211\\ni,j=1\\ncicj(\u2225xi\u22252 + \u2225xj \u22252) \u2212 2\\nm\u2211\\ni', '=1\\ncixi \u00b7\\nm\u2211\\nj=1\\ncjxj\\n=\\nm\u2211\\ni,j=1\\ncicj(\u2225xi\u22252 + \u2225xj \u22252) \u2212 2\\n\\ued79\\ued79\\nm\u2211\\ni=1\\ncixi\\n\\ued79\\ued792\\n\u2264\\nm\u2211\\ni,j=1\\ncicj(\u2225xi\u22252 + \u2225xj \u22252)\\n=\\n( m\u2211\\nj=1\\ncj\\n\u23a1( m\u2211\\ni=1\\nci(\u2225xi\u22252\\n\u23a1\\n+\\n( m\u2211\\ni=1\\nci\\n\u23a1( m\u2211\\nj=1\\ncj \u2225xj \u22252\\n\u23a1\\n=0 .\\nThe next theore', 'ms show connections between NDS and PDS kernels. These\\nresults provide another series of tools for designing PDS kernels.\\nTheorem 5.6\\nLet K\u2032 be de\ufb01ned for any x0 by\\nK\u2032(x, x\u2032)= K(x, x0)+ K(x\u2032,x0) \u2212 K(x', ', x\u2032) \u2212 K(x0,x0)\\nfor all x, x\u2032 \u2208X .T h e nK is NDS i\ufb00 K\u2032 is PDS.\\nProof Assume that K\u2032 is PDS and de\ufb01ne K such that for any x0 we have\\nK(x, x\u2032)= K(x, x0)+ K(x0,x \u2032) \u2212 K(x0,x0) \u2212 K\u2032(x, x\u2032). Then for any', ' c \u2208 Rm\\nsuch that c\u22a41 =0a n da n ys e to fp o i n t s(x1,...,x m) \u2208X m we have\\nm\u2211\\ni,j=1\\ncicjK(xi,xj)=\\n( m\u2211\\ni=1\\nciK(xi,x0)\\n\u23a1( m\u2211\\nj=1\\ncj\\n\u23a1\\n+\\n( m\u2211\\ni=1\\nci\\n\u23a1( m\u2211\\nj=1\\ncjK(x0,xj)\\n\u23a1\\n\u2212\\n( m\u2211\\ni=1\\nci\\n\u23a12\\nK(x0,x0) ', '\u2212\\nm\u2211\\ni,j=1\\ncicjK\u2032(xi,xj)= \u2212\\nm\u2211\\ni,j=1\\ncicjK\u2032(xi,xj) \u2264 0 .5.4 Negative de\ufb01nite symmetric kernels 105\\nwhich proves K is NDS.\\nNow, assume K is NDS and de\ufb01ne K\u2032 for any x0 as above. Then, for anyc \u2208 Rm,\\nwe', ' can de\ufb01ne c0 = \u2212c\u22a41 and the following holds by the NDS property for any points\\n(x1,...,x m) \u2208X m as well as x0 de\ufb01ned previously: \u2211m\\ni,j=0 cicjK(xi,xj) \u2264 0. This\\nimplies that\\n( m\u2211\\ni=0\\nciK(xi,x0)\\n\u23a1( m', '\u2211\\nj=0\\ncj\\n\u23a1\\n+\\n( m\u2211\\ni=0\\nci\\n\u23a1( m\u2211\\nj=0\\ncjK(x0,xj)\\n\u23a1\\n\u2212\\n( m\u2211\\ni=0\\nci\\n\u23a12\\nK(x0,x0) \u2212\\nm\u2211\\ni,j=0\\ncicjK\u2032(xi,xj)= \u2212\\nm\u2211\\ni,j=0\\ncicjK\u2032(xi,xj) \u2264 0 ,\\nwhich implies 2 \u2211m\\ni,j=1 cicjK\u2032(xi,xj) \u2265\u2212 2c0\\n\u2211m\\ni=0 ciK\u2032(xi,x0)+ c2\\n', '0K\u2032(x0,x0)=0 .\\nThe equality holds since \u2200x \u2208X ,K \u2032(x, x0)=0 .\\nThis theorem is useful in showing other connections, such the following theorems,\\nwhich are left as exercises (see exercises 5.14 and 5.15', ').\\nTheorem 5.7\\nLet K : X\u00d7 X \u2192 R be a symmetric kernel. Then,K is NDS i\ufb00exp(\u2212tK) is a PDS\\nkernel for all t> 0.\\nThe theorem provides another proof that Gaussian kernels are PDS: as seen earlier\\n(Example', ' 5.4), the squared distance ( x, x\u2032) \u21a6\u2192\u2225 x \u2212 x\u2032\u22252 in RN is NDS, thus\\n(x, x\u2032) \u21a6\u2192 exp(\u2212t||x \u2212 x\u2032||2)i sP D Sf o ra l lt> 0.\\nTheorem 5.8\\nLet K : X\u00d7 X\u2192 R be an NDS kernel such that for all x, x\u2032 \u2208X ,K (x,', ' x\u2032)=0 i\ufb00\\nx = x\u2032. Then, there exists a Hilbert space H and a mapping \u03a6: X\u2192 H such that\\nfor all x, x\u2032 \u2208X ,\\nK(x, x\u2032)= \u2225\u03a6(x) \u2212 \u03a6(x\u2032)\u22252.\\nThus, under the hypothesis of the theorem,\\n\u221a\\nK de\ufb01nes a metric.\\nThi', 's theorem can be used to show that the kernel (x, x\u2032) \u21a6\u2192 exp(\u2212|x \u2212 x\u2032|p)i n R\\nis not PDS for p> 2. Otherwise, for any t> 0, {x1,...,x m}\u2286X and c \u2208 Rm\u00d71,\\nwe would have:\\nm\u2211\\ni,j=1\\ncicje\u2212t|xi\u2212xj |p\\n=\\nm\u2211\\ni', ',j=1\\ncicje\u2212|t1/pxi\u2212t1/pxj |p\\n\u2265 0.\\nThis would imply that (x, x\u2032) \u21a6\u2192| x \u2212 x\u2032|p is NDS for p> 2, which can be proven\\n(via theorem 5.8) not to be valid.106 Kernel Methods\\n5.5 Sequence kernels\\nThe examples', ' given in the previous sections, including the commonly used poly-\\nnomial or Gaussian kernels, were all for PDS kernels over vector spaces. In many\\nlearning tasks found in practice, the input space X ', 'is not a vector space. The ex-\\namples to classify in practice could be protein sequences, images, graphs, parse\\ntrees, \ufb01nite automata, or other discrete structures which may not be directly given\\nas v', 'ectors. PDS kernels provide a method for extending algorithms such as SVMs\\noriginally designed for a vectorial space to the classi\ufb01cation of such objects. But,\\nh o wc a nw ed e \ufb01 n eP D Sk e r n e l s', 'f o rt h e s es t r u c t u r e s ?\\nThis section will focus on the speci\ufb01c case of sequence kernels,t h a t i s , k e r n e l s\\nfor sequences or strings. PDS kernels can be de\ufb01ned for other discrete s', 'tructures\\nin somewhat similar ways. Sequence kernels are particularly relevant to learning\\nalgorithms applied to computational biology or natural language processing, which\\nare both important applicat', 'ions.\\nHow can we de\ufb01ne PDS kernels for sequences, which are similarity measures for\\nsequences? One idea consists of declaring two sequences, e.g., two documents or\\ntwo biosequences, as similar when th', 'ey share common substrings or subsequences.\\nOne example could be the kernel between two sequences de\ufb01ned by the sum\\nof the product of the counts of their common substrings. But which substrings\\nshould', ' be used in that de\ufb01nition? Most likely, we would need some \ufb02exibility in\\nthe de\ufb01nition of the matching substrings. For computational biology applications,\\nfor example, the match could be imperfect. T', 'hus, we may need to consider some\\nnumber of mismatches, possibly gaps, or wildcards. More generally, we might need\\nto allow various substitutions and might wish to assign di\ufb00erent weights to common\\nsu', 'bstrings to emphasize some matching substrings and deemphasize others.\\nAs can be seen from this discussion, there are many di\ufb00erent possibilities and\\nwe need a general framework for de\ufb01ning such kerne', 'ls. In the following, we will\\nintroduce a general framework for sequence kernels, rational kernels, which will\\ninclude all the kernels considered in this discussion. We will also describe a general\\nan', 'd e\ufb03cient algorithm for their computation and will illustrate them with some\\nexamples.\\nThe de\ufb01nition of these kernels relies on that of weighted transducers.T h u s ,w e\\nstart with the de\ufb01nition of th', 'ese devices as well as some relevant algorithms.\\n5.5.1 Weighted transducers\\nSequence kernels can be e\ufb00ectively represented and computed usingweighted trans-\\nducers. In the following de\ufb01nition, let \u03a3 d', 'enote a \ufb01nite input alphabet, \u0394 a \ufb01nite\\noutput alphabet, and \u03f5 the empty string or null label, whose concatenation with5.5 Sequence kernels 107\\n2/8 b:b/2\\n0\\nb:b/2\\n3/2\\nb:a/3\\n1\\na:b/3\\na:a/2\\nb:a/4\\na:a/1\\nFi', 'gure 5.3 Example of weighted transducer.\\nany string leaves it unchanged.\\nDe\ufb01nition 5.4\\nA weighted transducer T is a 7-tuple T =( \u03a3, \u0394 ,Q ,I,F,E,\u03c1 ) where \u03a3 is a \ufb01nite\\ninput alphabet, \u0394 a \ufb01nite output ', 'alphabet, Q is a \ufb01nite set of states, I \u2286 Q the\\nset of initial states, F \u2286 Q the set of \ufb01nal states, E a \ufb01nite multiset of transitions\\nelements of Q \u00d7(\u03a3 \u222a{\u03f5}) \u00d7(\u0394 \u222a{\u03f5}) \u00d7 R \u00d7Q,a n d\u03c1: F \u2192 R a \ufb01nal wei', 'ght function\\nmapping F to R.T h esize of transducer T is the sum of its number of states and\\ntransitions and is denoted by |T |.2\\nThus, weighted transducers are \ufb01nite automata in which each transition', ' is labeled\\nwith both an input and an output label and carries some real-valued weight.\\nFigure 5.3 shows an example of a weighted \ufb01nite-state transducer. In this \ufb01gure,\\nthe input and output labels of ', 'a transition are separated by a colon delimiter, and\\nthe weight is indicated after the slash separator. The initial states are represented\\nby a bold circle and \ufb01nal states by double circles. The \ufb01nal ', 'weight \u03c1[q]a ta\ufb01 n a l\\nstate q is displayed after the slash.\\nThe input label of a path \u03c0 is a string element of \u03a3\\n\u2217 obtained by concatenating\\ninput labels along \u03c0. Similarly, the output label of a pat', 'h \u03c0 is obtained by\\nconcatenating output labels along \u03c0. A path from an initial state to a \ufb01nal state is\\nan accepting path. The weight of an accepting path is obtained by multiplying the\\nweights of its', ' constituent transitions and the weight of the \ufb01nal state of the path.\\nA weighted transducer de\ufb01nes a mapping from \u03a3 \u2217 \u00d7 \u0394 \u2217 to R.T h ew e i g h t\\nassociated by a weighted transducer T to a pair of st', 'rings ( x, y) \u2208 \u03a3\u2217 \u00d7 \u0394 \u2217 is\\ndenoted by T(x, y) and is obtained by summing the weights of all accepting paths\\n2. A multiset in the de\ufb01nition of the transitions is used to allow for the presence of seve', 'ral\\ntransitions from a state p to a state q with the same input and output label, and even the\\nsame weight, which may occur as a result of various operations.108 Kernel Methods\\nwith input label x and ', 'output label y. For example, the transducer of \ufb01gure 5.3\\nassociates to the pair (aab, baa)t h ew e i g h t3\u00d7 1 \u00d7 4 \u00d7 2+3 \u00d7 2 \u00d7 3 \u00d7 2, since there\\nis a path with input label aab and output label baa an', 'd weight 3 \u00d7 1 \u00d7 4 \u00d7 2, and\\nanother one with weight 3 \u00d7 2 \u00d7 3 \u00d7 2.\\nThe sum of the weights of all accepting paths of an acyclic transducer, that\\nis a transducer T with no cycle, can be computed in line', 'ar time, that is O(|T |),\\nusing a general shortest-distance or forward-backward algorithm. These are simple\\nalgorithms, but a detailed description would require too much of a digression from\\nthe main ', 'topic of this chapter.\\nComposition An important operation for weighted transducers is composition,\\nwhich can be used to combine two or more weighted transducers to form more\\ncomplex weighted transduce', 'rs. As we shall see, this operation is useful for the\\ncreation and computation of sequence kernels. Its de\ufb01nition follows that of com-\\nposition of relations. Given two weighted transducersT\\n1 =( \u03a3, \u0394 ', ',Q1,I1,F1,E1,\u03c11)\\nand T2 =( \u0394 , \u03a9,Q2,I2,F2,E2,\u03c12), the result of the composition of T1 and T2 is a\\nweighted transducer denoted by T1 \u25e6T2 and de\ufb01ned for all x \u2208 \u03a3\u2217 and y \u2208 \u03a9\u2217 by\\n(T1 \u25e6T2)(x, y)=\\n\u2211\\nz\u2208\u0394 \u2217\\n', 'T1(x, z) \u00b7 T2(z,y ), (5.19)\\nwhere the sum runs over all strings z over the alphabet \u0394. Thus, composition is\\nsimilar to matrix multiplication with in\ufb01nite matrices.\\nThere exists a general and e\ufb03cient a', 'lgorithm to compute the composition of two\\nweighted transducers. In the absence of \u03f5s on the input side of T1 or the output\\nside of T2, the states of T1 \u25e6T2 =( \u03a3, \u0394 ,Q ,I,F,E,\u03c1 ) can be identi\ufb01ed with', ' pairs\\nmade of a state of T1 and a state of T2, Q \u2286 Q1 \u00d7 Q2. Initial states are those\\nobtained by pairing initial states of the original transducers, I = I1 \u00d7 I2,a n d\\nsimilarly \ufb01nal states are de\ufb01ned', ' by F = Q \u2229 (F1 \u00d7 F2). The \ufb01nal weight at a state\\n(q1,q2) \u2208 F1 \u00d7 F2 is \u03c1(q)= \u03c11(q1)\u03c12(q2), that is the product of the \ufb01nal weights at\\nq1 and q2. Transitions are obtained by matching a transition of T1', ' with one of T2\\nfrom appropriate transitions of T1 and T2:\\nE =\\n\u2a04\\n(q1,a,b,w1,q2)\u2208E1\\n(q\u2032\\n1,b,c,w2,q\u2032\\n2)\u2208E2\\n{(\\n(q1,q \u2032\\n1),a ,c ,w1 \u2297 w2,(q2,q \u2032\\n2)\\n\u23a1}\\n.\\nHere, \u228e denotes the standard join operation of mult', 'isets as in {1, 2}\u228e{ 1, 3} =\\n{1,1,2, 3}, to preserve the multiplicity of the transitions.\\nIn the worst case, all transitions of T1 leaving a state q1 match all those of T2\\nleaving state q\u2032\\n1, thus the', ' space and time complexity of composition is quadratic:\\nO(|T1||T2|). In practice, such cases are rare and composition is very e\ufb03cient.\\nFigure 5.4 illustrates the algorithm in a particular case.5.5 Seq', 'uence kernels 109\\n0\\n1\\na:b/0.1\\na:b/0.2\\n2\\nb:b/0.3 3/0.7\\nb:b/0.4\\n          a:b/0.5\\na:a/0.6\\n0\\n1\\nb:b/0.1\\nb:a/0.2\\n2\\na:b/0.3 3/0.6\\na:b/0.4\\nb:a/0.5\\n(a) (b)\\n(0, 0) (1, 1)a:b/.01\\n(0, 1)\\na:a/.04\\n(2, 1)\\nb:a/.06 (', '3, 1)\\nb:a/.08\\na:a/.02\\na:a/0.1\\n(3, 2)\\na:b/.18\\n(3, 3)a:b/.24\\n(c)\\nFigure 5.4 (a) Weighted transducer T1. (b) Weighted transducer T2. (c) Result\\nof composition of T1 and T2, T1 \u25e6T2. Some states might be c', 'onstructed during the\\nexecution of the algorithm that are not co-accessible, that is, they do not admit a\\npath to a \ufb01nal state, e.g., (3, 2). Such states and the related transitions (in red) can\\nbe re', 'moved by a trimming (or connection) algorithm in linear time.\\nAs illustrated by \ufb01gure 5.5, when T1 admits output \u03f5 labels or T2 input \u03f5 labels,\\nthe algorithm just described may create redundant \u03f5-path', 's, which would lead to\\nan incorrect result. The weight of the matching paths of the original transducers\\nwould be counted p times, where p is the number of redundant paths in the result\\nof composition', '. To avoid with this problem, all but one\u03f5-path must be \ufb01ltered out\\nof the composite transducer. Figure 5.5 indicates in boldface one possible choice for\\nthat path, which in this case is the shortest.', ' Remarkably, that \ufb01ltering mechanism\\nitself can be encoded as a \ufb01nite-state transducer F (\ufb01gure 5.5b).\\nT oa p p l yt h a t\ufb01 l t e r ,w en e e dt o\ufb01 r s ta u g m e n tT\\n1 and T2 with auxiliary symbols\\n', 'that make the semantics of\u03f5 explicit: let \u02dcT1 ( \u02dcT2) be the weighted transducer obtained\\nfrom T1 (respectively T2) by replacing the output (respectively input)\u03f5 labels with\\n\u03f52 (respectively \u03f51) as ill', 'ustrated by \ufb01gure 5.5. Thus, matching with the symbol \u03f51\\ncorresponds to remaining at the same state ofT1 and taking a transition ofT2 with\\ninput \u03f5. \u03f52 can be described in a symmetric way. The \ufb01lter tr', 'ansducerF disallows a\\nmatching (\u03f52,\u03f52) immediately after (\u03f51,\u03f51) since this can be done instead via (\u03f52,\u03f51).110 Kernel Methods\\n/g1 /g2/g7/g6/g7 /g3/g8/g6/g1 /g4/g9/g6/g1 /g5/g10/g6/g10 /g1 /g2/g6/g5/g', '7 /g3/g1/g1/g8 /g4/g7/g5/g6\\nT1 T2\\n/g1\\n/g1/g3/g1/g1/g1/g3/g1/g1\\n/g2/g7/g6/g7\\n/g1/g3/g1/g1/g1/g3/g1/g1\\n/g3/g8/g6/g1/g2\\n/g1/g3/g1/g1/g1/g3/g1/g1\\n/g4/g9/g6/g1/g2\\n/g1/g3/g1/g1/g1/g3/g1/g1\\n/g5/g10/g6/g10\\n/g', '1/g3/g1/g1/g1/g3/g1/g1\\n/g2\\n/g1/g3/g4/g1\\n/g3/g7/g6/g8\\n/g1/g3/g4/g1\\n/g4/g1/g2/g4/g1/g1/g1/g1/g1/g1/g1/g9\\n/g1/g3/g4/g1\\n/g5/g8/g6/g7\\n/g1/g3/g4/g1\\n\u02dcT1 \u02dcT2\\n/g4/g3/g4 /g5/g3/g5 /g5/g3/g6\\n/g6/g3/g5 /g6/g3/g6\\n', '/g7/g3/g5 /g7/g3/g6\\n/g8/g3/g7\\n/g10/g9/g13 /g1/g9/g14\\n/g11/g9/g1\\n/g12/g9/g1\\n/g11/g9/g1\\n/g12/g9/g1\\n/g1/g9/g14\\n/g1/g9/g14\\n/g13/g9/g10\\n/g11/g9/g14\\n/g1/g15/g9/g15/g2 /g1/g1/g2/g4/g1/g2/g2\\n/g1/g1/g2/g4/g1/g', '2/g2\\n/g1/g1/g2/g4/g1/g2/g2\\n/g1/g1/g3/g4/g1/g3/g2/g1/g1/g3/g4/g1/g3/g2\\n/g1/g1/g3/g4/g1/g3/g2 /g1/g1/g3/g4/g1/g3/g2\\n/g1/g15/g9/g15/g2\\n/g1/g1/g3/g4/g1/g2/g1\\n/g1\\n/g5/g4/g5\\n/g1/g2/g3/g1/g1 /g2/g1/g1/g3/g1/', 'g1\\n/g3\\n/g1/g2/g3/g1/g2\\n/g5/g4/g5\\n/g1/g1/g3/g1/g1\\n/g5/g4/g5\\n/g1/g2/g3/g1/g2\\n(a) (b)\\nFigure 5.5 Redundant \u03f5-paths in composition. All transition and \ufb01nal weights are\\nequal to one. (a) A straightforward ', 'generalization of the\u03f5-free case would generate\\nall the paths from(1, 1) to (3, 2) when composing T1 and T2 and produce an incorrect\\nresults in non-idempotent semirings. (b) Filter transducer F. The s', 'horthand x is\\nused to represent an element of \u03a3.\\nBy symmetry, it also disallows a matching (\u03f51,\u03f51) immediately after (\u03f52,\u03f52). In the\\ns a m ew a y ,am a t c h i n g(\u03f51,\u03f51) immediately followed by ( \u03f52,', '\u03f51) is not permitted\\nby the \ufb01lter F since a path via the matchings ( \u03f52,\u03f51)(\u03f51,\u03f51) is possible. Similarly,\\n(\u03f52,\u03f52)(\u03f52,\u03f51) is ruled out. It is not hard to verify that the \ufb01lter transducer F is\\nprecisel', 'y a \ufb01nite automaton over pairs accepting the complement of the language\\nL = \u03c3\u2217((\u03f51,\u03f51)(\u03f52,\u03f52)+( \u03f52,\u03f52)(\u03f51,\u03f51)+( \u03f51,\u03f51)(\u03f52,\u03f51)+( \u03f52,\u03f52)(\u03f52,\u03f51))\u03c3\u2217,\\nwhere \u03c3 = {(\u03f51,\u03f51), (\u03f52,\u03f52), (\u03f52,\u03f51),x }.T h u s ,t h ', 'e\ufb01 l t e rF guarantees that exactly\\none \u03f5-path is allowed in the composition of each \u03f5 sequences. To obtain the correct\\nresult of composition, it su\ufb03ces then to use the\u03f5-free composition algorithm alr', 'eady\\ndescribed and compute\\n\u02dcT1 \u25e6F \u25e6 \u02dcT2. (5.20)\\nIndeed, the two compositions in \u02dcT1 \u25e6F \u25e6 \u02dcT2 no longer involve \u03f5s. Since the size of\\nthe \ufb01lter transducer F is constant, the complexity of general compo', 'sition is the5.5 Sequence kernels 111\\nsame as that of \u03f5-free composition, that is O(|T1||T2|). In practice, the augmented\\ntransducers \u02dcT1 and \u02dcT2 are not explicitly constructed, instead the presence o', 'f the\\nauxiliary symbols is simulated. Further \ufb01lter optimizations help limit the number of\\nnon-coaccessible states created, for example, by examining more carefully the case\\nof states with only outgoi', 'ng non-\u03f5-transitions or only outgoing \u03f5-transitions.\\n5.5.2 Rational kernels\\nThe following establishes a general framework for the de\ufb01nition of sequence kernels.\\nDe\ufb01nition 5.5 Rational kernels\\nA kernel', ' K :\u03a3\\n\u2217 \u00d7 \u03a3\u2217 \u2192 R is said to be rational if it coincides with the mapping\\nde\ufb01ned by some weighted transducer U: \u2200x, y \u2208 \u03a3\u2217,K (x, y)= U(x, y).\\nNote that we could have instead adopted a more general de\ufb01n', 'ition: instead of using\\nweighted transducers, we could have used more powerful sequence mappings such\\nas algebraic transductions, which are the functional counterparts of context-free\\nlanguages, or ev', 'en more powerful ones. However, an essential need for kernels is\\nan e\ufb03cient computation, and more complex de\ufb01nitions would lead to substantially\\nmore costly computational complexities for kernel compu', 'tation. For rational kernels,\\nthere exists a general and e\ufb03cient computation algorithm.\\nComputation We will assume that the transducer U de\ufb01ning a rational kernel\\nK does not admit any \u03f5-cycle with non', '-zero weight, otherwise the kernel value is\\nin\ufb01nite for all pairs. For any sequence x,l e tT\\nx denote a weighted transducer with\\njust one accepting path whose input and output labels are both x and it', 's weight\\nequal to one. Tx can be straightforwardly constructed from x in linear time O(|x|).\\nThen, for any x, y \u2208 \u03a3\u2217, U(x, y) can be computed by the following two steps:\\n1. Compute V = Tx \u25e6U \u25e6Ty using', ' the composition algorithm in timeO(|U ||Tx||Ty |).\\n2. Compute the sum of the weights of all accepting paths of V using a general\\nshortest-distance algorithm in time O(|V |).\\nBy de\ufb01nition of compositi', 'on, V is a weighted transducer whose accepting paths are\\nprecisely those accepting paths of U that have input label x and output label y.\\nT h es e c o n ds t e pc o m p u t e st h es u mo ft h ew e i ', 'g h t so ft h e s ep a t h s ,t h a ti s ,e x a c t l y\\nU(x, y). Since U admits no \u03f5-cycle, V is acyclic, and this step can be performed in\\nlinear time. The overall complexity of the algorithm for com', 'puting U(x, y)i st h e n\\nin O(|U ||Tx||Ty |). Since U is \ufb01xed for a rational kernel K and |Tx| = O(|x|) for any\\nx, this shows that the kernel values can be obtained in quadratic time O(|x||y|).\\nFor so', 'me speci\ufb01c weighted transducers U, the computation can be more e\ufb03cient,\\nfor example in O(|x| + |y|)( s e ee x e r c i s e5 . 1 7 ) .112 Kernel Methods\\nPDS rational kernels For any transducer T,l e tT ', '\u22121 denote the inverse of T,\\nthat is the transducer obtained from T by swapping the input and output labels of\\nevery transition. For all x, y,w eh a v eT \u22121(x, y)= T(y,x ). The following theorem\\ngives ', 'a general method for constructing a PDS rational kernel from an arbitrary\\nweighted transducer.\\nTheorem 5.9\\nFor any weighted transducerT =( \u03a3, \u0394 ,Q ,I,F,E,\u03c1 ), the function K = T \u25e6T\\n\u22121 is\\na PDS rationa', 'l kernel.\\nProof By de\ufb01nition of composition and the inverse operation, for all x, y \u2208 \u03a3\u2217,\\nK(x, y)=\\n\u2211\\nz\u2208\u0394 \u2217\\nT(x, z) T(y,z ).\\nK is the pointwise limit of the kernel sequence (Kn)n\u22650 de\ufb01ned by:\\n\u2200n \u2208 N, \u2200', 'x, y \u2208 \u03a3\u2217,K n(x, y)=\\n\u2211\\n|z|\u2264n\\nT(x, z) T(y,z ),\\nwhere the sum runs over all sequences in \u0394 \u2217 of length at most n. Kn is PDS\\nsince its corresponding kernel matrix Kn for any sample ( x1,...,x m)i sS P S ', 'D .\\nThis can be see form the fact that Kn c a nb ew r i t t e na sKn = AA\u22a4 with\\nA =( Kn(xi,z j))i\u2208[1,m],j\u2208[1,N],w h e r ez1,...,z N is some arbitrary enumeration of\\nthe set of strings in \u03a3 \u2217 with leng', 'th at most n.T h u s ,K is PDS as the pointwise\\nlimit of the sequence of PDS kernels (Kn)n\u2208N.\\nThe sequence kernels commonly used in computational biology, natural language\\nprocessing, computer vision,', ' and other applications are all special instances of\\nrational kernels of the formT \u25e6T \u22121. All of these kernels can be computed e\ufb03ciently\\nusing the same general algorithm for the computational of ratio', 'nal kernels presented\\nin the previous paragraph. Since the transducer U = T \u25e6T\\n\u22121 de\ufb01ning such PDS\\nrational kernels has a speci\ufb01c form, there are di\ufb00erent options for the computation\\nof the compositio', 'n T\\nx \u25e6U \u25e6Ty:\\ncompute U = T \u25e6T \u22121 \ufb01rst, then V = Tx \u25e6U \u25e6Ty;\\ncompute V1 = Tx \u25e6T and V2 = Ty \u25e6T \ufb01rst, then V = V1 \u25e6V \u22121\\n2 ;\\ncompute \ufb01rst V1 = Tx \u25e6T,t h e nV2 = V1 \u25e6T \u22121,t h e nV = V2 \u25e6Ty,o rt h es i m i', ' l a r\\nseries of operations with x and y permuted.\\nAll of these methods lead to the same result after computation of the sum of the\\nweights of all accepting paths, and they all have the same worst-cas', 'e complexity.\\nHowever, in practice, due to the sparsity of intermediate compositions, there may\\nbe substantial di\ufb00erences between their time and space computational costs. An5.5 Sequence kernels 113\\n0', '\\na:\u03b5/1\\nb:\u03b5/1\\n1a:a/1\\nb:b/1 2/1a:a/1\\nb:b/1\\na:\u03b5/1\\nb:\u03b5/1\\n0\\na:\u03b5/1\\nb:\u03b5/1\\n1a:a/1\\nb:b/1\\na:\u03b5/\u03bb\\nb:\u03b5/\u03bb\\n2/1a:a/1\\nb:b/1\\na:\u03b5/1\\nb:\u03b5/1\\n(a) (b)\\nFigure 5.6 (a) TransducerTbigram de\ufb01ning the bigram kernelTbigram \u25e6T \u2212 1\\n', 'bigram for \u03a3=\\n{a, b}. (b) Transducer Tgappy bigram de\ufb01ning the gappy bigram kernel Tgappy bigram \u25e6\\nT \u2212 1\\ngappy bigram with gap penalty \u03bb \u2208 (0, 1).\\nalternative method based on an n-way composition can ', 'further lead to signi\ufb01cantly\\nmore e\ufb03cient computations.\\nExample 5.5 Bigram and gappy bigram sequence kernels\\nFigure 5.6a shows a weighted transducer Tbigram de\ufb01ning a common sequence\\nkernel, the bigra', 'm sequence kernel , for the speci\ufb01c case of an alphabet reduced\\nto \u03a3 = {a, b}. The bigram kernel associates to any two sequences x and y the sum\\nof the product of the counts of all bigrams inx and y. ', 'For any sequencex \u2208 \u03a3\u2217 and\\nany bigram z \u2208{ aa, ab, ba, bb}, Tbigram(x, z) is exactly the number of occurrences\\nof the bigram z in x. Thus, by de\ufb01nition of composition and the inverse operation,\\nTbigra', 'm \u25e6T \u22121\\nbigram computes exactly the bigram kernel.\\nFigure 5.6b shows a weighted transducerTgappy bigram de\ufb01ning the so-called gappy\\nbigram kernel. The gappy bigram kernel associates to any two sequenc', 'es x and y\\nthe sum of the product of the counts of all gappy bigrams in x and y penalized\\nb yt h el e n g t ho ft h e i rgaps. Gappy bigrams are sequences of the form aua, aub,\\nbua,o r bub,w h e r eu ', '\u2208 \u03a3\u2217 is called the gap. The count of a gappy bigram is\\nmultiplied by |u|\u03bb for some \ufb01xed \u03bb \u2208 (0, 1) so that gappy bigrams with longer\\ngaps contribute less to the de\ufb01nition of the similarity measure. Wh', 'ile this de\ufb01nition\\ncould appear to be somewhat complex, \ufb01gure 5.6 shows that Tgappy bigram can be\\nstraightforwardly derived from Tbigram. The graphical representation of rational\\nkernels helps underst', 'anding or modifying their de\ufb01nition.\\nCounting transducers The de\ufb01nition of most sequence kernels is based on the\\ncounts of some common patterns appearing in the sequences. In the examples\\njust examine', 'd, these were bigrams or gappy bigrams. There exists a simple and\\ngeneral method for constructing a weighted transducer counting the number of\\noccurrences of patterns and using them to de\ufb01ne PDS ratio', 'nal kernels. Let X be\\na \ufb01nite automaton representing the set of patterns to count. In the case of bigram\\nkernels with \u03a3 = {a, b}, X would be an automaton accepting exactly the set of\\nstrings {aa, ab, ', 'ba, bb}. Then, the weighted transducer of \ufb01gure 5.7 can be used to\\ncompute exactly the number of occurrences of each pattern accepted by X.114 Kernel Methods\\n0\\na:\u03b5/1\\nb:\u03b5/1\\n1/1X:X/1\\na:\u03b5/1\\nb:\u03b5/1\\nFigure ', '5.7 Counting transducer Tcount for \u03a3= {a, b}. The \u201ctransition\u201d X : X/1\\nstands for the weighted transducer created from the automaton X by adding to\\neach transition an output label identical to the exi', 'sting label, and by making all\\ntransition and \ufb01nal weights equal to one.\\nTheorem 5.10\\nFor any x \u2208 \u03a3\\n\u2217 and any sequence z accepted by X, Tcount(x, z) is the number of\\noccurrences ofz in x.\\nProof Let x ', '\u2208 \u03a3\u2217 be an arbitrary sequence and let z be a sequence accepted by\\nX. Since all accepting paths of Tcount have weight one, Tcount(x, z) is equal to the\\nnumber of accepting paths in Tcount with input la', 'bel x and output z.\\nNow, an accepting path\u03c0 in Tcount with input x and output z can be decomposed\\nas \u03c0 = \u03c00 \u03c001 \u03c01,w h e r e\u03c00 is a path through the loops of state 0 with input label\\nsome pre\ufb01x x0 of ', 'x and output label \u03f5, \u03c001 an accepting path from 0 to 1 with input\\nand output labels equal to z,a n d\u03c01 a path through the self-loops of state 1 with\\ninput label a su\ufb03x x1 of x and output \u03f5. Thus, the', ' number of such paths is exactly\\nthe number of distinct ways in which we can write sequencex as x = x0zx1,w h i c h\\nis exactly the number of occurrences of z in x.\\nThe theorem provides a very general ', 'method for constructing PDS rational kernels\\nTcount \u25e6T \u22121\\ncount that are based on counts of some patterns that can be de\ufb01ned\\nvia a \ufb01nite automaton, or equivalently a regular expression. Figure 5.7 sho', 'ws the\\ntransducer for the case of an input alphabet reduced to \u03a3 = {a, b}. The general\\ncase can be obtained straightforwardly by augmenting states 0 and 1 with other\\nself-loops using other symbols tha', 'n a and b. In practice, a lazy evaluation can be\\nused to avoid the explicit creation of these transitions for all alphabet symbols and\\ninstead creating them on-demand based on the symbols found in the', ' input sequence\\nx. Finally, one can assign di\ufb00erent weights to the patterns counted to emphasize\\nor deemphasize some, as in the case of gappy bigrams. This can be done simply by\\nchanging the transitio', 'ns weight or \ufb01nal weights of the automaton X used in the\\nde\ufb01nition of T\\ncount.5.6 Chapter notes 115\\n5.6 Chapter notes\\nThe mathematical theory of PDS kernels in a general setting originated with the\\nfu', 'ndamental work of Mercer [1909] who also proved the equivalence of a condition\\nsimilar to that of theorem 5.1 for continuous kernels with the PDS property. The\\nconnection between PDS and NDS kernels, ', 'in particular theorems 5.8 and 5.7,\\nare due to Schoenberg [1938]. A systematic treatment of the theory of reproducing\\nkernel Hilbert spaces was presented in a long and elegant paper by Aronszajn [1950', '].\\nFor an excellent mathematical presentation of PDS kernels and positive de\ufb01nite\\nfunctions we refer the reader to Berg, Christensen, and Ressel [1984], which is also\\nthe source of several of the exer', 'cises given in this chapter.\\nThe fact that SVMs could be extended by using PDS kernels was pointed out\\nby Boser, Guyon, and Vapnik [1992]. The idea of kernel methods has been since\\nthen widely adopted', ' in machine learning and applied in a variety of di\ufb00erent tasks\\nand settings. The following two books are in fact speci\ufb01cally devoted to the study\\nof kernel methods: Sch\u00a8olkopf and Smola [2002] and Sh', 'awe-Taylor and Cristianini\\n[2004]. The classical representer theorem is due to Kimeldorf and Wahba [1971].\\nA generalization to non-quadratic cost functions was stated by Wahba [1990]. The\\ngeneral form', ' presented in this chapter was given by Sch\u00a8 olkopf, Herbrich, Smola,\\nand Williamson [2000].\\nRational kernels were introduced by Cortes, Ha\ufb00ner, and Mohri [2004]. A general\\nclass of kernels, convoluti', 'on kernels, was earlier introduced by Haussler [1999]. The\\nconvolution kernels for sequences described by Haussler [1999], as well as the pair-\\nHMM string kernels described by Watkins [1999], are spec', 'ial instances of rational\\nkernels. Rational kernels can be straightforwardly extended to de\ufb01ne kernels for\\n\ufb01nite automata and even weighted automata [Cortes et al., 2004]. Cortes, Mohri,\\nand Rostamiza', 'deh [2008b] study the problem of learning rational kernels such as\\nthose based on counting transducers.\\nThe composition of weighted transducers and the \ufb01lter transducers in the presence\\nof \u03f5-paths are', ' described in Pereira and Riley [1997], Mohri, Pereira, and Riley [2005],\\nand Mohri [2009]. Composition can be further generalized to theN-way composition\\nof weighted transducers [Allauzen and Mohri, ', '2009]. N-way composition of three\\nor more transducers can substantially speed up computation, in particular for PDS\\nrational kernels of the formT \u25e6T\\n\u22121. A genericshortest-distance algorithm which can\\n', 'be used with a large class of semirings and arbitrary queue disciplines is described by\\nMohri [2002]. A speci\ufb01c instance of that algorithm can be used to compute the sum\\nof the weights of all paths as', ' needed for the computation of rational kernels after\\ncomposition. For a study of the class of languages linearly separable with rational\\nkernels , see Cortes, Kontorovich, and Mohri [2007a].116 Kerne', 'l Methods\\n5.7 Exercises\\n5.1 Let K : X\u00d7 X \u2192 R be a PDS kernel, and let \u03b1: X\u2192 R be a positive function.\\nShow that the kernel K\u2032 de\ufb01ned for all x, y \u2208X by K\u2032(x, y)= K(x,y)\\n\u03b1(x)\u03b1(y) is a PDS\\nkernel.\\n5.2 S', 'how that the following kernels K are PDS:\\n(a) K(x, y)=c o s (x \u2212 y)o v e rR \u00d7 R.\\n(b) K(x, y)=c o s (x2 \u2212 y2)o v e rR \u00d7 R.\\n(c) K(x, y)=( x + y)\u22121 over (0, +\u221e ) \u00d7 (0, +\u221e ).\\n(d) K(x,x\u2032)=c o s\u2220(x,x\u2032)o v e', ' rRn \u00d7 Rn,w h e r e\u2220(x,x\u2032) is the angle between\\nx and x\u2032.\\n(e) \u2200\u03bb> 0,K (x, x\u2032)=e x p\\n(\\n\u2212 \u03bb[sin(x\u2032 \u2212 x)]2\u23a1\\nover R \u00d7 R.( Hint:r e w r i t e\\n[sin(x\u2032 \u2212 x)]2 as the square of the norm of the di\ufb00erence of tw', 'o vectors.)\\n5.3 Show that the following kernels K are NDS:\\n(a) K(x, y) = [sin(x \u2212 y)]2 over R \u00d7 R.\\n(b) K(x, y)=l o g (x + y)o v e r( 0,+\u221e ) \u00d7 (0, +\u221e ).\\n5.4 De\ufb01ne a di\ufb00erence kernel as K(x, x\u2032)= |x \u2212 x', '\u2032| for x, x\u2032 \u2208 R. Show that this\\nkernel is not positive de\ufb01nite symmetric (PDS).\\n5.5 Is the kernel K de\ufb01ned over Rn \u00d7 Rn by K(x,y)= \u2225x \u2212 y\u22253/2 PDS? Is it NDS?\\n5.6 Let H be a Hilbert space with the cor', 'responding dot product \u27e8\u00b7, \u00b7\u27e9. Show that\\nthe kernel K de\ufb01ned over H \u00d7 H by K(x, y)=1 \u2212\u27e8 x, y\u27e9 is negative de\ufb01nite.\\n5.7 For any p> 0, let Kp be the kernel de\ufb01ned over R+ \u00d7 R+ by\\nKp(x, y)= e\u2212(x+y)p\\n. (5', '.21)\\nShow that Kp is positive de\ufb01nite symmetric (PDS) i\ufb00 p \u2264 1. (Hint: you can use the\\nfact that if K is NDS, then for any 0 <\u03b1 \u2264 1, K\u03b1 is also NDS.)\\n5.8 Explicit mappings.\\n(a) Denote a data set x1,..', '.,x m and a kernel K(xi,xj) with a Gram matrix\\nK.A s s u m i n gK is positive semide\ufb01nite, then give a map \u03a6( \u00b7) such that5.7 Exercises 117\\nK(xi,xj)= \u27e8\u03a6(xi), \u03a6(xj)\u27e9.\\n(b) Show the converse of the previ', 'ous statement, i.e., if there exists a mapping\\n\u03a6(x) from input space to some Hilbert space, then the corresponding matrix\\nK is positive semide\ufb01nite.\\n5.9 Explicit polynomial kernel mapping. Let K be a ', 'polynomial kernel of degree d,\\ni.e., K : RN \u00d7RN \u2192 R, K(x,x\u2032)=( x\u00b7x\u2032 +c)d,w i t hc> 0, Show that the dimension\\nof the feature space associated to K is\\n(N + d\\nd\\n\u23a1\\n. (5.22)\\nWrite K in terms of kernels ki', ' :( x,x\u2032) \u21a6\u2192 (x \u00b7 x\u2032)i, i \u2208 [0,d ]. What is the weight\\nassigned to each ki in that expression? How does it vary as a function of c?\\n5.10 High-dimensional mapping. Let \u03a6: X\u2192 H b eaf e a t u r em a p p ', 'i n gs u c ht h a t\\nthe dimension N of H is very large and letK : X\u00d7 X \u2192 R be a PDS kernel de\ufb01ned\\nby\\nK(x, x\u2032)= E\\ni\u223cD\\n[\\n[\u03a6(x)]i[\u03a6(x\u2032)]i\\n]\\n, (5.23)\\nwhere [\u03a6(x)]i is the ith component of \u03a6( x) (and simil', 'arly for \u03a6 \u2032(x)) and where\\nD is a distribution over the indices i. We shall assume that |[\u03a6(x)]i|\u2264 R for all\\nx \u2208X and i \u2208 [1,N ]. Suppose that the only method available to compute K(x, x\u2032)\\ninvolved di', 'rect computation of the inner product (5.23), which would requireO(N)\\ntime. Alternatively, an approximation can be computed based on random selection\\nof a subset I of the N components of \u03a6(x)a n d\u03a6 (x', '\\n\u2032)a c c o r d i n gt oD,t h a ti s :\\nK\u2032(x, x\u2032)= 1\\nn\\n\u2211\\ni\u2208I\\nD(i)[\u03a6(x)]i[\u03a6(x\u2032)]i, (5.24)\\nwhere |I| = n.\\n(a) Fix x and x\u2032 in X.P r o v et h a t\\nPr\\nI\u223cDn\\n[|K(x, x\u2032) \u2212 K\u2032(x, x\u2032)| >\u03f5 ] \u2264 2e\\n\u2212 n\u03f52\\n2r2 . (5.25', ')\\n(Hint: use McDiarmid\u2019s inequality).\\n(b) Let K and K\u2032 b et h ek e r n e lm a t r i c e sa s s o c i a t e dt oK and K\u2032.S h o w\\nthat for any \u03f5, \u03b4 >0, for n> r2\\n\u03f52 log m(m+1)\\n\u03b4 , with probability at le', 'ast 1 \u2212 \u03b4,\\n|K\u2032\\nij \u2212 Kij |\u2264 \u03f5 for all i, j \u2208 [1,m].\\n5.11 Classi\ufb01er based kernel. Let S b eat r a i n i n gs a m p l eo fs i z em. Assume that118 Kernel Methods\\nS has been generated according to some pr', 'obability distribution D(x, y), where\\n(x, y) \u2208 X \u00d7{ \u22121, +1}.\\n(a) De\ufb01ne the Bayes classi\ufb01er h\u2217: X \u2192{ \u2212 1, +1}. Show that the kernel K\u2217\\nde\ufb01ned by K\u2217(x, x\u2032)= h\u2217(x)h\u2217(x\u2032) for any x, x\u2032 \u2208 X is positive de\ufb01', 'nite\\nsymmetric. What is the dimension of the natural feature space associated to\\nK\u2217?\\n(b) Give the expression of the solution obtained using SVMs with this kernel.\\nWhat is the number of support vectors', '? What is the value of the margin? What\\nis the generalization error of the solution obtained? Under what condition are\\nthe data linearly separable?\\n(c) Let h : X \u2192 R be an arbitrary real-valued functi', 'on. Under what condition\\non h is the kernel K de\ufb01ned by K(x, x\u2032)= h(x)h(x\u2032), x, x\u2032 \u2208 X, positive\\nde\ufb01nite symmetric?\\n5.12 Image classi\ufb01cation kernel. For \u03b1 \u2265 0, the kernel\\nK\u03b1 :( x,x\u2032) \u21a6\u2192\\nN\u2211\\nk=1\\nmin(|xk', '|\u03b1, |x\u2032\\nk|\u03b1) (5.26)\\nover RN \u00d7 RN is used in image classi\ufb01cation. Show that K\u03b1 is PDS for all \u03b1 \u2265 0.\\nTo do so, proceed as follows.\\n( a ) U s et h ef a c tt h a t(f,g ) \u21a6\u2192\\n\u222b +\u221e\\nt=0 f(t)g(t)dt is an inne', 'r product over the set\\nof measurable functions over [0 , +\u221e ) to show that ( x, x\u2032) \u21a6\u2192 min(x, x\u2032)i sa\\nPDS kernel. (Hint: associate an indicator function tox and another one to x\u2032.)\\n(b) Use the result ', 'from (a) to \ufb01rst show that K1 is PDS and similarly thatK\u03b1\\nwith other values of \u03b1 is also PDS.\\n5.13 Fraud detection. To prevent fraud, a credit-card company decides to contact\\nProfessor Villebanque and', ' provides him with a random list of several thousand\\nfraudulent and non-fraudulent events. There are many di\ufb00erent types of events,\\ne.g., transactions of various amounts, changes of address or card-ho', 'lder information,\\nor requests for a new card. Professor Villebanque decides to use SVMs with an\\nappropriate kernel to help predict fraudulent events accurately. It is di\ufb03cult for\\nProfessor Villebanque', ' to de\ufb01ne relevant features for such a diverse set of events.\\nHowever, the risk department of his company has created a complicated method to\\nestimate a probability Pr[U] for any event U. Thus, Profes', 'sor Villebanque decides\\nto make use of that information and comes up with the following kernel de\ufb01ned5.7 Exercises 119\\nover all pairs of events (U, V):\\nK(U, V)=P r [U \u2227 V ] \u2212 Pr[U]P r [V ]. (5.27)\\nHel', 'p Professor Villebanque show that his kernel is positive de\ufb01nite symmetric.\\n5.14 Relationship between NDS and PDS kernels. Prove the statement of theo-\\nrem 5.7. (Hint: Use the fact that if K is PDS th', 'en exp(K) is also PDS, along with\\ntheorem 5.6.)\\n5.15 Metrics and Kernels. Let X be a non-empty set and K : X\u00d7 X \u2192 R be a\\nnegative de\ufb01nite symmetric kernel such that K(x, x) = 0 for all x \u2208X .\\n(a) Show', ' that there exists a Hilbert space H and a mapping \u03a6(x)f r o mX to\\nH such that:\\nK(x, y)= ||\u03a6(x) \u2212 \u03a6(x\u2032)||2 .\\nAssume that K(x, x\u2032)=0 \u21d2 x = x\u2032. Use theorem 5.6 to show that\\n\u221a\\nK de\ufb01nes\\nam e t r i co nX.\\n', '(b) Use this result to prove that the kernelK(x, y)=e x p (\u2212|x\u2212x\u2032|p), x, x\u2032 \u2208 R,\\nis not positive de\ufb01nite for p> 2.\\n(c) The kernel K(x, x\u2032)=t a n h (a(x\u00b7x\u2032)+b) was shown to be equivalent to a two-\\nlaye', 'r neural network when combined with SVMs. Show that K is not positive\\nde\ufb01nite if a< 0o r b< 0. What can you conclude about the corresponding\\nneural network when a< 0o r b< 0?\\n5.16 Sequence kernels. Le', 't X = {a, c, g, t}. To classify DNA sequences using SVMs,\\nwe wish to de\ufb01ne a kernel between sequences de\ufb01ned over X. We are given a \ufb01nite\\nset I \u2282 X\u2217 of non-coding regions (introns). For x \u2208 X\u2217, denote', ' by |x| the length\\nof x and by F(x)t h es e to ff a c t o r so fx, i.e., the set of subsequences of x with\\ncontiguous symbols. For any two stringsx, y \u2208 X\u2217 de\ufb01ne K(x, y)b y\\nK(x, y)=\\n\u2211\\nz \u2208(F(x)\u2229F(y))\u2212I', '\\n\u03c1|z|, (5.28)\\nwhere \u03c1 \u2265 1 is a real number.\\n(a) Show that K is a rational kernel and that it is positive de\ufb01nite symmetric.\\n(b) Give the time and space complexity of the computation of K(x, y)w i t h\\n', 'respect to the size s of a minimal automaton representing X\u2217 \u2212 I.\\n(c) Long common factors between x and y of length greater than or equal to120 Kernel Methods\\nn are likely to be important coding regio', 'ns (exons). Modify the kernel K to\\nassign weight \u03c1|z|\\n2 to z when |z|\u2265 n, \u03c1|z|\\n1 otherwise, where 1 \u2264 \u03c11 \u226a \u03c12.S h o w\\nthat the resulting kernel is still positive de\ufb01nite symmetric.\\n5.17 n-gram kernel.', ' Show that for all n \u2265 1, and any n-gram kernel Kn, Kn(x, y)\\nc a nb ec o m p u t e di nl i n e a rt i m eO(|x| + |y|), for all x, y \u2208 \u03a3\u2217 assuming n and the\\nalphabet size are constants.\\n5.18 Mercer\u2019s c', 'ondition. Let X\u2282 RN be a compact set and K : X\u00d7 X \u2192 R a\\ncontinuous kernel function. Prove that ifK veri\ufb01es Mercer\u2019s condition (theorem 5.1),\\nthen it is PDS. (Hint: assume that K is not PDS and conside', 'r a set{x1,...,x m}\u2286\\nX and a column-vector c \u2208 Rm\u00d71 such that \u2211m\\ni,j=1 cicjK(xi,xj) < 0.)6B o o s t i n g\\nEnsemble methods are general techniques in machine learning for combining several\\npredictors t', 'o create a more accurate one. This chapter studies an important family of\\nensemble methods known asboosting, and more speci\ufb01cally theAdaBoost algorithm.\\nThis algorithm has been shown to be very e\ufb00ecti', 've in practice in some scenarios and\\nis based on a rich theoretical analysis. We \ufb01rst introduce AdaBoost, show how it can\\nrapidly reduce the empirical error as a function of the number of rounds of bo', 'osting,\\nand point out its relationship with some known algorithms. Then we present a\\ntheoretical analysis of its generalization properties based on the VC-dimension of\\nits hypothesis set and based on ', 'a notion of margin that we will introduce. Much of\\nthat margin theory can be applied to other similar ensemble algorithms. A game-\\ntheoretic interpretation of AdaBoost further helps analyzing its prop', 'erties. We end\\nwith a discussion of AdaBoost\u2019s bene\ufb01ts and drawbacks.\\n6.1 Introduction\\nIt is often di\ufb03cult, for a non-trivial learning task, to directly devise an accurate\\nalgorithm satisfying the str', 'ong PAC-learning requirements of chapter 2. But, there\\ncan be more hope for \ufb01nding simple predictors guaranteed only to perform slightly\\nbetter than random. The following gives a formal de\ufb01nition of s', 'uch weak learners.\\nDe\ufb01nition 6.1 Weak learning\\nA concept class C is said to be weakly PAC-learnable if there exists an algorithm\\nA, \u03b3> 0, and a polynomial functionpoly(\u00b7, \u00b7, \u00b7, \u00b7) such that for any \u03f5>', ' 0 and \u03b4> 0,\\nfor all distributions D on X and for any target concept c \u2208 C, the following holds\\nfor any sample size m \u2265 poly(1/\u03f5,1/\u03b4, n,size(c)):\\nPr\\nS\u223cDm\\n[\\nR(hS) \u2264 1\\n2 \u2212 \u03b3\\n]\\n\u2265 1 \u2212 \u03b4. (6.1)\\nWhen such a', 'n algorithm A exists, it is called a weak learning algorithm for C or a\\nweak learner. The hypotheses returned by a weak learning algorithm are calledbase\\nclassi\ufb01ers .122 Boosting\\nAdaBoost(S =( (x1,y1)', ',..., (xm,y m)))\\n1 for i \u2190 1 to m do\\n2 D1(i) \u2190 1\\nm\\n3 for t \u2190 1 to T do\\n4 ht \u2190 base classi\ufb01er in H with small error \u03f5t =P ri\u223cDt [ht(xi) \u0338= yi]\\n5 \u03b1t \u2190 1\\n2 log 1\u2212\u03f5t\\n\u03f5t\\n6 Zt \u2190 2[\u03f5t(1 \u2212 \u03f5t)]\\n1\\n2 \u22bf normaliz', 'ation factor\\n7 for i \u2190 1 to m do\\n8 Dt+1(i) \u2190 Dt(i)e x p (\u2212\u03b1tyiht(xi))\\nZt\\n9 g \u2190 \u2211T\\nt=1 \u03b1tht\\n10 return h = sgn(g)\\nFigure 6.1 AdaBoost algorithm for H \u2286{ \u2212 1, +1}X .\\nThe key idea behind boosting techniqu', 'es is to use a weak learning algorithm\\nto build a strong learner, that is, an accurate PAC-learning algorithm. To do so,\\nboosting techniques use an ensemble method: they combine di\ufb00erent base classi\ufb01e', 'rs\\nreturned by a weak learner to create a more accurate predictor. But which base\\nclassi\ufb01ers should be used and how should they be combined? The next section\\naddresses these questions by describing in', ' detail one of the most prevalent and\\nsuccessful boosting algorithms, AdaBoost.\\n6.2 AdaBoost\\nWe denote by H the hypothesis set out of which the base classi\ufb01ers are selected.\\nFigure 6.1 gives the pseud', 'ocode of AdaBoost in the case where the base classi\ufb01ers\\nare functions mapping from X to {\u22121, +1},t h u sH \u2286{ \u2212 1, +1}X .\\nThe algorithm takes as input a labeled sample S =( (x1,y1),..., (xm,y m)), with', '\\n(xi,y i) \u2208X\u00d7 { \u2212 1, +1} for all i \u2208 [1,m], and maintains a distribution over the\\nindices {1,...,m }. Initially (lines 1-2), the distribution is uniform ( D1). At each\\nround of boosting, that is each ', 'iterationt \u2208 [1,T ] of the loop 3\u20138, a new base classi\ufb01er\\nht \u2208 H is selected that minimizes the error on the training sample weighted by the6.2 AdaBoost 123\\nt = 1 t = 2 t = 3\\ndecision \\nboundary\\nupdate', 'd\\nweights\\n(a)\\n=\u03b11 +\u03b13+\u03b12\\n(b)\\nFigure 6.2 Example of AdaBoost with axis-aligned hyperplanes as base learners.\\n(a) The top row shows decision boundaries at each boosting round. The bottom row\\nshows how w', 'eights are updated at each round, with incorrectly (resp., correctly)\\npoints given increased (resp., decreased) weights. (b) Visualization of \ufb01nal classi\ufb01er,\\nconstructed as a linear combination of bas', 'e learners.\\ndistribution D\\nt:\\nht \u2208 argmin\\nh\u2208H\\nPr\\ni\u223cDt\\n[ht(xi) \u0338= yi] = argmin\\nh\u2208H\\nm\u2211\\ni=1\\nDt(i)1h(xi)\u0338=yi .\\nZt is simply a normalization factor to ensure that the weights Dt+1(i)s u mt oo n e .\\nThe pre', 'cise reason for the de\ufb01nition of the coe\ufb03cient \u03b1t will become clear later. For\\nnow, observe that if\u03f5t, the error of the base classi\ufb01er, is less than 1/2, then 1\u2212\u03f5t\\n\u03f5t\\n> 1\\nand \u03b1t > 0. Thus, the new dis', 'tribution Dt+1 is de\ufb01ned from Dt by substantially\\nincreasing the weight on i if point xi is incorrectly classi\ufb01ed (yiht(xi) < 0), and, on\\nthe contrary, decreasing it ifxi is correctly classi\ufb01ed. This ', 'has the e\ufb00ect of focusing\\nmore on the points incorrectly classi\ufb01ed at the next round of boosting, less on those\\ncorrectly classi\ufb01ed by h\\nt.124 Boosting\\nAfter T rounds of boosting, the classi\ufb01er return', 'ed by AdaBoost is based on the\\nsign of functiong, which is a linear combination of the base classi\ufb01ersht.T h ew e i g h t\\n\u03b1t assigned to ht in that sum is a logarithmic function of the ratio of the ac', 'curacy\\n1 \u2212 \u03f5t and error \u03f5t of ht. Thus, more accurate base classi\ufb01ers are assigned a larger\\nweight in that sum. Figure 6.2 illustrates the AdaBoost algorithm. The size of the\\npoints represents the dis', 'tribution weight assigned to them at each boosting round.\\nFor anyt \u2208 [1,T ], we will denote byg\\nt the linear combination of the base classi\ufb01ers\\nafter t rounds of boosting: ft = \u2211t\\ns=1 \u03b1tht.I np a r t ', 'i c u l a r ,w eh a v egT = g.T h e\\ndistribution Dt+1 can be expressed in terms ofgt and the normalization factors Zs,\\ns \u2208 [1,t ], as follows:\\n\u2200i \u2208 [1,m],D t+1(i)= e\u2212yigt(xi)\\nm \u220ft\\ns=1 Zs\\n. (6.2)\\nWe wi', 'll make use of this identity several times in the proofs of the following sections.\\nIt can be shown straightforwardly by repeatedly expanding the de\ufb01nition of the\\ndistribution over the point xi:\\nDt+1(', 'i)= Dt(i)e\u2212\u03b1tyiht(xi)\\nZt\\n= Dt\u22121(i)e\u2212\u03b1t\u2212 1yiht\u2212 1(xi)e\u2212\u03b1tyiht(xi)\\nZt\u22121Zt\\n= e\u2212yi\\nPt\\ns=1 \u03b1shs(xi)\\nm \u220ft\\ns=1 Zs\\n.\\nThe AdaBoost algorithm can be generalized in several ways:\\ninstead of a hypothesis with min', 'imal weighted error, ht can be more generally\\nthe base classi\ufb01er returned by a weak learning algorithm trained on Dt;\\nthe range of the base classi\ufb01ers could be [ \u22121,+1], or more generally R.T h e\\ncoe\ufb03', 'cients \u03b1t can then be di\ufb00erent and may not even admit a closed form. In\\ngeneral, they are chosen to minimize an upper bound on the empirical error, as\\ndiscussed in the next section. Of course, in that', ' general case, the hypothesis ht are\\nnot binary classi\ufb01ers , but the sign of their values could indicate the label, and their\\nmagnitude could be interpreted as a measure of con\ufb01dence.\\nIn the remainder', ' of this section, we will further analyze the properties of Ad-\\naBoost and discuss its typical use in practice.\\n6.2.1 Bound on the empirical error\\nWe \ufb01rst show that the empirical error of AdaBoost dec', 'reases exponentially fast as\\naf u n c t i o no ft h en u m b e ro fr o u n d so fb o o s t i n g .6.2 AdaBoost 125\\nTheorem 6.1\\nThe empirical error of the classi\ufb01er returned by AdaBoost veri\ufb01es:\\n\u02c6R(h) ', '\u2264 exp\\n[\\n\u2212 2\\nT\u2211\\nt=1\\n(1\\n2 \u2212 \u03f5t\\n\u23a12]\\n. (6.3)\\nFurthermore, if for all t \u2208 [1,T ], \u03b3 \u2264 (1\\n2 \u2212 \u03f5t),t h e n\\n\u02c6R(h) \u2264 exp(\u22122\u03b32T) . (6.4)\\nProof Using the general inequality 1 u\u22640 \u2264 exp(\u2212u) valid for all u \u2208 R an', 'd\\nidentity 6.2, we can write:\\n\u02c6R(h)= 1\\nm\\nm\u2211\\ni=1\\n1yig(xi)\u22640 \u2264 1\\nm\\nm\u2211\\ni=1\\ne\u2212yig(xi) = 1\\nm\\nm\u2211\\ni=1\\n[\\nm\\nT\u220f\\nt=1\\nZt\\n]\\nDT+1(i)=\\nT\u220f\\nt=1\\nZt.\\nSince, for all t \u2208 [1,T ], Zt is a normalization factor, it can be ex', 'pressed in terms of\\n\u03f5t by:\\nZt =\\nm\u2211\\ni=1\\nDt(i)e\u2212\u03b1tyiht(xi) =\\n\u2211\\ni:yiht(xi)=+1\\nDt(i)e\u2212\u03b1t +\\n\u2211\\ni:yiht(xi)=\u22121\\nDt(i)e\u03b1t\\n=( 1 \u2212 \u03f5t)e\u2212\u03b1t + \u03f5te\u03b1t\\n=( 1 \u2212 \u03f5t)\\n\u221a \u03f5t\\n1 \u2212 \u03f5t\\n+ \u03f5t\\n\u221a\\n1 \u2212 \u03f5t\\n\u03f5t\\n=2\\n\u221a\\n\u03f5t(1 \u2212 \u03f5t) .\\nThus, t', 'he product of the normalization factors can be expressed and upper bounded\\nas follows:\\nT\u220f\\nt=1\\nZt =\\nT\u220f\\nt=1\\n2\\n\u221a\\n\u03f5t(1 \u2212 \u03f5t)=\\nT\u220f\\nt=1\\n\u221a\\n1 \u2212 4\\n(1\\n2 \u2212 \u03f5t\\n\u23a12\\n\u2264\\nT\u220f\\nt=1\\nexp\\n[\\n\u2212 2\\n(1\\n2 \u2212 \u03f5t\\n\u23a12]\\n=e x p\\n[\\n\u2212 2\\nT\u2211\\nt', '=1\\n(1\\n2 \u2212 \u03f5t\\n\u23a12]\\n,\\nwhere the inequality follows from the identity 1 \u2212 x \u2264 e\u2212x valid for all x \u2208 R.\\nNote that the value of \u03b3,w h i c hi sk n o w na st h eedge, and the accuracy of the base\\nclassi\ufb01ers d', 'o not need to be known to the algorithm. The algorithm adapts to their\\naccuracy and de\ufb01nes a solution based on these values. This is the source of the\\nextended name of AdaBoost: adaptive boosting.\\nThe', ' proof of theorem 6.1 reveals several other important properties. First, observe\\nthat \u03b1\\nt is the minimizer of the function g: \u03b1 \u21a6\u2192 (1 \u2212 \u03f5t)e\u2212\u03b1 + \u03f5te\u03b1. Indeed, g is126 Boosting\\n-4 -2 0 2 4\\n0\\n1\\n2\\n3\\n4\\n5\\n', 'e\u2212x\\n0\u20131 loss\\nx\\nloss function\\nFigure 6.3 Visualization of the zero-one loss (blue) and the convex and di\ufb00eren-\\ntiable upper bound on the zero-one loss (red) that is optimized by AdaBoost.\\nconvex and di', '\ufb00erentiable, and setting its derivative to zero yields:\\ng\u2032(\u03b1)= \u2212(1 \u2212 \u03f5t)e\u2212\u03b1 + \u03f5te\u03b1 =0 \u21d4 (1 \u2212 \u03f5t)e\u2212\u03b1 = \u03f5te\u03b1 \u21d4 \u03b1 = 1\\n2 log 1 \u2212 \u03f5t\\n\u03f5t\\n. (6.5)\\nThus, \u03b1t is chosen to minimizeZt = g(\u03b1t), and in light of the', ' bound\u02c6R(h) \u2264 \u220fT\\nt=1 Zt\\nshown in the proof, these coe\ufb03cients are selected to minimize an upper bound on\\nthe empirical error. In fact, for base classi\ufb01ers whose range is [ \u22121, +1] or R, \u03b1t\\ncan be chose', 'n in a similar fashion to minimize Zt, and this is the way AdaBoost is\\nextended to these more general cases.\\nObserve also that the equality (1 \u2212 \u03f5t)e\u2212\u03b1t = \u03f5te\u03b1t just shown in (6.5) implies\\nthat at eac', 'h iteration, AdaBoost assigns equal distribution mass to correctly and\\nincorrectly classi\ufb01ed instances, since (1\u2212\u03f5\\nt)e\u2212\u03b1t is the total distribution assigned to\\ncorrectly classi\ufb01ed points and \u03f5te\u03b1t tha', 't of incorrectly classi\ufb01ed ones. This may seem\\nto contradict the fact that AdaBoost increases the weights of incorrectly classi\ufb01ed\\npoints and decreases that of others, but there is in fact no inconsis', 'tency: the reason\\nis that there are always fewer incorrectly classi\ufb01ed points, since the base classi\ufb01er\u2019s\\naccuracy is better than random.\\n6.2.2 Relationship with coordinate descent\\nAdaBoost was design', 'ed to address a novel theoretical question, that of designing a\\nstrong learning algorithm using a weak learning algorithm. We will show, however,\\nthat it coincides in fact with a very simple and class', 'ical algorithm, which consists\\nof applying a coordinate descent technique to a convex and di\ufb00erentiable objective\\nfunction. The objective function F for AdaBoost is de\ufb01ned for all samples S =6.2 AdaBo', 'ost 127\\n((x1,y1),..., (xm,y m)) and \u03b1 =( \u03b11,...,\u03b1 n) \u2208 Rn, n \u2265 1, by\\nF(\u03b1)=\\nm\u2211\\ni=1\\ne\u2212yign(xi) =\\nm\u2211\\ni=1\\ne\u2212yi\\nPn\\nt=1 \u03b1tht(xi), (6.6)\\nwhere gn = \u2211n\\nt=1 \u03b1tht. This function is an upper bound on the zero-on', 'e loss\\nfunction we wish to minimize, as shown in \ufb01gure 6.3. Let et denote the unit vector\\ncorresponding to the tth coordinate in Rn and let \u03b1t\u22121 denote the vector based\\non the ( t \u2212 1) \ufb01rst coe\ufb03cients', ', i.e. \u03b1t\u22121 =( \u03b11,...,\u03b1 t\u22121, 0,..., 0)\u22a4 if t \u2212 1 > 0,\\n\u03b1t\u22121 = 0 otherwise. At each iterationt \u2265 1, the direction et selected by coordinate\\ndescent is the one minimizing the directional derivative:\\net =', 'a r g m i n\\nt\\ndF(\u03b1t\u22121 + \u03b7et)\\nd\u03b7\\n\u23d0\u23d0\\n\u23d0\\n\u23d0\\n\u03b7=0\\n.\\nSince F(\u03b1t\u22121 + \u03b7et)= \u2211m\\ni=1 e\u2212yi\\nPt\u2212 1\\ns=1 \u03b1shs(xi)\u2212yi\u03b7ht(xi), the directional derivative\\nalong et can be expressed as follows:\\ndF(\u03b1t\u22121 + \u03b7et)\\nd\u03b7\\n\u23d0\u23d0\\n\u23d0\\n\u23d0\\n\u03b7=', '0\\n= \u2212\\nm\u2211\\ni=1\\nyiht(xi)e x p\\n[\\n\u2212 yi\\nt\u22121\u2211\\ns=1\\n\u03b1shs(xi)\\n]\\n= \u2212\\nm\u2211\\ni=1\\nyiht(xi)Dt(i)\\n[\\nm\\nt\u22121\u220f\\ns=1\\nZs\\n]\\n= \u2212\\n[ \u2211\\ni:yiht(xi)=+1\\nDt(i) \u2212\\n\u2211\\ni:yiht(xi)=\u22121\\nDt(i)\\n][\\nm\\nt\u22121\u220f\\ns=1\\nZs\\n]\\n= \u2212[(1 \u2212 \u03f5t) \u2212 \u03f5t]\\n[\\nm\\nt\u22121\u220f\\ns=1\\n', 'Zs\\n]\\n=[ 2\u03f5t \u2212 1]\\n[\\nm\\nt\u22121\u220f\\ns=1\\nZs\\n]\\n.\\nThe \ufb01rst equality holds by di\ufb00erentiation and evaluation at \u03b7 = 0, and the second\\none follows from (6.2). The third equality divides the sample set into points cor', 'rectly\\nand incorrectly classi\ufb01ed by ht, and the fourth equality uses the de\ufb01nition of \u03f5t.I n\\nview of the \ufb01nal equality, since m \u220ft\u22121\\ns=1 Zs is \ufb01xed, the direction et selected by\\ncoordinate descent is ', 'the one minimizing \u03f5t, which corresponds exactly to the base\\nlearner ht selected by AdaBoost.\\nThe step size \u03b7 is identi\ufb01ed by setting the derivative to zero in order to minimize\\nthe function in the ch', 'osen direction et. Thus, using identity 6.2 and the de\ufb01nition128 Boosting\\nsquare lossboosting loss\\nlogistic loss\\nhinge loss\\nzero-one loss\\n-4 -2 0 2 4\\n0\\n2\\n4\\n6\\n8\\n10\\nx\\nloss function\\nx \u21a6\u2192 (1 \u2212 x)2 1x\u22641x \u21a6', '\u2192 e\u2212x\\nx \u21a6\u2192 max(1 \u2212 x,0)\\nx \u21a6\u2192 1x<0\\nx \u21a6\u2192 log2(1 +e\u2212x)\\nFigure 6.4 Examples of several convex upper bounds on the zero-one loss.\\nof \u03f5t,w ec a nw r i t e :\\ndF(\u03b1t\u22121 + \u03b7et)\\nd\u03b7 =0 \u21d4\u2212\\nm\u2211\\ni=1\\nyiht(xi)e x p\\n[\\n\u2212 ', 'yi\\nt\u22121\u2211\\ns=1\\n\u03b1shs(xi)\\n]\\ne\u2212\u03b7yiht(xi) =0\\n\u21d4\u2212\\nm\u2211\\ni=1\\nyiht(xi)Dt(i)\\n[\\nm\\nt\u22121\u220f\\ns=1\\nZs\\n]\\ne\u2212yiht(xi)\u03b7 =0\\n\u21d4\u2212\\nm\u2211\\ni=1\\nyiht(xi)Dt(i)e\u2212yiht(xi)\u03b7 =0\\n\u21d4\u2212 [(1 \u2212 \u03f5t)e\u2212\u03b7 \u2212 \u03f5te\u03b7]=0\\n\u21d4 \u03b7= 1\\n2 log 1 \u2212 \u03f5t\\n\u03f5t\\n.\\nThis proves that', ' the step size chosen by coordinate descent matches the base\\nclassi\ufb01er weight \u03b1t of AdaBoost. Thus, coordinate descent applied to F precisely\\ncoincides with the AdaBoost algorithm.\\nIn light of this re', 'lationship, one may wish to consider similar applications of\\ncoordinate descent to other convex and di\ufb00erentiable functions of\u03b1 upper-bounding\\nthe zero-one loss. In particular, the logistic loss x \u21a6\u2192 ', 'log2(1 + e\u2212x)i sc o n v e xa n d\\ndi\ufb00erentiable and upper bounds the zero-one loss. Figure 6.4 shows other examples\\nof alternative convex loss functions upper-bounding the zero-one loss. Using the\\nlogi', 'stic loss, instead of the exponential loss used by AdaBoost, leads to an algorithm\\nthat coincides with logistic regression.6.2 AdaBoost 129\\n6.2.3 Relationship with logistic regression\\nLogistic regress', 'ion is a widely used binary classi\ufb01cation algorithm, a speci\ufb01c instance\\nof conditional maximum entropy models. For the purpose of this chapter, we\\n\ufb01rst give a very brief description of the algorithm. ', 'Logistic regression returns a\\nconditional probability distribution of the form\\np\\n\u03b1 [y|x]= 1\\nZ(x) exp(y \u03b1 \u00b7 \u03a6(x)), (6.7)\\nwhere y \u2208{ \u22121, +1} is the label predicted for x \u2208X , \u03a6(x) \u2208 RN is a feature vect', 'or\\nassociated to x, \u03b1 \u2208 RN a parameter weight vector, andZ(x)= e+\u03b1 \u00b7\u03a6(x) +e\u2212\u03b1 \u00b7\u03a6(x)\\na normalization factor. Dividing both the numerator and denominator by e+\u03b1 \u00b7\u03a6(x)\\nand taking the log leads to:\\nlog(p\u03b1', ' [y|x]) = \u2212 log(1 +e\u22122y\u03b1 \u00b7\u03a6(x)). (6.8)\\nThe parameter \u03b1 is learned via maximum likelihood by logistic regression, that is,\\nby maximizing the probability of the sampleS =( (x1,y s),..., (xm,y m)). Since', ' the\\npoints are sampled i.i.d., this can be written as follows: argmax \u03b1\\n\u220fm\\ni=1 p\u03b1 [yi|xi].\\nTaking the negative log of the probabilities shows that the objective function\\nminimized by logistic regress', 'ion is\\nG(\u03b1)=\\nm\u2211\\ni=1\\nlog(1 +e\u22122yi\u03b1 \u00b7\u03a6(xi)) . (6.9)\\nThus, modulo constants, which do not a\ufb00ect the solution sought, the objective\\nfunction coincides with the one based on the logistic loss. AdaBoost and', ' Logistic\\nregression have in fact many other relationships that we will not discuss in detail\\nhere. In particular, it can be shown that both algorithms solve exactly the same\\noptimization problem, exc', 'ept for a normalization constraint required for logistic\\nregression not imposed in the case of AdaBoost.\\n6.2.4 Standard use in practice\\nHere we brie\ufb02y describe the practical use of AdaBoost. An import', 'ant requirement\\nfor the algorithm is the choice of the base classi\ufb01ers or that of the weak learner. The\\nfamily of base classi\ufb01ers typically used with AdaBoost in practice is that ofdecision\\ntrees, whi', 'ch are equivalent to hierarchical partitions of the space (see chapter 8,\\nsection 8.3.3). In fact, more precisely, decision trees of depth one, also known as\\nstumps or boosting stumps are by far the m', 'ost frequently used base classi\ufb01ers.\\nBoosting stumps are threshold functions associated to a single feature. Thus,\\na stump corresponds to a single axis-aligned partition of the space, as illustrated13', '0 Boosting\\ntraining error\\ntest error\\nerror\\n0\\n10 100 1000\\nnumber of rounds -        log(T)\\nFigure 6.5 An empirical result using AdaBoost with C4.5 decision trees as base\\nlearners. In this example, the ', 'training error goes to zero after about 5 rounds of\\nboosting (T \u2248 5), yet the test error continues to decrease for larger values of T.\\nin \ufb01gure 6.2. If the data is in RN , we can associate a stump to ', 'each of the N\\ncomponents. Thus, to determine the stump with the minimal weighted error at\\neach of round of boosting, the best component and the best threshold for each\\ncomponent must be computed.\\nTo d', 'o so, we can \ufb01rst presort each component in O(m log m)t i m ew i t hat o t a l\\ncomputational cost of O(mN log m). For a given component, there are only m +1\\npossible distinct thresholds, since two thr', 'esholds between the same consecutive\\ncomponent values are equivalent. To \ufb01nd the best threshold at each round of\\nboosting, all of these possible m + 1 values can be compared, which can be done in\\nO(m)', ' time. Thus, the total computational complexity of the algorithm forT rounds\\nof boosting is O(mN log m + mNT ).\\nObserve, however, that while boosting stumps are widely used in combination\\nwith AdaBoos', 't and can perform well in practice, the algorithm that returns the\\nstump with the minimal empirical error is not a weak learner (see de\ufb01nition 6.1)!\\nConsider, for example, the simple XOR example with ', 'four data points lying in\\nR\\n2 (see \ufb01gure 5.2a), where points in the second and fourth quadrants are labeled\\npositively and those in the \ufb01rst and third quadrants negatively. Then, no decision\\nstump can', ' achieve an accuracy better than 1/2.\\n6.3 Theoretical results\\nIn this section we present a theoretical analysis of the generalization properties of\\nAdaBoost.6.3 Theoretical results 131\\n6.3.1 VC-dimens', 'ion-based analysis\\nW es t a r tw i t ha na n a l y s i so fA d a B o o s tb a s e do nt h eV C - d i m e n s i o no fi t sh y p o t h e s i s\\nset. For T rounds of boosting, its hypothesis set is\\nFT =\\n', '{\\nsgn\\n( T\u2211\\nt=1\\n\u03b1tht\\n\u23a1\\n: \u03b1t \u2208 R,h t \u2208 H,t \u2208 [1,T ]\\n}\\n. (6.10)\\nThe VC-dimension of FT can be bounded as follows in terms of the VC-dimension\\nd of the family of base hypothesis H (exercise 6.1):\\nVCdim(FT', ' ) \u2264 2(d +1 ) (T +1 )l o g2((T +1 )e) . (6.11)\\nThe upper bound grows as O(dT log T), thus the bound suggests that AdaBoost\\ncould over\ufb01t for large values of T, and indeed this can occur. However, in ma', 'ny\\ncases, it has been observed empirically that the generalization error of AdaBoost\\ndecreases as a function of the number of rounds of boosting T, as illustrated in\\n\ufb01gure 6.5! How can these empirical', ' results be explained? The following sections\\np r e s e n ta na n a l y s i sb a s e do nac o n c e p to fm a r g i n ,s i m i l a rt ot h eo n ep r e s e n t e df o r\\nSVMs.\\n6.3.2 Margin-based analysi', 's\\nIn chapter 4 we gave a de\ufb01nition of margin for linear classi\ufb01ers such as SVMs\\n(de\ufb01nition 4.2). Here we will need a somewhat di\ufb00erent but related de\ufb01nition of\\nmargin for linear combinations of base c', 'lassi\ufb01ers, as in the case of AdaBoost.\\nFirst note that a linear combination of base classi\ufb01ers g = \u2211\\nT\\nt=1 \u03b1tht can be\\nde\ufb01ned equivalently via g(x)= \u03b1 \u00b7 h(x) for all x \u2208X ,w i t h\u03b1 =( \u03b11,...,\u03b1 T )\u22a4 an', 'd\\nh(x)=[ h1(x),...,h T (x)]\u22a4. This makes their similarity with the linear hypotheses\\nconsidered in chapter 4 and chapter 5 evident:h(x) is the feature vector associated\\nto x, which was previously deno', 'ted by \u03a6( x), and \u03b1 is the weight vector that was\\ndenoted by w. The base classi\ufb01ers values ht(x) are the components of the feature\\nvector associated tox. For AdaBoost, additionally, the weight vector ', 'is non-negative:\\n\u03b1 \u2265 0.\\nWe will use the same notation to introduce the following de\ufb01nition.\\nDe\ufb01nition 6.2 L1-margin\\nThe L1-margin \u03c1(x) of a point x \u2208X with label y \u2208{ \u2212 1,+1} for a linear\\ncombination ', 'of base classi\ufb01ers g = \u2211T\\nt=1 \u03b1tht with \u03b1 \u0338=0 and ht \u2208 H for all\\nt \u2208 [1,T ] is de\ufb01ned as\\n\u03c1(x)= yg(x)\u2211m\\nt=T |\u03b1t| = y \u2211T\\nt=1 \u03b1tht(x)\\n\u2225\u03b1\u22251\\n= y\u03b1 \u00b7 h(x)\\n\u2225\u03b1\u22251\\n. (6.12)132 Boosting\\nThe L1-margin of a linear ', 'combination classi\ufb01er g w i t hr e s p e c tt oas a m p l eS =\\n(x1,...,x m) is the minimum margin of the points within the sample:\\n\u03c1=m i n\\ni\u2208[1,m]\\nyi\\n\u03b1 \u00b7 h(xi)\\n\u2225\u03b1\u22251\\n. (6.13)\\nWhen the coe\ufb03cients \u03b1t are', ' non-negative, as in the case of AdaBoost, \u03c1(x)i sa\\nconvex combination of the base classi\ufb01er values ht(x). In particular, if the base\\nclassi\ufb01ers ht take values in [ \u22121, +1], then \u03c1(x)i si n[ \u22121,+1]. T', 'he absolute value\\n|\u03c1(x)| can be interpreted as the con\ufb01dence of the classi\ufb01er g in that label.\\nThis de\ufb01nition of margin di\ufb00ers from de\ufb01nition 4.2 given for linear classi\ufb01ers only\\nby the norm used for ', 'the weight vector: L1 norm here, L2 norm in de\ufb01nition 4.2.\\nIndeed, in the case of a linear hypothesisx \u21a6\u2192 w \u00b7\u03a6(x), the margin for point x with\\nlabel y was de\ufb01ned as follows:\\n\u03c1(x)= yw \u00b7 \u03c6(x)\\n\u2225w\u22252\\nand w', 'as based on the L2 norm of w. When the prediction is correct, that is\\ny(\u03b1 \u00b7 h(x)) \u2265 0, the L1-margin and L2 margin of de\ufb01nition 4.2 can be rewritten as\\n\u03c11(x)= |\u03b1 \u00b7 h(x)|\\n\u2225\u03b1\u22251\\nand \u03c12(x)= |\u03b1 \u00b7 h(x)|\\n\u2225\u03b1\u2225', '2\\n.\\nIt is known that forp, q \u2265 1, p and q conjugate, i.e. 1/p+1/q =1 ,t h a t|\u03b1 \u00b7x|/\u2225\u03b1\u2225p\\nis the Lq distance of x to the hyperplane of equation\u03b1 \u00b7x =0 .T h u s ,b o t h\u03c11(x)a n d\\n\u03c12(x) measure the dist', 'ance of the feature vectorh(x)t ot h eh y p e r p l a n e\u03b1 \u00b7x =0i n\\nRT , \u03c11(x)i t s\u2225\u00b7\u2225 \u221e distance, \u03c12(x)i t s\u2225\u00b7\u2225 2 or Euclidean distance (see \ufb01gure 6.6).\\nTo examine the generalization properties of Ad', 'aBoost, we \ufb01rst analyze the\\nRademacher complexity of convex combinations of hypotheses such as those de\ufb01ned\\nby AdaBoost. Next, we use the margin-based analysis from chapter 4 to derive a\\nmargin-based ', 'generalization bound for boosting with the de\ufb01nition of margin just\\nintroduced.\\nFor any hypothesis set H of real-valued functions, we denote by conv( H)i t s\\nconvex hull de\ufb01ned by\\nconv(H)=\\n{\\np\u2211\\nk=1\\n\u03bck', 'hk : p \u2265 1, \u2200k \u2208 [1,p ],\u03bck \u2265 0,h k \u2208 H,\\np\u2211\\nk=1\\n\u03bck \u2264 1\\n}\\n. (6.14)\\nThe following theorem shows that, remarkably, the empirical Rademacher complex-\\nity of conv(H), which in general is a strictly larger s', 'et including H,c o i n c i d e sw i t h\\nthat of H.6.3 Theoretical results 133\\nTheorem 6.2\\nLet H be a set of functions mapping fromX to R.T h e n ,f o ra n ys a m p l eS, we have\\n\u02c6RS(conv(H)) = \u02c6RS(H) ', '.\\nProof The proof follows from a straightforward series of equalities:\\n\u02c6RS(conv(H)) = 1\\nm E\\n\u03c3\\n[\\nsup\\nh1,...,hp\u2208H,\u03bc \u22650,\u2225\u03bc \u22251\u22641\\nm\u2211\\ni=1\\n\u03c3i\\np\u2211\\nk=1\\n\u03bckhk(xi)\\n]\\n= 1\\nm E\\n\u03c3\\n[\\nsup\\nh1,...,hp\u2208H\\nsup\\n\u03bc \u22650,\u2225\u03bc \u22251\u22641\\np\u2211', '\\nk=1\\n\u03bck\\n( m\u2211\\ni=1\\n\u03c3ihk(xi)\\n\u23a1]\\n= 1\\nm E\\n\u03c3\\n[\\nsup\\nh1,...,hp\u2208H\\nmax\\nk\u2208[1,p]\\n( m\u2211\\ni=1\\n\u03c3ihk(xi)\\n\u23a1]\\n= 1\\nm E\\n\u03c3\\n[\\nsup\\nh\u2208H\\nm\u2211\\ni=1\\n\u03c3ih(xi)\\n]\\n= \u02c6RS(H).\\nThe main equality to recognize is the third one, which is based', ' on the observation\\nthat the maximizing vector \u03bc for the convex combination of p t e r m si st h eo n e\\nplacing all the weight on the largest term of the sum.\\nThis theorem can be used directly in comb', 'ination with theorem 4.4 to derive\\nthe following Rademacher complexity generalization bound for convex combination\\nensembles of hypotheses.\\nCorollary 6.1 Ensemble Rademacher margin bound\\nLet H denote ', 'a set of real-valued functions. Fix \u03c1> 0.T h e n ,f o ra n y\u03b4> 0,w i t h\\nprobability at least 1 \u2212 \u03b4, each of the following holds for all h \u2208 conv(H):\\nR(h) \u2264 \u02c6R\u03c1(h)+ 2\\n\u03c1Rm\\n(\\nH\\n\u23a1\\n+\\n\u221a\\nlog 1\\n\u03b4\\n2m (6.15)\\nR', '(h) \u2264 \u02c6R\u03c1(h)+ 2\\n\u03c1\\n\u02c6RS\\n(\\nH\\n\u23a1\\n+3\\n\u221a\\nlog 2\\n\u03b4\\n2m . (6.16)\\nUsing corollary 3.1 and corollary 3.3 to bound the Rademacher complexity in\\nterms of the VC-dimension yields immediately the following VC-dimension', '-based\\ngeneralization bounds for convex combination ensembles of hypotheses.\\nCorollary 6.2 Ensemble VC-Dimension margin bound\\nLet H be a family of functions taking values in{+1, \u22121} with VC-dimension ', 'd.F i x\\n\u03c1> 0.T h e n ,f o ra n y\u03b4> 0, with probability at least 1 \u2212 \u03b4, the following holds for134 Boosting\\nall h \u2208 conv(H):\\nR(h) \u2264 \u02c6R\u03c1(h)+ 2\\n\u03c1\\n\u221a\\n2d log em\\nd\\nm +\\n\u221a\\nlog 1\\n\u03b4\\n2m . (6.17)\\nThese bounds can ', 'be generalized to hold uniformly for all \u03c1> 0, instead of a \ufb01xed\\n\u03c1, at the price of an additional term of the form\\n\u221a\\n(log log2\\n2\\n\u03b4)/m as in theorem 4.5.\\nThey cannot be directly applied to the linear c', 'ombinationg generated by AdaBoost,\\nsince it is not a convex combination of base hypotheses, but they can be applied to\\nthe following normalized version of g:\\nx \u21a6\u2192 g(x)\\n\u2225\u03b1\u22251\\n=\\n\u2211T\\nt=1 \u03b1tht(x)\\n\u2225\u03b1\u22251\\n\u2208 con', 'v(H) . (6.18)\\nNote that from the point of view of binary classi\ufb01cation,g and g/\u2225\u03b1\u22251 are equivalent\\nsince sgn(g)=s g n (g/\u2225\u03b1\u22251), thus R(g)= R(g/\u2225\u03b1\u22251), but their empirical margin\\nloss are distinct. Let ', 'g = \u2211T\\nt=1 \u03b1tht denote the function de\ufb01ning the classi\ufb01er\\nreturned by AdaBoost after T rounds of boosting when trained on sampleS. Then,\\nin view of (6.15), for any \u03b4> 0, the following holds with proba', 'bility at least 1 \u2212 \u03b4:\\nR(g) \u2264 \u02c6R\u03c1(g/\u2225\u03b1\u22251)+ 2\\n\u03c1Rm\\n(\\nH\\n\u23a1\\n+\\n\u221a\\nlog 1\\n\u03b4\\n2m . (6.19)\\nSimilar bounds can be derived from (6.16) and (6.17). Remarkably, the number\\nof rounds of boosting T does not appear in t', 'he generalization bound (6.19). The\\nbound depends only on the margin \u03c1, the sample size m, and the Rademacher\\ncomplexity of the family of base classi\ufb01ers H. Thus, the bound guarantees an\\ne\ufb00ective gene', 'ralization if the margin loss \u02c6R\u03c1(g/\u2225\u03b1\u22251) is small for a relatively large\\n\u03c1. Recall that the margin loss can be upper bounded by the fraction of the points\\nx in the training sample with g(x)/\u2225\u03b1\u22251 \u2265 \u03c1(', 'see (4.39)). Thus, with our de\ufb01nition\\nof L1-margin, it can be bounded by the fraction of the points in S with L1-margin\\nmore than \u03c1:\\n\u02c6R\u03c1(g/\u2225\u03b1\u22251) \u2264 |{i \u2208 [1,m]: \u03c1(xi) \u2265 \u03c1}|\\nm . (6.20)\\nAdditionally, the', ' following theorem provides a bound on the empirical margin loss,\\nwhich decreases with T under conditions discussed later.\\nTheorem 6.3\\nLet g = \u2211T\\nt=1 \u03b1tht denote the function de\ufb01ning the classi\ufb01er ret', 'urned by AdaBoost\\nafter T rounds of boosting and assume for all t \u2208 [1,T ] that \u03f5t < 1\\n2, which implies6.3 Theoretical results 135\\nat > 0.T h e n ,f o ra n y\u03c1> 0, the following holds:\\n\u02c6R\u03c1\\n( g\\n\u2225\u03b1\u22251\\n\u23a1\\n\u2264', ' 2T\\nT\u220f\\nt=1\\n\u221a\\n\u03f51\u2212\u03c1\\nt (1 \u2212 \u03f5t)1+\u03c1 .\\nProof Using the general inequality 1 u\u22640 \u2264 exp(\u2212u) valid for all u \u2208 R, iden-\\ntity 6.2, that is Dt+1(i)= e\u2212 yig(xi)\\nm QT\\nt=1 Zt\\n, the equality Zt =2\\n\u221a\\n\u03f5t(1 \u2212 \u03f5t) from', ' the proof\\nof theorem 6.1, and the de\ufb01nition of \u03b1 in AdaBoost, we can write:\\n1\\nm\\nm\u2211\\ni=1\\n1yig(xi)\u2212\u03c1\u2225\u03b1 \u22251\u22640 \u2264 1\\nm\\nm\u2211\\ni=1\\nexp(\u2212yig(xi)+ \u03c1\u2225\u03b1\u22251)\\n= 1\\nm\\nm\u2211\\ni=1\\ne\u03c1\u2225\u03b1 \u22251\\n[\\nm\\nT\u220f\\nt=1\\nZt\\n]\\nDT+1(i)\\n= e\u03c1\u2225\u03b1 \u22251\\nT\u220f\\nt=', '1\\nZt = e\u03c1 P\\ni \u03b1i\\nT\u220f\\nt=1\\nZt\\n=2 T\\nT\u220f\\nt=1\\n[\u221a\\n1\u2212\u03f5t\\n\u03f5t\\n]\u03c1\u221a\\n\u03f5t(1 \u2212 \u03f5t) ,\\nwhich concludes the proof.\\nMoreover, if for all t \u2208 [1,T ]w eh a v e\u03b3 \u2264 (1\\n2 \u2212 \u03f5t)a n d\u03c1 \u2264 2\u03b3, then the expression\\n4\u03f51\u2212\u03c1\\nt (1\u2212\u03f5t)1+\u03c1 ', 'is maximized at\u03f5t = 1\\n2 \u2212\u03b3.1 Thus, the upper bound on the empirical\\nm a r g i nl o s sc a nt h e nb eb o u n d e db y\\n\u02c6R\u03c1\\n( g\\n\u2225\u03b1\u22251\\n\u23a1\\n\u2264\\n[\\n(1 \u2212 2\u03b3)1\u2212\u03c1(1 + 2\u03b3)1+\u03c1\\n]T/2\\n. (6.21)\\nObserve that (1 \u2212 2\u03b3)1\u2212\u03c1(1', ' + 2\u03b3)1+\u03c1 =( 1 \u2212 4\u03b32)\\n(1+2\u03b3\\n1\u22122\u03b3\\n\u23a1\u03c1\\n.T h i si sa ni n c r e a s i n g\\nfunction of \u03c1 s i n c ew eh a v e\\n(1+2\u03b3\\n1\u22122\u03b3\\n\u23a1\\n> 1 as a consequence of \u03b3> 0. Thus, if \u03c1<\u03b3 ,\\nit can be strictly upper bounded as fo', 'llows\\n(1 \u2212 2\u03b3)1\u2212\u03c1(1 + 2\u03b3)1+\u03c1 < (1 \u2212 2\u03b3)1\u2212\u03b3(1 + 2\u03b3)1+\u03b3.\\nThe function \u03b3 \u21a6\u2192 (1 \u2212 2\u03b3)1\u2212\u03b3(1 + 2\u03b3)1+\u03b3 is strictly upper bounded by 1 over the\\ninterval (0, 1/2), thus, if \u03c1<\u03b3 ,t h e n( 1\u2212 2\u03b3)1\u2212\u03c1(1+2 \u03b3)1+\u03c1 < ', '1 and the right-hand\\nside of (6.21) decreases exponentially with T. Since the condition \u03c1 \u226b O(1/\u221am)i s\\nnecessary in order for the given margin bounds to converge, this places a condition\\n1. The di\ufb00ere', 'ntial of f : \u03f5 \u21a6\u2192 log[\u03f51\u2212\u03c1(1 \u2212 \u03f5)1+\u03c1]=( 1 \u2212 \u03c1)l o g\u03f5 +( 1+ \u03c1)l o g( 1\u2212 \u03f5)o v e rt h e\\ninterval (0, 1) is given byf \u2032(\u03f5)= 1\u2212\u03c1\\n\u03f5 \u2212 1+\u03c1\\n1\u2212\u03f5 =2\\n( 1\\n2 \u2212 \u03c1\\n2 )\u2212\u03f5\\n\u03f5(1\u2212e) .T h u s ,f is an increasing function', '\\nover (0, 1\\n2 \u2212 \u03c1\\n2 ), which implies that it is increasing over (0, 1\\n2 \u2212 \u03b3) when \u03b3 \u2265 \u03c1\\n2 .136 Boosting\\nNorm | |\u00b7| |2. Norm | |\u00b7| |\u221e.\\nFigure 6.6 Maximum margins with respect to both the L2 and L\u221e norm', '.\\nof \u03b3 \u226b O(1/\u221am) on the edge value. In practice, the error\u03f5t of the base classi\ufb01er at\\nround t may increase as a function of t. Informally, this is because boosting presses\\nthe weak learner to concentr', 'ate on instances that are harder and harder to classify,\\nfor which even the best base classi\ufb01er could not achieve an error signi\ufb01cantly better\\nthan random. If \u03f5t becomes close to 1/2 relatively fast a', 's a function of t, then the\\nbound of theorem 6.3 becomes uninformative.\\nThe margin bounds of corollary 6.1 and corollary 6.2, combined with the bound\\non the empirical margin loss of theorem 6.3, sugge', 'st that under some conditions,\\nAdaBoost can achieve a large margin on the training sample. They could also serve\\nas a theoretical explanation of the empirical observation that in some tasks the\\ngenera', 'lization error increases as a function ofT even after the error on the training\\nsample is zero: the margin would continue to increase. But does AdaBoost maximize\\nthe L\\n1-margin?\\nNo. It has been shown ', 'that AdaBoost may converge to a margin that is signi\ufb01-\\ncantly smaller than the maximum margin (e.g., 1/3 instead of 3/8 ) .H o w e v e r ,u n d e r\\nsome general assumptions, when the data is separable', ' and the base learners satisfy\\nparticular conditions, it has been proven that AdaBoost can asymptotically achieve\\na margin that is at least half the maximum margin, \u03c1max/2.\\n6.3.3 Margin maximization\\nI', 'n view of these results, several algorithms have been devised with the explicit goal\\nof maximizing theL\\n1-margin. These algorithms correspond to di\ufb00erent methods for\\nsolving a linear program (LP).\\nBy ', 'de\ufb01nition of the L1-margin, the maximum margin for a sample S =\\n((x1,y1),..., (xm,y m)) is given by\\n\u03c1=m a x\\n\u03b1\\nmin\\ni\u2208[1,m]\\nyi\\n\u03b1 \u00b7 h(xi)\\n\u2225\u03b1\u22251\\n. (6.22)6.3 Theoretical results 137\\nBy de\ufb01nition of the maxi', 'mization, the optimization problem can be written as:\\nmax\\n\u03b1\\n\u03c1\\nsubject to : yi\\n\u03b1 \u00b7 h(xi)\\n\u2225\u03b1\u22251\\n\u2265 \u03c1, \u2200i \u2208 [1,m].\\nSince \u03b1 \u00b7h(xi)\\n\u2225\u03b1 \u22251\\nis invariant to the scaling of\u03b1, we can restrict ourselves to \u2225\u03b1\u22251 =1', ' .\\nFurther seeking a non-negative \u03b1 as in the case of AdaBoost leads to the following\\noptimization:\\nmax\\n\u03b1\\n\u03c1\\nsubject to : yi(\u03b1 \u00b7 h(xi)) \u2265 \u03c1, \u2200i \u2208 [1,m]\\n( T\u2211\\nt=1\\n\u03b1t =1\\n\u23a1\\n\u2227 (\u03b1t \u2265 0, \u2200t \u2208 [1,T ]).\\nThis is', ' a linear program (LP), that is, an optimization problem with a linear\\nobjective function and linear constraints. There are several di\ufb00erent methods for\\nsolving relative large LPs in practice, using t', 'he simplex method, interior-point\\nmethods, or a variety of special-purpose solutions.\\nNote that the solution of this algorithm di\ufb00ers from the margin-maximization\\nde\ufb01ning SVMs in the separable case on', 'ly by the de\ufb01nition of the margin used ( L\\n1\\nversus L2) and the non-negativity constraint on the weight vector. Figure 6.6 illus-\\ntrates the margin-maximizing hyperplanes found using these two distinc', 't margin\\nde\ufb01nitions in a simple case. The left \ufb01gure shows the SVM solution, where the dis-\\ntance to the closest points to the hyperplane is measured with respect to the norm\\n\u2225\u00b7\u2225\\n2. The right \ufb01gure sh', 'ows the solution for the L1-margin, where the distance to\\nthe closest points to the hyperplane is measured with respect to the norm \u2225\u00b7\u2225 \u221e .\\nBy de\ufb01nition, the solution of the LP just described admits a', 'n L1-margin that\\nis larger or equal to that of the AdaBoost solution. However, empirical results do\\nnot show a systematic bene\ufb01t for the solution of the LP. In fact, it appears that in\\nmany cases, Ada', 'Boost outperforms that algorithm. The margin theory described\\ndoes not seem su\ufb03cient to explain that performance.\\n6.3.4 Game-theoretic interpretation\\nIn this section, we \ufb01rst show that AdaBoost admits', ' a natural game-theoretic\\ninterpretation. The application of von Neumann\u2019s theorem then helps us relate the\\nmaximum margin and the optimal edge and clarify the connection of AdaBoost\u2019s\\nweak-learning a', 'ssumption with the notion of L\\n1-margin. We \ufb01rst introduce the\\nde\ufb01nition of the edge for a speci\ufb01c classi\ufb01er and a particular distribution.138 Boosting\\nrock paper scissors\\nrock 0 +1 -1\\npaper -1 0 +1\\ns', 'cissors +1 -1 0\\nT able 6.1 The loss matrix for the standard rock-paper-scissors game.\\nDe\ufb01nition 6.3\\nThe edge of a base classi\ufb01er ht for a distribution D o v e rt h et r a i n i n gs a m p l ei s\\nde\ufb01ne', 'd by\\n\u03b3t(D)= 1\\n2 \u2212 \u03f5t = 1\\n2\\nm\u2211\\ni=1\\nyiht(xi)D(i). (6.23)\\nAdaBoost\u2019s weak learning condition can now be formulated as: there exists \u03b3> 0\\nsuch that for any distributionD over the training sample and any b', 'ase classi\ufb01erht,\\nthe following holds:\\n\u03b3t(D) \u2265 \u03b3. (6.24)\\nThis condition is required for the analysis of theorem 6.1 and the non-negativity of\\nthe coe\ufb03cients \u03b1t. We will frame boosting as a two-person z', 'ero-sum game.\\nDe\ufb01nition 6.4 Zero-sum game\\nA two-person zero-sum game consists of a loss matrix M \u2208 Rm\u00d7n,w h e r em is the\\nnumber of possible actions (or pure strategies) for the row player andn the nu', 'mber\\nof possible actions for the column player. The entry Mij i st h el o s sf o rt h er o w\\nplayer (or equivalently the payo\ufb00 for the column payer) when the row player takes\\naction i and the column p', 'layer takes action j.\\n2\\nAn example of a loss matrix for the familiar \u201crock-paper-scissors\u201d game is shown\\nin table 6.1.\\nDe\ufb01nition 6.5 Mixed strategy\\nA mixed strategy for the row player is a distributio', 'n p over the m possible row\\nactions, a distribution q over the n possible column actions for the column player.\\nThe expected loss for the row player (expected payo\ufb00 for the column player) with\\n2. To b', 'e consistent with the results discussed in other chapters, we consider the loss matrix\\nas opposed to the payo\ufb00 matrix (its opposite).6.3 Theoretical results 139\\nrespect to the mixed strategiesp and q ', 'is\\nE[loss]= p\u22a4Mq =\\nm\u2211\\ni=1\\nn\u2211\\nj=1\\npiMijqj .\\nThe following is a fundamental result in game theory proven in chapter 7.\\nTheorem 6.4 Von Neumann\u2019s minimax theorem\\nFor any two-person zero-sum game de\ufb01ned b', 'y matrix M,\\nmin\\np\\nmax\\nq\\np\u22a4Mq =m a x\\nq\\nmin\\np\\np\u22a4Mq. (6.25)\\nThe common value in (6.25) is called the value of the game . The theorem states\\nthat for any two-person zero-sum game, there exists a mixed str', 'ategy for each player\\nsuch that the expected loss for one is the same as the expected payo\ufb00 for the other,\\nboth of which are equal to the value of the game. Note that, given the row player\u2019s\\nstrategy,', ' the column player can choose an optimal pure strategy, that is, the column\\nplayer can choose the single strategy corresponding the smallest coordinate of the\\nvector p\\n\u22a4M. A similar comment applies to', ' the reverse. Thus, an alternative and\\nequivalent form of the minimax theorem is\\nmax\\np\\nmin\\nj\u2208[1,n]\\np\u22a4Mej =m i n\\nq\\nmax\\ni\u2208[1,m]\\ne\u22a4\\ni Mq, (6.26)\\nwhere ei denotes the ith unit vector. We can now view AdaB', 'oost as a zero-sum\\ngame, where an action of the row player is the selection of a training instance\\nx\\ni, i \u2208 [1,m ], and an action of the column player the selection of a base learner\\nht, t \u2208 [1,T ]. A', ' mixed strategy for the row player is thus a distribution D over\\nthe training points\u2019 indices [1 ,m]. A mixed strategy for the column player is a\\ndistribution over the based classi\ufb01ers\u2019 indices [1 ,T ', ']. This can be de\ufb01ned from a\\nnon-negative vector \u03b1 \u2265 0: the weight assigned to t \u2208 [1,T ]i s \u03b1t/\u2225\u03b1\u22251.T h e\\nloss matrix M \u2208{ \u2212 1, +1}m\u00d7T for AdaBoost is de\ufb01ned by Mit = yiht(xi)f o r\\nall (i, t) \u2208 [1,m]', ' \u00d7 [1,T ]. By von Neumann\u2019s theorem (6.26), the following holds:\\nmin\\nD\u2208D\\nmax\\nt\u2208[1,T]\\nm\u2211\\ni=1\\nD(i)yiht(xi)=m a x\\n\u03b1 \u22650\\nmin\\ni\u2208[1,m]\\nT\u2211\\nt=1\\n\u03b1t\\n\u2225\u03b1\u22251\\nyiht(xi), (6.27)\\nwhere D denotes the set of all distribut', 'ions over the training sample. Let \u03c1\u03b1 (x)\\ndenote the margin of point x for the classi\ufb01er de\ufb01ned by g = \u2211T\\nt=1 \u03b1tht.T h er e s u l t\\nc a nb er e w r i t t e na sf o l l o w si nt e r m so ft h em a r g', ' i n sa n de d g e s :\\n2\u03b3\u2217 =2m i n\\nD\\nmax\\nt\u2208[1,T]\\n\u03b3t(D)=m a x\\n\u03b1\\nmin\\ni\u2208[1,m]\\n\u03c1\u03b1 (xi)= \u03c1\u2217, (6.28)140 Boosting\\nwhere \u03c1\u2217 is the maximum margin of a classi\ufb01er and \u03b3\u2217 the best possible edge. This\\nresult has ', 'several implications. First, it shows that the weak learning condition\\n(\u03b3\\n\u2217 > 0) implies \u03c1\u2217 > 0 and thus the existence of a classi\ufb01er with positive margin,\\nwhich motivates the search for a non-zero ma', 'rgin. AdaBoost can be viewed as an\\nalgorithm seeking to achieve such a non-zero margin, though, as discussed earlier,\\nAdaBoost does not always achieve an optimal margin and is thus suboptimal in that\\n', 'respect. Furthermore, we see that the \u201cweak learning\u201d assumption, which originally\\nappeared to be the weakest condition one could require for an algorithm (that of\\nperforming better than random), is i', 'n fact a strong condition: it implies that the\\ntraining sample is linearly separable with margin 2\u03b3\\n\u2217 > 0. Linear separability often\\ndoes not hold for the data sets found in practice.\\n6.4 Discussion\\nA', 'daBoost o\ufb00ers several advantages: it is simple, its implementation is straightfor-\\nward, and the time complexity of each round of boosting as a function of the sample\\nsize is rather favorable. As alre', 'ady discussed, when using decision stumps, the time\\ncomplexity of each round of boosting is in O(mN). Of course, if the dimension of\\nthe feature space N is very large, then the algorithm could become ', 'in fact quite\\nslow.\\nAdaBoost additionally bene\ufb01ts from a rich theoretical analysis. Nevertheless,\\nthere are still many theoretical questions. For example, as we saw, the algorithm in\\nfact does not max', 'imize the margin, and yet algorithms that do maximize the margin\\ndo not always outperform it. This suggests that perhaps a \ufb01ner analysis based on a\\nnotion di\ufb00erent from that of margin could shed more ', 'light on the properties of the\\nalgorithm.\\nThe main drawbacks of the algorithm are the need to select the parameterT and\\nthe base classi\ufb01ers, and its poor performance in the presence of noise. The choi', 'ce of\\nthe number of rounds of boostingT (stopping criterion) is crucial to the performance\\nof the algorithm. As suggested by the VC-dimension analysis, larger values ofT can\\nlead to over\ufb01tting. In pra', 'ctice, T is typically determined via cross-validation.\\nThe choice of the base classi\ufb01ers is also crucial. The complexity of the family\\nof base classi\ufb01ers H appeared in all the bounds presented and it ', 'is important to\\ncontrol it in order to guarantee generalization. On the other hand, insu\ufb03ciently\\ncomplex hypothesis sets could lead to low margins.\\nProbably the most serious disadvantage of AdaBoost i', 's its performance in the\\npresence of noise; it has been shown empirically that noise severely damages its\\naccuracy. The distribution weight assigned to examples that are harder to classify\\nsubstantial', 'ly increases with the number of rounds of boosting, by the nature of the6.5 Chapter notes 141\\nalgorithm. These examples end up dominating the selection of the base classi\ufb01ers,\\nwhich, with a large enou', 'gh number of rounds, will play a detrimental role in the\\nde\ufb01nition of the linear combination de\ufb01ned by AdaBoost.\\nSeveral solutions have been proposed to address these issues. One consists of using\\na \u201c', 'less aggressive\u201d objective function than the exponential function of AdaBoost,\\nsuch as the logistic loss, to penalize less incorrectly classi\ufb01ed points. Another\\nsolution is based on a regularization, ', 'e.g., an L\\n1-regularization, which consists of\\nadding a term to the objective function to penalize larger weights. This could be\\nviewed as a soft margin approach for boosting. However, recent theoreti', 'cal results\\nshow that boosting algorithms based on convex potentials do not tolerate even low\\nlevels of random noise, even with L1-regularization or early stopping.\\nThe behavior of AdaBoost in the pre', 'sence of noise can be used, however, as a\\nuseful feature for detecting outliers, that is, examples that are incorrectly labeled\\nor that are hard to classify. Examples with large weights after a certai', 'n number of\\nrounds of boosting can be identi\ufb01ed as outliers.\\n6.5 Chapter notes\\nThe question of whether a weak learning algorithm could be boosted to derive a\\nstrong learning algorithm was \ufb01rst posed b', 'y Kearns and Valiant [1988, 1994], who\\nalso gave a negative proof of this result for a distribution-dependent setting. The\\n\ufb01rst positive proof of this result in a distribution-independent setting was ', 'given by\\nSchapire [1990], and later by Freund [1990].\\nThese early boosting algorithms, boosting by \ufb01ltering [Schapire, 1990] or boosting\\nby majority [Freund, 1990, 1995] were not practical. The AdaBoo', 'st algorithm\\nintroduced by Freund and Schapire [1997] solved several of these practical issues.\\nFreund and Schapire [1997] further gave a detailed presentation and analysis of the\\nalgorithm including ', 'the bound on its empirical error, a VC-dimension analysis, and\\nits applications to multi-class classi\ufb01cation and regression.\\nEarly experiments with AdaBoost were carried out by Drucker, Schapire, and\\n', 'Simard [1993], who gave the \ufb01rst implementation in OCR with weak learners based\\non neural networks and Drucker and Cortes [1995], who reported the empirical\\nperformance of AdaBoost combined with decis', 'ion trees, in particular decision\\nstumps.\\nThe fact that AdaBoost coincides with coordinate descent applied to an expo-\\nnential objective function was later shown by Du\ufb00y and Helmbold [1999], Mason\\net ', 'al. [1999], and Friedman [2000]. Friedman, Hastie, and Tibshirani [2000] also\\ngave an interpretation of boosting in terms of additive models. They also pointed\\nout the close connections between AdaBoo', 'st and logistic regression, in particular142 Boosting\\nthe fact that their objective functions have a similar behavior near zero or the\\nfact that their expectation admit the same minimizer, and derived', ' an alternative\\nboosting algorithm, LogitBoost, based on the logistic loss. La\ufb00erty [1999] showed\\nhow an incremental family of algorithms, including LogitBoost, can be derived from\\nBregman divergences', ' and designed to closely approximate AdaBoost when varying\\na parameter. Kivinen and Warmuth [1999] observed that boosting can be viewed\\nas a type of entropy projection. Collins, Schapire, and Singer [', '2002] later showed\\nthat boosting and logistic regression were special instances of a common framework\\nbased on Bregman divergences and used that to give the \ufb01rst convergence proof\\nof AdaBoost. Probabl', 'y the most direct relationship between AdaBoost and logis-\\ntic regression is the proof by Lebanon and La\ufb00erty [2001] that the two algorithms\\nminimize the same extended relative entropy objective funct', 'ion subject to the same\\nfeature constraints, except from an additional normalization constraint for logistic\\nregression.\\nA margin-based analysis of AdaBoost was \ufb01rst presented by Schapire, Freund,\\nBar', 'tlett, and Lee [1997], including theorem 6.3 which gives a bound on the empirical\\nmargin loss. Our presentation is based on the elegant derivation of margin bounds\\nby Koltchinskii and Panchenko [2002]', ' using the notion of Rademacher complexity.\\nRudin et al. [2004] gave an example showing that, in general, AdaBoost does not\\nmaximize the L\\n1-margin. R\u00a8atsch and Warmuth [2002] provided asymptotic lowe', 'r\\nbounds for the margin achieved by AdaBoost under some conditions. TheL1-margin\\nmaximization based on a LP is due to Grove and Schuurmans [1998]. The game-\\ntheoretic interpretation of boosting and th', 'e application of von Neumann\u2019s minimax\\ntheorem [von Neumann, 1928] in that context were pointed out by Freund and\\nSchapire [1996, 1999b]; see also Grove and Schuurmans [1998], Breiman [1999].\\nDietteri', 'ch [2000] provided extensive empirical evidence for the fact that noise can\\nseverely damage the accuracy of AdaBoost. This has been reported by a number of\\nother authors since then. R\u00a8atsch, Onoda, an', 'd M\u00a8uller [2001] suggested the use of a\\nsoft margin for AdaBoost based on a regularization of the objective function and\\npointed out its connections with SVMs. Long and Servedio [2010] recently showed', '\\nthe failure of boosting algorithms based on convex potentials to tolerate random\\nnoise, even with L\\n1-regularization or early stopping.\\nThere are several excellent surveys and tutorials related to bo', 'osting [Schapire,\\n2003, Meir and R\u00a8atsch, 2002, Meir and R\u00a8atsch, 2003].\\n6.6 Exercises\\n6.1 VC-dimension of the hypothesis set of AdaBoost.\\nProve the upper bound on the VC-dimension of the hypothesis s', 'etFT of AdaBoost6.6 Exercises 143\\nafter T rounds of boosting, as stated in equation 6.11.\\n6.2 Alternative objective functions.\\nThis problem studies boosting-type algorithms de\ufb01ned with objective funct', 'ions\\ndi\ufb00erent from that of AdaBoost. We assume that the training data are given as\\nm labeled examples ( x\\n1,y1),..., (xm,y m) \u2208 X \u00d7{ \u2212 1, +1}.W ef u r t h e ra s s u m e\\nthat \u03a6 is a strictly increasin', 'g convex and di\ufb00erentiable function over R such that:\\n\u2200x \u2265 0, \u03a6(x) \u2265 1a n d\u2200x< 0, \u03a6(x) > 0.\\n(a) Consider the loss function L(\u03b1)= \u2211m\\ni=1 \u03a6(\u2212yig(xi)) where g is a linear\\ncombination of base classi\ufb01ers, ', 'i.e., g = \u2211T\\nt=1 \u03b1tht (as in AdaBoost). Derive a\\nnew boosting algorithm using the objective function L. In particular, charac-\\nterize the best base classi\ufb01er hu to select at each round of boosting if ', 'we use\\ncoordinate descent.\\n(b) Consider the following functions: (1) zero-one loss \u03a61(\u2212u)=1 u\u22640; (2) least\\nsquared loss \u03a62(\u2212u)=( 1 \u2212 u)2; (3) SVM loss \u03a63(\u2212u)=m a x{0, 1 \u2212 u};a n d( 4 )\\nlogistic loss \u03a6', '4(\u2212u)=l o g ( 1+e\u2212u). Which functions satisfy the assumptions\\non \u03a6 stated earlier in this problem?\\n(c) For each loss function satisfying these assumptions, derive the correspond-\\ning boosting algorith', 'm. How do the algorithm(s) di\ufb00er from AdaBoost?\\n6.3 Update guarantee. Assume that the main weak learner assumption of AdaBoost\\nholds. Let ht be the base learner selected at round t. Show that the base', ' learner\\nht+1 selected at round t + 1 must be di\ufb00erent fromht.\\n6.4 Weighted instances. Let the training sample be S =( (x1,y1),..., (xm,y m)).\\nS u p p o s ew ew i s ht op e n a l i z ed i \ufb00 e r e n t ', 'l ye r r o r sm a d eo nxi versus xj.T od ot h a t ,w e\\nassociate some non-negative importance weight wi to each point xi and de\ufb01ne the\\nobjective function F(\u03b1)= \u2211m\\ni=1 wie\u2212yig(xi),w h e r eg = \u2211T\\nt=1 ', '\u03b1tht. Show that this\\nfunction is convex and di\ufb00erentiable and use it to derive a boosting-type algorithm.\\n6.5 De\ufb01ne the unnormalized correlation of two vectorsx and x\u2032 as the inner product\\nbetween the', 'se vectors. Prove that the distribution vector ( Dt+1(1),...,D t+1(m))\\nde\ufb01ned by AdaBoost and the vector of components yiht(xi) are uncorrelated.\\n6.6 Fix \u03f5 \u2208 (0, 1/2 ) .L e tt h et r a i n i n gs a m ', 'p l eb ed e \ufb01 n e db ym points in the plane\\nwith m\\n4 negative points all at coordinate (1 , 1), another m\\n4 negative points all at\\ncoordinate ( \u22121, \u22121), m(1\u2212\u03f5)\\n4 positive points all at coordinate (1 ,', ' \u22121), and m(1+\u03f5)\\n4\\npositive points all at coordinate (\u22121, +1). Describe the behavior of AdaBoost when144 Boosting\\nrun on this sample using boosting stumps. What solution does the algorithm return\\nafte', 'r T rounds?\\n6.7 Noise-tolerant AdaBoost. AdaBoost may signi\ufb01cantly over\ufb01tting in the presence\\nof noise, in part due to the high penalization of misclassi\ufb01ed examples. To reduce\\nthis e\ufb00ect, one could u', 'se instead the following objective function:\\nF =\\nm\u2211\\ni=1\\nG(\u2212yig(xi)), (6.29)\\nwhere G is the function de\ufb01ned on R by\\nG(x)=\\n{\\nex if x \u2264 0\\nx + 1 otherwise .\\n(6.30)\\n(a) Show that the function G is convex a', 'nd di\ufb00erentiable.\\n(b) Use F and greedy coordinate descent to derive an algorithm similar to\\nAdaBoost.\\n(c) Compare the reduction of the empirical error rate of this algorithm with\\nthat of AdaBoost.\\n6.8', ' Simpli\ufb01ed AdaBoost. Suppose we simplify AdaBoost by setting the parameter\\n\u03b1t to a \ufb01xed value \u03b1t = \u03b1> 0, independent of the boosting round t.\\n(a) Let \u03b3 be such that (1\\n2 \u2212 \u03f5t) \u2265 \u03b3> 0. Find the best va', 'lue of\u03b1 as a function\\nof \u03b3 by analyzing the empirical error.\\n(b) For this value of \u03b1, does the algorithm assign the same probability mass\\nto correctly classi\ufb01ed and misclassi\ufb01ed examples at each round', '? If not, which\\nset is assigned a higher probability mass?\\n(c) Using the previous value of \u03b1, give a bound on the empirical error of the\\nalgorithm that depends only on \u03b3 and the number of rounds of bo', 'osting T.\\n(d) Using the previous bound, show that forT>\\nlog m\\n2\u03b32 , the resulting hypothesis\\nis consistent with the sample of size m.\\n(e) Let s be the VC-dimension of the base learners used. Give a bo', 'und on the\\ngeneralization error of the consistent hypothesis obtained afterT =\\n\u230a\\nlog m\\n2\u03b32\\n\u230b\\n+1\\nrounds of boosting. ( Hint: Use the fact that the VC-dimension of the family\\nof functions {sgn(\u2211T\\nt=1 \u03b1t', 'ht): \u03b1t \u2208 R} is bounded by 2( s +1 )T log2(eT)).\\nSuppose now that \u03b3 varies with m. Based on the bound derived, what can be\\nsaid if \u03b3(m)= O(\\n\u221a\\nlog m\\nm )?)6.6 Exercises 145\\nMatrix-based AdaBoost(M,tmax)', '\\n1 \u03bb1,j \u2190 0f o ri =1 ,...,m\\n2 for t \u2190 1 to tmax do\\n3 dt,i \u2190 exp(\u2212(M\u03bbt)i)Pm\\nk=1 exp(\u2212(M\u03bbt)k) for i =1 ,...,m\\n4 jt \u2190 argmaxj(d\u22a4\\nt M)j\\n5 rt \u2190 (d\u22a4\\nt M)jt\\n6 \u03b1t \u2190 1\\n2 log\\n(1+rt\\n1\u2212rt\\n\u23a1\\n7 \u03bbt+1 \u2190 \u03bbt + \u03b1tejt, w', 'here ejt is 1 in position jt and 0 elsewhere.\\n8 return \u03bbtmax\\n\u2225\u03bbtmax \u22251\\nFigure 6.7 Matrix-based AdaBoost.\\n6.9 Matrix-based AdaBoost.\\n(a) De\ufb01ne an m\u00d7n matrix M where Mij = yihj(xi), i.e.,Mij = +1 if tra', 'ining\\nexample i is classi\ufb01ed correctly by weak classi\ufb01er hj,a n d \u22121 otherwise. Let\\ndt, \u03bbt \u2208 Rn, \u2225dt\u22251 = 1 anddt,i (respectively \u03bbt,i) equal theith component ofdt\\n(respectively \u03bbt). Now, consider the ', 'matrix-based form of AdaBoost described\\nin \ufb01gure 6.7 and de\ufb01ne M as below with eight training points and eight weak\\nclassi\ufb01ers.\\nM =\\n\u239b\\n\u239c\u239c\\n\u239c\\n\u239c\\n\u239c\u239c\u239c\\n\u239c\\n\u239c\\n\u239c\\n\u239c\u239c\u239c\\n\u239c\u239d\\n\u221211 1 1 1 \u22121 \u221211\\n\u221211 1 \u22121 \u221211 1 1\\n1 \u221211 1 ', '1 \u221211 1\\n1 \u221211 1 \u221211 1 1\\n1 \u221211 \u221211 1 1 \u22121\\n11 \u221211 1 1 1 \u22121\\n11 \u221211 1 1 \u221211\\n1111 \u22121 \u221211 \u22121\\n\u239e\\n\u239f\u239f\\n\u239f\\n\u239f\\n\u239f\u239f\u239f\\n\u239f\\n\u239f\\n\u239f\\n\u239f\u239f\u239f\\n\u239f\u23a0\\nAssume that we start with the following initial distribution over the datapoints:\\nd\\n1 =', '\\n(3 \u2212\\n\u221a\\n5\\n8 , 3 \u2212\\n\u221a\\n5\\n8 , 1\\n6, 1\\n6, 1\\n6,\\n\u221a\\n5 \u2212 1\\n8 ,\\n\u221a\\n5 \u2212 1\\n8 , 0\\n\u23a1\u22a4\\nCompute the \ufb01rst few steps of the matrix-based AdaBoost algorithm usingM,\\nd1,a n dtmax = 7. What weak classi\ufb01er is picked at each ', 'round of boosting?146 Boosting\\nDo you notice any pattern?\\n(b) What is the L1 norm margin produced by AdaBoost for this example?\\n(c) Instead of using AdaBoost, imagine we combined our classi\ufb01ers using ', 'the\\nfollowing coe\ufb03cients: [2,3, 4, 1, 2, 2, 1,1] \u00d7 1\\n16 . What is the margin in this case?\\nDoes AdaBoost maximize the margin?7 On-Line Learning\\nThis chapter presents an introduction to on-line learnin', 'g, an important area with a\\nrich literature and multiple connections with game theory and optimization that\\nis increasingly in\ufb02uencing the theoretical and algorithmic advances in machine\\nlearning. In ', 'addition to the intriguing novel learning theory questions that they\\nraise, on-line learning algorithms are particularly attractive in modern applications\\nsince they form an attractive solution for la', 'rge-scale problems.\\nT h e s ea l g o r i t h m sp r o c e s so n es a m p l ea tat i m ea n dc a nt h u sb es i g n i \ufb01 c a n t l y\\nmore e\ufb03cient both in time and space and more practical than batch al', 'gorithms,\\nwhen processing modern data sets of several million or billion points. They are\\nalso typically easy to implement. Moreover, on-line algorithms do not require any\\ndistributional assumption; t', 'heir analysis assumes an adversarial scenario. This\\nm a k e st h e ma p p l i c a b l ei nav a r i e t yo fs c e n a r i o sw h e r et h es a m p l ep o i n t sa r en o t\\ndrawn i.i.d. or according to ', 'a \ufb01xed distribution.\\nWe \ufb01rst introduce the general scenario of on-line learning, then present and\\nanalyze several key algorithms for on-line learning with expert advice, including\\nthe deterministic an', 'd randomized weighted majority algorithms for the zero-one\\nloss and an extension of these algorithms for convex losses. We also describe and\\nanalyze two standard on-line algorithms for linear classi\ufb01c', 'ations, the Perceptron and\\nWinnow algorithms, as well as some extensions. While on-line learning algorithms\\nare designed for an adversarial scenario, they can be used, under some assumptions,\\nto deriv', 'e accurate predictors for a distributional scenario. We derive learning\\nguarantees for this on-line to batch conversion. Finally, we brie\ufb02y point out the\\nconnection of on-line learning with game theor', 'y by describing their use to derive a\\nsimple proof of von Neumann\u2019s minimax theorem.\\n7.1 Introduction\\nThe learning framework for on-line algorithms is in stark contrast to the PAC\\nlearning or stochast', 'ic models discussed up to this point. First, instead of learning\\nfrom a training set and then testing on a test set, the on-line learning scenario mixes148 On-Line Learning\\nthe training and test phase', 's. Second, PAC learning follows the key assumption\\nthat the distribution over data points is \ufb01xed over time, both for training and test\\npoints, and that points are sampled in an i.i.d. fashion. Under ', 'this assumption, the\\nnatural goal is to learn a hypothesis with a small expected loss or generalization\\nerror. In contrast, with on-line learning, no distributional assumption is made,\\nand thus there ', 'is no notion of generalization. Instead, the performance of on-line\\nlearning algorithms is measured using a mistake model and the notion of regret.T o\\nderive guarantees in this model, theoretical anal', 'yses are based on a worst-case or\\nadversarial assumption.\\nThe general on-line setting involves T rounds. At the tth round, the algorithm\\nreceives an instancex\\nt \u2208X and makes a prediction \u02c6yt \u2208Y . It t', 'hen receives the true\\nlabel yt \u2208 Y and incurs a loss L(\u02c6yt,y t), where L: Y\u00d7Y \u2192 R+ is a loss function.\\nMore generally, the prediction domain for the algorithm may beY\u2032 \u0338= Y and the loss\\nfunction de\ufb01ne', 'd over Y\u2032 \u00d7Y . For classi\ufb01cation problems, we often have Y = {0, 1}\\nand L(y,y \u2032)= |y\u2032 \u2212 y|, while for regression Y\u2286 R and typically L(y,y \u2032)=( y\u2032 \u2212 y)2.\\nThe objective in the on-line setting is to mini', 'mize the cumulative loss:\u2211T\\nt=1 L(\u02c6yt,y t)\\nover T rounds.\\n7.2 Prediction with expert advice\\nWe \ufb01rst discuss the setting of online learning with expert advice, and the associated\\nnotion of regret.I nt ', 'h i ss e t t i n g ,a tt h etth round, in addition to receiving xt \u2208X ,\\nthe algorithm also receives advice yt,i \u2208 Y , i \u2208 [1,N ], from N experts. Following\\nthe general framework of on-line algorithms,', ' it then makes a prediction, receives\\nthe true label, and incurs a loss. After T rounds, the algorithm has incurred a\\ncumulative loss. The objective in this setting is to minimize the regret RT ,a l s', ' o\\ncalled external regret, which compares the cumulative loss of the algorithm to that\\nof the best expert in hindsight after T rounds:\\nRT =\\nT\u2211\\nt=1\\nL(\u02c6yt,y t) \u2212\\nN\\nmin\\ni=1\\nT\u2211\\nt=1\\nL(\u02c6yt,i,y t). (7.1)\\nThi', 's problem arises in a variety of di\ufb00erent domains and applications. Figure 7.1\\nillustrates the problem of predicting the weather using several forecasting sources\\nas experts.\\n7.2.1 Mistake bounds and ', 'Halving algorithm\\nHere, we assume that the loss function is the standard zero-one loss used in\\nclassi\ufb01cation. To analyze the expert advice setting, we \ufb01rst consider the realizable7.2 Prediction with e', 'xpert advice 149\\nwunderground.com bbc.com weather.com cnn.com\\n?\\nalgorithm\\nbb\\nFigure 7.1 Weather forecast: an example of a prediction problem based on expert\\nadvice.\\ncase. As such, we discuss the mista', 'ke bound model,w h i c ha s k st h es i m p l eq u e s t i o n\\n\u201cHow many mistakes before we learn a particular concept?\u201d Since we are in the\\nrealizable case, after some number of rounds T, we will lea', 'rn the concept and no\\nlonger make errors in subsequent rounds. For any \ufb01xed concept c,w ed e \ufb01 n et h e\\nmaximum number of mistakes a learning algorithm A makes as\\nMA(c)= m a x\\nx1,...,xT\\n|mistakes(A,c)', '|. (7.2)\\nFurther, for any concept in a concept class C, the maximum number of mistakes a\\nlearning algorithm makes is\\nMA(C)=m a x\\nc\u2208C\\nMA(c). (7.3)\\nOur goal in this setting is to derivemistake bounds,t ', 'h a ti s ,ab o u n dM on MA(C).\\nWe will \ufb01rst do this for the Halving algorithm, an elegant and simple algorithm for\\nwhich we can generate surprisingly favorable mistake bounds. At each round, the\\nHalv', 'ing algorithm makes its prediction by taking the majority vote over all active\\nexperts. After any incorrect prediction, it deactivates all experts that gave faulty\\nadvice. Initially, all experts are a', 'ctive, and by the time the algorithm has converged\\nto the correct concept, the active set contains only those experts that are consistent\\nwith the target concept. The pseudocode for this algorithm is ', 'shown in \ufb01gure 7.2.\\nWe also present straightforward mistake bounds in theorems 7.1 and 7.2, where\\nthe former deals with \ufb01nite hypothesis sets and the latter relates mistake bounds to\\nVC-dimension. Not', 'e that the hypothesis complexity term in theorem 7.1 is identical\\nto the corresponding complexity term in the PAC model bound of theorem 2.1.\\nTheorem 7.1\\nLet H be a \ufb01nite hypothesis set. Then\\nMHalving', ' (H) \u2264 log2 |H|. (7.4)\\nProof Since the algorithm makes predictions using majority vote from the active\\nset, at each mistake, the active set is reduced by at least half. Hence, after log2 |H|\\nmistakes,', ' there can only remain one active hypothesis, and since we are in the150 On-Line Learning\\nHalving(H)\\n1 H1 \u2190 H\\n2 for t \u2190 1 to T do\\n3 Receive(xt)\\n4 \u02c6yt \u2190 MajorityVote(Ht,xt)\\n5 Receive(yt)\\n6 if (\u02c6yt \u0338= y', 't) then\\n7 Ht+1 \u2190{ c \u2208 Ht : c(xt)= yt}\\n8 return HT+1\\nFigure 7.2 Halving algorithm.\\nrealizable case, this hypothesis must coincide with the target concept.\\nTheorem 7.2\\nLet opt(H) be the optimal mistake ', 'boundfor H.T h e n ,\\nVCdim(H) \u2264 opt(H) \u2264 MHalving (H) \u2264 log2 |H|. (7.5)\\nProof The second inequality is true by de\ufb01nition and the third inequality holds\\nbased on theorem 7.1. To prove the \ufb01rst inequali', 'ty, we let d =V C d i m (H). Then\\nthere exists a shattered set ofd points, for which we can form a complete binary tree\\nof the mistakes with heightd, and we can choose labels at each round of learning', ' to\\nensure that d mistakes are made. Note that this adversarial argument is valid since\\nthe on-line setting makes no statistical assumptions about the data.\\n7.2.2 Weighted majority algorithm\\nIn the pr', 'evious section, we focused on the realizable setting in which the Halving\\nalgorithm simply discarded experts after a single mistake. We now move to the\\nnon-realizable setting and use a more general an', 'd less extreme algorithm, the\\nWeighted Majority (WM) algorithm, that weights the importance of experts as\\na function of their mistake rate. The WM algorithm begins with uniform weights\\nover allN exper', 'ts. At each round, it generates predictions using a weighted majority\\nvote. After receiving the true label, the algorithm then reduces the weight of each\\nincorrect expert by a factor of \u03b2 \u2208 [0, 1). No', 'te that this algorithm reduces to the\\nHalving algorithm when \u03b2 = 0. The pseudocode for the WM algorithm is shown in7.2 Prediction with expert advice 151\\nWeighted-Majority(N)\\n1 for i \u2190 1 to N do\\n2 w1,i', ' \u2190 1\\n3 for t \u2190 1 to T do\\n4 Receive(xt)\\n5 if \u2211\\ni: yt,i=1 wt,i \u2265 \u2211\\ni: yt,i=0 wt,i then\\n6 \u02c6yt \u2190 1\\n7 else \u02c6yt \u2190 0\\n8 Receive(yt)\\n9 if (\u02c6yt \u0338= yt) then\\n10 for i \u2190 1 to N do\\n11 if (yt,i \u0338= yt) then\\n12 wt+1,i', ' \u2190 \u03b2wt,i\\n13 else wt+1,i \u2190 wt,i\\n14 return wT+1\\nFigure 7.3 Weighted majority algorithm, yt,y t,i \u2208{ 0, 1}.\\n\ufb01gure 7.3.\\nSince we are not in the realizable setting, the mistake bounds of theorem 7.1\\ncannot', ' apply. However, the following theorem presents a bound on the number of\\nmistakes mT made by the WM algorithm after T \u2265 1 rounds of on-line learning as\\na function of the number of mistakes made by the', ' best expert, that is the expert\\nwho achieves the smallest number of mistakes for the sequence y\\n1,...,y T .L e tu s\\nemphasize that this is the best expert in hindsight.\\nTheorem 7.3\\nFix \u03b2 \u2208 (0, 1).L e', ' tmT be the number of mistakes made by algorithm WM afterT \u2265 1\\nrounds, and m\u2217\\nT be the number of mistakes made by the best of theN experts. Then,\\nthe following inequality holds:\\nmT \u2264\\nlog N + m\u2217\\nT log ', '1\\n\u03b2\\nlog 2\\n1+\u03b2\\n. (7.6)\\nProof To prove this theorem, we \ufb01rst introduce a potential function. We then\\nderive upper and lower bounds for this function, and combine them to obtain our152 On-Line Learning\\nr', 'esult. This potential function method is a general proof technique that we will use\\nthroughout this chapter.\\nFor any t \u2265 1, we de\ufb01ne our potential function as Wt = \u2211N\\ni=1 wt,i.S i n c e\\npredictions ar', 'e generated using weighted majority vote, if the algorithm makes\\nan error at round t,t h i si m p l i e st h a t\\nW\\nt+1 \u2264\\n[\\n1/2+( 1/2)\u03b2\\n]\\nWt =\\n[1+ \u03b2\\n2\\n]\\nWt. (7.7)\\nSince W1 = N and mT mistakes are made ', 'afterT rounds, we thus have the following\\nupper bound:\\nWT \u2264\\n[1+ \u03b2\\n2\\n]mT\\nN. (7.8)\\nNext, since the weights are all non-negative, it is clear that for any expert i,\\nWT \u2265 wT,i = \u03b2mT,i ,w h e r emT,i is th', 'e number of mistakes made by the ith expert\\nafter T rounds. Applying this lower bound to the best expert and combining it with\\nthe upper bound in (7.8) gives us:\\n\u03b2m\u2217\\nT \u2264 WT \u2264\\n[1+ \u03b2\\n2\\n]mT\\nN\\n\u21d2 m\u2217\\nT log ', '\u03b2 \u2264 log N + mT log\\n[1+ \u03b2\\n2\\n]\\n\u21d2 mT log\\n[ 2\\n1+ \u03b2\\n]\\n\u2264 log N + m\u2217\\nT log 1\\n\u03b2,\\nwhich concludes the proof.\\nThus, the theorem guarantees a bound of the following form for algorithm WM:\\nmT \u2264 O(log N)+c o n s t', ' a n t\u00d7| mistakes of best expert|.\\nSince the \ufb01rst term varies only logarithmically as a function of N,t h et h e o r e m\\nguarantees that the number of mistakes is roughly a constant times that of the ', 'best\\nexpert in hindsight. This is a remarkable result, especially because it requires no\\nassumption about the sequence of points and labels generated. In particular, the\\nsequence could be chosen adver', 'sarially. In the realizable case where m\\n\u2217\\nT =0 ,t h e\\nbound reduces to mT \u2264 O(log N) as for the Halving algorithm.\\n7.2.3 Randomized weighted majority algorithm\\nIn spite of the guarantees just discuss', 'ed, the WM algorithm admits a drawback that\\na\ufb00ects all deterministic algorithms in the case of the zero-one loss: no deterministic\\nalgorithm can achieve a regret R\\nT = o(T) over all sequences. Clearly', ', for any7.2 Prediction with expert advice 153\\nRandomized-Weighted-Majority (N)\\n1 for i \u2190 1 to N do\\n2 w1,i \u2190 1\\n3 p1,i \u2190 1/N\\n4 for t \u2190 1 to T do\\n5 for i \u2190 1 to N do\\n6 if (lt,i =1 ) then\\n7 wt+1,i \u2190 \u03b2wt,', 'i\\n8 else wt+1,i \u2190 wt,i\\n9 Wt+1 \u2190 \u2211N\\ni=1 wt+1,i\\n10 for i \u2190 1 to N do\\n11 pt+1,i \u2190 wt+1,i/Wt+1\\n12 return wT+1\\nFigure 7.4 Randomized weighted majority algorithm.\\ndeterministic algorithm A and any t \u2208 [1,T ', '], we can adversarially select yt to\\nbe 1 if the algorithm predicts 0, and choose it to be 0 otherwise. Thus, A errs at\\nevery point of such a sequence and its cumulative mistake is mT = T.A s s u m ef', ' o r\\nexample that N = 2 and that one expert always predicts 0, the other one always 1.\\nThe error of the best expert over that sequence (and in fact any sequence of that\\nl e n g t h )i st h e na tm o s', ' tm\\n\u2217\\nT \u2264 T/2. Thus, for that sequence, we have\\nRT = mT \u2212 m\u2217\\nT \u2265 T/2,\\nwhich shows that RT = o(T) cannot be achieved in general. Note that this does\\nnot contradict the bound proven in the previous sect', 'ion, since for any \u03b2 \u2208 (0, 1),\\nlog 1\\n\u03b2\\nlog 2\\n1+\u03b2\\n\u2265 2. As we shall see in the next section, this negative result does not hold\\nfor any loss that is convex with respect to one of its arguments. But for ', 'the zero-one\\nloss, this leads us to consider randomized algorithms instead.\\nIn the randomized scenario of on-line learning, we assume that a set A =\\n{1,...,N } of N actions is available. At each round', ' t \u2208 [1,T ], an on-line algorithm\\nA selects a distributionpt over the set of actions, receives a loss vectorlt,w h o s eith\\ncomponent lt,i \u2208 [0,1] is the loss associated with action i, and incurs the ', 'expected\\nloss Lt = \u2211N\\ni=1 pt,i lt,i. The total loss incurred by the algorithm over T rounds\\nis LT = \u2211T\\nt=1 Lt.T h et o t a ll o s sa s s o c i a t e dt oa c t i o ni is LT,i = \u2211T\\nt=1 lt,i.T h e154 On-', 'Line Learning\\nminimal loss of a single action is denoted by Lmin\\nT =m i ni\u2208A LT,i.T h er e g r e tRT of\\nthe algorithm after T rounds is then typically de\ufb01ned by the di\ufb00erence of the loss\\nof the algori', 'thm and that of the best single action:1\\nRT = LT \u2212L min\\nT .\\nHere, we consider speci\ufb01cally the case of zero-one losses and assume thatlt,i \u2208{ 0,1}\\nfor all t \u2208 [1,T ]a n di \u2208 A.\\nThe WM algorithm admits ', 'a straightforward randomized version, the randomized\\nweighted majority (RWM) algorithm. The pseudocode of this algorithm is given in\\n\ufb01gure 7.4. The algorithm updates the weightw\\nt,i of experti as in t', 'he case of the WM\\nalgorithm by multiplying it by \u03b2. The following theorem gives a strong guarantee\\non the regret RT of the RWM algorithm, showing that it is in O(\u221aT log N).\\nTheorem 7.4\\nFix \u03b2 \u2208 [1/2,1)', '.T h e n ,f o ra n yT \u2265 1, the loss of algorithm RWM on any sequence\\ncan be bounded as follows:\\nLT \u2264 log N\\n1 \u2212 \u03b2 +( 2 \u2212 \u03b2)Lmin\\nT . (7.9)\\nIn particular, for \u03b2 =m a x{1/2, 1 \u2212\\n\u221a\\n(log N)/T }, the loss ca', 'n be bounded as:\\nLT \u2264L min\\nT +2\\n\u221a\\nT log N. (7.10)\\nProof As in the proof of theorem 7.3, we derive upper and lower bounds for the\\npotential function Wt = \u2211N\\ni=1 wt,i, t \u2208 [1,T ], and combine these boun', 'ds to obtain\\nthe result. By de\ufb01nition of the algorithm, for any t \u2208 [1,T ], Wt+1 can be expressed\\nas follows in terms of Wt:\\nWt+1 =\\n\u2211\\ni: lt,i=0\\nwt,i + \u03b2\\n\u2211\\ni: lt.i=1\\nwt,i = Wt +( \u03b2 \u2212 1)\\n\u2211\\ni: lt,i=1\\nwt,', 'i\\n= Wt +( \u03b2 \u2212 1)Wt\\n\u2211\\ni: lt,i=1\\npt,i\\n= Wt +( \u03b2 \u2212 1)WtLt\\n= Wt(1 \u2212 (1 \u2212 \u03b2)Lt).\\nThus, since W1 = N, it follows that WT+1 = N \u220fT\\nt=1(1 \u2212 (1 \u2212 \u03b2)Lt). On the other\\nhand, the following lower bound clearly hol', 'ds: WT+1 \u2265 maxi\u2208[1,N] wT+1,i = \u03b2Lmin\\nT .\\nThis leads to the following inequality and series of derivations after taking the log\\n1. Alternative de\ufb01nitions of the regret with comparison classes di\ufb00erent ', 'from the set of\\nsingle actions can be considered.7.2 Prediction with expert advice 155\\nand using the inequalities log(1 \u2212 x) \u2264\u2212 x valid for all x< 1, and \u2212 log(1 \u2212 x) \u2264\\nx + x2 valid for all x \u2208 [0, 1/', '2]:\\n\u03b2Lmin\\nT \u2264 N\\nT\u220f\\nt=1\\n(1 \u2212 (1 \u2212 \u03b2)Lt)= \u21d2L min\\nT log \u03b2 \u2264 log N +\\nT\u2211\\nt=1\\nlog(1 \u2212 (1 \u2212 \u03b2)Lt)\\n=\u21d2L min\\nT log \u03b2 \u2264 log N \u2212 (1 \u2212 \u03b2)\\nT\u2211\\nt=1\\nLt\\n=\u21d2L min\\nT log \u03b2 \u2264 log N \u2212 (1 \u2212 \u03b2)LT\\n=\u21d2L T \u2264 log N\\n1 \u2212 \u03b2 \u2212 log \u03b2\\n1', ' \u2212 \u03b2Lmin\\nT\\n=\u21d2L T \u2264 log N\\n1 \u2212 \u03b2 \u2212 log(1 \u2212 (1 \u2212 \u03b2))\\n1 \u2212 \u03b2 Lmin\\nT\\n=\u21d2L T \u2264 log N\\n1 \u2212 \u03b2 +( 2 \u2212 \u03b2)Lmin\\nT .\\nThis shows the \ufb01rst statement. Since Lmin\\nT \u2264 T, this also implies\\nLT \u2264 log N\\n1 \u2212 \u03b2 +( 1 \u2212 \u03b2)T + Lm', 'in\\nT . (7.11)\\nDi\ufb00erentiating the upper bound with respect to \u03b2 and setting it to zero gives\\nlog N\\n(1\u2212\u03b2)2 \u2212 T =0 ,t h a ti s \u03b2 = \u03b20 =1 \u2212\\n\u221a\\n(log N)/T,s i n c e\u03b2< 1. Thus, if\\n1 \u2212\\n\u221a\\n(log N)/T \u2265 1/2, \u03b20 is', ' the minimizing value of\u03b2, otherwise 1/2 is the optimal\\nvalue. The second statement follows by replacing \u03b2 with \u03b20 in (7.11).\\nThe bound (7.10) assumes that the algorithm additionally receives as a par', 'ameter\\nthe number of rounds T.A sw es h a l ls e ei nt h en e x ts e c t i o n ,h o w e v e r ,t h e r ee x i s t s\\na general doubling trick that can be used to relax this requirement at the price of\\n', 'a small constant factor increase. Inequality 7.10 can be written directly in terms of\\nthe regret R\\nT of the RWM algorithm:\\nRT \u2264 2\\n\u221a\\nT log N. (7.12)\\nThus, for N constant, the regret veri\ufb01es RT = O(\\n\u221a\\nT', ')a n dt h eaverage regret or\\nregret per roundRT /T decreases as O(1/\\n\u221a\\nT) .T h e s er e s u l t sa r eo p t i m a l ,a ss h o w n\\nby the following theorem.\\nTheorem 7.5\\nLet N =2 . There exists a stocha', 'stic sequence of losses for which the regret of any\\non-line learning algorithm veri\ufb01es E[RT ] \u2265\\n\u221a\\nT/8.\\nProof For any t \u2208 [1,T ], let the vector of losses lt take the values l01 =( 0,1)\u22a4\\nand l10 =( 1, ', '0)\u22a4 with equal probability. Then, the expected loss of any randomized156 On-Line Learning\\nalgorithm A is\\nE[LT ]=E\\n[ T\u2211\\nt=1\\npt \u00b7 lt\\n]\\n=\\nT\u2211\\nt=1\\npt \u00b7 E[lt]=\\nT\u2211\\nt=1\\n1\\n2pt,1 + 1\\n2(1 \u2212 pt,1)= T/2,\\nw h e r e', 'w ed e n o t e db ypt the distribution selected by A at round t. By de\ufb01nition,\\nLmin\\nT c a nb ew r i t t e na sf o l l o w s :\\nLmin\\nT =m i n{LT,1, LT,2} = 1\\n2(LT,1 + LT,2 \u2212| LT,1 \u2212L T,2|)= T/2 \u2212| LT,1 ', '\u2212 T/2|,\\nusing the fact that LT,1 + LT,2 = T. Thus, the expected regret of A is\\nE[RT ]=E [ LT ] \u2212 E[Lmin\\nT ]=E [ |LT,1 \u2212 T/2|].\\nLet \u03c3t, t \u2208 [1,T ], denote Rademacher variables taking values in {\u22121,+1},', 't h e n\\nLT,1 can be rewritten as LT,1 = \u2211T\\nt=1\\n1+\u03c3t\\n2 = T/2+ 1\\n2\\n\u2211T\\nt=1 \u03c3t. Thus, introducing\\nscalars xt =1 /2, t \u2208 [1,T ], by the Khintchine-Kahane inequality (D.22), we have:\\nE[RT ]=E\\n[\\n|\\nT\u2211\\nt=1\\n\u03c3tx', 't|\\n]\\n\u2265\\n\\ued6a\\ued6b\\ued6b\\n\u221a\\n1\\n2\\nT\u2211\\nt=1\\nx2\\nt =\\n\u221a\\nT/8,\\nwhich concludes the proof.\\nMore generally, for T \u2265 N, a lower bound of RT =\u03a9 (\u221aT log N) can be proven for\\nthe regret of any algorithm.\\n7.2.4 Exponential weighted ', 'average algorithm\\nThe WM algorithm can be extended to other loss functions L taking values in\\n[0,1]. The Exponential Weighted Average algorithm presented here can be viewed\\nas that extension for the c', 'ase whereL is convex in its \ufb01rst argument. Note that this\\nalgorithm is deterministic and yet, as we shall see, admits a very favorable regret\\nguarantee. Figure 7.5 gives its pseudocode. At round t \u2208 [', '1,T ], the algorithm\u2019s\\nprediction is\\n\u02c6y\\nt =\\n\u2211N\\ni=1 wt,iyt,i\\n\u2211N\\ni=1 wt,i\\n, (7.13)\\nwhere yt,i is the prediction by experti and wt,i the weight assigned by the algorithm\\nto that expert. Initially, all we', 'ights are set to one. The algorithm then updates the\\nweights at the end of round t according to the following rule:\\nwt+1,i \u2190 wt,i e\u2212\u03b7L(byt,i,yt) = e\u2212\u03b7Lt,i , (7.14)7.2 Prediction with expert advice 157', '\\nExponential-Weighted-A verage(N)\\n1 for i \u2190 1 to N do\\n2 w1,i \u2190 1\\n3 for t \u2190 1 to T do\\n4 Receive(xt)\\n5 \u02c6yt \u2190\\nPN\\ni=1 wt,iyt,iPN\\ni=1 wt,i\\n6 Receive(yt)\\n7 for i \u2190 1 to N do\\n8 wt+1,i \u2190 wt,i e\u2212\u03b7L(byt,i,yt)\\n9', ' return wT+1\\nFigure 7.5 Exponential weighted average, L(byt,i,y t) \u2208 [0, 1].\\nwhere Lt,i is the total loss incurred by expert i after t rounds. Note that this\\nalgorithm, as well as the others presented', ' in this chapter, are simple, since they\\ndo not require keeping track of the losses incurred by each expert at all previous\\nrounds but only of their cumulative performance. Furthermore, this property ', 'is also\\ncomputationally advantageous. The following theorem presents a regret bound for\\nthis algorithm.\\nTheorem 7.6\\nAssume that the loss function L is convex in its \ufb01rst argument and takes values\\nin [', '0,1].T h e n ,f o ra n y\u03b7> 0 and any sequence y\\n1,...,y T \u2208 Y , the regret of the\\nExponential Weighted Average algorithm after T rounds satis\ufb01es\\nRT \u2264 log N\\n\u03b7 + \u03b7T\\n8 . (7.15)\\nIn particular, for \u03b7=\\n\u221a\\n8l', ' o gN/T, the regret is bounded as\\nRT \u2264\\n\u221a\\n(T/2) logN. (7.16)\\nProof We apply the same potential function analysis as in previous proofs but\\nusing as potential \u03a6t =l o g\u2211N\\ni=1 wt,i, t \u2208 [1,T ]. Let pt de', 'note the distribution over\\n{1,...,N } with pt,i = wt,iPN\\ni=1 wt,i\\n. To derive an upper bound on \u03a6t,w e\ufb01 r s te x a m i n e158 On-Line Learning\\nthe di\ufb00erence of two consecutive potential values:\\n\u03a6t+1 \u2212', ' \u03a6t =l o g\\n\u2211N\\ni=1 wt,i e\u2212\u03b7L(byt,i,yt)\\n\u2211N\\ni=1 wt,i\\n=l o g\\n(\\nE\\npt\\n[e\u03b7X]\\n\u23a1\\n,\\nwith X = \u2212L(\u02c6yt,i,y t) \u2208 [\u22121, 0]. To upper bound the expression appearing in the\\nright-hand side, we apply Hoe\ufb00ding\u2019s lemma (l', 'emma D.1) to the centered random\\nvariable X \u2212 Ept [X], then Jensen\u2019s inequality (theorem B.4) using the convexity of\\nL with respect to its \ufb01rst argument:\\n\u03a6t+1 \u2212 \u03a6t =l o g\\n(\\nE\\npt\\n[\\ne\u03b7(X\u2212E[X])+\u03b7E[X]]\u23a1\\n\u2264', ' \u03b72\\n8 + \u03b7E\\npt\\n[X]= \u03b72\\n8 \u2212 \u03b7E\\npt\\n[L(\u02c6yt,i,y t)] (Hoe\ufb00ding\u2019s lemma)\\n\u2264\u2212 \u03b7L\\n(\\nE\\npt\\n[\u02c6yt,i],y t\\n\u23a1\\n+ \u03b72\\n8 (convexity of \ufb01rst arg. of L)\\n= \u2212\u03b7L(\u02c6yt,y t)+ \u03b72\\n8 .\\nSumming up these inequalities yields the follow', 'ing upper bound:\\n\u03a6T+1 \u2212 \u03a61 \u2264\u2212 \u03b7\\nT\u2211\\nt=1\\nL(\u02c6yt,y t)+ \u03b72T\\n8 . (7.17)\\nWe obtain a lower bound for the same quantity as follows:\\n\u03a6T+1 \u2212\u03a61 =l o g\\nN\u2211\\ni=1\\ne\u2212\u03b7LT,i \u2212log N \u2265 log\\nN\\nmax\\ni=1\\ne\u2212\u03b7LT,i \u2212log N = \u2212\u03b7\\nN\\n', 'min\\ni=1\\nLT,i \u2212log N.\\nCombining the upper and lower bounds yields:\\n\u2212 \u03b7\\nN\\nmin\\ni=1\\nLT,i \u2212 log N \u2264\u2212 \u03b7\\nT\u2211\\nt=1\\nL(\u02c6yt,y t)+ \u03b72T\\n8\\n=\u21d2\\nT\u2211\\nt=1\\nL(\u02c6yt,y t) \u2212\\nN\\nmin\\ni=1\\nLT,i \u2264 log N\\n\u03b7 + \u03b7T\\n8 ,\\nand concludes the pr', 'oof.\\nThe optimal choice of\u03b7in theorem 7.6 requires knowledge of the horizonT,w h i c hi s\\nan apparent disadvantage of this analysis. However, we can use a standarddoubling\\ntrick to eliminate this requ', 'irement, at the price of a small constant factor. This\\nconsists of dividing time into periods [2 k,2k+1 \u2212 1] of length 2k with k =0 ,...,n\\nand T \u2265 2n\u22121, and then choose\u03b7k =\\n\u221a\\n8 logN\\n2k in each period.', ' The following theorem\\npresents a regret bound when using the doubling trick to select \u03b7. A more general7.3 Linear classi\ufb01cation 159\\nmethod consists of interpreting \u03b7 as a function of time, i.e., \u03b7t =', '\\n\u221a\\n(8 logN)/t,\\nwhich can lead to a further constant factor improvement over the regret bound of\\nthe following theorem.\\nTheorem 7.7\\nAssume that the loss function L is convex in its \ufb01rst argument and ta', 'kes values\\nin [0,1].T h e n ,f o ra n yT \u2265 1 and any sequence y\\n1,...,y T \u2208 Y , the regret of the\\nExponential Weighted Average algorithm after T rounds is bounded as follows:\\nRT \u2264\\n\u221a\\n2\u221a\\n2 \u2212 1\\n\u221a\\n(T/2) l', 'ogN +\\n\u221a\\nlog N/2. (7.18)\\nProof Let T \u2265 1a n dl e tIk =[ 2k, 2k+1 \u2212 1], for k \u2208 [0,n ], with n = \u230alog(T +1 )\u230b.\\nLet LIk denote the loss incurred in the interval Ik. By theorem 7.6 (7.16), for any\\nk \u2208 [0,', 'n ], we have\\nLIk \u2212\\nN\\nmin\\ni=1\\nLIk,i \u2264\\n\u221a\\n2k/2l o gN. (7.19)\\nThus, we can bound the total loss incurred by the algorithm after T rounds as:\\nLT =\\nn\u2211\\nk=0\\nLIk \u2264\\nn\u2211\\nk=0\\nN\\nmin\\ni=1\\nLIk,i +\\nn\u2211\\nk=0\\n\u221a\\n2k (log N)/', '2\\n\u2264\\nN\\nmin\\ni=1\\nLT,i +\\n\u221a\\n(log N)/2 \u00b7\\nn\u2211\\nk=0\\n2\\nk\\n2 , (7.20)\\nwhere the second inequality follows from the super-additivity of min, that is\\nmini Xi +m i ni Yi \u2264 mini(Xi + Yi) for any sequences (Xi)i and (Y', 'i)i, which implies\u2211n\\nk=0 minN\\ni=1 LIk,i \u2264 minN\\ni=1\\n\u2211n\\nk=0 LIk,i. The geometric sum appearing in the right-\\nhand side of (7.20) can be expressed as follows:\\nn\u2211\\nk=0\\n2\\nk\\n2 = 2(n+1)/2 \u2212 1\u221a\\n2 \u2212 1 \u2264\\n\u221a\\n2\\n\u221a\\nT', ' +1 \u2212 1\u221a\\n2 \u2212 1 \u2264\\n\u221a\\n2(\\n\u221a\\nT +1 ) \u2212 1\u221a\\n2 \u2212 1 =\\n\u221a\\n2\\n\u221a\\nT\u221a\\n2 \u2212 1 +1 .\\nPlugging back into (7.20) and rearranging terms yields (7.18).\\nThe O(\\n\u221a\\nT) dependency on T presented in this bound cannot be improved fo', 'r\\ngeneral loss functions.\\n7.3 Linear classi\ufb01cation\\nThis section presents two well-known on-line learning algorithms for linear classi\ufb01-\\ncation: the Perceptron and Winnow algorithms.160 On-Line Learnin', 'g\\nPerceptron(w0)\\n1 w1 \u2190 w0 \u22bf typically w0 = 0\\n2 for t \u2190 1 to T do\\n3 Receive(xt)\\n4 \u02c6yt \u2190 sgn(wt \u00b7 xt)\\n5 Receive(yt)\\n6 if (\u02c6yt \u0338= yt) then\\n7 wt+1 \u2190 wt + ytxt \u22bf more generally \u03b7ytxt,\u03b7> 0.\\n8 else wt+1 \u2190 w', 't\\n9 return wT+1\\nFigure 7.6 Perceptron algorithm.\\n7.3.1 Perceptron algorithm\\nThe Perceptron algorithm is one of the earliest machine learning algorithms. It is\\nan on-line linear classi\ufb01cation algorithm', '. Thus, it learns a decision function based\\non a hyperplane by processing training points one at a time. Figure 7.6 gives its\\npseudocode.\\nThe algorithm maintains a weight vector w\\nt \u2208 RN de\ufb01ning the h', 'yperplane\\nlearned, starting with an arbitrary vector w0. At each round t \u2208 [1,T ], it predicts\\nthe label of the point xt \u2208 RN received, using the current vectorwt (line 4). When\\nthe prediction made do', 'es not match the correct label (lines 6-7), it updates wt by\\nadding ytxt. More generally, when a learning rate \u03b7> 0 is used, the vector added\\nis \u03b7ytxt. This update can be partially motivated by examin', 'ing the inner product of\\nthe current weight vector with ytxt, whose sign determines the classi\ufb01cation of xt.\\nJust before an update, xt is misclassi\ufb01ed and thus ytwt \u00b7 xt is negative; afterward,\\nytwt+1', ' \u00b7xt = ytwt \u00b7ytxt +\u03b7\u2225xt\u22252, thus, the update corrects the weight vector in the\\ndirection of making the inner product positive by augmenting it with this quantity\\nwith \u03b7\u2225x\\nt\u22252 > 0.\\nThe Perceptron algori', 'thm can be shown in fact to seek a weight vector w\\nminimizing an objective function F precisely based on the quantities ( \u2212ytw \u00b7 xt),\\nt \u2208 [1,T ]. Since (\u2212ytw \u00b7 xt) is positive when xt is misclassi\ufb01ed ', 'by w, F is de\ufb01ned7.3 Linear classi\ufb01cation 161\\nw1\\nw2\\nw3\\nw4\\nw5\\nFigure 7.7 An example path followed by the iterative stochastic gradient descent\\ntechnique. Each inner contour indicates a region of lower ', 'elevation.\\nfor all w \u2208 RN by\\nF(w)= 1\\nT\\nT\u2211\\nt=1\\nmax\\n(\\n0, \u2212yt(w \u00b7 xt)\\n\u23a1\\n=E\\nx\u223c bD\\n[ \u02dcF(w,x)], (7.21)\\nwhere \u02dcF(w,x)=m a x\\n(\\n0, \u2212f(x)(w \u00b7 x)\\n\u23a1\\nwith f(x) denoting the label of x,a n d\\n\u02c6D is the empirical dis', 'tribution associated with the sample ( x1,..., xT ). For any\\nt \u2208 [1,T ], w \u21a6\u2192\u2212 yt(w \u00b7 xt) is linear and thus convex. Since the max operator pre-\\nserves convexity, this shows thatF is convex. However,F', ' is not di\ufb00erentiable. Nev-\\nertheless, the Perceptron algorithm coincides with the application of the stochastic\\ngradient descent technique to F.\\nThe stochastic (or on-line) gradient descent technique', ' examines one point xt at\\na time. For a function \u02dcF, a generalized version of this technique can be de\ufb01ned by\\nthe execution of the following update for each point xt:\\nwt+1 \u2190\\n{\\nwt \u2212 \u03b7\u2207w \u02dcF(wt,xt)i f w ', '\u21a6\u2192 \u02dcF(w,xt) di\ufb00erentiable at wt,\\nwt otherwise,\\n(7.22)\\nwhere \u03b7> 0 is a learning rate parameter. Figure 7.7 illustrates an example path\\nthe gradient descent follows. In the speci\ufb01c case we are consideri', 'ng, w \u21a6\u2192 \u02dcF(w,xt)\\nis di\ufb00erentiable at any w such that yt(w \u00b7 xt) \u0338=0w i t h \u2207w \u02dcF(w,xt)= \u2212yxt if\\nyt(w \u00b7 xt) < 0a n d\u2207w \u02dcF(w,xt)=0i f yt(w \u00b7 xt) > 0. Thus, the stochastic gradient\\ndescent update become', 's\\nwt+1 \u2190\\n\u23a7\\n\u23aa\u23aa\u23a8\\n\u23aa\u23aa\\n\u23a9\\nw\\nt + \u03b7ytxt if yt(w \u00b7 xt) < 0;\\nwt if yt(w \u00b7 xt) > 0;\\nwt otherwise,\\n(7.23)\\nwhich coincides exactly with the update of the Perceptron algorithm.\\nThe following theorem gives a margin-', 'based upper bound on the number of\\nmistakes or updates made by the Perceptron algorithm when processing a sequence162 On-Line Learning\\nof T points that can be linearly separated by a hyperplane with m', 'argin \u03c1> 0.\\nTheorem 7.8\\nLet x1,..., xT \u2208 RN be a sequence of T points with \u2225xt\u2225\u2264 r for all t \u2208 [1,T ],f o r\\nsome r> 0. Assume that there exist \u03c1> 0 and v \u2208 RN such that for all t \u2208 [1,T ],\\n\u03c1 \u2264 yt(v\u00b7xt', ')\\n\u2225v\u2225 . Then, the number of updates made by the Perceptron algorithm when\\nprocessing x1,..., xT is bounded by r2/\u03c12.\\nProof Let I be the subset of the T rounds at which there is an update, and let\\nM be', ' the total number of updates, i.e., |I| = M. Summing up the assumption\\ninequalities yields:\\nM\u03c1 \u2264 v \u00b7 \u2211\\nt\u2208I ytxt\\n\u2225v\u2225 \u2264\\n\\ued79\\ued79\\n\\ued79\\n\u2211\\nt\u2208I\\nytxt\\n\\ued79\\ued79\\n\\ued79 (Cauchy-Schwarz inequality )\\n=\\n\\ued79\\ued79\\n\\ued79\\n\u2211\\nt\u2208I\\n(wt+1 \u2212 wt)\\n\\ued79\\ued79\\n\\ued79 (d', 'e\ufb01nition of updates)\\n= \u2225w\\nT+1\u2225 (telescoping sum, w0 =0 )\\n=\\n\u221a\u2211\\nt\u2208I\\n\u2225wt+1\u22252 \u2212\u2225 wt\u22252 (telescoping sum, w0 =0 )\\n=\\n\u221a\u2211\\nt\u2208I\\n\u2225wt + ytxt\u22252 \u2212\u2225 wt\u22252 (de\ufb01nition of updates)\\n=\\n\\ued6a\\ued6b\\ued6b\\n\u221a\\n\u2211\\nt\u2208I\\n2 ytwt \u00b7 xt\\ued19 \\ued18\\ued17 \\ued1a\\n\u22640\\n+\u2225xt', '\u22252\\n\u2264\\n\u221a\u2211\\nt\u2208I\\n\u2225xt\u22252 \u2264\\n\u221a\\nMr2.\\nComparing the left- and right-hand sides gives\\n\u221a\\nM \u2264 r/\u03c1,t h a ti s ,M \u2264 r2/\u03c12.\\nBy de\ufb01nition of the algorithm, the weight vector wT after processing T points is a\\nlinear com', 'bination of the vectorsxt at which an update was made:wT = \u2211\\nt\u2208I ytxt.\\nThus, as in the case of SVMs, these vectors can be referred to as support vectors\\nfor the Perceptron algorithm.\\nThe bound of theo', 'rem 7.8 is remarkable, since it depends only on the normalized\\nmargin \u03c1/r and not on the dimension N of the space. This bound can be shown\\nto be tight, that is the number of updates can be equal to r2', '/\u03c12 in some instances\\n(see exercise 7.3 to show the upper bound is tight).\\nThe theorem required no assumption about the sequence of points x1,..., xT .\\nA standard setting for the application of the Pe', 'rceptron algorithm is one where a\\n\ufb01nite sample S of size m<T is available and where the algorithm makes multiple7.3 Linear classi\ufb01cation 163\\npasses over thesem points. The result of the theorem implie', 's that whenS is linearly\\nseparable, the Perceptron algorithm converges after a \ufb01nite number of updates and\\nthus passes. For a small margin \u03c1, the convergence of the algorithm can be quite\\nslow, howeve', 'r. In fact, for some samples, regardless of the order in which the points\\nin S are processed, the number of updates made by the algorithm is in \u03a9(2\\nN )( s e e\\nexercise 7.1). Of course, ifS is not line', 'arly separable, the Perceptron algorithm does\\nnot converge. In practice, it is stopped after some number of passes over S.\\nThere are many variants of the standard Perceptron algorithm which are used\\ni', 'n practice and have been theoretically analyzed. One notable example is thevoted\\nPerceptron algorithm, which predicts according to the rule sgn\\n(\\n(\u2211\\nt\u2208I ctwt) \u00b7 x\\n\u23a1\\n,\\nwhere ct is a weight proportional', ' to the number of iterations thatwt survives, i.e.,\\nthe number of iterations between wt and wt+1.\\nFor the following theorem, we consider the case where the Perceptron algorithm\\nis trained via multiple', ' passes till convergence over a \ufb01nite sample that is linearly\\nseparable. In view of theorem 7.8, convergence occurs after a \ufb01nite number of\\nupdates.\\nFor a linearly separable sample S,w ed e n o t eb y', 'r\\nS the radius of the smallest\\nsphere containing all points in S and by \u03c1S the largest margin of a separating\\nhyperplane for S.W ea l s od e n o t eb yM(S) the number of updates made by the\\nalgorithm ', 'after training over S.\\nTheorem 7.9\\nAssume that the data is linearly separable. LethS be the hypothesis returned by the\\nPerceptron algorithm after training over a sample S of size m drawn according to\\n', 'some distribution D. Then, the expected error ofhS is bounded as follows:\\nE\\nS\u223cDm\\n[R(hS)] \u2264 E\\nS\u223cDm+1\\n[min\\n(\\nM(S),r 2\\nS/\u03c12\\nS\\n\u23a1\\nm +1\\n]\\n.\\nProof Let S be a linearly separable sample of size m + 1 drawn i.i', '.d. according\\nto D and let x be a point in S.I f hS\u2212{x} misclassi\ufb01es x,t h e nx must be a support\\nvector for hS. Thus, the leave-one-out error of the Perceptron algorithm on sample\\nS is at most M(S)\\nm', '+1 . The result then follows lemma 4.1, which relates the expected\\nleave-one-out error to the expected error, along with the upper bound on M(S)\\ngiven by theorem 7.8.\\nThis result can be compared with ', 'a similar one given for the SVM algorithm (with\\nno o\ufb00set) in the following theorem, which is an extension of theorem 4.1. We denote\\nby N\\nSV(S) the number of support vectors that de\ufb01ne the hypothesis h', 'S returned\\nb yS V M sw h e nt r a i n e do nas a m p l eS.164 On-Line Learning\\nTheorem 7.10\\nAssume that the data is linearly separable. Let hS be the hypothesis returned by\\nSVMs used with no o\ufb00set (b ', '=0 ) after training over a sample S of size m drawn\\naccording to some distribution D. Then, the expected error of hS is bounded as\\nfollows:\\nE\\nS\u223cDm\\n[R(hS)] \u2264 E\\nS\u223cDm+1\\n[min\\n(\\nNSV(S),r 2\\nS/\u03c12\\nS\\n\u23a1\\nm +1\\n]\\n', '.\\nProof The fact that the expected error can be upper bounded by the average\\nfraction of support vectors ( NSV(S)/(m + 1)) was already shown by theorem 4.1.\\nThus, it su\ufb03ces to show that it is also upp', 'er bounded by the expected value of\\n(r\\n2\\nS/\u03c12\\nS\\n)/(m + 1). To do so, we will bound the leave-one-out error of the SVM\\nalgorithm for a sample S of size m +1b y( r2\\nS/\u03c12\\nS\\n)/(m + 1). The result will the', 'n\\nfollow by lemma 4.1, which relates the expected leave-one-out error to the expected\\nerror.\\nLet S =( x\\n1,..., xm+1) be a linearly separable sample drawn i.i.d. according to\\nD and let x be a point in ', 'S that is misclassi\ufb01ed by hS\u2212{x}. We will analyze the\\ncase where x = xm+1, the analysis of other cases is similar. We denote by S\u2032 the\\nsample (x1,..., xm).\\nFor any q \u2208 [1,m + 1], let Gq denote the fun', 'ction de\ufb01ned over Rq by Gq : \u03b1 \u21a6\u2192\u2211q\\ni=1 \u03b1i \u2212 1\\n2\\n\u2211q\\ni,j=1 \u03b1i\u03b1jyiyj(xi \u00b7 xj). Then, Gm+1 is the objective function of the\\ndual optimization problem for SVMs associated to the sample S and Gm the one\\nfo', 'r the sample S\u2032.L e t \u03b1 \u2208 Rm+1 denote a solution of the dual SVM problem\\nmax\u03b1 \u22650 Gm+1(\u03b1)a n d\u03b1\u2032 \u2208 Rm+1 the vector such that ( \u03b1\u2032\\n1,...,\u03b1 \u2032\\nm)\u22a4 \u2208 Rm is a\\nsolution of max \u03b1 \u22650 Gm(\u03b1)a n d \u03b1\u2032\\nm+1 =0 .L e ', 't em+1 denote the ( m +1 ) t hu n i t\\nvector in Rm+1.B yd e \ufb01 n i t i o no f\u03b1 and \u03b1\u2032 as maximizers, max \u03b2\u22650 Gm+1(\u03b1\u2032 +\\n\u03b2em+1) \u2264 Gm+1(\u03b1)a n d Gm+1(\u03b1 \u2212 \u03b1m+1em+1) \u2264 Gm(\u03b1\u2032). Thus, the quantity\\nA = Gm+1(\u03b1) ', '\u2212 Gm(\u03b1\u2032) admits the following lower and upper bounds:\\nmax\\n\u03b2\u22650\\nGm+1(\u03b1\u2032 + \u03b2em+1) \u2212 Gm(\u03b1\u2032) \u2264 A \u2264 Gm+1(\u03b1) \u2212 Gm+1(\u03b1 \u2212 \u03b1m+1em+1).\\nLet w = \u2211m+1\\ni=1 yi\u03b1ixi denote the weight vector returned by SVMs for the sa', 'mple\\nS.S i n c ehS\u2032 misclassi\ufb01es xm+1, xm+1 must be a support vector for hS,t h u s7.3 Linear classi\ufb01cation 165\\nym+1w \u00b7 xm+1 = 1. In view of that, the upper bound can be rewritten as follows:\\nGm+1(\u03b1) ', '\u2212 Gm+1(\u03b1 \u2212 \u03b1m+1em+1)\\n= \u03b1m+1 \u2212\\nm+1\u2211\\ni=1\\n(yi\u03b1ixi) \u00b7 (ym+1\u03b1m+1xm+1)+ 1\\n2\u03b12\\nm+1\u2225xm+1\u22252\\n= \u03b1m+1(1 \u2212 ym+1w \u00b7 xm+1)+ 1\\n2\u03b12\\nm+1\u2225xm+1\u22252\\n= 1\\n2\u03b12\\nm+1\u2225xm+1\u22252.\\nSimilarly, let w\u2032 = \u2211m\\ni=1 yi\u03b1\u2032\\nixi. Then, for any \u03b2 \u2265', ' 0, the quantity maximized in\\nt h el o w e rb o u n dc a nb ew r i t t e na s\\nGm+1(\u03b1\u2032 + \u03b2em+1) \u2212 Gm(\u03b1\u2032)\\n= \u03b2\\n(\\n1 \u2212 ym+1(w\u2032 + \u03b2xm+1) \u00b7 xm+1\\n\u23a1\\n+ 1\\n2\u03b22\u2225xm+1\u22252\\n= \u03b2(1 \u2212 ym+1w\u2032 \u00b7 xm+1) \u2212 1\\n2\u03b22\u2225xm+1\u22252.\\nThe ri', 'ght-hand side is maximized for the following value of \u03b2: 1\u2212ym+1w\u2032\u00b7xm+1\\n\u2225xm+1\u22252 .\\nPlugging in this value in the right-hand side gives 1\\n2\\n(1\u2212ym+1w\u2032 \u00b7xm+1)\\n\u2225xm+1\u22252 .T h u s ,\\nA \u2265 1\\n2\\n(1 \u2212 ym+1w\u2032 \u00b7 xm+1)', '\\n\u2225xm+1\u22252 \u2265 1\\n2\u2225xm+1\u22252 ,\\nusing the fact thatym+1w\u2032 \u00b7xm+1 < 0, sincexm+1 is misclassi\ufb01ed by w\u2032. Comparing\\nthis lower bound onA with the upper bound previously derived leads to 1\\n2\u2225xm+1\u22252 \u2264\\n1\\n2 \u03b12\\nm+1\u2225xm', '+1\u22252,t h a ti s\\n\u03b1m+1 \u2265 1\\n\u2225xm+1\u22252 \u2265 1\\nr2\\nS\\n.\\nThe analysis carried out in the casex = xm+1 holds similarly for anyxi in S that is\\nmisclassi\ufb01ed by hS\u2212{xi}.L e tI denote the set of such indicesi. Then, we', ' can write:\\n\u2211\\ni\u2208I\\n\u03b1i \u2265 |I|\\nr2\\nS\\n.\\nBy (4.18), the following simple expression holds for the margin: \u2211m+1\\ni=1 \u03b1i =1 /\u03c12\\nS.\\nUsing this identity leads to\\n|I|\u2264 r2\\nS\\n\u2211\\ni\u2208I\\n\u03b1i \u2264 r2\\nS\\nm+1\u2211\\ni=1\\n\u03b1i = r2\\nS\\n\u03c12\\nS\\n', '.166 On-Line Learning\\nSince by de\ufb01nition |I| is the total number of leave-one-out errors, this concludes the\\nproof.\\nThus, the guarantees given by theorem 7.9 and theorem 7.10 in the separable\\ncase hav', 'e a similar form. These bounds do not seem su\ufb03cient to distinguish the\\ne\ufb00ectiveness of the SVM and Perceptron algorithms. Note, however, that while the\\nsame margin quantity \u03c1\\nS appears in both bounds,', ' the radius rS can be replaced by\\na \ufb01ner quantity that is di\ufb00erent for the two algorithms: in both cases, instead of the\\nradius of the sphere containing all sample points, rS can be replaced by the ra', 'dius\\nof the sphere containing the support vectors, as can be seen straightforwardly from\\nthe proof of the theorems. Thus, the position of the support vectors in the case\\nof SVMs can provide a more fav', 'orable guarantee than that of the support vectors\\n(update vectors) for the Perceptron algorithm. Finally, the guarantees given by\\nthese theorems are somewhat weak. These are not high probability bound', 's, they\\nhold only for the expected error of the hypotheses returned by the algorithms and\\nin particular provide no information about the variance of their error.\\nThe following theorem presents a bound', ' on the number of updates or mistakes\\nmade by the Perceptron algorithm in the more general scenario of a non-linearly\\nseparable sample.\\nTheorem 7.11\\nLet x\\n1,..., xT \u2208 RN be a sequence of T points with', ' \u2225xt\u2225\u2264 r for all t \u2208 [1,T ],f o r\\nsome r> 0.L e tv \u2208 RN be any vector with \u2225v\u2225 =1 and let \u03c1> 0. De\ufb01ne the\\ndeviation of xt by dt =m a x{0,\u03c1 \u2212 yt(v \u00b7 xt)},a n dl e t\u03b4 =\\n\u221a\u2211T\\nt=1 d2\\nt . Then, the\\nnumber o', 'f updates made by the Perceptron algorithm when processingx1,..., xT is\\nbounded by (r + \u03b4)2/\u03c12.\\nProof We \ufb01rst reduce the problem to the separable case by mapping each input\\nvector xt \u2208 RN to a vector ', 'in x\u2032\\nt \u2208 RN+T as follows:\\nxt =\\n\u23a1\\n\u23a2\u23a2\u23a3\\nxt,1\\n..\\n.\\nx\\nt,N\\n\u23a4\\n\u23a5\u23a5\u23a6 \u21a6\u2192 x\u2032\\nt =\\n\u23a1\\n\u23a3xt,1 ... x t,N 0 ... 0\u0394 \\ued19\\ued18\\ued17\\ued1a\\n(N + t)th\\ncomponent\\n0 ... 0\\n\u23a4\\n\u23a6\\n\u22a4\\n,\\nwhere the \ufb01rst N components of x\u2032\\nt are identical to those of ', 'x and the only other\\nnon-zero component is the ( N + t)th component and is equal to \u0394. The value of\\nthe parameter \u0394 will be set later. The vector v is replaced by the vectorv\u2032 de\ufb01ned\\nas follows:\\nv\u2032 =\\n', '[\\nv1/Z . . . v N /Z y 1d1/(\u0394 Z) ... y T dT /(\u0394 Z)\\n]\u22a4\\n.\\nThe \ufb01rst N components ofv\u2032 are equal to the components ofv/Z and the remaining7.3 Linear classi\ufb01cation 167\\nDualPerceptron(\u03b10)\\n1 \u03b1 \u2190 \u03b10 \u22bf typicall', 'y \u03b10 = 0\\n2 for t \u2190 1 to T do\\n3 Receive(xt)\\n4 \u02c6yt \u2190 sgn(\u2211T\\ns=1 \u03b1sys(xs \u00b7 xt))\\n5 Receive(yt)\\n6 if (\u02c6yt \u0338= yt) then\\n7 \u03b1t+1 \u2190 \u03b1t +1\\n8 else \u03b1t+1 \u2190 \u03b1t\\n9 return \u03b1\\nFigure 7.8 Dual Perceptron algorithm.\\nT comp', 'onents are functions of the labels and deviations. Z is chosen to guarantee\\nthat \u2225v\u2032\u2225 =1 : Z =\\n\u221a\\n1+ \u03b42\\n\u0394 2 . The predictions made by the Perceptron algorithm\\nfor x\u2032\\nt, t \u2208 [1,T ] coincide with those m', 'ade in the original space for xt, t \u2208 [1,T ].\\nFurthermore, by de\ufb01nition of v\u2032 and x\u2032\\nt, we can write for any t \u2208 [1,T ]:\\nyt(v\u2032 \u00b7 x\u2032\\nt)= yt\\n(v \u00b7 xt\\nZ +\u0394 ytdt\\nZ\u0394\\n\u23a1\\n= ytv \u00b7 xt\\nZ + dt\\nZ\\n\u2265 ytv \u00b7 xt\\nZ + \u03c1\u2212 ', 'yt(v \u00b7 xt)\\nZ = \u03c1\\nZ ,\\nwhere the inequality results from the de\ufb01nition of the deviationdt. This shows that\\nthe sample formed by x\u2032\\n1,..., x\u2032\\nT is linearly separable with margin \u03c1/Z.T h u s ,i n\\nview of ', 'theorem 7.8, since ||x\u2032\\nt||2 \u2264 r2 +\u0394 2, the number of updates made by the\\nPerceptron algorithm is bounded by (r2+\u0394 2)(1+\u03b42/\u0394 2)\\n\u03c12 .C h o o s i n g\u03942 to minimize\\nthis bound leads to \u0394 2 = r\u03b4. Plugging', ' in this value yields the statement of the\\ntheorem.\\nThe main idea behind the proof of the theorem just presented is to map input points\\nto a higher-dimensional space where linear separation is possibl', 'e, which coincides\\nwith the idea of kernel methods. In fact, the particular kernel used in the proof is\\nclose to a straightforward one with a feature mapping that maps each data point\\nto a distinct di', 'mension.\\nThe Perceptron algorithm can in fact be generalized, as in the case of SVMs,168 On-Line Learning\\nKernelPerceptron(\u03b10)\\n1 \u03b1 \u2190 \u03b10 \u22bf typically \u03b10 = 0\\n2 for t \u2190 1 to T do\\n3 Receive(xt)\\n4 \u02c6yt \u2190 sgn', '(\u2211T\\ns=1 \u03b1sysK(xs,xt))\\n5 Receive(yt)\\n6 if (\u02c6yt \u0338= yt) then\\n7 \u03b1t+1 \u2190 \u03b1t +1\\n8 else \u03b1t+1 \u2190 \u03b1t\\n9 return \u03b1\\nFigure 7.9 Kernel Perceptron algorithm for PDS kernel K.\\nto de\ufb01ne a linear separation in a high-dim', 'ensional space. It admits an equivalent\\ndual form, the dual Perceptron algorithm, which is presented in \ufb01gure 7.8. The\\ndual Perceptron algorithm maintains a vector \u03b1 \u2208 R\\nT of coe\ufb03cients assigned to\\nea', 'ch point xt, t \u2208 [1,T ]. The label of a point xt is predicted according to the rule\\nsgn(w \u00b7xt), where w = \u2211T\\ns=1 \u03b1sysxs. The coe\ufb03cient \u03b1t is incremented by one when\\nthis prediction does not match the ', 'correct label. Thus, an update forxt is equivalent\\nto augmenting the weight vectorw with ytxt, which shows that the dual algorithm\\nmatches exactly the standard Perceptron algorithm. The dual Perceptro', 'n algorithm\\ncan be written solely in terms of inner products between training instances. Thus, as\\ni nt h ec a s eo fS V M s ,i n s t e a do ft h ei n n e rp r o d u c tb e t w e e np o i n t si nt h e', 'i n p u ts p a c e ,\\nan arbitrary PDS kernel can be used, which leads to the kernel Perceptron algorithm\\ndetailed in \ufb01gure 7.9. The kernel Perceptron algorithm and its average variant,\\ni.e., voted Per', 'ceptron with uniform weights c\\nt, are commonly used algorithms in a\\nvariety of applications.\\n7.3.2 Winnow algorithm\\nThis section presents an alternative on-line linear classi\ufb01cation algorithm, the\\nWin', 'now algorithm. Thus, it learns a weight vector de\ufb01ning a separating hyperplane\\nby sequentially processing the training points. As suggested by the name, the\\nalgorithm is particularly well suited to ca', 'ses where a relatively small number of\\ndimensions or experts can be used to de\ufb01ne an accurate weight vector. Many of the\\nother dimensions may then be irrelevant.7.3 Linear classi\ufb01cation 169\\nWinnow(\u03b7)\\n', '1 w1 \u2190 1/N\\n2 for t \u2190 1 to T do\\n3 Receive(xt)\\n4 \u02c6yt \u2190 sgn(wt \u00b7 xt)\\n5 Receive(yt)\\n6 if (\u02c6yt \u0338= yt) then\\n7 Zt \u2190 \u2211N\\ni=1 wt,i exp(\u03b7ytxt,i)\\n8 for i \u2190 1 to N do\\n9 wt+1,i \u2190 wt,i exp(\u03b7ytxt,i)\\nZt\\n10 else wt+1 \u2190', ' wt\\n11 return wT+1\\nFigure 7.10 Winnow algorithm, with yt \u2208{ \u2212 1, +1} for all t \u2208 [1,T ].\\nThe Winnow algorithm is similar to the Perceptron algorithm, but, instead of\\nthe additive update of the weight ', 'vector in the Perceptron case, Winnow\u2019s update\\nis multiplicative. The pseudocode of the algorithm is given in \ufb01gure 7.10. The\\nalgorithm takes as input a learning parameter \u03b7> 0. It maintains a non-neg', 'ative\\nweight vector wt with components summing to one ( \u2225wt\u22251 =1 )s t a r t i n gw i t ht h e\\nuniform weight vector (line 1). At each round t \u2208 [1,T ], if the prediction does not\\nmatch the correct lab', 'el (line 6), each component wt,i, i \u2208 [1,N ], is updated by\\nmultiplying it by exp(\u03b7ytxt,i) and dividing by the normalization factorZt to ensure\\nthat the weights sum to one (lines 7\u20139). Thus, if the la', 'belyt and xt,i share the same\\nsign, then wt,i is increased, while, in the opposite case, it is signi\ufb01cantly decreased.\\nThe Winnow algorithm is closely related to the WM algorithm: when xt,i \u2208\\n{\u22121,+1},', ' sgn(wt \u00b7xt) coincides with the majority vote, since multiplying the weight\\nof correct or incorrect experts bye\u03b7 or e\u2212\u03b7 is equivalent to multiplying the weight of\\nincorrect ones by \u03b2 = e\u22122\u03b7. The multi', 'plicative update rule of Winnow is of course\\nalso similar to that of AdaBoost.\\nThe following theorem gives a mistake bound for the Winnow algorithm in\\nthe separable case, which is similar in form to t', 'he bound of theorem 7.8 for the\\nPerceptron algorithm.170 On-Line Learning\\nTheorem 7.12\\nLet x1,..., xT \u2208 RN be a sequence of T points with \u2225xt\u2225\u221e \u2264 r\u221e for all t \u2208 [1,T ],\\nfor some r\u221e > 0. Assume that th', 'ere existv \u2208 RN, v \u2265 0,a n d\u03c1\u221e > 0 such that for\\nall t \u2208 [1,T ], \u03c1\u221e \u2264 yt(v\u00b7xt)\\n\u2225v\u22251\\n.T h e n ,f o r\u03b7= \u03c1\u221e\\nr2\\n\u221e\\n, the number of updates made by the\\nWinnow algorithm when processingx1,..., xT is upper bo', 'unded by2(r2\\n\u221e /\u03c12\\n\u221e\\n)l o gN.\\nProof Let I \u2286{ 1,...,T } be the set of iterations at which there is an update,\\nand let M be the total number of updates, i.e., |I| = M. The potential function \u03a6t,\\nt \u2208 [1,', 'T ], used for this proof is the relative entropy of the distribution de\ufb01ned by the\\nnormalized weights vi/\u2225v\u22251 \u2265 0, i \u2208 [1,N ], and the one de\ufb01ned by the components\\nof the weight vector wt,i, i \u2208 [1,N ', ']:\\n\u03a6t =\\nN\u2211\\ni=1\\nvi\\n\u2225v\u22251\\nlog vi/\u2225v\u22251\\nwt,i\\n.\\nTo derive an upper bound on \u03a6t, we analyze the di\ufb00erence of the potential functions\\nat two consecutive rounds. For all t \u2208 I, this di\ufb00erence can be expressed ', 'and\\nbounded as follows:\\n\u03a6t+1 \u2212 \u03a6t =\\nN\u2211\\ni=1\\nvi\\n\u2225v\u22251\\nlog wt,i\\nwt+1,i\\n=\\nN\u2211\\ni=1\\nvi\\n\u2225v\u22251\\nlog Zt\\nexp(\u03b7ytxt,i)\\n=l o gZt \u2212 \u03b7\\nN\u2211\\ni=1\\nvi\\n\u2225v\u22251\\nytxt,i\\n\u2264 log\\n[ N\u2211\\ni=1\\nwt,i exp(\u03b7ytxt,i)\\n]\\n\u2212 \u03b7\u03c1\u221e\\n= log E\\nwt\\n[\\nexp(\u03b7yt', 'xt)\\n]\\n\u2212 \u03b7\u03c1\u221e\\n\u2264 log\\n[\\nexp(\u03b72(2r\u221e )2/8)\\n]\\n\u2212 \u03b7\u03c1\u221e\\n= \u03b72r2\\n\u221e /2 \u2212 \u03b7\u03c1\u221e .\\nThe \ufb01rst inequality follows the de\ufb01nition \u03c1\u221e . The subsequent equality rewrites\\nthe summation as an expectation over the distribution d', 'e\ufb01ned by wt.T h en e x t\\ninequality uses Hoe\ufb00ding\u2019s lemma (lemma D.1). Summing up these inequalities\\nover all t \u2208 I yields:\\n\u03a6T+1 \u2212 \u03a61 \u2264 M(\u03b72r2\\n\u221e /2 \u2212 \u03b7\u03c1\u221e ).7.4 On-line to batch conversion 171\\nNext, we', ' derive a lower bound by noting that\\n\u03a61 =\\nN\u2211\\ni=1\\nvi\\n\u2225v\u22251\\nlog vi/\u2225v\u22251\\n1/N =l o gN +\\nN\u2211\\ni=1\\nvi\\n\u2225v\u22251\\nlog vi\\n\u2225v\u22251\\n\u2264 log N.\\nAdditionally, since the relative entropy is always non-negative, we have \u03a6T+1 \u2265 0', '.\\nThis yields the following lower bound:\\n\u03a6T+1 \u2212 \u03a61 \u2265 0 \u2212 log N = \u2212 log N.\\nCombining the upper and lower bounds we see that \u2212 log N \u2264 M(\u03b72r2\\n\u221e /2 \u2212 \u03b7\u03c1\u221e ).\\nSetting \u03b7= \u03c1\u221e\\nr2\\n\u221e\\nyields the statement of the', ' theorem.\\nThe margin-based mistake bounds of theorem 7.8 and theorem 7.12 for the Percep-\\ntron and Winnow algorithms have a similar form, but they are based on di\ufb00erent\\nnorms. For both algorithms, the', ' norm\u2225\u00b7\u2225\\np used for the input vectorsxt, t \u2208 [1,T ],\\nis the dual of the norm \u2225\u00b7\u2225 q used for the margin vector v,t h a ti sp and q are\\nconjugate: 1/p +1 /q = 1: in the case of the Perceptron algorithm ', 'p = q = 2, while\\nfor Winnow p = \u221e and q =1 .\\nThese bounds imply di\ufb00erent types of guarantees. The bound for Winnow is\\nfavorable when a sparse set of the experts i \u2208 [1,N ] can predict well. For exampl', 'e,\\nif v = e1 where e1 is the unit vector along the \ufb01rst axis inRN and ifxt \u2208{ \u22121, +1}N\\nfor all t, then the upper bound on the number of mistakes given for Winnow by\\ntheorem 7.12 is only logN, while th', 'e upper bound of theorem 7.8 for the Perceptron\\nalgorithm is N. The guarantee for the Perceptron algorithm is more favorable in\\nthe opposite situation, where sparse solutions are not e\ufb00ective.\\n7.4 On-', 'line to batch conversion\\nThe previous sections presented several algorithms for the scenario of on-line\\nlearning, including the Perceptron and Winnow algorithms, and analyzed their\\nbehavior within the', ' mistake model, where no assumption is made about the way the\\ntraining sequence is generated. Can these algorithms be used to derive hypotheses\\nwith small generalization error in the standard stochast', 'ic setting? How can the\\nintermediate hypotheses they generate be combined to form an accurate predictor?\\nThese are the questions addressed in this section.\\nLet H be a hypothesis of functions mapping X', ' to Y\\n\u2032,a n dl e tL: Y\u2032 \u00d7Y \u2192 R+\\nbe a bounded loss function, that is L \u2264 M for some M \u2265 0. We assume a standard\\nsupervised learning setting where a labeled sample S =( (x1,y1),..., (xT ,y T )) \u2208\\n(X\u00d7 Y ', ')T is drawn i.i.d. according to some \ufb01xed but unknown distribution D.T h e\\nsample is sequentially processed by an on-line learning algorithmA. The algorithm172 On-Line Learning\\nstarts with an initial ', 'hypothesis h1 \u2208 H and generates a new hypothesis hi+1 \u2208 H,\\nafter processing pair ( xi,y i), i \u2208 [1,m]. The regret of the algorithm is de\ufb01ned as\\nbefore by\\nRT =\\nT\u2211\\ni=1\\nL(hi(xi),y i) \u2212 min\\nh\u2208H\\nT\u2211\\ni=1\\nL(h', '(xi),y i). (7.24)\\nThe generalization error of a hypothesis h \u2208 H is its expected loss R(h)=\\nE(x,y)\u223cD[L(h(x),y )].\\nThe following lemma gives a bound on the average of the generalization errors of\\nthe h', 'ypotheses generated by A in terms of its average loss 1\\nT\\n\u2211T\\ni=1 L(hi(xi),y i).\\nLemma 7.1\\nLet S =( (x1,y1),..., (xT ,y T )) \u2208 (X\u00d7 Y)T be a labeled sample drawn i.i.d. according\\nto D, L a loss bounded ', 'byM and h1,...,h T+1 the sequence of hypotheses generated\\nby an on-line algorithm A sequentially processing S.T h e n ,f o ra n y\u03b4> 0,w i t h\\nprobability at least 1 \u2212 \u03b4, the following holds:\\n1\\nT\\nT\u2211\\ni=', '1\\nR(hi) \u2264 1\\nT\\nT\u2211\\ni=1\\nL(hi(xi),y i)+ M\\n\u221a\\n2l o g1\\n\u03b4\\nT . (7.25)\\nProof For any i \u2208 [1,T ], let Vi be the random variable de\ufb01ned by Vi = R(hi) \u2212\\nL(hi(xi),y i). Observe that for any i \u2208 [1,T ],\\nE[Vi|x1,...,', 'x i\u22121]= R(hi) \u2212 E[L(hi(xi),y i)|hi]= R(hi) \u2212 R(hi)=0 .\\nSince the loss is bounded by M, Vi takes values in the interval [ \u2212M, +M]f o r\\nall i \u2208 [1,T ]. Thus, by Azuma\u2019s inequality (theorem D.2), Pr[ 1\\nT', '\\n\u2211T\\ni=1 Vi \u2265 \u03f5] \u2264\\nexp(\u22122T\u03f52/(2M)2)). Setting the right-hand side to be equal to \u03b4> 0y i e l d st h e\\nstatement of the lemma.\\nW h e nt h el o s sf u n c t i o ni sc o n v e xw i t hr e s p e c tt oi t ', 's\ufb01 r s ta r g u m e n t ,t h el e m m a\\ncan be used to derive a bound on the generalization error of the average of the\\nhypotheses generated by A, 1\\nT\\n\u2211T\\nt=1 hi, in terms of the average loss of A on S', ',o r\\nin terms of the regret RT and the in\ufb01mum error of hypotheses in H.\\nTheorem 7.13\\nLet S =( ( x1,y1),..., (xT ,y T )) \u2208 (X\u00d7 Y )T be a labeled sample drawn i.i.d.\\naccording toD, L a loss bounded byM ', 'and convex with respect to its \ufb01rst argument,\\nand h1,...,h T+1 the sequence of hypotheses generated by an on-line algorithm A\\nsequentially processing S.T h e n ,f o ra n y\u03b4> 0, with probability at lea', 'st 1 \u2212 \u03b4,e a c h7.4 On-line to batch conversion 173\\nof the following holds:\\nR\\n( 1\\nT\\nT\u2211\\ni=1\\nhi\\n\u23a1\\n\u2264 1\\nT\\nT\u2211\\ni=1\\nL(hi(xi),y i)+ M\\n\u221a\\n2l o g1\\n\u03b4\\nT (7.26)\\nR\\n( 1\\nT\\nT\u2211\\ni=1\\nhi\\n\u23a1\\n\u2264 inf\\nh\u2208H\\nR(h)+ RT\\nT +2 M\\n\u221a\\n2l o ', 'g2\\n\u03b4\\nT . (7.27)\\nProof By the convexity of L with respect to its \ufb01rst argument, for any ( x, y) \u2208\\nX\u00d7 Y ,w eh a v eL( 1\\nT\\n\u2211T\\ni=1 hi(x),y ) \u2264 1\\nT\\n\u2211T\\ni=1 L(hi(x),y ). Taking the expectation\\ngives R( 1\\nT\\n\u2211', 'T\\ni=1 hi) \u2264 1\\nT\\n\u2211T\\ni=1 R(hi). The \ufb01rst inequality then follows by lemma 7.1.\\nThus, by de\ufb01nition of the regret RT , for any \u03b4> 0, the following holds with\\nprobability at least 1 \u2212 \u03b4/2:\\nR\\n( 1\\nT\\nT\u2211\\ni=1\\nh', 'i\\n\u23a1\\n\u2264 1\\nT\\nT\u2211\\ni=1\\nL(hi(xi),y i)+ M\\n\u221a\\n2l o g2\\n\u03b4\\nT\\n\u2264 min\\nh\u2208H\\n1\\nT\\nT\u2211\\ni=1\\nL(h(xi),y i)+ RT\\nT + M\\n\u221a\\n2l o g2\\n\u03b4\\nT .\\nBy de\ufb01nition of inf h\u2208H R(h), for any \u03f5> 0, there exists h\u2217 \u2208 H with R(h\u2217) \u2264\\ninfh\u2208H R(h)+ \u03f5.', ' By Hoe\ufb00ding\u2019s inequality, for any\u03b4> 0, with probability at least\\n1 \u2212 \u03b4/2, 1\\nT\\n\u2211T\\ni=1 L(h\u2217(xi),y i) \u2264 R(h\u2217)+ M\\n\u221a\\n2 log 2\\n\u03b4\\nT . Thus, for any \u03f5> 0, by the\\nunion bound, the following holds with probabil', 'ity at least 1 \u2212 \u03b4:\\nR\\n( 1\\nT\\nT\u2211\\ni=1\\nhi\\n\u23a1\\n\u2264 1\\nT\\nT\u2211\\ni=1\\nL(h\u2217(xi),y i)+ RT\\nT + M\\n\u221a\\n2l o g2\\n\u03b4\\nT\\n\u2264 R(h\u2217)+ M\\n\u221a\\n2l o g2\\n\u03b4\\nT + RT\\nT + M\\n\u221a\\n2l o g2\\n\u03b4\\nT\\n= R(h\u2217)+ RT\\nT +2 M\\n\u221a\\n2l o g2\\n\u03b4\\nT\\n\u2264 inf\\nh\u2208H\\nR(h)+ \u03f5 + RT\\nT +', '2 M\\n\u221a\\n2l o g2\\n\u03b4\\nT .\\nSince this inequality holds for all \u03f5> 0, it implies the second statement of the\\ntheorem.\\nThe theorem can be applied to a variety of on-line regret minimization algorithms,\\nfor exa', 'mple when R\\nT /T = O(1/\\n\u221a\\nT). In particular, we can apply the theorem to\\nthe exponential weighted average algorithm. Assuming that the loss L is bounded174 On-Line Learning\\nby M = 1 and that the numbe', 'r of rounds T is known to the algorithm, we can use\\nthe regret bound of theorem 7.6. The doubling trick (used in theorem 7.7) can be\\nused to derive a similar bound if T is not known in advance. Thus, ', 'for any \u03b4> 0,\\nwith probability at least 1 \u2212 \u03b4, the following holds for the generalization error of\\nthe average of the hypotheses generated by exponential weighted average:\\nR\\n( 1\\nT\\nT\u2211\\ni=1\\nhi\\n\u23a1\\n\u2264 inf\\nh\u2208', 'H\\nR(h)+\\n\u221a\\nlog N\\n2T +2\\n\u221a\\n2l o g2\\n\u03b4\\nT ,\\nwhere N is the number of experts, or the dimension of the weight vectors.\\n7.5 Game-theoretic connection\\nThe existence of regret minimization algorithms can be use', 'd to give a simple proof\\nof von Neumann\u2019s theorem. For any m \u2265 1, we will denote by \u0394 m the set of all\\ndistributions over {1,...,m },t h a ti s\u0394m = {p \u2208 Rm : p \u2265 0 \u2227\u2225p\u22251 =1 }.\\nTheorem 7.14 Von Neumann', '\u2019s minimax theorem\\nLet m, n \u2265 1. Then, for any two-person zero-sum game de\ufb01ned by matrix M \u2208\\nRm\u00d7n,\\nmin\\np\u2208\u0394 m\\nmax\\nq\u2208\u0394 n\\np\u22a4Mq =m a x\\nq\u2208\u0394 n\\nmin\\np\u2208\u0394 m\\np\u22a4Mq. (7.28)\\nProof The inequality maxq minp p\u22a4Mq \u2264 mi', 'np maxq p\u22a4Mq is straightforward,\\nsince by de\ufb01nition of min, for all p \u2208 \u0394 m,q \u2208 \u0394 n,w eh a v em i np p\u22a4Mq \u2264 p\u22a4Mq.\\nTaking the maximum overq of both sides gives: maxq minp p\u22a4Mq \u2264 maxq p\u22a4Mq\\nfor all p, su', 'bsequently taking the minimum over p proves the inequality.2\\nTo show the reverse inequality, consider an on-line learning setting where at each\\nround t \u2208 [1,T ], algorithm A returns pt and incurs loss', ' Mqt. We can assume that\\nqt is selected in the optimal adversarial way, that is qt \u2208 argmaxq\u2208\u0394 m p\u22a4\\nt Mq,\\nand that A is a regret minimization algorithm, that is RT /T \u2192 0, where RT =\u2211T\\nt=1 p\u22a4\\nt Mqt \u2212 ', 'minp\u2208\u0394 m\\n\u2211T\\nt=1 p\u22a4Mqt. Then, the following holds:\\nmin\\np\u2208\u0394 m\\nmax\\nq\u2208\u0394 n\\np\u22a4Mq \u2264 max\\nq\\n(1\\nT\\nT\u2211\\nt=1\\npt\\n\u23a1\u22a4\\nMq \u2264 1\\nT\\nT\u2211\\nt=1\\nmax\\nq\\np\u22a4\\nt Mq = 1\\nT\\nT\u2211\\nt=1\\np\u22a4\\nt Mqt.\\n2. More generally, the maxmin is always upper ', 'bounded by the minmax for any function\\nor two arguments and any constraint sets, following the same proof.7.6 Chapter notes 175\\nBy de\ufb01nition of regret, the right-hand side can be expressed and bounded', ' as follows:\\n1\\nT\\nT\u2211\\nt=1\\np\u22a4\\nt Mqt =m i n\\np\u2208\u0394 m\\n1\\nT\\nT\u2211\\nt=1\\np\u22a4Mqt + RT\\nT =m i n\\np\u2208\u0394 m\\np\u22a4M\\n(1\\nT\\nT\u2211\\nt=1\\nqt\\n\u23a1\\n+ RT\\nT\\n\u2264 max\\nq\u2208\u0394 n\\nmin\\np\u2208\u0394 m\\np\u22a4Mq + RT\\nT .\\nThis implies that the following bound holds for the m', 'inmax for all T \u2265 1:\\nmin\\np\u2208\u0394 m\\nmax\\nq\u2208\u0394 n\\np\u22a4Mq \u2264 max\\nq\u2208\u0394 n\\nmin\\np\u2208\u0394 m\\np\u22a4Mq + RT\\nT\\nSince limT \u2192 +\u221e\\nRT\\nT = 0, this shows that minp maxq p\u22a4Mq \u2264 maxq minp p\u22a4Mq.\\n7.6 Chapter notes\\nAlgorithms for regret minim', 'ization were initiated with the pioneering work of\\nHannan [1957] who gave an algorithm whose regret decreases asO(\\n\u221a\\nT) as a function\\nof T but whose dependency on N is linear. The weighted majority al', 'gorithm and\\nthe randomized weighted majority algorithm, whose regret is only logarithmic inN,\\nare due to Littlestone and Warmuth [1989]. The exponentiated average algorithm\\nand its analysis, which can', ' be viewed as an extension of the WM algorithm to\\nconvex non-zero-one losses is due to the same authors [Littlestone and Warmuth,\\n1989, 1994]. The analysis we presented follows Cesa-Bianchi [1999] and', ' Cesa-Bianchi\\nand Lugosi [2006]. The doubling trick technique appears in Vovk [1990] and Cesa-\\nBianchi et al. [1997]. The algorithm of exercise 7.7 and the analysis leading to a\\nsecond-order bound on ', 'the regret are due to Cesa-Bianchi et al. [2005]. The lower\\nbound presented in theorem 7.5 is from Blum and Mansour [2007].\\nWhile the regret bounds presented are logarithmic in the number of the exper', 'ts\\nN,w h e nN is exponential in the size of the input problem, the computational\\ncomplexity of an expert algorithm could be exponential. For example, in the on-\\nline shortest paths problem, N is the n', 'umber of paths between two vertices of\\na directed graph. However, several computationally e\ufb03cient algorithms have been\\npresented for broad classes of such problems by exploiting their structure [Takim', 'oto\\nand Warmuth, 2002, Kalai and Vempala, 2003, Zinkevich, 2003].\\nThe notion of regret (or external regret) presented in this chapter can be gener-\\nalized to that of internal regret or even swap regre', 't, by comparing the loss of the\\nalgorithm not just to that of the best expert in retrospect, but to that of any modi-\\n\ufb01cation of the actions taken by the algorithm by replacing each occurrence of some', '\\nspeci\ufb01c action with another one (internal regret), or even replacing actions via an ar-176 On-Line Learning\\nbitrary mapping (swap regret) [Foster and Vohra, 1997, Hart and Mas-Colell, 2000,\\nLehrer, 2', '003]. Several algorithms for low internal regret have been given [Foster\\nand Vohra, 1997, 1998, 1999, Hart and Mas-Colell, 2000, Cesa-Bianchi and Lugosi,\\n2001, Stoltz and Lugosi, 2003], including a co', 'nversion of low external regret to low\\nswap regret by Blum and Mansour [2005].\\nThe Perceptron algorithm was introduced by Rosenblatt [1958]. The algorithm\\nraised a number of reactions, in particular b', 'y Minsky and Papert [1969], who\\nobjected that the algorithm could not be used to recognize the XOR function.\\nOf course, the kernel Perceptron algorithm already given by Aizerman et al. [1964]\\ncould st', 'raightforwardly succeed to do so using second-degree polynomial kernels.\\nThe margin bound for the Perceptron algorithm was proven by Noviko\ufb00 [1962]\\nand is one of the \ufb01rst results in learning theory. T', 'he leave-one-out analysis for\\nSVMs is described by Vapnik [1998]. The upper bound presented for the Perceptron\\nalgorithm in the non-separable case is by Freund and Schapire [1999a]. The Winnow\\nalgorit', 'hm was introduced by Littlestone [1987].\\nThe analysis of the on-line to batch conversion and exercise 7.10 are from Cesa-\\nBianchi et al. [2001, 2004] (see also Littlestone [1989]). Von Neumann\u2019s minim', 'ax\\ntheorem admits a number of di\ufb00erent generalizations. See Sion [1958] for a gener-\\nalization to quasi-concave-convex functions semi-continuous in each argument and\\nthe references therein. The simple', ' proof of von Neumann\u2019s theorem presented here\\nis entirely based on learning-related techniques. A proof of a more general version\\nusing multiplicative updates was presented by Freund and Schapire [19', '99b].\\nOn-line learning is a very broad and fast-growing research area in machine\\nlearning. The material presented in this chapter should be viewed only as an\\nintroduction to the topic, but the proofs ', 'and techniques presented should indicate\\nthe \ufb02avor of most results in this area. For a more comprehensive presentation of on-\\nline learning and related game theory algorithms and techniques, the reade', 'r could\\nconsult the book of Cesa-Bianchi and Lugosi [2006].\\n7.7 Exercises\\n7.1 Perceptron lower bound. Let S be a labeled sample of m points in RN with\\nxi =( (\u22121)i,..., (\u22121)i, (\u22121)i+1\\n\\ued19 \\ued18\\ued17 \\ued1a\\ni \ufb01rst com', 'ponents\\n, 0,..., 0) and yi =( \u22121)i+1. (7.29)\\nShow that the Perceptron algorithm makes \u03a9(2 N ) updates before \ufb01nding a sepa-\\nrating hyperplane, regardless of the order in which it receives the points.\\n', '7.2 Generalized mistake bound. Theorem 7.8 presents a margin bound on the7.7 Exercises 177\\nOn-line-SVM(w0)\\n1 w1 \u2190 w0 \u22bf typically w0 = 0\\n2 for t \u2190 1 to T do\\n3 Receive(xt,y t)\\n4 if yt(wt \u00b7 xt) < 1 then\\n', '5 wt+1 \u2190 wt \u2212 \u03b7(wt \u2212 Cytxt)\\n6 elseif yt(wt \u00b7 xt) > 1 then\\n7 wt+1 \u2190 wt \u2212 \u03b7wt\\n8 else wt+1 \u2190 wt\\n9 return wT+1\\nFigure 7.11 On-line SVM algorithm.\\nmaximum number of updates for the Perceptron algorithm for', ' the special case\\n\u03b7 = 1. Consider now the general Perceptron update wt+1 \u2190 wt + \u03b7ytxt,w h e r e\\n\u03b7> 0. Prove a bound on the maximum number of mistakes. How does \u03b7 a\ufb00ect the\\nbound?\\n7.3 Sparse instances.', ' Suppose each input vector xt, t \u2208 [1,T ], coincides with the\\ntth unit vector of RT . How many updates are required for the Perceptron algorithm\\nto converge? Show that the number of updates matches th', 'e margin bound of\\ntheorem 7.8.\\n7.4 Tightness of lower bound. Is the lower bound of theorem 7.5 tight? Explain why\\nor show a counter-example.\\n7.5 On-line SVM algorithm. Consider the algorithm described', ' in \ufb01gure 7.11. Show\\nthat this algorithm corresponds to the stochastic gradient descent technique applied\\nto the SVM problem (4.23) with hinge loss and no o\ufb00set (i.e., \ufb01x p = 1 and b =0 ) .\\n7.6 Margin', ' Perceptron. Given a training sample S that is linearly separable with\\na maximum margin \u03c1> 0, theorem 7.8 states that the Perceptron algorithm run\\ncyclically over S is guaranteed to converge after at ', 'mostR\\n2/\u03c12 updates, where R is\\nthe radius of the sphere containing the sample points. However, this theorem does\\nnot guarantee that the hyperplane solution of the Perceptron algorithm achieves\\na margi', 'n close to \u03c1. Suppose we modify the Perceptron algorithm to ensure that178 On-Line Learning\\nMarginPerceptron()\\n1 w1 \u2190 0\\n2 for t \u2190 1 to T do\\n3 Receive(xt)\\n4 Receive(yt)\\n5 if\\n(\\n(wt =0 ) or (ytwt\u00b7xt\\n\u2225wt\u2225', ' < \u03c1\\n2 )\\n\u23a1\\nthen\\n6 wt+1 \u2190 wt + ytxt\\n7 else wt+1 \u2190 wt\\n8 return wT+1\\nFigure 7.12 Margin Perceptron algorithm.\\nthe margin of the hyperplane solution is at least \u03c1/2. In particular, consider the\\nalgorithm ', 'described in \ufb01gure 7.12. In this problem we show that this algorithm\\nconverges after at most 16R2/\u03c12 updates. Let I denote the set of times t \u2208 [1,T ]\\nat which the algorithm makes an update and let M ', '= |I| be the total number of\\nupdates.\\n(a) Using an analysis similar to the one given for the Perceptron algorithm,\\nshow that M\u03c1 \u2264\u2225 wT+1\u2225. Conclude that if \u2225wT+1\u2225 < 4R2\\n\u03c1 ,t h e nM< 4R2/\u03c12.\\n(For the re', 'mainder of this problem, we will assume that \u2225wT+1\u2225\u2265 4R2\\n\u03c1 .)\\n(b) Show that for any t \u2208 I (including t = 0), the following holds:\\n\u2225wt+1\u22252 \u2264 (\u2225wt\u2225 + \u03c1/2)2 + R2.\\n(c) From (b), infer that for any t \u2208 I w', 'e have\\n\u2225wt+1\u2225\u2264\u2225 wt\u2225 + \u03c1/2+ R2\\n\u2225wt\u2225 + \u2225wt+1\u2225 + \u03c1/2.\\n(d) Using the inequality from (c), show that for any t \u2208 I such that either\\n\u2225wt\u2225\u2265 4R2\\n\u03c1 or \u2225wt+1\u2225\u2265 4R2\\n\u03c1 ,w eh a v e\\n\u2225wt+1\u2225\u2264\u2225 wt\u2225 + 3\\n4\u03c1.\\n(e) Show th', 'at \u2225w1\u2225\u2264 R \u2264 4R2/\u03c1. Since by assumption we have \u2225wT+1\u2225\u2265\\n4R2\\n\u03c1 , conclude that there must exist a largest timet0 \u2208 I such that \u2225wt0 \u2225\u2264 4R2\\n\u03c17.7 Exercises 179\\nand \u2225wt0+1\u2225\u2265 4R2\\n\u03c1 .\\n(f) Show that \u2225wT+1\u2225\u2264\u2225', ' wt0 \u2225 + 3\\n4 M\u03c1. Conclude that M \u2264 16R2/\u03c12.\\n7.7 Second-order regret bound. Consider the randomized algorithm that di\ufb00ers from\\nthe RWM algorithm only by the weight update, i.e., wt+1,i \u2190 (1 \u2212 (1 \u2212 \u03b2)lt', ',i)wt,i,\\nt \u2208 [1,T ], which is applied to all i \u2208 [1,N ]w i t h1/2 \u2264 \u03b2< 1. This algorithm can\\nbe used in a more general setting than RWM since the losses lt,i are only assumed\\nto be in [0,1]. The objec', 'tive of this problem is to show that a similar upper bound\\ncan be shown for the regret.\\n(a) Use the same potential Wt as for the RWM algorithm and derive a simple\\nupper bound for log WT+1:\\nlog WT+1 \u2264 ', 'log N \u2212 (1 \u2212 \u03b2)LT .\\n(Hint:U s et h ei d e n t i t yl o g ( 1\u2212 x) \u2264\u2212 x for x \u2208 [0,1/2].)\\n(b) Prove the following lower bound for the potential for all i \u2208 [1,N ]:\\nlog WT+1 \u2265\u2212 (1 \u2212 \u03b2)LT,i \u2212 (1 \u2212 \u03b2)2\\nT\u2211\\n', 't=1\\nl2\\nt,i .\\n(Hint:U s et h ei d e n t i t yl o g ( 1\u2212 x) \u2265\u2212 x \u2212 x2, which is valid for allx \u2208 [0, 1/2].)\\n(c) Use upper and lower bounds to derive the following regret bound for the\\nalgorithm: RT \u2264 2\u221a', 'T log N.\\n7.8 Polynomial weighted algorithm. The objective of this problem is to show how\\nanother regret minimization algorithm can be de\ufb01ned and studied. Let L be a loss\\nfunction convex in its \ufb01rst ar', 'gument and taking values in [0,M ].\\nWe will assume N>e 2 and then for any expert i \u2208 [1,N ], we denote by rt,i the\\ni n s t a n t a n e o u sr e g r e to ft h a te x p e r ta tt i m et \u2208 [1,T ], rt,i =', ' L(\u02c6yt,y t)\u2212L(yt,i,y t), and\\nby Rt,i his cumulative regret up to timet: Rt,i = \u2211t\\ns=1 rt,i. For convenience, we also\\nde\ufb01ne R0,i =0f o ra l li \u2208 [1,N ]. For anyx \u2208 R,( x)+ denotes max(x,0), that is the', '\\npositive part of x,a n df o rx =( x1,...,x N )\u22a4 \u2208 RN ,( x)+ =( (x1)+,..., (xN )+)\u22a4.\\nLet \u03b1> 2 and consider the algorithm that predicts at round t \u2208 [1,T ]a c c o r d i n g\\nto \u02c6yt =\\nPn\\ni=1 wt,iyt,iPn\\ni', '=1 wt,i\\n,w i t ht h ew e i g h twt,i de\ufb01ned based on the \u03b1th power of\\nt h er e g r e tu pt ot i m e(t \u2212 1): wt,i =( Rt\u22121,i)\u03b1\u22121\\n+ .T h ep o t e n t i a lf u n c t i o nw e\\nuse to analyze the algorithm ', 'is based on the function \u03a6 de\ufb01ned over RN by\\n\u03a6: x \u21a6\u2192\u2225 (x)+\u22252\\n\u03b1 =\\n[\u2211N\\ni=1(xi)\u03b1\\n+\\n]2\\n\u03b1 .180 On-Line Learning\\n(a) Show that \u03a6 is twice di\ufb00erentiable over RN \u2212 B,w h e r eB is de\ufb01ned as\\nfollows:\\nB = {u \u2208 ', 'RN :( u)+ =0 }.\\n(b) For any t \u2208 [1,T ], let rt denote the vector of instantaneous regrets,\\nrt =( rt,1,...,r t,N )\u22a4,a n ds i m i l a r l yRt =( Rt,1,...,R t,N )\u22a4.W ed e \ufb01 n et h e\\npotential function as', ' \u03a6( Rt)= \u2225(Rt)+\u22252\\n\u03b1.C o m p u t e\u2207\u03a6(Rt\u22121)f o rRt\u22121 \u0338\u2208 B\\nand show that \u2207\u03a6(Rt\u22121) \u00b7 rt \u2264 0( Hint: use the convexity of the loss with\\nrespect to the \ufb01rst argument).\\n(c) Prove the inequality r\u22a4[\u22072\u03a6(u)]r \u2264 ', '2(\u03b1 \u2212 1)\u2225r\u22252\\n\u03b1 valid for all r \u2208 RN and\\nu \u2208 RN \u2212 B (Hint: write the Hessian \u22072\u03a6(u) as a sum of a diagonal matrix\\nand a positive semi-de\ufb01nite matrix multiplied by (2 \u2212 \u03b1). Also, use H\u00a8older\u2019s\\ninequalit', 'y generalizing Cauchy-Schwarz : for anyp> 1a n dq> 1w i t h1\\np +1\\nq =1\\nand u,v \u2208 RN , |u \u00b7 v|\u2264\u2225 u\u2225p\u2225v\u2225q).\\n(d) Using the answers to the two previous questions and Taylor\u2019s formula, show\\nthat for all t ', '\u2265 1, \u03a6(Rt) \u2212 \u03a6(Rt\u22121) \u2264 (\u03b1 \u2212 1)\u2225rt\u22252\\n\u03b1,i f \u03b3Rt\u22121 +( 1\u2212 \u03b3)Rt \u0338\u2208 B\\nfor all \u03b3 \u2208 [0, 1].\\n(e) Suppose there exists \u03b3 \u2208 [0,1] such that (1\u2212 \u03b3)Rt\u22121 +\u03b3Rt \u2208 B. Show that\\n\u03a6(Rt) \u2264 (\u03b1 \u2212 1)\u2225rt\u22252\\n\u03b1.\\n(f) Using the tw', 'o previous questions, derive an upper bound on \u03a6( RT )e x -\\npressed in terms of T, N,a n dM.\\n(g) Show that \u03a6( RT )a d m i t sa sal o w e rb o u n dt h es q u a r eo ft h er e g r e tRT of\\nthe algorith', 'm.\\n(h) Using the two previous questions give an upper bound on the regret RT .\\nFor what value of\u03b1 is the bound the most favorable? Give a simple expression\\nof the upper bound on the regret for a suita', 'ble approximation of that optimal\\nvalue.\\n7.9 General inequality. In this exercise we generalize the result of exercise 7.7 by\\nusing a more general inequality: log(1 \u2212 x) \u2265\u2212 x \u2212\\nx2\\n\u03b1 for some 0 <\u03b1< 2.\\n', '(a) First prove that the inequality is true for x \u2208 [0, 1 \u2212 \u03b1\\n2 ]. What does this\\nimply about the valid range of \u03b2?\\n(b) Give a generalized version of the regret bound derived in exercise 7.7 in\\nterms ', 'of \u03b1,w h i c hs h o w s :\\nRT \u2264 log N\\n1 \u2212 \u03b2 + 1 \u2212 \u03b2\\n\u03b1 T.\\nWhat is the optimal choice of \u03b2 and the resulting bound in this case?7.7 Exercises 181\\n(c) Explain how \u03b1 may act as a regularization parameter. ', 'What is the optimal\\nchoice of \u03b1?\\n7.10 On-line to batch. Consider the margin loss (4.3), which is convex. Our goal is\\nto apply theorem 7.13 to the kernel Perceptron algorithm using the margin loss.\\n(a)', ' Show that the regret RT can be bounded as RT \u2264\\n\u221a\\nTr[K]/\u03c12 where \u03c1 is\\nthe margin and K is the kernel matrix associated to the sequence x1,...,x T .\\n(b) Apply theorem 7.13. How does this result compare', ' with the margin bounds\\nfor kernel-based hypotheses given by corollary 5.1?\\n7.11 On-line to batch \u2014 non-convex loss. The on-line to batch result of theorem 7.13\\nheavily relies on the fact that the los', 's in convex in order to provide a generalization\\nguarantee for the uniformly averaged hypothesis 1\\nT\\n\u2211T\\ni=1 hi. For general losses,\\ninstead of using the averaged hypothesis we will use a di\ufb00erent stra', 'tegy and try\\nto estimate the best single base hypothesis and show the expected loss of this\\nhypothesis is bounded.\\nLet m\\ni denote the number of errors of hypothesis hi makes on the points\\n(xi,...,x T ', '), i.e. the subset of points in the sequence that are not used to train\\nhi. Then we de\ufb01ne the penalized risk estimate of hypothesis hi as,\\nmi\\nT \u2212 i +1 + c\u03b4(T \u2212 i +1 ) w h e r ec\u03b4(x)=\\n\u221a\\n1\\n2x log T(T +1', ' )\\n\u03b4 .\\nThe term c\u03b4 penalizes the empirical error when the test sample is small. De\ufb01ne\\n\u02c6h = hi\u2217 where i\u2217=a r g m i ni mi/(T \u2212 i)+ c\u03b4(T \u2212 i+1). We will then show under the\\nsame conditions of theorem 7.1', '3 (withM = 1 for simplicity), but without requiring\\nthe convexity of L, that the following holds with probability at least 1 \u2212 \u03b4:\\nR(\u02c6h) \u2264 1\\nT\\nT\u2211\\ni=1\\nL(hi(xi),y i)+6\\n\u221a\\n1\\nT log 2(T +1 )\\n\u03b4 . (7.30)\\n(a) P', 'rove the following inequality:\\nmin\\ni\u2208[1,T]\\n(R(hi)+2 c\u03b4(T \u2212 i +1 ) )\u2264 1\\nT\\nT\u2211\\ni=1\\nR(hi)+4\\n\u221a\\n1\\nT log T +1\\n\u03b4 .182 On-Line Learning\\n(b) Use part (a) to show that with probability at least 1 \u2212 \u03b4,\\nmin\\ni\u2208[1,T', ']\\n(R(hi)+2 c\u03b4(T \u2212 i +1 ) )\\n<\\nT\u2211\\ni=1\\nL(hi(xi),y i)+\\n\u221a\\n2\\nT log 1\\n\u03b4 +4\\n\u221a\\n1\\nT log T +1\\n\u03b4 .\\n(c) By design, the de\ufb01nition of c\u03b4 ensures that with probability at least 1 \u2212 \u03b4\\nR(\u02c6h) \u2264 min\\ni\u2208[1,T]\\n(R(hi)+2 c\u03b4(T', ' \u2212 i +1 ) ).\\nUse this property to complete the proof of (7.30).8 Multi-Class Classi\ufb01cation\\nThe classi\ufb01cation problems we examined in the previous chapters were all binary.\\nHowever, in most real-world ', 'classi\ufb01cation problems the number of classes is greater\\nthan two. The problem may consist of assigning a topic to a text document, a\\ncategory to a speech utterance or a function to a biological sequen', 'ce. In all of these\\ntasks, the number of classes may be on the order of several hundred or more.\\nIn this chapter, we analyze the problem of multi-class classi\ufb01cation. We \ufb01rst in-\\ntroduce the multi-cla', 'ss classi\ufb01cation learning problem and discuss its multiple set-\\ntings, and then derive generalization bounds for it using the notion of Rademacher\\ncomplexity. Next, we describe and analyze a series of', ' algorithms for tackling the\\nmulti-class classi\ufb01cation problem. We will distinguish between two broad classes\\nof algorithms: uncombined algorithms that are speci\ufb01cally designed for the multi-\\nclass se', 'tting such as multi-class SVMs, decision trees, or multi-class boosting, and\\naggregated algorithms that are based on a reduction to binary classi\ufb01cation and re-\\nquire training multiple binary classi\ufb01e', 'rs. We will also brie\ufb02y discuss the problem of\\nstructured prediction, which is a related problem arising in a variety of applications.\\n8.1 Multi-class classi\ufb01cation problem\\nLet X denote the input spac', 'e and Y denote the output space, and let D be an\\nunknown distribution over X according to which input points are drawn. We will\\ndistinguish between two cases: themono-label case,w h e r eY is a \ufb01nite ', 'set of classes\\nthat we mark with numbers for convenience, Y = {1,...,k },a n dt h emulti-label\\ncase where Y = {\u22121, +1}k. In the mono-label case, each example is labeled with a\\nsingle class, while in t', 'he multi-label case it can be labeled with several. The latter\\ncan be illustrated by the case of text documents, which can be labeled with several\\ndi\ufb00erent relevant topics, e.g.,sports, business,a n d', 'society. The positive components\\nof a vector in {\u22121, +1}k indicate the classes associated with an example.\\nIn either case, the learner receives a labeled sampleS =\\n(\\n(x1,y1),..., (xm,y m)\\n\u23a1\\n\u2208\\n(X\u00d7 Y )m', ' with x1,...,x m drawn i.i.d. according to D,a n d yi = f(xi) for all\\ni \u2208 [1,m ], where f : X\u2192 Y is the target labeling function. Thus, we consider a184 Multi-Class Classi\ufb01cation\\ndeterministic scenari', 'o, which, as discussed in section 2.4.1, can be straightforwardly\\nextended to a stochastic one where we have a distribution over X\u00d7 Y .\\nGiven a hypothesis set H of functions mapping X to Y, the multi-', 'class classi\ufb01-\\ncation problem consists of using the labeled sample S to \ufb01nd a hypothesis h \u2208 H\\nwith small generalization error R(h) with respect to the target f:\\nR(h)= E\\nx\u223cD\\n[1h(x)\u0338=f(x)] mono-label c', 'ase (8.1)\\nR(h)= E\\nx\u223cD\\n[ k\u2211\\nl=1\\n1[h(x)]l \u0338=[f(x)]l\\n]\\nmulti-label case. (8.2)\\nThe notion of Hamming distance dH, that is, the number of corresponding compo-\\nnents in two vectors that di\ufb00er, can be used ', 'to give a common formulation for both\\nerrors:\\nR(h)= E\\nx\u223cD\\n[\\ndH(h(x),f (x))\\n]\\n. (8.3)\\nThe empirical error of h \u2208 H is denoted by \u02c6R(h) and de\ufb01ned by\\n\u02c6R(h)= 1\\nm\\nm\u2211\\ni=1\\ndH(h(xi),y i) . (8.4)\\nSeveral issu', 'es, both computational and learning-related, often arise in the multi-\\nclass setting. Computationally, dealing with a large number of classes can be\\nproblematic. The number of classes k directly enter', 's the time complexity of the\\nalgorithms we will present. Even for a relatively small number of classes such as\\nk = 100 or k =1 ,000, some techniques may become prohibitive to use in practice.\\nThis dep', 'endency is even more critical in the case where k is very large or even\\nin\ufb01nite as in the case of some structured prediction problems.\\nA learning-related issue that commonly appears in the multi-class', ' setting is the\\nexistence of unbalanced classes. Some classes may be represented by less than 5\\npercent of the labeled sample, while others may dominate a very large fraction\\nof the data. When separat', 'e binary classi\ufb01ers are used to de\ufb01ne the multi-class\\nsolution, we may need to train a classi\ufb01er distinguishing between two classes with\\nonly a small representation in the training sample. This implie', 's training on a small\\nsample, with poor performance guarantees. Alternatively, when a large fraction\\nof the training instances belong to one class, it may be tempting to propose a\\nhypothesis always re', 'turning that class, since its generalization error as de\ufb01ned\\nearlier is likely to be relatively low. However, this trivial solution is typically not the\\none intended. Instead, the loss function may ne', 'ed to be reformulated by assigning\\ndi\ufb00erent misclassi\ufb01cation weights to each pair of classes.\\nAnother learning-related issue is the relationship between classes, which can8.2 Generalization bounds 185', '\\nbe hierarchical. For example, in the case of document classi\ufb01cation, the error of\\nmisclassifying a document dealing with world politics as one dealing with real\\nestate should naturally be penalized m', 'ore than the error of labeling a document\\nwith sports instead of the more speci\ufb01c label baseball. Thus, a more complex and\\nmore useful multi-class classi\ufb01cation formulation would take into considerati', 'on the\\nhierarchical relationships between classes and de\ufb01ne the loss function in accordance\\nwith this hierarchy. More generally, there may be a graph relationship between\\nclasses as in the case of the', ' GO ontology in computational biology. The use of\\nhierarchical relationships between classes leads to a richer and more complex multi-\\nclass classi\ufb01cation problem.\\n8.2 Generalization bounds\\nIn this se', 'ction, we present margin-based generalization bounds for multi-class\\nclassi\ufb01cation in the mono-label case. In the binary setting, classi\ufb01ers are often\\nde\ufb01ned based on the sign of a scoring function. I', 'n the multi-class setting, a\\nhypothesis is de\ufb01ned based on a scoring functionh: X\u00d7 Y \u2192 R.T h el a b e la s s o c i a t e d\\nto point x is the one resulting in the largest scoreh(x, y), which de\ufb01nes the', ' following\\nmapping from X to Y:\\nx \u21a6\u2192 argmax\\ny\u2208Y\\nh(x, y).\\nThis naturally leads to the following de\ufb01nition of themargin \u03c1h(x, y) of the function\\nh at a labeled example (x, y):\\n\u03c1h(x, y)= h(x, y) \u2212 max\\ny\u2032', ' \u0338=y\\nh(x, y\u2032).\\nThus, h misclassi\ufb01es (x, y)i \ufb00\u03c1h(x, y) \u2264 0. For any\u03c1> 0, we can de\ufb01ne theempirical\\nmargin loss of a hypothesis h for multi-class classi\ufb01cation as\\n\u02c6R\u03c1(h)= 1\\nm\\nm\u2211\\ni=1\\n\u03a6\u03c1(\u03c1h(xi,y i)), (8.5', ')\\nwhere \u03a6\u03c1 is the margin loss function (de\ufb01nition 4.3). Thus, the empirical margin\\nloss for multi-class classi\ufb01cation is upper bounded by the fraction of the training\\npoints misclassi\ufb01ed by h or corre', 'ctly classi\ufb01ed but with con\ufb01dence less than or equal\\nto \u03c1:\\n\u02c6R\\n\u03c1(h) \u2264 1\\nm\\nm\u2211\\ni=1\\n1\u03c1h(xi,yi)\u2264\u03c1. (8.6)186 Multi-Class Classi\ufb01cation\\nThe following lemma will be used in the proof of the main result of thi', 's section.\\nLemma 8.1\\nLet F1,..., Fl be l hypothesis sets in RX , l \u2265 1,a n dl e tG = {max{h1,...,h l}: hi \u2208\\nFi,i \u2208 [1,l ]}. Then, for any sample S of size m, the empirical Rademacher\\ncomplexity of G c', 'an be upper bounded as follows:\\n\u02c6RS(G) \u2264\\nl\u2211\\nj=1\\n\u02c6RS(Fj). (8.7)\\nProof Let S =( x1,...,x m) be a sample of size m. We \ufb01rst prove the result in\\nthe case l = 2. By de\ufb01nition of the max operator, for any h', '1 \u2208F 1 and h2 \u2208F 2,\\nmax{h1,h2} = 1\\n2[h1 + h2 + |h1 \u2212 h2|].\\nThus, we can write:\\n\u02c6RS(G)= 1\\nm E\\n\u03c3\\n[\\nsup\\nh1\u2208F1\\nh2\u2208F2\\nm\u2211\\ni=1\\n\u03c3i max{h1(xi),h2(xi)}\\n]\\n= 1\\n2m E\\n\u03c3\\n[\\nsup\\nh1\u2208F1\\nh2\u2208F2\\nm\u2211\\ni=1\\n\u03c3i\\n(\\nh1(xi)+ h2(xi)+', ' |(h1 \u2212 h2)(xi)|\\n\u23a1]\\n\u2264 1\\n2\\n\u02c6RS(F1)+ 1\\n2\\n\u02c6RS(F2)+ 1\\n2m E\\n\u03c3\\n[\\nsup\\nh1\u2208F1\\nh2\u2208F2\\nm\u2211\\ni=1\\n\u03c3i|(h1 \u2212 h2)(xi)|\\n]\\n, (8.8)\\nusing the sub-additivity of sup. Since x \u21a6\u2192| x| is 1-Lipschitz, by Talagrand\u2019s lemma\\n(lemm', 'a 4.2), the last term can be bounded as follows\\n1\\n2m E\\n\u03c3\\n[\\nsup\\nh1\u2208F1\\nh2\u2208F2\\nm\u2211\\ni=1\\n\u03c3i|(h1 \u2212 h2)(xi)|\\n]\\n\u2264 1\\n2m E\\n\u03c3\\n[\\nsup\\nh1\u2208F1\\nh2\u2208F2\\nm\u2211\\ni=1\\n\u03c3i(h1 \u2212 h2)(xi)\\n]\\n\u2264 1\\n2\\n\u02c6RS(F1)+ 1\\n2m E\\n\u03c3\\n[\\nsup\\nh2\u2208F2\\nm\u2211\\ni=1\\n\u2212', '\u03c3ih2(xi)\\n]\\n= 1\\n2\\n\u02c6RS(F1)+ 1\\n2\\n\u02c6RS(F2), (8.9)\\nwhere we again use the sub-additivity of sup for the second inequality and the fact\\nthat \u03c3i and \u2212\u03c3i have the same distribution for any i \u2208 [1,m] for the la', 'st equality.\\nCombining (8.8) and (8.9) yields \u02c6RS(G) \u2264 \u02c6RS(F1)+ \u02c6RS(F2). The general case can\\nbe derived from the case l =2u s i n gm a x{h1,...,h l} =m a x{h1, max{h2,...,h l}}\\nand an immediate recur', 'rence.8.2 Generalization bounds 187\\nFor any family of hypotheses mapping X\u00d7 Y to R,w ed e \ufb01 n e\u03a01(H)b y\\n\u03a01(H)= {x \u21a6\u2192 h(x, y): y \u2208Y ,h \u2208 H}.\\nThe following theorem gives a general margin bound for multi', '-class classi\ufb01cation.\\nTheorem 8.1 Margin bound for multi-class classi\ufb01cation\\nLet H \u2286 RX\u00d7 Y be a hypothesis set with Y = {1,...,k }.F i x \u03c1> 0.T h e n ,f o r\\nany \u03b4> 0, with probability at least 1 \u2212 \u03b4, ', 'the following multi-class classi\ufb01cation\\ngeneralization bound holds for all h \u2208 H:\\nR(h) \u2264 \u02c6R\u03c1(h)+ 2k2\\n\u03c1 Rm(\u03a01(H)) +\\n\u221a\\nlog 1\\n\u03b4\\n2m . (8.10)\\nProof The \ufb01rst part of the proof is similar to that of theorem ', '4.4. Let \u02dcH be\\nthe family of hypotheses mapping X\u00d7 Y to R de\ufb01ned by \u02dcH = {z =( x, y) \u21a6\u2192\\n\u03c1h(x, y): h \u2208 H}. Consider the family of functions \u02dcH = {\u03a6\u03c1 \u25e6r: r \u2208 \u02dcH} derived\\nfrom \u02dcH, which take values in [0', ', 1]. By theorem 3.1, with probability at least 1\u2212 \u03b4,\\nfor all h \u2208 H,\\nE\\n[\\n\u03a6\u03c1(\u03c1h(x, y))\\n]\\n\u2264 \u02c6R\u03c1(h)+2 Rm\\n(\\n\u03a6\u03c1 \u25e6 \u02dcH\\n\u23a1\\n+\\n\u221a\\nlog 1\\n\u03b4\\n2m .\\nSince 1u\u22640 \u2264 \u03a6\u03c1(u) for all u \u2208 R, the generalization error R(h) is a ', 'lower bound on\\nthe left-hand side, R(h)=E [ 1y[h(x\u2032)\u2212h(x)]\u22640] \u2264 E\\n[\\n\u03a6\u03c1(\u03c1h(x, y))\\n]\\n,a n dw ec a nw r i t e :\\nR(h) \u2264 \u02c6R\u03c1(h)+2 Rm\\n(\\n\u03a6\u03c1 \u25e6 \u02dcH\\n\u23a1\\n+\\n\u221a\\nlog 1\\n\u03b4\\n2m .\\nAs in the proof of theorem 4.4, we can show', ' that Rm\\n(\\n\u03a6\u03c1 \u25e6 \u02dcH\\n\u23a1\\n\u2264 1\\n\u03c1Rm( \u02dcH)u s i n g188 Multi-Class Classi\ufb01cation\\nthe (1/\u03c1)-Lipschitzness of \u03a6\u03c1. Here, Rm( \u02dcH) can be upper bounded as follows:\\nRm( \u02dcH)= 1\\nm E\\nS,\u03c3\\n[\\nsup\\nh\u2208H\\nm\u2211\\ni=1\\n\u03c3i\u03c1h(xi,y i)\\n]', '\\n= 1\\nm E\\nS,\u03c3\\n[\\nsup\\nh\u2208H\\nm\u2211\\ni=1\\n\u2211\\ny\u2208Y\\n\u03c3i\u03c1h(xi,y )1y=yi\\n]\\n\u2264 1\\nm\\n\u2211\\ny\u2208Y\\nE\\nS,\u03c3\\n[\\nsup\\nh\u2208H\\nm\u2211\\ni=1\\n\u03c3i\u03c1h(xi,y )1y=yi\\n]\\n(sub-additivity of sup)\\n= 1\\nm\\n\u2211\\ny\u2208Y\\nE\\nS,\u03c3\\n[\\nsup\\nh\u2208H\\nm\u2211\\ni=1\\n\u03c3i\u03c1h(xi,y )\\n(2(1y=yi )\u22121\\n2 + 1\\n2', '\\n\u23a1]\\n\u2264 1\\n2m\\n\u2211\\ny\u2208Y\\nE\\nS,\u03c3\\n[\\nsup\\nh\u2208H\\nm\u2211\\ni=1\\n\u03c3i\u03f5i\u03c1h(xi,y )\\n]\\n+\\n(\\n\u03f5i =2 ( 1y=yi ) \u2212 1\\n\u23a1\\n1\\n2m\\n\u2211\\ny\u2208Y\\nE\\nS,\u03c3\\n[\\nsup\\nh\u2208H\\nm\u2211\\ni=1\\n\u03c3i\u03c1h(xi,y )\\n]\\n(sub-additivity of sup)\\n= 1\\nm\\n\u2211\\ny\u2208Y\\nE\\nS,\u03c3\\n[\\nsup\\nh\u2208H\\nm\u2211\\ni=1\\n\u03c3i\u03c1h(xi,y )', '\\n]\\n,\\nwhere by de\ufb01nition \u03f5i \u2208{ \u2212 1, +1} and we use the fact that \u03c3i and \u03c3i\u03f5i have the\\nsame distribution.\\nLet \u03a01(H)(k\u22121) = {max{h1,...,h l}: hi \u2208 \u03a01(H),i \u2208 [1,k \u2212 1]}.N o w ,r e w r i t i n g\\n\u03c1h(xi,y ) ', 'explicitly, using again the sub-additivity of sup, observing that \u2212\u03c3i and\\n\u03c3i are distributed in the same way, and using lemma 8.1 leads to\\nRm( \u02dcH) \u2264 1\\nm\\n\u2211\\ny\u2208Y\\nE\\nS,\u03c3\\n[\\nsup\\nh\u2208H\\nm\u2211\\ni=1\\n\u03c3i\\n(\\nh(xi,y ) \u2212 ma', 'x\\ny\u2032 \u0338=y\\nh(xi,y \u2032)\\n\u23a1]\\n\u2264\\n\u2211\\ny\u2208Y\\n[\\n1\\nm E\\nS,\u03c3\\n[\\nsup\\nh\u2208H\\nm\u2211\\ni=1\\n\u03c3ih(xi,y )\\n]\\n+ 1\\nm E\\nS,\u03c3\\n[\\nsup\\nh\u2208H\\nm\u2211\\ni=1\\n\u2212\u03c3i max\\ny\u2032 \u0338=y\\nh(xi,y \u2032)\\n]]\\n=\\n\u2211\\ny\u2208Y\\n[\\n1\\nm E\\nS,\u03c3\\n[\\nsup\\nh\u2208H\\nm\u2211\\ni=1\\n\u03c3ih(xi,y )\\n]\\n+ 1\\nm E\\nS,\u03c3\\n[\\nsup\\nh\u2208H', '\\nm\u2211\\ni=1\\n\u03c3i max\\ny\u2032 \u0338=y\\nh(xi,y \u2032)\\n]]\\n\u2264\\n\u2211\\ny\u2208Y\\n[\\n1\\nm E\\nS,\u03c3\\n[\\nsup\\nh\u2208\u03a01(H)\\nm\u2211\\ni=1\\n\u03c3ih(xi)\\n]\\n+ 1\\nm E\\nS,\u03c3\\n[\\nsup\\nh\u2208\u03a01(H)(k\u2212 1)\\nm\u2211\\ni=1\\n\u03c3ih(xi)\\n]]\\n\u2264 k\\n[\\nk\\nm E\\nS,\u03c3\\n[\\nsup\\nh\u2208\u03a01(H)\\nm\u2211\\ni=1\\n\u03c3ih(xi)\\n]]\\n= k2Rm(\u03a01(H)).\\nT', 'his concludes the proof.8.2 Generalization bounds 189\\nThese bounds can be generalized to hold uniformly for all \u03c1> 0a tt h ec o s to f\\nan additional term\\n\u221a\\n(log log2(2/\u03c1))/m, as in theorem 4.5 and exe', 'rcise 4.2. As for\\nother margin bounds presented in previous sections, they show the con\ufb02ict between\\ntwo terms: the larger the desired pairwise ranking margin\u03c1, the smaller the middle\\nterm, at the pric', 'e of a larger empirical multi-class classi\ufb01cation margin loss\u02c6R\\n\u03c1. Note,\\nhowever, that here there is additionally a quadratic dependency on the number of\\nclasses k.T h i ss u g g e s t sw e a k e rg u', ' a r a n t e e sw h e nl e a r n i n gw i t hal a r g en u m b e ro f\\nclasses or the need for even larger margins \u03c1 for which the empirical margin loss\\nwould be small.\\nFor some hypothesis sets, a simp', 'le upper bound can be derived for the\\nRademacher complexity of \u03a01(H), thereby making theorem 8.1 more explicit. We\\nwill show this for kernel-based hypotheses. LetK : X\u00d7 X \u2192 R be a PDS kernel and\\nlet \u03a6', ' : X\u2192 H be a feature mapping associated to K. In multi-class classi\ufb01cation, a\\nkernel-based hypothesis is based on k weight vectors w1,..., wk \u2208 H. Each weight\\nvector wl, l \u2208 [1,k ], de\ufb01nes a scoring f', 'unctionx \u21a6\u2192 wl \u00b7\u03a6(x) and the class associated\\nto point x \u2208X is given by\\nargmax\\ny\u2208Y\\nwy \u00b7 \u03a6(x).\\nWe denote byW the matrix formed by these weight vectors:W =( w\u22a4\\n1 ,..., w\u22a4\\nk )\u22a4\\nand for any p \u2265 1 denote b', 'y \u2225W\u2225H,p the LH,p group norm of W de\ufb01ned by\\n\u2225W\u2225H,p =\\n( k\u2211\\nl=1\\n\u2225wl\u2225p\\nH\\n\u23a11/p\\n.\\nFor any p \u2265 1, the family of kernel-based hypotheses we will consider is1\\nHK,p = {(x, y) \u2208X\u00d7{ 1,...,k } \u21a6\u2192 wy \u00b7 \u03a6(x): W =( ', 'w1,..., wk)\u22a4, \u2225W\u2225H,p \u2264 \u039b}.\\nProposition 8.1 Rademacher complexity of multi-class kernel-based hy-\\npotheses\\nLet K : X\u00d7 X \u2192 R be a PDS kernel and let \u03a6 : X\u2192 H be a feature mapping\\nassociated to K. Assume', ' that there exists r> 0 such that K(x, x) \u2264 r\\n2 for all\\nx \u2208X .T h e n ,f o ra n ym \u2265 1, Rm(\u03a01(HK,p)) can be bounded as follows:\\nRm(\u03a01(HK,p)) \u2264\\n\u221a\\nr2\u039b2\\nm .\\nProof Let S =( x1,...,x m) denote a sample of ', 'size m. Observe that for all\\n1. The hypothesis set H can also be de\ufb01ned via H = {h \u2208 RX\u00d7 Y : h(\u00b7,y ) \u2208 H \u2227\u2225 h\u2225K,p \u2264\\n\u039b}, where \u2225h\u2225K,p =\\n` Pk\\ny=1 \u2225h(\u00b7,y )\u2225p\\nH\\n\u00b41/p\\n, without referring to a feature mappi', 'ng for K.190 Multi-Class Classi\ufb01cation\\nl \u2208 [1,k ], the inequality \u2225wl\u2225H \u2264\\n(\u2211k\\nl=1 \u2225wl\u2225p\\nH\\n\u23a11/p\\n= \u2225W\u2225H,p holds. Thus, the\\ncondition \u2225W\u2225H,p \u2264 \u039bi m p l i e st h a t\u2225wl\u2225H \u2264 \u039b for all l \u2208 [1,k ]. In view o', 'f that,\\nthe Rademacher complexity of the hypothesis set \u03a0 1(HK,p) can be expressed and\\nbounded as follows:\\n\u02c6RS(\u03a01(HK,p)) = 1\\nm E\\nS,\u03c3\\n[\\nsup\\ny\u2208Y\\n\u2225W\u2225\u2264\u039b\\n\u28e8\\nwy,\\nm\u2211\\ni=1\\n\u03c3i\u03a6(xi)\\n\u27e9]\\n\u2264 1\\nm E\\nS,\u03c3\\n[\\nsup\\ny\u2208Y\\n\u2225W\u2225\u2264\u039b', '\\n\u2225wy \u2225H\\n\\ued79\\ued79\\ued79\\nm\u2211\\ni=1\\n\u03c3i\u03a6(xi)\\n\\ued79\\ued79\\ued79\\nH\\n]\\n(Cauchy-Schwarz ineq. )\\n\u2264 \u039b\\nm E\\nS,\u03c3\\n[\\ued79\\ued79\\ued79\\nm\u2211\\ni=1\\n\u03c3i\u03a6(xi)\\n\\ued79\\ued79\\ued79\\nH\\n]\\n\u2264 \u039b\\nm\\n[\\nE\\nS,\u03c3\\n[\\ued79\\ued79\\n\\ued79\\nm\u2211\\ni=1\\n\u03c3i\u03a6(xi)\\n\\ued79\\ued79\\n\\ued79\\n2\\nH\\n]]1/2\\n(Jensen\u2019s inequality)\\n= \u039b\\nm\\n[\\nE\\nS,\u03c3\\n[ m\u2211\\ni=1\\n\u2225\u03a6(xi)', '\u22252\\nH\\n]]1/2\\n(i \u0338= j \u21d2 E\\n\u03c3\\n[\u03c3i\u03c3j]=0 )\\n= \u039b\\nm\\n[\\nE\\nS,\u03c3\\n[ m\u2211\\ni=1\\nK(xi,xi)\\n]]1/2\\n\u2264 \u039b\\n\u221a\\nmr2\\nm =\\n\u221a\\nr2\u039b2\\nm ,\\nwhich concludes the proof.\\nCombining theorem 8.1 and proposition 8.1 yields directly the following re', 'sult.\\nCorollary 8.1 Margin bound for multi-class classi\ufb01cation with kernel-\\nbased hypotheses\\nLet K : X\u00d7 X \u2192 R be a PDS kernel and let \u03a6 : X\u2192 H be a feature mapping\\nassociated to K. Assume that there e', 'xists r> 0 such that K(x, x) \u2264 r\\n2 for all\\nx \u2208X .F i x\u03c1> 0.T h e n ,f o ra n y\u03b4> 0, with probability at least1 \u2212 \u03b4, the following\\nmulti-class classi\ufb01cation generalization bound holds for all h \u2208 HK,p:', '\\nR(h) \u2264 \u02c6R\u03c1(h)+2 k2\\n\u221a\\nr2\u039b2/\u03c12\\nm +\\n\u221a\\nlog 1\\n\u03b4\\n2m . (8.11)\\nIn the next two sections, we describe multi-class classi\ufb01cation algorithms that\\nbelong to two distinct families:uncombined algorithms, which are', ' de\ufb01ned by a single\\noptimization problem, and aggregated algorithms, which are obtained by training\\nmultiple binary classi\ufb01cations and by combining their outputs.8.3 Uncombined multi-class algorithms ', '191\\n8.3 Uncombined multi-class algorithms\\nIn this section, we describe three algorithms designed speci\ufb01cally for multi-class\\nclassi\ufb01cation. We start with a multi-class version of SVMs, then describe a', ' boosting-\\ntype multi-class algorithm, and conclude with decision trees, which are often used\\nas base learners in boosting.\\n8.3.1 Multi-class SVMs\\nWe describe an algorithm that can be derived directly', ' from the theoretical guar-\\nantees presented in the previous section. Proceeding as in section 4.4 for classi\ufb01-\\ncation, the guarantee of corollary 8.1 can be expressed as follows: for any \u03b4> 0,\\nwith p', 'robability at least 1 \u2212 \u03b4, for all h \u2208 H\\nK,2 = {(x, y) \u2192 wy \u00b7 \u03a6(x): W =\\n(w1,..., wk)\u22a4, \u2211k\\nl=1 \u2225wl\u22252 \u2264 \u039b2},\\nR(h) \u2264 1\\nm\\nm\u2211\\ni=1\\n\u03bei +4 k2\\n\u221a\\nr2\u039b2\\nm +\\n\u221a\\nlog 1\\n\u03b4\\n2m , (8.12)\\nwhere \u03bei =m a x\\n(\\n1 \u2212 [wyi \u00b7 \u03a6(xi', ') \u2212 maxy\u2032 \u0338=yi wy\u2032 \u00b7 \u03a6(xi)], 0\\n\u23a1\\nfor all i \u2208 [1,m].\\nAn algorithm based on this theoretical guarantee consists of minimizing the\\nright-hand side of (8.12), that is, minimizing an objective function wit', 'h a term\\ncorresponding to the sum of the slack variables \u03be\\ni, and another one minimizing\\n\u2225W\u2225H,2 or equivalently \u2211k\\nl=1 \u2225wl\u22252.T h i si sp r e c i s e l yt h eo p t i m i z a t i o np r o b l e m\\nde\ufb01nin', 'g the multi-class SVM algorithm:\\nmin\\nW,\u03be\\n1\\n2\\nk\u2211\\nl=1\\n\u2225wl\u22252 + C\\nm\u2211\\ni=1\\n\u03bei\\nsubject to: \u2200i \u2208 [1,m], \u2200l \u2208Y\u2212{ yi},\\nwyi \u00b7 \u03a6(xi) \u2265 wl \u00b7 \u03a6(xi)+1 \u2212 \u03bei.\\nThe decision function learned is of the form x \u21a6\u2192 argmaxl\u2208', 'Y wl \u00b7 \u03a6(x). As with\\nthe primal problem of SVMs, this is a convex optimization problem: the objective\\nfunction is convex, since it is a sum of convex functions, and the constraints are\\na\ufb03ne and thus q', 'uali\ufb01ed. The objective and constraint functions are di\ufb00erentiable,\\nand the KKT conditions hold at the optimum. De\ufb01ning the Lagrangian and applying\\nthese conditions leads to the equivalent dual optimiz', 'ation problem, which can be192 Multi-Class Classi\ufb01cation\\nexpressed in terms of the kernel function K alone:\\nmax\\n\u03b1 \u2208Rm\u00d7k\\nm\u2211\\ni=1\\n\u03b1i \u00b7 eyi \u2212 1\\n2\\nm\u2211\\ni=1\\n(\u03b1i \u00b7 \u03b1j)K(xi,xj)\\nsubject to: 0 \u2264 \u03b1i \u2264 C \u2227 \u03b1i \u00b7 1 =', '0 , \u2200i \u2208 [1,m].\\nHere, \u03b1 \u2208 Rm\u00d7k is a matrix, \u03b1i denotes the ith row of \u03b1,a n del the lth unit vector\\nin Rk, l \u2208 [1,k ]. Both the primal and dual problems are simple QPs generalizing\\nthose of the standa', 'rd SVM algorithm. However, the size of the solution and the\\nnumber of constraints for both problems is in \u03a9(mk), which, for a large number of\\nclasses k, can make it di\ufb03cult to solve. However, there ex', 'ist speci\ufb01c optimization\\nsolutions designed for this problem based on a decomposition of the problem into\\nm disjoint sets of constraints.\\n8.3.2 Multi-class boosting algorithms\\nWe describe a boosting a', 'lgorithm for multi-class classi\ufb01cation calledAdaBoost.MH,\\nwhich in fact coincides with a special instance of AdaBoost. An alternative multi-\\nclass classi\ufb01cation algorithm based on similar boosting ide', 'as, AdaBoost.MR, is\\ndescribed and analyzed in exercise 9.5. AdaBoost.MH applies to the multi-label\\nsetting whereY = {\u22121, +1}\\nk. As in the binary case, it returns a convex combination\\nof base classi\ufb01er', 's selected from a hypothesis setH.L e tF be the following objective\\nfunction de\ufb01ned for all samples S =( ( x1,y1),..., (xm,y m)) \u2208 (X\u00d7 Y )m and\\n\u03b1 =( \u03b11,...,\u03b1 n) \u2208 Rn, n \u2265 1, by\\nF(\u03b1)=\\nm\u2211\\ni=1\\nk\u2211\\nl=1\\ne\u2212y', 'i[l]gn(xi,l) =\\nm\u2211\\ni=1\\nk\u2211\\nl=1\\ne\u2212yi[l] Pn\\nt=1 \u03b1tht(xi,l), (8.13)\\nwhere gn = \u2211n\\nt=1 \u03b1tht and where yi[l] denotes the lth coordinate of yi for any\\ni \u2208 [1,m]a n d l \u2208 [1,k ]. F is a convex and di\ufb00erentiabl', 'e upper bound on the\\nmulti-class multi-label loss:\\nm\u2211\\ni=1\\nk\u2211\\nl=1\\n1yi[l]\u0338=gn(xi,l) \u2264\\nm\u2211\\ni=1\\nk\u2211\\nl=1\\ne\u2212yi[l]gn(xi,l), (8.14)\\nsince for any x \u2208X with label y = f(x) and any l \u2208 [1,k ], the inequality\\n1y[l', ']\u0338=gn(x,l) \u2264 e\u2212y[l]gn(x,l) holds. AdaBoost.MH coincides exactly with the appli-\\ncation of coordinate descent to the objective function F.F i g u r e8 . 1g i v e st h e\\npseudocode of the algorithm in t', 'he case where the base classi\ufb01ers are functions\\nmapping from X\u00d7 Y to {\u22121, +1}. The algorithm takes as input a labeled sam-\\nple S =( (x\\n1,y1),..., (xm,y m)) \u2208 (X\u00d7 Y )m and maintains a distribution Dt o', 'ver\\n{1,...,m }\u00d7 Y . The remaining details of the algorithm are similar to AdaBoost. In8.3 Uncombined multi-class algorithms 193\\nAdaBoost.MH(S =( (x1,y1),..., (xm,y m)))\\n1 for i \u2190 1 to m do\\n2 for l \u2190 1', ' to k do\\n3 D1(i, l) \u2190 1\\nmk\\n4 for t \u2190 1 to T do\\n5 ht \u2190 base classi\ufb01er in H with small error \u03f5t =P r(i,l)\u223cDt [ht(xi,l ) \u0338= yi[l]]\\n6 \u03b1t \u2190 1\\n2 log 1\u2212\u03f5t\\n\u03f5t\\n7 Zt \u2190 2[\u03f5t(1 \u2212 \u03f5t)]\\n1\\n2 \u22bf normalization factor\\n8', ' for i \u2190 1 to m do\\n9 for l \u2190 1 to k do\\n10 Dt+1(i, l) \u2190 Dt(i,l)e x p (\u2212\u03b1tyi[l]ht(xi,l))\\nZt\\n11 g \u2190 \u2211T\\nt=1 \u03b1tht\\n12 return h = sgn(g)\\nFigure 8.1 AdaBoost.MH algorithm, for H \u2286 ({\u22121, +1}k)X\u00d7 Y.\\nfact, AdaBo', 'ost.MH exactly coincides with AdaBoost applied to the training sam-\\nple derived from S by splitting each labeled point (xi,y i)i n t ok labeled examples\\n((xi,l ),y i[l]), with each example (xi,l )i n ', 'X\u00d7 Y and its label in {\u22121, +1}:\\n(xi,y i) \u2192 ((xi, 1),y i[1]),..., ((xi,k ),y i[k]),i \u2208 [1,m].\\nLet S\u2032 denote the resulting sample, then S\u2032 =( (x1,1),y1[1]),..., (xm,k ),y m[k])).\\nS\u2032 contains mk examples', ' and the expression of the objective function F in (8.13)\\ncoincides exactly with that of the objective function of AdaBoost for the sampleS\u2032.\\nIn view of this connection, the theoretical analysis along', ' with the other observations\\nwe presented for AdaBoost in chapter 6 also apply here. Hence, we will focus on\\naspects related to the computational e\ufb03ciency and to the weak learning condition\\nthat are s', 'peci\ufb01c to the multi-class scenario.\\nThe complexity of the algorithm is that of AdaBoost applied to a sample of\\nsize mk.F o rX\u2286 RN , using boosting stumps as base classi\ufb01ers, the complexity of\\nthe algo', 'rithm is therefore in O((mk) log(mk)+ mkNT ). Thus, for a large number\\nof classes k, the algorithm may become impractical using a single processor. The\\nweak learning condition for the application of A', 'daBoost in this scenario requires\\nthat at each round there exists a base classi\ufb01er h\\nt : X\u00d7 Y\u2192{ \u2212 1,+1} such that\\nPr(i,l)\u223cDt [ht(xi,l ) \u0338= yi[l]] < 1/2. This may be hard to achieve if classes are clos', 'e194 Multi-Class Classi\ufb01cation\\nX1 < a1\\nX1 < a2 X2 < a3\\nX2 < a4 R3 R4 R5\\nR1 R2\\nX1\\nX2\\na4\\na2 a1\\na3\\nR2\\nR1\\nR3\\nR5\\nR4\\nFigure 8.2 Left: example of a decision tree with numerical questions based on two\\nvariabl', 'es X1 and X2. Here, each leaf is marked with the region it de\ufb01nes. The class\\nlabeling for a leaf is obtained via majority vote based on the training points falling\\nin the region it de\ufb01nes. Right: Part', 'ition of the two-dimensional space induced by\\nthat decision tree.\\nand it is di\ufb03cult to distinguish between them. It is also more di\ufb03cult in this context\\nto come up with \u201crules of thumb\u201d h\\nt de\ufb01ned ove', 'r X\u00d7 Y .\\n8.3.3 Decision trees\\nWe present and discuss the general learning method of decision trees that can\\nbe used in multi-class classi\ufb01cation, but also in other learning problems such as\\nregression', ' (chapter 10) and clustering. Although the empirical performance of\\ndecision trees often is not state-of-the-art, decision trees can be used as weak learners\\nwith boosting to de\ufb01ne e\ufb00ective learning a', 'lgorithms. Decision trees are also typically\\nfast to train and evaluate and relatively easy to interpret.\\nDe\ufb01nition 8.1 Binary decision tree\\nA binary decision tree is a tree representation of a partit', 'ion of the feature space.\\nFigure 8.2 shows a simple example in the case of a two-dimensional space based\\non two features X\\n1 and X2, as well as the partition it represents. Each interior\\nnode of a dec', 'ision tree corresponds to a question related to features. It can be a\\nnumerical question of the form X\\ni \u2264 a for a feature variable Xi, i \u2208 [1,N ],a n d\\nsome threshold a \u2208 R, as in the example of \ufb01gur', 'e 8.2, or a categorical question\\nsuch as Xi \u2208{ blue,white,red},w h e nf e a t u r eXi takes a categorical value such as a\\ncolor. Each leaf is labeled with a labell \u2208Y .\\nDecision trees can be de\ufb01ned us', 'ing more complex node questions, resulting in\\npartitions based on more complex decision surfaces. For example, binary space8.3 Uncombined multi-class algorithms 195\\nGreedyDecisionTrees(S =( (x1,y1),..', '., (xm,y m)))\\n1 tree \u2190{ n0} \u22bf root node.\\n2 for t \u2190 1 to T do\\n3( nt, qt) \u2190 argmin(n,q) \u02dcF(n, q)\\n4 Split(tree, nt, qt)\\n5 return tree\\nFigure 8.3 Greedy algorithm for building a decision tree from a label', 'ed sampleS.\\nThe procedure Split(tree, nt, qt) splits node nt by making it an internal node with\\nquestion qt and leaf children n\u2212 (n, q) and n+(n, q), each labeled with the dominating\\nclass of the regi', 'on it de\ufb01nes, with ties broken arbitrarily.\\npartition (BSP) trees partition the space with convex polyhedral regions, based\\non questions of the form \u2211n\\ni=1 \u03b1iXi \u2264 a,a n d sphere trees partition with p', 'ieces\\nof spheres based on questions of the form \u2225X \u2212 a0\u2225\u2264 a,w h e r eX is a feature\\nvector, a0 a \ufb01xed vector, and a is a \ufb01xed positive real number. More complex\\ntree questions lead to richer partition', 's and thus hypothesis sets, which can cause\\nover\ufb01tting in the absence of a su\ufb03ciently large training sample. They also increase\\nthe computational complexity of prediction and training. Decision trees ', 'can also\\nbe generalized to branching factors greater than two, but binary trees are most\\ncommonly used due to computational considerations.\\nPrediction/partitioning: To predict the label of any point x', ' \u2208X we start\\nat the root node of the decision tree and go down the tree until a leaf is found,\\nby moving to the right child of a node when the response to the node question is\\npositive, and to the lef', 't child otherwise. When we reach a leaf, we associatex with\\nthe label of this leaf.\\nT h u s ,e a c hl e a fd e \ufb01 n e saregion of X formed by the set of points corresponding\\nexactly to the same node re', 'sponses and thus the same traversal of the tree. By\\nde\ufb01nition, no two regions intersect and all points belong to exactly one region.\\nThus, leaf regions de\ufb01ne a partition of X, as shown in the example ', 'of \ufb01gure 8.2. In\\nmulti-class classi\ufb01cation, the label of a leaf is determined using the training sample:\\nthe class with the majority representation among the training points falling in a\\nleaf region d', 'e\ufb01nes the label of that leaf, with ties broken arbitrarily.\\nLearning: We will discuss two di\ufb00erent methods for learning a decision tree\\nusing a labeled sample. The \ufb01rst method is a greedy technique. T', 'his is motivated\\nby the fact that the general problem of \ufb01nding a decision tree with the smallest\\nerror is NP-hard. The method consists of starting with a tree reduced to a single196 Multi-Class Class', 'i\ufb01cation\\n(root) node, which is a leaf whose label is the class that has majority over the\\nentire sample. Next, at each round, a node nt is split based on some question\\nqt. The pair ( nt, qt)i sc h o s', ' e ns ot h a tt h enode impurity is maximally decreased\\naccording to some measure of impurity F. We denote by F(n) the impurity of n.\\nThe decrease in node impurity after a split of noden based on ques', 'tion q is de\ufb01ned\\nas follows. Let n+(n, q) denote the right child of n after the split, n\u2212 (q, n)t h e\\nleft child, and \u03b7(n, q) the fraction of the points in the region de\ufb01ned by n that are\\nmoved to n\u2212 ', '(n, q). The total impurity of the leavesn\u2212 (n, q)a n dn+(n, q) is therefore\\n\u03b7(n, q)F(n\u2212 (n, q))+(1 \u2212 \u03b7(n, q))F(n+(n, q)). Thus, the decrease in impurity \u02dcF(n, q)\\nby that split is given by\\n\u02dcF(n, q)= F(', 'n) \u2212 [\u03b7(n, q)F(n\u2212 (n, q)) + (1\u2212 \u03b7(n, q))F(n+(n, q))].\\nFigure 8.3 shows the pseudocode of this greedy construction based on\u02dcF.I np r a c t i c e ,\\nthe algorithm is stopped once all nodes have reached a', ' su\ufb03cient level of purity, when\\nthe number of points per leaf has become too small for further splitting or based\\non some other similar heuristic.\\nFor any noden and class l \u2208 [1,k ], let p\\nl(n) denote', ' the fraction of points atn that\\nbelong to class l. Then, the three most commonly used measures of node impurity\\nF are de\ufb01ned as follows:\\nF(n)=\\n\u23a7\\n\u23aa\u23aa\u23a8\\n\u23aa\u23aa\\n\u23a9\\n1 \u2212 max\\nl\u2208[1,k] pl(n) misclassi\ufb01cation ;\\n\u2212 \u2211k', '\\nl=1 pl(n)l o g2 pl(n) entropy;\\n\u2211k\\nl=1 pl(n)(1 \u2212 pl(n)) Gini index.\\nFigure 8.4 illustrates these de\ufb01nitions in the special cases of two classes (k =2 ) .T h e\\nentropy and Gini index impurity functions', ' are upper bounds on the misclassi\ufb01cation\\nimpurity function. All three functions are convex, which ensures that\\nF(n) \u2212 [\u03b7(n, q)F(n\u2212 (n, q)) + (1\u2212 \u03b7(n, q))F(n+(n, q))] \u2265 0.\\nHowever, the misclassi\ufb01catio', 'n function is piecewise linear, so \u02dcF(n, q)i sz e r oi ft h e\\nfraction of positive points remains less than (or more than) half after a split. In\\nsome cases, the impurity cannot be decreased by any sp', 'lit using that criterion. In\\ncontrast, the entropy and Gini functions are strictly convex, which guarantees a\\nstrict decrease in impurity. Furthermore, they are di\ufb00erentiable which is a useful\\nfeature', ' for numerical optimization. Thus, the Gini index and the entropy criteria\\nare typically preferred in practice.\\nThe greedy method just described faces some issues. One issue relates to the\\ngreedy natu', 're of the algorithm: a seemingly bad split may dominate subsequent\\nuseful splits, which could lead to trees with less impurity overall. This can be\\naddressed to a certain extent by using a look-ahead ', 'of some depth d to determine8.3 Uncombined multi-class algorithms 197\\n0 0.2 0.4 0.6 0.8 10\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\np\\nimpurity\\nFigure 8.4 Node impurity plotted as a function of the fraction of positive exa', 'mples\\nin the binary case: misclassi\ufb01cation (in black), entropy (in green, scaled by.5 to set\\nthe maximum to the same value for all three functions), and the Gini index (in red).\\nthe splitting decision', 's, but such look-aheads can be computationally very costly.\\nAnother issue relates to the size of the resulting tree. To achieve some desired level\\nof impurity, trees of relatively large sizes may be n', 'eeded. But larger trees de\ufb01ne\\noverly complex hypotheses with high VC-dimensions (see exercise 9.6) and thus\\ncould over\ufb01t.\\nAn alternative method for learning decision trees using a labeled training sam', 'ple\\nis based on the so-called grow-then-prune strategy. First a very large tree is grown\\nuntil it fully \ufb01ts the training sample or until no more than a very small number of\\npoints are left at each lea', 'f. Then, the resulting tree, denoted astree, is pruned back\\nto minimize an objective function de\ufb01ned based on generalization bounds as the\\nsum of an empirical error and a complexity term that can be e', 'xpressed in terms of\\nt h es i z eo f\u02dctree, the set of leaves of tree:\\nG\\n\u03bb(tree)=\\n\u2211\\nn\u2208gtree\\n|n|F(n)+ \u03bb|\u02dctree|. (8.15)\\n\u03bb \u2265 0 is a regularization parameter determining the trade-o\ufb00 between misclassi\ufb01-\\nca', 'tion, or more generally impurity, versus tree complexity. For any tree tree\u2032,w e\\ndenote by \u02c6R(tree\u2032) the total empirical error \u2211\\nn\u2208gtree\u2032 |n|F(n). We seek a sub-tree\\ntree\u03bb of tree that minimizes G\u03bb an', 'd that has the smallest size. tree\u03bb can be shown\\nto be unique. To determine tree\u03bb, the following pruning method is used, which de-\\n\ufb01nes a \ufb01nite sequence of nested sub-trees tree(0),..., tree(n). We st', 'art with the full\\ntree tree(0) = tree and for anyi \u2208 [0,n \u2212 1], de\ufb01ne tree(i+1) from tree(i) by collapsing\\nan internal node n\u2032 of tree(i),t h a ti sb yr e p l a c i n gt h es u b - t r e er o o t e da', ' tn\u2032 with a\\nleaf, or equivalently by combining the regions of all the leaves dominated by n\u2032. n\u2032\\nis chosen so that collapsing it causes the smallest per node increase in \u02c6R(tree(i)),198 Multi-Class Cl', 'assi\ufb01cation\\nthat is the smallest r(tree(i), n\u2032)d e \ufb01 n e db y\\nr(tree(i), n\u2032)= |n\u2032|F(n\u2032) \u2212 \u02c6R(tree\u2032)\\n|\u02dctree\\n\u2032\\n|\u2212 1\\n,\\nwhere n\u2032 is an internal node of tree(i). If several nodes n\u2032 in tree(i) cause the sa', 'me\\nsmallest increase per noder(tree(i), n\u2032), then all of them are pruned to de\ufb01netree(i+1)\\nfrom tree(i). This procedure continues until the tree tree(n) obtained has a single\\nnode. The sub-tree tree\u03bb ', 'can be shown to be among the elements of the sequence\\ntree(0),..., tree(n).T h ep a r a m e t e r\u03bb is determined via n-fold cross-validation.\\nDecision trees seem relatively easy to interpret, and this', ' is often underlined as\\none of their most useful features. However, such interpretations should be carried\\nout with care since decision trees are unstable: small changes in the training data\\nmay lead ', 'to very di\ufb00erent splits and thus entirely di\ufb00erent trees, as a result of their\\nhierarchical nature. Decision trees can also be used in a natural manner to deal\\nwith the problem ofmissing features , wh', 'ich often appears in learning applications;\\nin practice, some features values may be missing because the proper measurements\\nwere not taken or because of some noise source causing their systematic abs', 'ence. In\\nsuch cases, only those variables available at a node can be used in prediction. Finally,\\ndecision trees can be used and learned from data in a similar way inregression (see\\nchapter 10).\\n2\\n8.4', ' Aggregated multi-class algorithms\\nIn this section, we discuss a di\ufb00erent approach to multi-class classi\ufb01cation that\\nreduces the problem to that of multiple binary classi\ufb01cation tasks. A binary clas-\\n', 'si\ufb01cation algorithm is then trained for each of these tasks independently, and the\\nmulti-class predictor is de\ufb01ned as a combination of the hypotheses returned by each\\nof these algorithms. We \ufb01rst disc', 'uss two standard techniques for the reduction of\\nmulti-class classi\ufb01cation to binary classi\ufb01cation, and then show that they are both\\nspecial instances of a more general framework.\\n8.4.1 One-versus-all', '\\nLet S =( ( x\\n1,y1),...,x m,y m)) \u2208 (X\u00d7 Y )m be a labeled training sample. A\\nstraightforward reduction of the multi-class classi\ufb01cation to binary classi\ufb01cation\\n2. The only changes to the description f', 'or classi\ufb01cation are the following. For prediction,\\nthe label of a leaf is de\ufb01ned as the mean squared average of the labels of the points falling\\nin that region. For learning, the impurity function is', ' the mean squared error.8.4 Aggregated multi-class algorithms 199\\nis based on the so-called one-versus-all (OVA) or one-versus-the-rest technique .\\nThis technique consists of learning k binary classi\ufb01', 'ers hl : X\u2192 { \u2212 1, +1}, l \u2208Y ,\\neach seeking to discriminate one class l \u2208Y from all the others. For any l \u2208Y , hl\\nis obtained by training a binary classi\ufb01cation algorithm on the full sample S after\\nre', 'labeling points in class l with 1 and all others with \u22121. For l \u2208Y , assume that\\nhl is derived from the sign of a scoring function fl : X\u2192 R,t h a ti shl = sgn(fl), as\\nin the case of many of the binar', 'y classi\ufb01cation algorithms discussed in the previous\\nchapters. Then, the multi-class hypothesish: X\u2192 Y de\ufb01ned by the OVA technique\\nis given by:\\n\u2200x \u2208X ,h (x) = argmax\\nl\u2208Y\\nfl(x). (8.16)\\nThis formula may', ' seem similar to those de\ufb01ning a multi-class classi\ufb01cation hypoth-\\nesis in the case of uncombined algorithms. Note, however, that for uncombined\\nalgorithms the functions f\\nl are learned together, whil', 'e here they are learned in-\\ndependently. Formula (8.16) is well-founded when the scores given by functions fl\\ncan be interpreted as con\ufb01dence scores, that is when fl(x) is learned as an esti-\\nmate of ', 'the probability of x conditioned on class l. However, in general, the scores\\ngiven by functions fl, l \u2208Y , are not comparable and the OVA technique based\\non (8.16) admits no principled justi\ufb01cation. T', 'his is sometimes referred to as a cal-\\nibration problem.C l e a r l y ,t h i sp r o b l e mc a n n o tb ec o r r e c t e db ys i m p l yn o r m a l i z i n g\\nthe scores of each function to make their ', 'magnitudes uniform, or by applying other\\nsimilar heuristics. When it is justi\ufb01able, the OVA technique is simple and its com-\\nputational cost is k times that of training a binary classi\ufb01cation algorith', 'm, which\\nis similar to the computation costs for many uncombined algorithms.\\n8.4.2 One-versus-one\\nAn alternative technique, known as the one-versus-one (OVO) technique, consists\\nof using the training ', 'data to learn (independently), for each pair of distinct classes\\n(l,l\\n\u2032) \u2208Y 2, l \u0338= l\u2032, a binary classi\ufb01er hll\u2032 : X\u2192 { \u2212 1, 1} discriminating between\\nclasses l and l\u2032. For any (l,l \u2032) \u2208Y 2, hll\u2032 is ob', 'tained by training a binary classi\ufb01cation\\nalgorithm on the sub-sample containing exactly the points labeled with l or l\u2032,\\nwith the value +1 returned for class l\u2032 and \u22121 for class l.T h i sr e q u i r ', 'e st r a i n i n g(k\\n2\\n\u23a1\\n= k(k \u2212 1)/2 classi\ufb01ers, which are combined to de\ufb01ne a multi-class classi\ufb01cation\\nhypothesis h via majority vote:\\n\u2200x \u2208X ,h (x) = argmax\\nl\u2032 \u2208Y\\n\u23d0\u23d0{l: hll\u2032 (x)=1 }\\n\u23d0\u23d0. (8.17)\\nThus', ', for a \ufb01xed point x \u2208X , if we describe the prediction values hll\u2032 (x)a st h e\\nresults of the matches in a tournament between two playersl and l\u2032,w i t hhll\u2032 (x)=1200 Multi-Class Classi\ufb01cation\\nTraini', 'ng Testing\\nOVA O(km\u03b1) O(kct)\\nOVO O(k2\u2212\u03b1m\u03b1) O(k2ct)\\nT able 8.1 Comparison of the time complexity the OVA and OVO techniques for\\nboth training and testing. The table assumes a full training sample of si', 'ze m with\\neach class represented by m/k points. The time for training a binary classi\ufb01cation\\nalgorithm on a sample of size n is assumed to be in O(n\u03b1 ). Thus, the training time\\nfor the OVO technique i', 's inO(k2(m/k)\u03b1 )= O(k2\u2212\u03b1 m\u03b1 ). ct denotes the cost of testing\\na single classi\ufb01er.\\nindicating l\u2032 winning over l, then the class predicted by h can be interpreted as the\\no n ew i t ht h el a r g e s tn ', 'u m b e ro fw i n si nt h a tt o u r n a m e n t .\\nLet x \u2208X b eap o i n tb e l o n g i n gt oc l a s sl\u2032.B yd e \ufb01 n i t i o no ft h eO V Ot e c h n i q u e ,\\nif hll\u2032 (x) = 1 for all l \u0338= l\u2032, then the ', 'class associated to x by OVO is the correct\\nclass l\u2032 since\\n\u23d0\u23d0{l: hll\u2032 (x)=1 }\\n\u23d0\u23d0 = k \u2212 1 and no other class can reach (k \u2212 1) wins. By\\ncontraposition, if the OVO hypothesis misclassi\ufb01esx, then at leas', 't one of the (k \u22121)\\nbinary classi\ufb01ers hll\u2032 , l \u0338= l\u2032, incorrectly classi\ufb01es x. Assume that the generalization\\nerror of all binary classi\ufb01ers hll\u2032 used by OVO is at most r, then, in view of this\\ndiscus', 'sion, the generalization error of the hypothesis returned by OVO is at most\\n(k \u2212 1)r.\\nThe OVO technique is not subject to the calibration problem pointed out in the\\ncase of the OVA technique. However,', ' when the size of the sub-sample containing\\nmembers of the classes l and l\\n\u2032 is relatively small, hll\u2032 may be learned without\\nsu\ufb03cient data or with increased risk of over\ufb01tting. Another concern often ', 'raised for\\nthe use of this technique is the computational cost of training k(k \u2212 1)/2 binary\\nclassi\ufb01ers versus that of the OVA technique.\\nTaking a closer look at the computational requirements of thes', 'e two methods\\nreveals, however, that the disparity may not be so great and that in fact under\\nsome assumptions the time complexity of training for OVO could be less than that\\nof OVA. Table 8.1 compare', 's the computational complexity of these methods both\\nfor training and testing assuming that the complexity of training a binary classi\ufb01er\\non a sample of size m is in O(m\u03b1) and that each class is equal', 'ly represented in\\nt h et r a i n i n gs e t ,t h a ti sb ym/k points. Under these assumptions, if \u03b1 \u2208 [2, 3) as in\\nthe case of some algorithms solving a QP problem, such as SVMs, then the time\\ncomplex', 'ity of training for the OVO technique is in fact more favorable than that\\nof OVA. For\u03b1 = 1, the two are comparable and it is only for sub-linear algorithms\\nthat the OVA technique would bene\ufb01t from a b', 'etter complexity. In all cases, at test\\ntime, OVO requiresk(k\u22121)/2 classi\ufb01er evaluations, which is (k\u22121) times more than8.4 Aggregated multi-class algorithms 201\\nOVA. However, for some algorithms the ', 'evaluation time for each classi\ufb01er could be\\nmuch smaller for OVO. For example, in the case of SVMs, the average number of\\nsupport vectors may be signi\ufb01cantly smaller for OVO, since each classi\ufb01er is t', 'rained\\non a signi\ufb01cantly smaller sample. If the number of support vectors isk times smaller\\nand if sparse feature representations are used, then the time complexities of both\\ntechniques for testing ar', 'e comparable.\\n8.4.3 Error-correction codes\\nA more general method for the reduction of multi-class to binary classi\ufb01cation is\\nbased on the idea of error-correction codes (ECOC).T h i st e c h n i q u e', 'c o n s i s t so f\\nassigning to each classl \u2208Y a code word of length c \u2265 1, which in the simplest case\\nis a binary vectorM\\nl \u2208{ \u22121, +1}c. Ml serves as a signature for classl, and together\\nthese vector', 's de\ufb01ne a matrix M \u2208{ \u22121, +1}k\u00d7c whose lth row is Ml, as illustrated\\nby \ufb01gure 8.5. Next, for each column j \u2208 [1,c], a binary classi\ufb01er hj : X\u2192 { \u2212 1, +1}\\nis learned using the full training sample S, a', 'fter relabeling points that belong to\\na class of column l labeled with +1, and all others with \u22121. For any x \u2208X ,l e t\\nh(x) denote the vectorh(x)=( h1(x),...,h c(x))\u22a4. Then, the multi-class hypothesis', '\\nh: X\u2192 Y is de\ufb01ned by\\n\u2200x \u2208X ,h (x) = argmax\\nl\u2208Y\\ndH\\n(\\nMl,h(x)\\n\u23a1\\n. (8.18)\\nT h u s ,t h ec l a s sp r e d i c t e di st h eo n ew h o s es i g n a t u r e si st h ec l o s e s tt oh(x)i n\\nHamming distanc', 'e. Figure 8.5 illustrates this de\ufb01nition: no row of matrix M\\nmatches the vector of predictions h(x) in that case, but the third row shares the\\nlargest number of components with h(x).\\nThe success of th', 'e ECOC technique depends on the minimal Hamming distance\\nbetween the class code words. Let d denote that distance, then up to r0 =\\n\u230ad\u22121\\n2\\n\u230b\\nbinary classi\ufb01cation errors can be corrected by this techniq', 'ue: by de\ufb01nition of d,\\neven if r<r 0 binary classi\ufb01ers hl misclassify x \u2208X , h(x) is closest to the code\\nword of the correct class of x. For a \ufb01xed c, the design of error-correction matrix\\nM is subjec', 't to a trade-o\ufb00, since larger d values may imply substantially more\\ndi\ufb03cult binary classi\ufb01cation tasks. In practice, each column may correspond to a\\nclass feature determined based on domain knowledge.', '\\nThe ECOC technique just described can be extended in two ways. First, instead\\nof using only the label predicted by each classi\ufb01er hl the magnitude of the scores\\nde\ufb01ning hl is used. Thus, if hl = sgn(', 'fl) for some function fl whose values can\\nbe interpreted as con\ufb01dence scores, then the multi-class hypothesis h: X\u2192 Y is202 Multi-Class Classi\ufb01cation\\n123456\\n1 000100\\n2 100000\\n3 011010\\n4 110000\\n5 11001', '0\\n6 001101\\n7 001000\\n8 010100\\n011011\\nclasses\\ncodes\\nnew example    x\\nf1(x) f2(x) f3(x) f4(x) f5(x) f6(x)\\nFigure 8.5 Illustration of error-correction codes for multi-class classi\ufb01cation. Left:\\nbinary cod', 'e matrix M, with each row representing the code word of length c =6\\nof a class l \u2208 [1, 8].R i g h t :v e c t o ro fp r e d i c t i o n sh(x) for a test point x.T h eE C O C\\nclassi\ufb01er assigns label 3 t', 'o x, since the binary code for the third class yields the\\nminimal Hamming distance with h(x) (distance of 1).\\nde\ufb01ned by\\n\u2200x \u2208X ,h (x) = argmin\\nl\u2208Y\\nc\u2211\\nj=1\\nL(mljfj(x)), (8.19)\\nwhere (mlj) are the entries', ' ofM and where L: R \u2192 R+ is a loss function. When L\\nis de\ufb01ned by L(x)= 1\u2212sgn(x)\\n2 for all x \u2208X and hl = fl,w ec a nw r i t e :\\nc\u2211\\nj=1\\nL(mljfj(x)) =\\nc\u2211\\nj=1\\n1 \u2212 sgn(mljhj(x))\\n2 = dh(Ml,h(x)),\\nand (8.19)', ' coincides with (8.18). Furthermore, ternary codes can be used with ma-\\ntrix entries in {\u22121, 0, +1} so that examples in classes labeled with 0 are disregarded\\nwhen training a binary classi\ufb01er for each', ' column. With these extensions, both OVA\\nand OVO become special instances of the ECOC technique. The matrix M for\\nOVA is a square matrix, that is c = k, with all terms equal to \u22121e x c e p tf r o mt h', ' e\\ndiagonal ones which are all equal to +1. The matrixM for OVO hasc = k(k \u2212 1)/2\\ncolumns. Each column corresponds to a pair of distinct classes ( l,l \u2032), l \u0338= l\u2032,w i t h\\nall entries equal to 0 except', ' from the one with rowl,w h i c hi s\u22121, and the one with\\nrow l\u2032, which is +1.\\nSince the values of the scoring functions are assumed to be con\ufb01dence scores,\\nmljfj(x) can be interpreted as the margin of', ' classi\ufb01er j on point x and (8.19) is\\nthus based on some loss L de\ufb01ned with respect to the binary classi\ufb01er\u2019s margin.\\nA further extension of ECOC consists of extending discrete codes to continuous8.5 ', 'Structured prediction algorithms 203\\nones by letting the matrix entries take arbitrary real values and by using the training\\nsample to learn matrix M. Starting with a discrete version ofM, c binary cl', 'assi\ufb01ers\\nwith scoring functions fl, l \u2208 [1,c], are \ufb01rst learned as described previously. We will\\ndenote by F(x) the vector (f1(x),...,f c(x))\u22a4 for any x \u2208X . Next, the entries of\\nM are relaxed to take', ' real values and learned from the training sample with the\\nobjective of making the row of M corresponding to the class of any point x \u2208X\\nmore similar to F(x) than other rows. The similarity can be mea', 'sured using any\\nPDS kernel K. An example of an algorithm for learning M using a PDS kernel K\\nand the idea just discussed is in fact multi-class SVMs, which, in this context, can\\nbe formulated as follo', 'ws:\\nmin\\nM,\u03be\\n\u2225M\u22252\\nF + C\\nm\u2211\\ni=1\\n\u03bei\\nsubject to: \u2200(i, l) \u2208 [1,m] \u00d7Y ,\\nK(f(xi),Myi ) \u2265 K(f(xi),Ml)+1 \u2212 \u03bei.\\nSimilar algorithms can be de\ufb01ned using other matrix norms. The resulting multi-\\nclass classi\ufb01catio', 'n decision function has the following form:\\nh: x \u21a6\u2192 argmax\\nl\u2208{1,...,k}\\nK(f(x),Ml).\\n8.5 Structured prediction algorithms\\nIn this section, we brie\ufb02y discuss an important class of problems related to mul', 'ti-\\nclass classi\ufb01cation that frequently arises in computer vision, computational biology,\\nand natural language processing. These include all sequence labeling problems and\\ncomplex problems such as par', 'sing, machine translation, and speech recognition.\\nIn these applications, the output labels have a rich internal structure. For exam-\\nple, in part-of-speech tagging the problem consists of assigning a', ' part-of-speech tag\\nsuch as N (noun), V (verb), or A (adjective), to every word of a sentence. Thus, the\\nlabel of the sentence \u03c9\\n1 ...\u03c9 n made of the words \u03c9i is a sequence of part-of-speech\\ntags t1 .', '..t n. This can be viewed as a multi-class classi\ufb01cation problem where each\\nsequence of tags is a possible label. However, several critical aspects common to\\nsuch structured output problems make them ', 'distinct from the standard multi-class\\nclassi\ufb01cation.\\nFirst, the label set is exponentially large as a function of the size of the output.\\nFor example, if \u03a3 denotes the alphabet of part-of-speech tags', ', for a sentence of\\nlength n there are |\u03a3|\\nn possible tag sequences. Second, there are dependencies204 Multi-Class Classi\ufb01cation\\nbetween the substructures of a label that are important to take into ac', 'count for\\nan accurate prediction. For example, in part-of-speech tagging, some tag sequences\\nmay be ungrammatical or unlikely. Finally, the loss function used is typically not a\\nzero-one loss but one ', 'that depends on the substructures. LetL: Y\u00d7Y \u2192 R denote\\nal o s sf u n c t i o ns u c ht h a tL(y\\n\u2032,y ) measures the penalty of predicting the labely\u2032 \u2208Y\\ninstead of the correct label y \u2208Y .3 In part-of', '-speech tagging, L(y\u2032,y ) could be for\\nexample the Hamming distance between y\u2032 and y.\\nThe relevant features in structured output problems often depend on both the\\ninput and the output. Thus, we will d', 'enote by \u03a6(x, y) \u2208 RN the feature vector\\nassociated to a pair (x, y) \u2208X\u00d7Y .\\nTo model the label structures and their dependency, the label set Y is typically\\nassumed to be endowed with a graphical mode', 'l structure, that is, a graph giving a\\nprobabilistic model of the conditional dependence between the substructures. It is\\nalso assumed that both the feature vector\u03a6(x, y) associated to an inputx \u2208X an', 'd\\noutput y \u2208Y and the lossL(y\u2032,y ) factorize according to the cliques of that graphical\\nmodel.4 A detailed treatment of this topic would require a further background in\\ngraphical models, and is thus b', 'eyond the scope of this section.\\nThe hypothesis set used by most structured prediction algorithms is then de\ufb01ned\\nas the set of functions h: X\u2192 Y such that\\n\u2200x \u2208X ,h (x) = argmax\\ny\u2208Y\\nw \u00b7 \u03a6(x, y), (8.20)', '\\nfor some vector w \u2208 RN .L e tS =( (x1,y1),...,x m,y m)) \u2208 (X\u00d7 Y )m be an i.i.d.\\nlabeled sample. Since the hypothesis set is linear, we can seek to de\ufb01ne an algorithm\\nsimilar to multi-class SVMs. The ', 'optimization problem for multi-class SVMs can\\nbe rewritten equivalently as follows:\\nmin\\nw\\n1\\n2 \u2225w\u22252+C\\nm\u2211\\ni=1\\nmax\\ny\u0338=yi\\nmax\\n(\\n0, 1 \u2212 w \u00b7 [\u03a6(xi,y i)\u2212\u03a6(xi,y )]\\n\u23a1\\n, (8.21)\\nHowever, here we need to take int', 'o account the loss functionL,t h a ti sL(y,y i)f o r\\neach i \u2208 [1,m]a n dy \u2208Y , and there are multiple ways to proceed. One possible way\\nis to let the margin violation be penalized additively with L(y,', 'y i). Thus, in that\\ncase L(y,y i) is added to the margin violation. Another natural method consists of\\npenalizing the margin violation by multiplying it with L(y,y i). A margin violation\\nwith a larger', ' loss is then penalized more than one with a smaller one.\\n3. More generally, in some applications, the loss function could also depend on the input.\\nThus, L is then a function mapping L: X\u00d7 Y\u00d7 Y\u2192 R,w ', 'i t hL(x, y\u2032,y ) measuring the\\npenalty of predicting the label y\u2032 instead of y given the input x.\\n4. In an undirected graph, a clique is a set of fully connected vertices.8.5 Structured prediction alg', 'orithms 205\\nThe additive penalization leads to the following algorithm known as Maximum\\nMargin Markov Networks (M3N):\\nmin\\nw\\n1\\n2 \u2225w\u22252+C\\nm\u2211\\ni=1\\nmax\\ny\u0338=yi\\nmax\\n(\\n0,L(yi,y ) \u2212 w \u00b7 [\u03a6(xi,y i)\u2212\u03a6(xi,y )]\\n\u23a1\\n. ', '(8.22)\\nAn advantage of this algorithm is that, as in the case of SVMs, it admits a natural\\nuse of PDS kernels. As already indicated, the label setY is assumed to be endowed\\nwith a graph structure with', ' a Markov property, typically a chain or a tree, and\\nthe loss function is assumed to be decomposable in the same way. Under these\\nassumptions, by exploiting the graphical model structure of the labels', ', a polynomial-\\ntime algorithm can be given to determine its solution.\\nA multiplicative combination of the loss with the margin leads to the following\\nalgorithm known as SVMStruct:\\nmin\\nw\\n1\\n2 \u2225w\u22252+C\\nm\u2211', '\\ni=1\\nmax\\ny\u0338=yi\\nL(yi,y )m a x\\n(\\n0,1 \u2212 w \u00b7 [\u03a6(xi,y i)\u2212\u03a6(xi,y )]\\n\u23a1\\n. (8.23)\\nThis problem can be equivalently written as a QP with an in\ufb01nite number of\\nconstraints. In practice, it is solved iteratively b', 'y augmenting at each round the\\n\ufb01nite set of constraints of the previous round with the most violating constraint.\\nThis method can be applied in fact under very general assumptions and for arbitrary\\nlo', 'ss de\ufb01nitions. As in the case of M\\n3N, SVMStruct naturally admits the use of PDS\\nkernels and thus an extension to non-linear models for the solution.\\nAnother standard algorithm for structured predicti', 'on problems is Conditional\\nRandom Fields (CRFs). We will not describe this algorithm in detail, but point\\nout its similarity with the algorithms just described, in particular M 3N. The\\noptimization pr', 'oblem for CRFs can be written as\\nmin\\nw\\n1\\n2 \u2225w\u22252+C\\nm\u2211\\ni=1\\nlog\\n\u2211\\ny\u2208Y\\nexp\\n(\\nL(yi,y ) \u2212 w \u00b7 [\u03a6(xi,y i)\u2212\u03a6(xi,y )]\\n\u23a1\\n. (8.24)\\nAssume for simplicity that Y is \ufb01nite and has cardinality k and let f denote the', '\\nfunction (x1,...,x k) \u21a6\u2192 log(\u2211k\\nj=1 exj ). f is a convex function known as the soft-\\nmax, since it provides a smooth approximation of ( x1,...,x k) \u21a6\u2192 max(x1,...,x k).\\nThen, problem (8.24) is similar', ' to (8.22) modulo the replacement of the max\\noperator with the soft-max function just described.206 Multi-Class Classi\ufb01cation\\n8.6 Chapter notes\\nThe margin-based generalization for multi-class classi\ufb01c', 'ation presented in theo-\\nrem 8.1 is based on an adaptation of the result and proof due to Koltchinskii and\\nPanchenko [2002]. Proposition 8.1 bounding the Rademacher complexity of multi-\\nclass kernel-b', 'ased hypotheses and corollary 8.1 are new.\\nAn algorithm generalizing SVMs to the multi-class classi\ufb01cation setting was \ufb01rst\\nintroduced by Weston and Watkins [1999]. The optimization problem for that\\na', 'lgorithm was based on k(k \u2212 1)/2 slack variables for a problem withk classes and\\nthus could be ine\ufb03cient for a relatively large number of classes. A simpli\ufb01cation of\\nthat algorithm by replacing the su', 'm of the slack variables\u2211\\nj\u0338=i \u03beij related to point\\nxi by its maximum \u03bei =m a xj\u0338=i \u03beij considerably reduces the number of variables\\nand leads to the multi-class SVM algorithm presented in this chapte', 'r [Crammer\\nand Singer, 2001, 2002].\\nThe AdaBoost.MH algorithm is presented and discussed by Schapire and Singer\\n[1999, 2000]. As we showed in this chapter, the algorithm is a special instance\\nof AdaBo', 'ost. Another boosting-type algorithm for multi-class classi\ufb01cation, Ad-\\naBoost.MR, is presented by Schapire and Singer [1999, 2000]. That algorithm is\\nalso a special instance of the RankBoost algorith', 'm presented in chapter 9. See ex-\\nercise 9.5 for a detailed analysis of this algorithm, including generalization bounds.\\nThe most commonly used tools for learning decision trees are CART (classi\ufb01catio', 'n\\nand regression tree) [Breiman et al., 1984] and C4.5 [Quinlan, 1986, 1993]. The\\ngreedy technique we described for learning decision trees bene\ufb01ts in fact from an\\ninteresting analysis: remarkably, it', ' has been shown by Kearns and Mansour [1999],\\nMansour and McAllester [1999] that, under a weak learner hypothesis assumption,\\nsuch decision tree algorithms produce a strong hypothesis. The grow-then-p', 'rune\\nmethod is from CART. It has been analyzed by a variety of di\ufb00erent studies, in\\nparticular by Kearns and Mansour [1998] and Mansour and McAllester [2000], who\\ngive generalization bounds for the re', 'sulting decision trees with respect to the error\\nand size of the best sub-tree of the original tree pruned.\\nThe idea of the ECOC framework for multi-class classi\ufb01cation is due to Dietterich\\nand Bakiri', ' [1995]. Allwein et al. [2000] further extended and analyzed this method\\nto margin-based losses, for which they presented a bound on the empirical error\\nand a generalization bound in the more speci\ufb01c ', 'case of boosting. While the OVA\\ntechnique is in general subject to a calibration issue and does not have any\\njusti\ufb01cation, it is very commonly used in practice. Rifkin [2002] reports the results\\nof ex', 'tensive experiments with several multi-class classi\ufb01cation algorithms that are\\nrather favorable to the OVA technique, with performances often very close or better\\nthan for those of several uncombined ', 'algorithms, unlike what has been claimed by\\nsome authors (see also Rifkin and Klautau [2004]).8.7 Exercises 207\\nThe CRFs algorithm was introduced by La\ufb00erty, McCallum, and Pereira [2001].\\nM3N is due t', 'o Taskar, Guestrin, and Koller [2003] and StructSVM was presented by\\nTsochantaridis, Joachims, Hofmann, and Altun [2005]. An alternative technique for\\ntackling structured prediction as a regression pr', 'oblem was presented and analyzed\\nby Cortes, Mohri, and Weston [2007c].\\n8.7 Exercises\\n8.1 Generalization bounds for multi-label case. Use similar techniques to those used\\nin the proof of theorem 8.1 to', ' derive a margin-based learning bound in the multi-\\nlabel case.\\n8.2 Multi-class classi\ufb01cation with kernel-based hypotheses constrained by an L\\np\\nnorm. Use corollary 8.1 to de\ufb01ne alternative multi-clas', 's classi\ufb01cation algorithms\\nwith kernel-based hypotheses constrained by an Lp norm with p \u0338= 2. For which\\nvalue ofp \u2265 1 is the bound of proposition 8.1 tightest? Derive the dual optimization\\nof the mul', 'ti-class classi\ufb01cation algorithm de\ufb01ned with p = \u221e .\\n8.3 Alternative multi-class boosting algorithm. Consider the objective function\\nG de\ufb01ned for any sample S =( ( x1,y1),..., (xm,y m)) \u2208 (X\u00d7 Y )m and', ' \u03b1 =\\n(\u03b11,...,\u03b1 n) \u2208 Rn, n \u2265 1, by\\nG(\u03b1)=\\nm\u2211\\ni=1\\ne\u2212 1\\nk\\nPk\\nl=1 yi[l]gn(xi,l) =\\nm\u2211\\ni=1\\ne\u2212 1\\nk\\nPk\\nl=1 yi[l] Pn\\nt=1 \u03b1tht(xi,l). (8.25)\\nUse the convexity of the exponential function to compareG with the obj', 'ective func-\\ntion F de\ufb01ning AdaBoost.MH. Show that G is a convex function upper bounding\\nthe multi-label multi-class error. Discuss the properties ofG and derive an algorithm\\nde\ufb01ned by the application', ' of coordinate descent to G. Give theoretical guarantees\\nfor the performance of the algorithm and analyze its running-time complexity when\\nusing boosting stumps.\\n8.4 Multi-class algorithm based on Ran', 'kBoost. This problem requires familiarity\\nwith the material presented both in this chapter and in chapter 9. An alternative\\nboosting-type multi-class classi\ufb01cation algorithm is one based on a ranking ', 'criterion.\\nWe will de\ufb01ne and examine that algorithm in the mono-label setting. Let H be a\\nfamily of base hypothesis mapping X\u00d7 Y to {\u22121, +1}.L e t F be the following\\nobjective function de\ufb01ned for all ', 'samples S =( (x\\n1,y1),..., (xm,y m)) \u2208 (X\u00d7 Y )m208 Multi-Class Classi\ufb01cation\\nand \u03b1 =( \u03b11,...,\u03b1 n) \u2208 Rn, n \u2265 1, by\\nF(\u03b1)=\\nm\u2211\\ni=1\\n\u2211\\nl\u0338=yi\\ne\u2212(gn(xi,yi)\u2212gn(xi,l)) =\\nm\u2211\\ni=1\\n\u2211\\nl\u0338=yi\\ne\u2212 Pn\\nt=1 \u03b1t(ht(xi,yi)\u2212ht', '(xi,l)). (8.26)\\nwhere gn = \u2211n\\nt=1 \u03b1tht.\\n(a) Show that F is convex and di\ufb00erentiable.\\n(b) Show that 1\\nm\\n\u2211m\\ni=1 1\u03c1gn (xi,y i) \u2264 1\\nk\u22121 F(\u03b1), where gn = \u2211n\\nt=1 \u03b1tht.\\n(c) Give the pseudocode of the algorit', 'hm obtained by applying coordinate\\ndescent to F. The resulting algorithm is known as AdaBoost.MR. Show that\\nAdaBoost.MR exactly coincides with the RankBoost algorithm applied to the\\nproblem of ranking', ' pairs (x, y) \u2208X\u00d7Y . Describe exactly the ranking target\\nfor these pairs.\\n(d) Use question (8.4b) and the learning bounds of this chapter to derive\\nmargin-based generalization bounds for this algorith', 'm.\\n(e) Use the connection of the algorithm with RankBoost and the learning\\nbounds of chapter 9 to derive alternative generalization bounds for this al-\\ngorithm. Compare these bounds with those of the ', 'previous question.\\n8.5 Decision trees. Show that VC-dimension of a binary decision tree with n nodes\\nin dimension N is in O(n log N).\\n8.6 Give an example where the generalization error of each of thek', '(k \u2212 1)/2 binary\\nclassi\ufb01ers h\\nll\u2032 , l \u0338= l\u2032, used in the de\ufb01nition of the OVO technique is r and that of\\nthe OVO hypothesis (k \u2212 1)r.9R a n k i n g\\nThe learning problem of ranking arises in many moder', 'n applications, including\\nthe design of search engines, information extraction platforms, and movie recom-\\nmendation systems. In these applications, the ordering of the documents or movies\\nreturned is', ' a critical aspect of the system. The main motivation for ranking over\\nclassi\ufb01cation in the binary case is the limitation of resources: for very large data\\nsets, it may be impractical or even impossib', 'le to display or process all items labeled\\nas relevant by a classi\ufb01er. A standard user of a search engine is not willing to con-\\nsult all the documents returned in response to a query, but only the to', 'p ten or so.\\nSimilarly, a member of the fraud detection department of a credit card company\\ncannot investigate thousands of transactions classi\ufb01ed as potentially fraudulent, but\\nonly a few dozens of t', 'he most suspicious ones.\\nIn this chapter, we study in depth the learning problem of ranking. We distinguish\\ntwo general settings for this problem: the score-based and the preference-based set-\\ntings. ', 'For the score-based setting, which is the most widely explored one, we present\\nmargin-based generalization bounds using the notion of Rademacher complexity.\\nWe then describe an SVM-based ranking algor', 'ithm that can be derived from these\\nbounds and describe and analyze RankBoost, a boosting algorithm for ranking.\\nWe further study speci\ufb01cally the bipartite setting of the ranking problem where,\\nas in ', 'binary classi\ufb01cation, each point belongs to one of two classes. We discuss an\\ne\ufb03cient implementation of RankBoost in that setting and point out its connec-\\ntions with AdaBoost. We also introduce the n', 'otions of ROC curves and area under\\nthe ROC curves (AUC) which are directly relevant to bipartite ranking. For the\\npreference-based setting, we present a series of results, in particular regret-based\\n', 'guarantees for both a deterministic and a randomized algorithm, as well as a lower\\nb o u n di nt h ed e t e r m i n i s t i cc a s e .\\n9.1 The problem of ranking\\nWe \ufb01rst introduce the most commonly st', 'udied scenario of the ranking problem in\\nmachine learning. We will refer to this scenario as the score-based setting of the210 Ranking\\nranking problem. In section 9.6, we present and analyze an altern', 'ative setting, the\\npreference-based setting.\\nThe general supervised learning problem of ranking consists of using labeled\\ninformation to de\ufb01ne an accurate ranking prediction function for all points. I', 'n the\\nscenario examined here, the labeled information is supplied only for pairs of points\\nand the quality of a predictor is similarly measured in terms of its average pairwise\\nmisranking. The predict', 'or is a real-valued function, a scoring function: the scores\\nassigned to input points by this function determine their ranking.\\nLet X denote the input space. We denote by D an unknown distribution ove', 'r\\nX\u00d7 X according to which pairs of points are drawn and byf : X\u00d7 X \u2192{ \u2212 1,0, +1}\\na target labeling function or preference function. The three values assigned by f\\nare interpreted as follows: f(x, x\\n\u2032)', ' = +1 if x\u2032 is preferred to x or ranked higher\\nthan x, f(x, x\u2032)= \u22121i f x is preferred to x\u2032,a n df(x, x\u2032) = 0 if both x and x\u2032 have\\nthe same preference or ranking, or if there is no information about ', 'their respective\\nranking. This formulation corresponds to a deterministic scenario which we adopt\\nfor simpli\ufb01cation. As discussed in section 2.4.1, it can be straightforwardly extended\\nto a stochastic', ' scenario where we have a distribution over X\u00d7 X\u00d7 { \u2212 1,0, +1}.\\nNote that in general no particular assumption is made about the transitivity of the\\norder induced by f:w em a yh a v ef(x, x\\n\u2032)=1a n d f', '(x\u2032,x \u2032\u2032)=1b u t f(x, x\u201d) = \u22121\\nfor three points x, x\u2032,a n d x\u2032\u2032. While this may contradict an intuitive notion of\\npreference, such preference orders are in fact commonly encountered in practice, in\\npa', 'rticular when they are based on human judgments. This is sometimes because the\\npreference between two items are decided based on di\ufb00erent features: for example,\\nan individual may prefer movie x\\n\u2032 to x', ' because x\u2032 is an action movie and x a\\nmusical, and prefer x\u2032\u2032 to x\u2032 because x\u2032\u2032 is an action movie with more active scenes\\nthan x\u2032. Nevertheless, he may prefer x to x\u2032\u2032 because the cost of renting a ', 'DVD\\nfor x\u2032\u2032 is prohibitive. Thus, in this example, two features, the genre and the price,\\nare invoked, each a\ufb00ecting the decision for di\ufb00erent pairs. In fact, in general, no\\nassumption is made about t', 'he preference function, not even the antisymmetry of\\nthe order induced; thus, we may have f(x, x\u2032)=1a n d f(x\u2032,x)=1a n dy e t x \u0338= x\u2032.\\nThe learner receives a labeled sample S =\\n(\\n(x1,x \u2032\\n1,y1),..., (x', 'm,x \u2032\\nm,y m)\\n\u23a1\\n\u2208\\nX\u00d7 X\u00d7 { \u2212 1, 0, +1} with (x1,x \u2032\\n1),..., (xm,x \u2032\\nm) drawn i.i.d. according to D and\\nyi = f(xi,x \u2032\\ni) for all i \u2208 [1,m]. Given a hypothesis setH of functions mapping X to\\nR, the rankin', 'g problem consists of selecting a hypothesish \u2208 H with small expected\\npairwise misranking or generalization error R(h) with respect to the target f:\\nR(h)= P r\\n(x,x\u2032)\u223cD\\n[(\\nf(x, x\u2032) \u0338=0\\n\u23a1\\n\u2227\\n(\\nf(x, x\u2032)(h', '(x\u2032) \u2212 h(x)) \u2264 0\\n\u23a1]\\n. (9.1)\\nThe empirical pairwise misranking or empirical error of h is denoted by \u02c6R(h)a n d9.2 Generalization bound 211\\nde\ufb01ned by\\n\u02c6R(h)= 1\\nm\\nm\u2211\\ni=1\\n1(yi\u0338=0)\u2227(yi(h(x\u2032\\ni)\u2212h(xi))\u22640) . ', '(9.2)\\nNote that while the target preference function f is in general not transitive, the\\nlinear ordering induced by a scoring functionh \u2208 H is by de\ufb01nition transitive. This\\nis a drawback of the score-', 'based setting for the ranking problem since, regardless of\\nthe complexity of the hypothesis set H, if the preference function is not transitive,\\nno hypothesis h \u2208 H can faultlessly predict the target ', 'pairwise ranking.\\n9.2 Generalization bound\\nIn this section, we present margin-based generalization bounds for ranking. To\\nsimplify the presentation, we will assume for the results of this section that', ' the\\npairwise labels are in {\u22121, +1}.T h u s ,i fap a i r(x, x\\n\u2032) is drawn according toD,t h e n\\neither x is preferred to x\u2032 or the opposite. The learning bounds for the general case\\nhave a very simil', 'ar form but require more details. As in the case of classi\ufb01cation,\\nfor any \u03c1> 0, we can de\ufb01ne the empirical margin loss of a hypothesish for pairwise\\nranking as\\n\u02c6R\u03c1(h)= 1\\nm\\nm\u2211\\ni=1\\n\u03a6\u03c1(yi(h(x\u2032\\ni) \u2212 h(xi', ')), (9.3)\\nwhere \u03a6\u03c1 is the margin loss function (de\ufb01nition 4.3). Thus, the empirical margin\\nloss for ranking is upper bounded by the fraction of the pairs ( xi,x \u2032\\ni)t h a th is\\nmisranking or correctly', ' ranking but with con\ufb01dence less than \u03c1:\\n\u02c6R\u03c1(h) \u2264 1\\nm\\nm\u2211\\ni=1\\n1yi(h(x\u2032\\ni)\u2212h(xi))\u2264\u03c1. (9.4)\\nWe denote byD1 t h em a r g i n a ld i s t r i b u t i o no ft h e\ufb01 r s te l e m e n to ft h ep a i r si nX\u00d7 X\\n', 'derived from D,a n db y D2 the marginal distribution with respect to the second\\nelement of the pairs. Similarly,S1 is the sample derived fromS by keeping only the\\n\ufb01rst element of each pair: S1 =\\n(\\n(x1', ',y1),..., (xm,y m)\\n\u23a1\\nand S2 the one obtained by\\nkeeping only the second element: S2 =\\n(\\n(x\u2032\\n1,y1),..., (x\u2032\\nm,y m)\\n\u23a1\\n.W ea l s od e n o t eb y\\nRD1\\nm (H) the Rademacher complexity ofH with respect to th', 'e marginal distribution\\nD1,t h a ti sRD1\\nm (H)=E [ \u02c6RS1 (H)], and similarly RD2\\nm (H)=E [ \u02c6RS2 (H) ] .C l e a r l y ,i f\\nthe distribution D is symmetric, the marginal distributionsD1 and D2 coincide a', 'nd\\nRD1\\nm (H)= RD2\\nm (H).212 Ranking\\nTheorem 9.1 Margin bound for ranking\\nLet H be a set of real-valued functions. Fix \u03c1> 0;t h e n ,f o ra n y\u03b4> 0,w i t h\\nprobability at least 1 \u2212 \u03b4 over the choice of', ' a sample S of size m,e a c ho ft h e\\nfollowing holds for all h \u2208 H:\\nR(h) \u2264 \u02c6R\u03c1(h)+ 2\\n\u03c1\\n(\\nRD1\\nm (H)+ RD2\\nm (H)\\n\u23a1\\n+\\n\u221a\\nlog 1\\n\u03b4\\n2m (9.5)\\nR(h) \u2264 \u02c6R\u03c1(h)+ 2\\n\u03c1\\n(\u02c6RS1 (H)+ \u02c6RS2 (H)\\n\u23a1\\n+3\\n\u221a\\nlog 2\\n\u03b4\\n2m . (9.6)\\nP', 'roof The proof is similar to that of theorem 4.4. Let \u02dcH be the family of\\nhypotheses mapping (X\u00d7 X ) \u00d7{ \u22121,+1} to R de\ufb01ned by \u02dcH = {z =( (x, x\u2032),y ) \u21a6\u2192\\ny[h(x\u2032) \u2212 h(x)]: h \u2208 H}. Consider the family of ', 'functions \u02dcH = {\u03a6\u03c1 \u25e6f : f \u2208 \u02dcH}\\nderived from \u02dcH which are taking values in [0, 1]. By theorem 3.1, for any\u03b4> 0w i t h\\nprobability at least 1 \u2212 \u03b4, for all h \u2208 H,\\nE\\n[\\n\u03a6\u03c1(y[h(x\u2032) \u2212 h(x)])\\n]\\n\u2264 \u02c6R\u03c1(h)+2 Rm', '\\n(\\n\u03a6\u03c1 \u25e6 \u02dcH\\n\u23a1\\n+\\n\u221a\\nlog 1\\n\u03b4\\n2m .\\nSince 1u\u22640 \u2264 \u03a6\u03c1(u) for all u \u2208 R, the generalization error R(h)i sal o w e rb o u n d\\non left-hand side, R(h)=E [ 1y[h(x\u2032)\u2212h(x)]\u22640] \u2264 E\\n[\\n\u03a6\u03c1(y[h(x\u2032) \u2212 h(x)])\\n]\\n,a n dw ec', ' a n\\nwrite:\\nR(h) \u2264 \u02c6R\u03c1(h)+2 Rm\\n(\\n\u03a6\u03c1 \u25e6 \u02dcH\\n\u23a1\\n+\\n\u221a\\nlog 1\\n\u03b4\\n2m .\\nExactly as in the proof of theorem 4.4, we can show that Rm\\n(\\n\u03a6\u03c1 \u25e6 \u02dcH\\n\u23a1\\n\u2264 1\\n\u03c1Rm( \u02dcH)\\nusing the (1/\u03c1)-Lipschitzness of \u03a6\u03c1. Here, Rm( \u02dcH) can ', 'be upper bounded as follows:\\nRm( \u02dcH)= 1\\nm E\\nS,\u03c3\\n[\\nsup\\nh\u2208H\\nm\u2211\\ni=1\\n\u03c3iyi(h(x\u2032\\ni) \u2212 h(xi))\\n]\\n= 1\\nm E\\nS,\u03c3\\n[\\nsup\\nh\u2208H\\nm\u2211\\ni=1\\n\u03c3i(h(x\u2032\\ni) \u2212 h(xi))\\n]\\n(yi\u03c3i and \u03c3i: same distrib.)\\n\u2264 1\\nm E\\nS,\u03c3\\n[\\nsup\\nh\u2208H\\nm\u2211\\ni=1\\n\u03c3i', 'h(x\u2032\\ni)+s u p\\nh\u2208H\\nm\u2211\\ni=1\\n\u03c3ih(xi)\\n]\\n(by sub-additivity of sup)\\n=E\\nS\\n[\\nRS2 (H)+ RS1 (H)\\n]\\n(de\ufb01nition of S1 and S2)\\n= RD2\\nm (H)+ RD1\\nm (H) ,\\nwhich proves (9.5). The second inequality, (9.6), can be deriv', 'ed in the same way by\\nusing the second inequality of theorem 3.1, (3.4), instead of (3.3).9.3 Ranking with SVMs 213\\nThese bounds can be generalized to hold uniformly for all \u03c1> 0 at the cost of an\\nadd', 'itional term\\n\u221a\\n(log log2(2/\u03c1))/m, as in theorem 4.5 and exercise 4.2. As for other\\nmargin bounds presented in previous sections, they show the con\ufb02ict between two\\nterms: the larger the desired pairwis', 'e ranking margin\u03c1, the smaller the middle term.\\nHowever, the \ufb01rst term, the empirical pairwise ranking margin loss \u02c6R\\n\u03c1, increases as\\naf u n c t i o no f\u03c1.\\nKnown upper bounds for the Rademacher comple', 'xity of a hypothesisH, including\\nbounds in terms of VC-dimension, can be used directly to make theorem 9.1 more\\nexplicit. In particular, using theorem 9.1, we obtain immediately the following\\nmargin b', 'ound for pairwise ranking using kernel-based hypotheses.\\nCorollary 9.1 Margin bounds for ranking with kernel-based hypotheses\\nLet K : X\u00d7 X\u2192 R be a PDS kernel with r =s u px\u2208X K(x, x).L e t\u03a6: X\u2192 H be a', '\\nfeature mapping associated toK and let H = {x \u21a6\u2192 w \u00b7 \u03a6(x): \u2225w\u2225H \u2264 \u039b} for some\\n\u039b \u2265 0.F i x\u03c1> 0.T h e n ,f o ra n y\u03b4> 0, the following pairwise margin bound holds\\nwith probability at least 1 \u2212 \u03b4for any', ' h \u2208 H:\\nR(h) \u2264 \u02c6R\u03c1(h)+4\\n\u221a\\nr2\u039b2/\u03c12\\nm +\\n\u221a\\nlog 1\\n\u03b4\\n2m . (9.7)\\nAs with theorem 4.4, the bound of this corollary can be generalized to hold\\nuniformly for all \u03c1> 0 at the cost of an additional term\\n\u221a\\n(log l', 'og2(2/\u03c1))/m.T h i s\\ngeneralization bound for kernel-based hypotheses is remarkable, since it does not\\ndepend directly on the dimension of the feature space, but only on the pairwise\\nranking margin. It', ' suggests that a small generalization error can be achieved when\\n\u03c1/r is large (small second term) while the empirical margin loss is relatively small\\n(\ufb01rst term). The latter occurs when few points are', ' either classi\ufb01ed incorrectly or\\ncorrectly but with margin less than \u03c1.\\n9.3 Ranking with SVMs\\nIn this section, we discuss an algorithm that is derived directly from the theoretical\\nguarantees just pre', 'sented. The algorithm turns out to be a special instance of the\\nSVM algorithm.\\nProceeding as in section 4.4 for classi\ufb01cation, the guarantee of corollary 9.1 can\\nbe expressed as follows: for any \u03b4> 0,', ' with probability at least 1 \u2212 \u03b4, for all\\nh \u2208 H = {x \u21a6\u2192 w \u00b7 \u03a6(x): \u2225w\u2225\u2264 \u039b},\\nR(h) \u2264 1\\nm\\nm\u2211\\ni=1\\n\u03bei +4\\n\u221a\\nr2\u039b2\\nm +\\n\u221a\\nlog 1\\n\u03b4\\n2m , (9.8)214 Ranking\\nwhere \u03bei =m a x\\n(\\n1 \u2212 yi\\n[\\nw \u00b7\\n(\\n\u03a6(x\u2032\\ni) \u2212 \u03a6(xi)\\n\u23a1]\\n, 0\\n\u23a1\\n', 'for all i \u2208 [1,m], and where\\n\u03a6 : X\u2192 H is a feature mapping associated to a PDS kernelK. An algorithm based\\non this theoretical guarantee consists of minimizing the right-hand side of (9.8),\\nthat is mi', 'nimizing an objective function with a term corresponding to the sum of\\nthe slack variables \u03be\\ni, and another one minimizing \u2225w\u2225 or equivalently \u2225w\u22252.I t s\\noptimization problem can thus be formulated as', '\\nmin\\nw,\u03be\\n1\\n2 \u2225w\u22252 + C\\nm\u2211\\ni=1\\n\u03bei (9.9)\\nsubject to: yi\\n[\\nw \u00b7\\n(\\n\u03a6(x\u2032\\ni) \u2212 \u03a6(xi)\\n\u23a1]\\n\u2265 1 \u2212 \u03bei\\n\u03bei \u2265 0, \u2200i \u2208 [1,m] .\\nThis coincides exactly with the primal optimization problem of SVMs, with a feature\\nmappin', 'g \u03a8 : X\u00d7 X \u2192 H de\ufb01ned by \u03a8(x, x\u2032)= \u03a6(x\u2032) \u2212\u03a6(x) for all (x, x\u2032) \u2208X\u00d7 X ,\\nand with a hypothesis set of functions of the form ( x, x\u2032) \u21a6\u2192 w \u00b7 \u03a8(x, x\u2032). Thus,\\nclearly, all the properties already presented ', 'for SVMs apply in this instance. In\\nparticular, the algorithm can bene\ufb01t from the use of PDS kernels. Problem (9.9)\\nadmits an equivalent dual that can be expressed in terms of the kernel matrix K\\n\u2032\\nde', '\ufb01ned by\\nK\u2032\\nij = \u03a8(xi,x \u2032\\ni) \u00b7\u03a8(xj,x \u2032\\nj)= K(xi,xj)+ K(x\u2032\\ni,x \u2032\\nj\\n) \u2212 K(x\u2032\\ni,xj) \u2212 K(xi,x \u2032\\nj), (9.10)\\nfor all i, j \u2208 [1,m]. This algorithm can provide an e\ufb00ective solution for pairwise\\nranking in prac', 'tice. The algorithm can also be used and extended to the case where\\nthe labels are in {\u22121, 0, +1}. The next section presents an alternative algorithm for\\nranking in the score-based setting.\\n9.4 RankBo', 'ost\\nThis section presents a boosting algorithm for pairwise ranking,RankBoost,s i m i l a r\\nto the AdaBoost algorithm for binary classi\ufb01cation. RankBoost is based on ideas\\nanalogous to those discussed', ' for classi\ufb01cation: it consists of combining di\ufb00erent\\nbase rankers to create a more accurate predictor. The base rankers are hypotheses\\nreturned by a weak learning algorithm for ranking. As for classi', '\ufb01cation, these\\nbase hypotheses must satisfy a minimal accuracy condition that will be described\\nprecisely later.\\nLet H denote the hypothesis set from which the base rankers are selected.\\nAlgorithm 9.1', ' gives the pseudocode of the RankBoost algorithm whenH is a set of9.4 RankBoost 215\\nRankBoost(S =( (x1,x \u2032\\n1,y1) ..., (xm,x \u2032\\nm,y m)))\\n1 for i \u2190 1 to m do\\n2 D1(i) \u2190 1\\nm\\n3 for t \u2190 1 to T do\\n4 ht \u2190 base', ' ranker in H with smallest \u03f5\u2212\\nt \u2212 \u03f5+\\nt = \u2212 E\\ni\u223cDt\\n[\\nyi\\n(\\nht(x\u2032\\ni) \u2212 ht(xi)\\n\u23a1]\\n5 \u03b1t \u2190 1\\n2 log \u03f5+\\nt\\n\u03f5\u2212\\nt\\n6 Zt \u2190 \u03f50\\nt +2 [\u03f5+\\nt \u03f5\u2212\\nt\\n]\\n1\\n2 \u22bf normalization factor\\n7 for i \u2190 1 to m do\\n8 Dt+1(i) \u2190\\nDt(i)e x p', '\\n[\\n\u2212\u03b1tyi\\n(\\nht(x\u2032\\ni)\u2212ht(xi)\\n\u23a1]\\nZt\\n9 g \u2190 \u2211T\\nt=1 \u03b1tht\\n10 return g\\nFigure 9.1 RankBoost algorithm for H \u2286{ 0, 1}X .\\nfunctions mapping from X to {0, 1}. For any s \u2208{ \u22121, 0, +1},w ed e \ufb01 n e\u03f5s\\nt by\\n\u03f5s\\nt =\\nm', '\u2211\\ni=1\\nDt(i)1yi(ht(x\u2032\\ni)\u2212ht(xi))=s =E\\ni\u223cDt\\n[1yi(ht(x\u2032\\ni)\u2212ht(xi))=s], (9.11)\\nand simplify the notation \u03f5+1\\nt into \u03f5+\\nt and similarly write \u03f5\u2212\\nt instead of \u03f5\u22121\\nt .W i t h\\nthese de\ufb01nitions, clearly the fo', 'llowing equality holds: \u03f50\\nt + \u03f5+\\nt + \u03f5\u2212\\nt =1 .\\nThe algorithm takes as input a labeled sampleS =\\n(\\n(x1,x \u2032\\n1,y1),..., (xm,x \u2032\\nm,y m)\\n\u23a1\\nwith elements in X\u00d7 X\u00d7 { \u2212 1,0, +1}, and maintains a distribution', ' over the subset\\nof the indices i \u2208{ 1,...,m } for which yi \u0338= 0. To simplify the presentation, we will\\nassume that yi \u0338=0f o ra l l i \u2208{ 1,...,m } and consider distributions de\ufb01ned over\\n{1,...,m }. T', 'his can be guaranteed by simply \ufb01rst removing from the sample the\\npairs labeled with zero.\\nInitially (lines 1\u20132), the distribution is uniform (D1) .A te a c hr o u n do fb o o s t i n g ,\\nthat is at e', 'ach iteration t \u2208 [1,T ] of the loop 3\u20138, a new base ranker ht \u2208 H is\\nselected with the smallest di\ufb00erence\u03f5\u2212\\nt \u2212 \u03f5+\\nt , that is one with the smallest pairwise\\nmisranking error and largest correct pair', 'wise ranking accuracy for the distribution\\nDt:\\nht \u2208 argmin\\nh\u2208H\\n{\\n\u2212 E\\ni\u223cDt\\n[\\nyi\\n(\\nh(x\u2032\\ni) \u2212 h(xi)\\n\u23a1]}\\n.\\nNote that \u03f5\u2212\\nt \u2212 \u03f5+\\nt = \u03f5\u2212\\nt \u2212 (1 \u2212 \u03f5\u2212\\nt \u2212 \u03f50\\nt )=2 \u03f5\u2212\\nt + \u03f50\\nt \u2212 1. Thus, \ufb01nding the smallest216', ' Ranking\\ndi\ufb00erence \u03f5\u2212\\nt \u2212\u03f5+\\nt is equivalent to seeking the smallest 2\u03f5\u2212\\nt +\u03f50\\nt , which itself coincides\\nwith seeking the smallest \u03f5\u2212\\nt when \u03f50\\nt =0 . Zt is simply a normalization factor to\\nensure tha', 't the weights Dt+1(i) sum to one. RankBoost relies on the assumption\\nthat at each roundt \u2208 [1,T ], for the hypothesisht found, the inequality\u03f5+\\nt \u2212 \u03f5\u2212\\nt > 0\\nholds; thus, the probability mass of the pa', 'irs correctly ranked byht (ignoring pairs\\nwith label zero) is larger than that of misranked pairs. We denote by\u03b3t the edge of\\nthe base ranker ht: \u03b3t = \u03f5+\\nt \u2212\u03f5\u2212\\nt\\n2 .\\nThe precise reason for the de\ufb01niti', 'on of the coe\ufb03cient \u03b1t (line 5) will become\\nclear later. For now, observe that if \u03f5+\\nt \u2212 \u03f5\u2212\\nt > 0, then \u03f5+\\nt /\u03f5\u2212\\nt\\n> 1a n d \u03b1t > 0.\\nThus, the new distribution Dt+1 is de\ufb01ned from Dt by increasing the ', 'weight on\\ni if the pair ( xi,x \u2032\\ni)i sm i s r a n k e d(yi(ht(x\u2032\\ni) \u2212 ht(xi) < 0), and, on the contrary,\\ndecreasing it if ( xi,x \u2032\\ni) is ranked correctly ( yi(ht(x\u2032\\ni) \u2212 ht(xi) > 0). The relative\\nweig', 'ht is unchanged for a pair with ht(x\u2032\\ni) \u2212 ht(xi) = 0. This distribution update\\nhas the e\ufb00ect of focusing more on misranked points at the next round of boosting.\\nAfter T rounds of boosting, the hypoth', 'esis returned by RankBoost is g,w h i c hi s\\na linear combination of the base classi\ufb01ers ht.T h ew e i g h t\u03b1t assigned to ht in that\\nsum is a logarithmic function of the ratio of \u03f5+\\nt and \u03f5\u2212\\nt . Thus', ', more accurate base\\nrankers are assigned a larger weight in that sum.\\nFor anyt \u2208 [1,T ], we will denote bygt the linear combination of the base rankers\\nafter t rounds of boosting: gt = \u2211t\\ns=1 \u03b1tht.I ', 'np a r t i c u l a r ,w eh a v egT = g.T h e\\ndistribution Dt+1 can be expressed in terms ofgt and the normalization factors Zs,\\ns \u2208 [1,t ], as follows:\\n\u2200i \u2208 [1,m],D t+1(i)= e\u2212yi(gt(x\u2032\\ni))\u2212gt(xi))\\nm \u220ft', '\\ns=1 Zs\\n. (9.12)\\nWe will make use of this identity several times in the proofs of the following sections.\\nIt can be shown straightforwardly by repeatedly expanding the de\ufb01nition of the\\ndistribution ov', 'er the point x\\ni:\\nDt+1(i)= Dt(i)e\u2212\u03b1tyi(ht(x\u2032\\ni)\u2212ht(xi))\\nZt\\n= Dt\u22121(i)e\u2212\u03b1t\u2212 1yi(ht\u2212 1(x\u2032\\ni)\u2212ht\u2212 1(xi))e\u2212\u03b1tyi(ht(x\u2032\\ni)\u2212ht(xi))\\nZt\u22121Zt\\n= e\u2212yi\\nPt\\ns=1 \u03b1s(hs(x\u2032\\ni)\u2212hs(xi))\\nm \u220ft\\ns=1 Zs\\n.\\n9.4.1 Bound on the em', 'pirical error\\nWe \ufb01rst show that the empirical error of RankBoost decreases exponentially fast\\nas a function of the number of rounds of boosting when the edge \u03b3t of each base9.4 RankBoost 217\\nranker ht', ' is lower bounded by some positive value \u03b3> 0.\\nTheorem 9.2\\nThe empirical error of the hypothesish: X\u2192 { 0, 1} returned by RankBoost veri\ufb01es:\\n\u02c6R(h) \u2264 exp\\n[\\n\u2212 2\\nT\u2211\\nt=1\\n(\u03f5+\\nt \u2212 \u03f5\u2212\\nt\\n2\\n\u23a12]\\n. (9.13)\\nFurthe', 'rmore, if there exists \u03b3 such that for all t \u2208 [1,T ], 0 <\u03b3 \u2264 \u03f5+\\nt \u2212\u03f5\u2212\\nt\\n2 ,t h e n\\n\u02c6R(h) \u2264 exp(\u22122\u03b32T) . (9.14)\\nProof Using the general inequality 1 u\u22640 \u2264 exp(\u2212u) valid for all u \u2208 R and\\nidentity 9.12', ', we can write:\\n\u02c6R(h)= 1\\nm\\nm\u2211\\ni=1\\n1yi(g(x\u2032\\ni)\u2212g(xi))\u22640 \u2264 1\\nm\\nm\u2211\\ni=1\\ne\u2212yi(g(x\u2032\\ni)\u2212g(xi))\\n\u2264 1\\nm\\nm\u2211\\ni=1\\n[\\nm\\nT\u220f\\nt=1\\nZt\\n]\\nDT+1(i)=\\nT\u220f\\nt=1\\nZt.\\nBy the de\ufb01nition of normalization factor, for all t \u2208 [1,T ], w', 'e have Zt =\u2211m\\ni=1 Dt(i)e\u2212\u03b1tyi(ht(x\u2032\\ni)\u2212ht(xi)).B yg r o u p i n gt o g e t h e rt h ei n d i c e si for which\\nyi(ht(x\u2032\\ni) \u2212 ht(xi)) takes the values in +1, \u22121, or 0, Zt c a nb er e w r i t t e na s\\nZt', ' = \u03f5+\\nt e\u2212\u03b1t + \u03f5\u2212\\nt e\u03b1t + \u03f50\\nt = \u03f5+\\nt\\n\u221a\\n\u03f5\u2212\\nt\\n\u03f5+\\nt\\n+ \u03f5\u2212\\nt\\n\u221a\\n\u03f5+\\nt\\n\u03f5\u2212\\nt\\n+ \u03f50\\nt =2\\n\u221a\\n\u03f5+\\nt \u03f5\u2212\\nt\\n+ \u03f50\\nt .\\nSince \u03f5+\\nt =1 \u2212 \u03f5\u2212\\nt \u2212 \u03f50\\nt ,w eh a v e\\n4\u03f5+\\nt \u03f5\u2212\\nt\\n=( \u03f5+\\nt + \u03f5\u2212\\nt )2 \u2212 (\u03f5+\\nt \u2212 \u03f5\u2212\\nt )2 =( 1 \u2212 \u03f50\\nt )', '2 \u2212 (\u03f5+\\nt \u2212 \u03f5\u2212\\nt )2.\\nThus, assuming that \u03f50\\nt < 1, Zt can be upper bounded as follows:\\nZt =\\n\u221a\\n(1 \u2212 \u03f50\\nt )2 \u2212 (\u03f5+\\nt \u2212 \u03f5\u2212\\nt )2 + \u03f50\\nt\\n=( 1 \u2212 \u03f50\\nt )\\n\u221a\\n1 \u2212 (\u03f5+\\nt \u2212 \u03f5\u2212\\nt )2\\n(1 \u2212 \u03f50\\nt )2 + \u03f50\\nt\\n\u2264 (1 \u2212 \u03f50\\nt ', ')e x p\\n(\\n\u2212 (\u03f5+\\nt \u2212 \u03f5\u2212\\nt )2\\n2(1 \u2212 \u03f50\\nt )2\\n\u23a1\\n+ \u03f50\\nt\\n\u2264 exp\\n(\\n\u2212 (\u03f5+\\nt \u2212 \u03f5\u2212\\nt )2\\n2(1 \u2212 \u03f50\\nt )\\n\u23a1\\n\u2264 exp\\n(\\n\u2212 (\u03f5+\\nt \u2212 \u03f5\u2212\\nt )2\\n2\\n\u23a1\\n\u2264 exp\\n(\\n\u22122[(\u03f5+\\nt \u2212 \u03f5\u2212\\nt )/2]2\u23a1\\n,\\nwhere we used for the \ufb01rst inequality the iden', 'tity 1 \u2212 x \u2264 e\u2212x valid for all x \u2208 R218 Ranking\\nand for the second inequality the convexity of the exponential function and the fact\\nthat 0 < 1 \u2212 \u03f50\\nt \u2264 1. This upper bound on Zt also trivially holds ', 'when \u03f50\\nt =1s i n c e\\nin that case \u03f5+\\nt = \u03f5\u2212\\nt = 0. This concludes the proof.\\nA sc a nb es e e nf r o mt h ep r o o fo ft h et h e o r e m ,t h ew e a kr a n k i n ga s s u m p t i o n\\n\u03b3 \u2264 \u03f5+\\nt \u2212\u03f5\u2212\\nt\\n', '2 with \u03b3> 0 can be replaced with the somewhat weaker requirement\\n\u03b3 \u2264 \u03f5+\\nt \u2212\u03f5\u2212\\nt\\n2\\n\u221a\\n1\u2212\u03f50\\nt\\n,w i t h\u03f50\\nt \u0338=1 ,w h i c hc a nb er e w r i t t e na s\u03b3 \u2264 1\\n2\\n\u03f5+\\nt \u2212\u03f5\u2212\\nt\u221a\\n\u03f5+\\nt\\n+\u03f5\u2212\\nt\\n,w i t h\u03f5+\\nt +\u03f5\u2212\\nt \u0338=0 ', ',\\nwhere the quantity \u03f5+\\nt \u2212\u03f5\u2212\\nt\u221a\\n\u03f5+\\nt\\n+\u03f5\u2212\\nt\\ncan be interpreted as a (normalized) relative di\ufb00erence\\nbetween \u03f5+\\nt and \u03f5\u2212\\nt .\\nThe proof of the theorem also shows that the coe\ufb03cient\u03b1t is selected to mini', 'mize\\nZt. Thus, overall, these coe\ufb03cients are chosen to minimize the upper bound on\\nthe empirical error \u220fT\\nt=1 Zt, as for AdaBoost. The RankBoost algorithm can be\\ngeneralized in several ways:\\ninstead o', 'f a hypothesis with minimal di\ufb00erence\u03f5\u2212\\nt \u2212 \u03f5+\\nt , ht can be more generally\\na base ranker returned by a weak ranking algorithm trained on Dt with \u03f5+\\nt >\u03f5 \u2212\\nt\\n;\\nthe range of the base rankers could be [', '0, +1], or more generallyR. The coe\ufb03cients\\n\u03b1t can then be di\ufb00erent and may not even admit a closed form. However, in general,\\nthey are chosen to minimize the upper bound \u220fT\\nt=1 Zt on the empirical err', 'or.\\n9.4.2 Relationship with coordinate descent\\nRankBoost coincides with the application of the coordinate descent technique\\nto a convex and di\ufb00erentiable objective function F de\ufb01ned for all samples S ', '=(\\n(x\\n1,x \u2032\\n1,y1),..., (xm,x \u2032\\nm,y m)\\n\u23a1\\n\u2208X\u00d7X\u00d7{ \u2212 1,0, +1} and \u03b1 =( \u03b11,...,\u03b1 n) \u2208 Rn,\\nn \u2265 1b y\\nF(\u03b1)=\\nm\u2211\\ni=1\\ne\u2212yi[gn(x\u2032\\ni)\u2212gn(xi)] =\\nm\u2211\\ni=1\\ne\u2212yi\\nPn\\nt=1 \u03b1t[ht(x\u2032\\ni)\u2212ht(xi)] , (9.15)\\nwhere gn = \u2211n\\nt=1 \u03b1th', 't. This loss function is a convex upper bound on the zero-one\\npairwise loss function \u03b1 \u21a6\u2192 \u2211m\\ni=1 1yi[gn(x\u2032\\ni)\u2212gn(xi)]\u22640, which is not convex. Let et\\ndenote the unit vector corresponding to thetth coor', 'dinate inRn and let\u03b1t\u22121 denote\\nthe vector based on the (t \u2212 1) \ufb01rst coe\ufb03cients, i.e. \u03b1t\u22121 =( \u03b11,...,\u03b1 t\u22121, 0,..., 0)\u22a4\\nif t \u2212 1 > 0, \u03b1t\u22121 = 0 otherwise. At each iteration t \u2265 1, the direction et select', 'ed\\nby coordinate descent is the one minimizing the directional derivative:\\net =a r g m i n\\nt\\ndF(\u03b1t\u22121 + \u03b7et)\\nd\u03b7\\n\u23d0\u23d0\u23d0\\n\u23d0\\n\u03b7=0\\n.9.4 RankBoost 219\\nSince F(\u03b1t\u22121 + \u03b7et)= \u2211m\\ni=1 e\u2212yi\\nPt\u2212 1\\ns=1 \u03b1s(hs(x\u2032\\ni)\u2212hs(xi', '))\u2212\u03b7yi(ht(x\u2032\\ni)\u2212ht(xi)), the direc-\\ntional derivative along et can be expressed as follows:\\ndF(\u03b1t\u22121 + \u03b7et)\\nd\u03b7\\n\u23d0\u23d0\\n\u23d0\\n\u23d0\\n\u03b7=0\\n= \u2212\\nm\u2211\\ni=1\\nyi(ht(x\u2032\\ni) \u2212 ht(xi)) exp\\n[\\n\u2212 yi\\nt\u22121\u2211\\ns=1\\n\u03b1s(hs(x\u2032\\ni) \u2212 hs(xi))\\n]\\n= ', '\u2212\\nm\u2211\\ni=1\\nyi(ht(x\u2032\\ni) \u2212 ht(xi))Dt(i)\\n[\\nm\\nt\u22121\u220f\\ns=1\\nZs\\n]\\n= \u2212\\n[ m\u2211\\ni=1\\nDt(i)1yi(ht(x\u2032\\ni)\u2212ht(xi))=+1 \u2212\\nm\u2211\\ni=1\\nDt(i)1yi(ht(x\u2032\\ni)\u2212ht(xi))=\u22121\\n][\\nm\\nt\u22121\u220f\\ns=1\\nZs\\n]\\n= \u2212[\u03f5+\\nt \u2212 \u03f5\u2212\\nt ]\\n[\\nm\\nt\u22121\u220f\\ns=1\\nZs\\n]\\n.\\nThe \ufb01rst ', 'equality holds by di\ufb00erentiation and evaluation at \u03b7 = 0 and the second\\none follows from (9.12). In view of the \ufb01nal equality, since m \u220ft\u22121\\ns=1 Zs is \ufb01xed,\\nthe direction et selected by coordinate desc', 'ent is the one minimizing \u03f5t,w h i c h\\ncorresponds exactly to the base ranker ht selected by RankBoost.\\nThe step size \u03b7 is identi\ufb01ed by setting the derivative to zero in order to minimize\\nthe function', ' in the chosen directionet. Thus, using identity 9.12 and the de\ufb01nition\\nof \u03f5t,w ec a nw r i t e :\\ndF(\u03b1t\u22121 + \u03b7et)\\nd\u03b7 =0\\n\u21d4\u2212\\nm\u2211\\ni=1\\nyi(ht(x\u2032\\ni) \u2212 ht(xi))e\u2212yi\\nPt\u2212 1\\ns=1 \u03b1s(hs(x\u2032\\ni)\u2212hs(xi))e\u2212\u03b7yi(ht(x\u2032\\ni)\u2212h', 't(xi)) =0\\n\u21d4\u2212\\nm\u2211\\ni=1\\nyi(ht(x\u2032\\ni) \u2212 ht(xi))Dt(i)\\n[\\nm\\nt\u22121\u220f\\ns=1\\nZs\\n]\\ne\u2212\u03b7yi(ht(x\u2032\\ni)\u2212ht(xi)) =0\\n\u21d4\u2212\\nm\u2211\\ni=1\\nyi(ht(x\u2032\\ni) \u2212 ht(xi))Dt(i)e\u2212\u03b7yi(ht(x\u2032\\ni)\u2212ht(xi)) =0\\n\u21d4\u2212 [\u03f5+\\nt e\u2212\u03b7 \u2212 \u03f5\u2212\\nt e\u03b7]=0\\n\u21d4 \u03b7= 1\\n2 log \u03f5+\\nt\\n\u03f5\u2212\\n', 't\\n.\\nThis proves that the step size chosen by coordinate descent matches the base\\nranker weight \u03b1t of RankBoost. Thus, coordinate descent applied to F precisely\\ncoincides with the RankBoost algorithm. ', 'As in the classi\ufb01cation case, other convex\\nloss functions upper bounding the zero-one pairwise misranking loss can be used.220 Ranking\\nIn particular, the following objective function based on the logi', 'stic loss can be\\nused: \u03b1 \u21a6\u2192 \u2211m\\ni=1 log(1 + e\u2212yi[gn(x\u2032\\ni)\u2212gn(xi)]) to derive an alternative boosting-type\\nalgorithm.\\n9.4.3 Margin bound for ensemble methods in ranking\\nTo simplify the presentation, we ', 'will assume for the results of this section, as\\nin section 9.2, that the pairwise labels are in {\u22121, +1}. By theorem 6.2, the\\nempirical Rademacher complexity of the convex hull conv( H)e q u a l st h ', 'a to fH.\\nThus, theorem 9.1 immediately implies the following guarantee for ensembles of\\nhypotheses in ranking.\\nCorollary 9.2\\nLet H be a set of real-valued functions. Fix \u03c1> 0;t h e n ,f o ra n y\u03b4> 0,w', ' i t h\\nprobability at least 1 \u2212 \u03b4 over the choice of a sample S of size m,e a c ho ft h e\\nfollowing ranking guarantees holds for all h \u2208 conv(H):\\nR(h) \u2264 \u02c6R\\n\u03c1(h)+ 2\\n\u03c1\\n(\\nRD1\\nm (H)+ RD2\\nm (H)\\n\u23a1\\n+\\n\u221a\\nlog 1', '\\n\u03b4\\n2m (9.16)\\nR(h) \u2264 \u02c6R\u03c1(h)+ 2\\n\u03c1\\n(\u02c6RS1 (H)+ \u02c6RS2 (H)\\n\u23a1\\n+3\\n\u221a\\nlog 2\\n\u03b4\\n2m . (9.17)\\nFor RankBoost, these bounds apply to g/\u2225\u03b1\u22251,w h e r eg is the hypothesis returned\\nby the algorithm. Since g and g/\u2225\u03b1\u22251 in', 'duce the same ordering of the points, for\\nany \u03b4> 0, the following holds with probability at least 1 \u2212 \u03b4:\\nR(g) \u2264 \u02c6R\u03c1(g/\u2225\u03b1\u22251)+ 2\\n\u03c1\\n(\\nRD1\\nm (H)+ RD2\\nm (H)\\n\u23a1\\n+\\n\u221a\\nlog 1\\n\u03b4\\n2m (9.18)\\nRemarkably, the number o', 'f rounds of boosting T does not appear in this bound.\\nThe bound depends only on the margin \u03c1, the sample size m, and the Rademacher\\ncomplexity of the family of base classi\ufb01ers H. Thus, the bound guara', 'ntees an e\ufb00ec-\\ntive generalization if the pairwise margin loss \u02c6R\u03c1(g/\u2225\u03b1\u22251) is small for a relatively\\nlarge \u03c1. A bound similar to that of theorem 6.3 for AdaBoost can be derived for the\\nempirical pairw', 'ise ranking margin loss of RankBoost (see exercise 9.3) and similar\\ncomments on that result apply here.\\nThese results provide a margin-based analysis in support of ensemble methods\\nin ranking and Rank', 'Boost in particular. As in the case of AdaBoost, however,\\nRankBoost in general does not achieve a maximum margin. But, in practice, it has\\nbeen observed to obtain excellent pairwise ranking performanc', 'es.9.5 Bipartite ranking 221\\n9.5 Bipartite ranking\\nThis section examines an important ranking scenario within the score-based setting,\\nthe bipartite ranking problem.I nt h i ss c e n a r i o ,t h es e', ' to fp o i n t sX is partitioned\\ninto two classes: X+ the class of positive points, and X\u2212 that of negative ones. The\\nproblem consists of ranking positive points higher than negative ones. For example', ',\\nfor a \ufb01xed search engine query, the task consists of ranking relevant (positive)\\ndocuments higher than irrelevant (negative) ones.\\nThe bipartite problem could be treated in the way already discussed', ' in the\\nprevious sections with exactly the same theory and algorithms. However, the setup\\ntypically adopted for this problem is di\ufb00erent: instead of assuming that the learner\\nreceives a sample of rand', 'om pairs, here pairs of positive and negative elements, it\\nis assumed that he receives a sample of positive points from some distribution and\\na sample of negative points from another. This leads to th', 'e set of all pairs made of\\na positive point of the \ufb01rst sample and a negative point of the second.\\nMore formally, the learner receives a sample S\\n+ =( x\u2032\\n1,...,x \u2032\\nm) drawn i.i.d.\\naccording to some di', 'stribution D+ over X+, and a sample S\u2212 =( x1,...,x n)d r a w n\\ni.i.d. according to some distribution D\u2212 over X\u2212 .1 Given a hypothesis set H of\\nfunctions mapping X to R, the learning problem consists o', 'f selecting a hypothesis\\nh \u2208 H with small expected bipartite misranking or generalization error R(h):\\nR(h)= P r\\nx\u223cD\u2212\\nx\u2032\u223cD+\\n[h(x\u2032) <h (x)]. (9.19)\\nThe empirical pairwise misranking or empirical error o', 'f h is denoted by \u02c6R(h)a n d\\nde\ufb01ned by\\n\u02c6R(h)= 1\\nmn\\nm\u2211\\ni=1\\nn\u2211\\nj=1\\n1h(x\u2032\\ni)<h(xj) . (9.20)\\nNote that while the bipartite ranking problem bears some similarity with binary\\nclassi\ufb01cation, in particular, t', 'he presence of two classes, they are distinct problems,\\nsince their objectives and measures of success clearly di\ufb00er.\\n1. This two-distribution formulation also avoids a potential dependency issue that', ' can\\narise for some modeling of the problem: if pairs are drawn according to some distribution\\nD over X\\n\u2212 \u00d7X + and the learner makes use of this information to augment his training\\nsample, then the re', 'sulting sample is in general not i.i.d. This is because if ( x1,x \u2032\\n1)a n d\\n(x2,x \u2032\\n2) are in the sample, then so are the pairs (x1,x \u2032\\n2) and (x2,x \u2032\\n1) and thus the pairs are\\nnot independent. Howeve', 'r, without sample augmentation, the points are i.i.d., and this\\nissue does not arise.222 Ranking\\nBy the de\ufb01nition of the formulation of the bipartite ranking just presented, the\\nlearning algorithm mus', 't typically deal withmn pairs. For example, the application\\nof SVMs to ranking in this scenario leads to an optimization withmn slack variables\\nor constraints. With just a thousand positive and a thou', 'sand negative points,\\none million pairs would need to be considered. This can lead to a prohibitive\\ncomputational cost for some learning algorithms. The next section shows that\\nRankBoost admits an e\ufb03c', 'ient implementation in the bipartite scenario.\\n9.5.1 Boosting in bipartite ranking\\nThis section shows the e\ufb03ciency of RankBoost in the bipartite scenario and dis-\\ncusses the connection between AdaBoos', 't and RankBoost in this context.\\nThe key property of RankBoost leading to an e\ufb03cient algorithm in the bipartite\\nsetting is the fact that its objective function is based on the exponential function.\\nAs', ' a result, it can be decomposed into the product of two functions, one depending\\non only the positive and the other on only the negative points. Similarly, the\\ndistribution D\\nt maintained by the algor', 'ithm can be factored as the product of\\ntwo distributions D+\\nt and D\u2212\\nt . This is clear for the uniform distribution D1 at the\\n\ufb01rst round as for any i \u2208 [1,m]a n dj \u2208 [1,n ], D1(i, j)=1 /(mn)= D+\\n1 (i)', 'D\u2212\\n1 (j)\\nwith D+\\n1 (i)=1 /m and D\u2212\\n1 (j)=1 /n. This property is recursively preserved since,\\nin view of the following, the decomposition of Dt implies that of Dt+1 for any\\nt \u2208 [1,T ]. For any i \u2208 [1,m', ']a n d j \u2208 [1,n ], by de\ufb01nition of the update, we can\\nwrite:\\nDt+1(i, j)= Dt(i, j)e\u2212\u03b1t[ht(x\u2032\\ni)\u2212ht(xj)]\\nZt\\n= D+\\nt (i)e\u2212\u03b1tht(x\u2032\\ni)\\nZt,+\\nD\u2212\\nt (j)e\u03b1tht(xj)\\nZt,\u2212\\n,\\nsince the normalization factor Zt can als', 'o be decomposed as Zt = Z\u2212\\nt Z+\\nt ,w i t h\\nZ+\\nt = \u2211m\\ni=1 D+\\nt (i)e\u2212\u03b1tht(x\u2032\\ni) and Z\u2212\\nt = \u2211n\\nj=1 D\u2212\\nt (j)e\u03b1tht(xj).F u r t h e r m o r e ,t h e\\npairwise misranking of a hypothesis h \u2208 H based on the di', 'stribution Dt used to\\ndetermine ht can also be computed as the di\ufb00erence of two quantities, one depending\\nonly on positive points, the other only on negative ones:\\nE\\n(i,j)\u223cDt\\n[h(x\u2032\\ni) \u2212 h(xj)] = E\\ni\u223cD', '+\\nt\\n[E\\nj\u223cD\u2212\\nt\\n[h(x\u2032\\ni) \u2212 h(xj)]] = E\\ni\u223cD+\\nt\\n[h(x\u2032\\ni)] \u2212 E\\nj\u223cD\u2212\\nt\\n[h(xj)].\\nThus, the time and space complexity of RankBoost depends only on the total\\nnumber of pointsm+n and not the number of pairsmn. ', 'More speci\ufb01cally, ignoring\\nthe call to the weak ranker or the cost of determining ht, the time and space\\ncomplexity of each round is linear, that is, in O(m + n). Furthermore, the cost of\\ndetermining ', 'ht depends only on O(m + n) and not on O(mn). Figure 9.2 gives the\\npseudocode of the algorithm adapted to the bipartite scenario.\\nIn the bipartite scenario, a connection can be made between the classi', '\ufb01cation algo-9.5 Bipartite ranking 223\\nBipartiteRankBoost(S =( x\u2032\\n1,...,x \u2032\\nm,x1,...,x n))\\n1 for j \u2190 1 to m do\\n2 D+\\n1 (j) \u2190 1\\nm\\n3 for i \u2190 1 to n do\\n4 D\u2212\\n1 (i) \u2190 1\\nn\\n5 for t \u2190 1 to T do\\n6 ht \u2190 base ran', 'ker in H with smallest \u03f5\u2212\\nt \u2212 \u03f5+\\nt =E\\nj\u223cD\u2212\\nt\\n[h(xj)] \u2212 E\\ni\u223cD+\\nt\\n[h(x\u2032\\ni)]\\n7 \u03b1t \u2190 1\\n2 log \u03f5+\\nt\\n\u03f5\u2212\\nt\\n8 Z+\\nt \u2190 1 \u2212 \u03f5+\\nt +\\n\u221a\\n\u03f5+\\nt \u03f5\u2212\\nt\\n9 for i \u2190 1 to m do\\n10 D+\\nt+1(i) \u2190\\nD+\\nt (i)e x p\\n[\\n\u2212\u03b1tht(x\u2032\\ni)\\n]\\nZ+\\nt', '\\n11 Z\u2212\\nt \u2190 1 \u2212 \u03f5\u2212\\nt +\\n\u221a\\n\u03f5+\\nt \u03f5\u2212\\nt\\n12 for j \u2190 1 to n do\\n13 D\u2212\\nt+1(j) \u2190\\nD\u2212\\nt (j)e x p\\n[\\n+\u03b1tht(xj)\\n]\\nZt\\n14 g \u2190 \u2211T\\nt=1 \u03b1tht\\n15 return g\\nFigure 9.2 Pseudocode of RankBoost in a bipartite setting, with H \u2286{', ' 0, 1}X ,\\n\u03f5+\\nt =E i\u223cD+\\nt\\n[h(x\u2032\\ni)] and \u03f5\u2212\\nt =E j\u223cD\u2212\\nt\\n[h(xj)].\\nrithm AdaBoost and the ranking algorithm RankBoost. In particular, the objective\\nfunction of RankBoost can be expressed as follows for an', 'y \u03b1 =( \u03b11,...,\u03b1 T ) \u2208 RT ,\\nT \u2265 1:\\nFRankBoost(\u03b1)=\\nm\u2211\\nj=1\\nn\u2211\\ni=1\\nexp(\u2212[g(x\u2032\\ni) \u2212 g(xj)])\\n=\\n( m\u2211\\ni=1\\ne\u2212 PT\\nt=1 \u03b1tht(x\u2032\\ni)\\n\u23a1( n\u2211\\nj=1\\ne+ PT\\nt=1 \u03b1tht(xj)\\n\u23a1\\n= F+(\u03b1)F\u2212 (\u03b1),\\nwhere F+ denotes the function de\ufb01ne', 'd by the sum over the positive points and F\u2212\\nthe function de\ufb01ned over the negative points. The objective function of AdaBoost224 Ranking\\ncan be de\ufb01ned in terms of these same two functions as follows:\\n', 'FAdaBoost(\u03b1)=\\nm\u2211\\ni=1\\nexp(\u2212y\u2032\\nig(x\u2032\\ni)) +\\nn\u2211\\nj=1\\nexp(\u2212yjg(xj))\\n=\\nm\u2211\\ni=1\\ne\u2212 PT\\nt=1 \u03b1tht(x\u2032\\ni) +\\nn\u2211\\nj=1\\ne+ PT\\nt=1 \u03b1tht(xj)\\n= F+(\u03b1)+ F\u2212 (\u03b1).\\nNote that the gradient of the objective function of RankBoost c', 'an be expressed in\\nterms of AdaBoost as follows:\\n\u2207\u03b1 FRankBoost(\u03b1)= F\u2212 (\u03b1)\u2207\u03b1 F+(\u03b1)+ F+(\u03b1)\u2207\u03b1 F\u2212 (\u03b1) (9.21)\\n= F\u2212 (\u03b1)(\u2207\u03b1 F+(\u03b1)+ \u2207\u03b1 F\u2212 (\u03b1)) + (F+(\u03b1) \u2212 F\u2212 (\u03b1))\u2207\u03b1 F\u2212 (\u03b1)\\n= F\u2212 (\u03b1)\u2207\u03b1 FAdaBoost(\u03b1)+( F+(\u03b1) \u2212 F\u2212 ', '(\u03b1))\u2207\u03b1 F\u2212 (\u03b1).\\nIf \u03b1 is a minimizer ofFAdaBoost,t h e n\u2207\u03b1 FAdaBoost(\u03b1)=0a n di tc a nb es h o w nt h a t\\nthe equality F+(\u03b1) \u2212 F\u2212 (\u03b1)=0a l s oh o l d sf o r\u03b1, provided that the family of base\\nhypotheses', ' H used for AdaBoost includes the constant hypothesish0 : x \u21a6\u2192 1, which\\noften is the case in practice. Then, by (9.21), this implies that\u2207\u03b1 FRankBoost(\u03b1)=0\\nand therefore that \u03b1 is also a minimizer of ', 'the convex function FRankBoost.I n\\ngeneral, FAdaBoost does not admit a minimizer. Nevertheless, it can be shown that\\nif limk\u2192\u221e FAdaBoost(\u03b1k)=i n f \u03b1 FAdaBoost(\u03b1) for some sequence ( \u03b1k)k\u2208N, then,\\nunde', 'r the same assumption on the use of a constant base hypothesis and for\\na non-linearly separable dataset, the following holds: lim k\u2192\u221e FRankBoost(\u03b1k)=\\ninf\u03b1 FRankBoost(\u03b1).\\nThe connections between AdaBoo', 'st and RankBoost just mentioned suggest that\\nAdaBoost could achieve a good ranking performance as well. This is often observed\\nempirically, a fact that brings strong support to the use of AdaBoost bot', 'h as a\\nclassi\ufb01er and a ranking algorithm. Nevertheless, RankBoost may converge faster\\nand achieve a good ranking faster than AdaBoost.\\n9.5.2 Area under the ROC curve\\nThe performance of a bipartite ran', 'king algorithm is typically reported in terms of\\nthe area under the receiver operating characteristic (ROC) curve,o rt h earea under\\nthe curve (AUC) for short.\\nLet U be a test sample used to evaluate ', 'the performance of h (or a training\\nsample) with m positive points z\\n\u2032\\n1,...,z \u2032\\nm and n negative points z1,...,z n. For any\\nh \u2208 H,l e t \u02c6R(h, U) denote the average pairwise misranking of h over U. Th', 'en, the\\nAUC of h for the sample U is precisely 1 \u2212 \u02c6R(h, U), that is, its average pairwise9.5 Bipartite ranking 225\\nFalse positive rate\\nTrue positive rate\\nAUC\\n.2\\n.4\\n0 .2 .4 .6 .8 1\\n.6\\n.8\\n1\\n0\\nFigure 9.', '3 The AUC (area under the ROC curve) is a measure of the performance\\nof a bipartite ranking.\\nranking accuracy on U:\\nAUC(h, U)= 1\\nmn\\nm\u2211\\ni=1\\nn\u2211\\nj=1\\n1h(z\u2032\\ni)\u2265h(zj) =P r\\nz\u223c bD\u2212\\nU\\nz\u2032\u223c bD+\\nU\\n[h(z\u2032) \u2265 h(z)].', '\\nHere, \u02c6D+\\nU denotes the empirical distribution corresponding to the positive points in\\nU and \u02c6D+\\nU the empirical distribution corresponding to the negative ones. AUC(h, U)\\nis thus an empirical estima', 'te of the pairwise ranking accuracy based on the sample\\nU, and by de\ufb01nition it is in [0, 1]. Higher AUC values correspond to a better ranking\\nperformance. In particular, an AUC of one indicates that t', 'he points ofU are ranked\\nperfectly using h. AUC(h, U) can be computed in linear time from a sorted array\\ncontaining them+n elements h(z\\n\u2032\\ni)a n dh(zj), fori \u2208 [1,m]a n dj \u2208 [1,n ]. Assuming\\nthat the a', 'rray is sorted in increasing order (with a positive point placed higher than a\\nnegative one if they both have the same scores) the total number of correctly ranked\\npairs r can be computed as follows. ', 'Starting with r =0 ,t h ea r r a yi si n s p e c t e di n\\nincreasing order of the indices while maintaining at any time the number of negative\\npoints seen n and incrementing the current value of r wit', 'h n whenever a positive\\npoint is found. After full inspection of the array, the AUC is given byr/(mn). Thus,\\nassuming that a comparison-based sorting algorithm is used, the complexity of the\\nc o m p u', ' t a t i o no ft h eA U Ci si nO((m + n) log(m + n)).\\nAs indicated by its name, the AUC coincides with the area under the ROC curve\\n(\ufb01gure 9.3). An ROC curve plots the true positive rate, that is, the', ' percentage of\\npositive points correctly predicted as positive as a function of the false positive\\nrate, that is, the percentage of negative points incorrectly predicted as positive.\\nFigure 9.4 illust', 'rates the de\ufb01nition and construction of an ROC curve.226 Ranking\\nFalse positive rate\\nTrue positive rate\\n.2\\n.4\\n0 .2 .4 .6 .8 1\\n.6\\n.8\\n1\\n0 h(x3) h(x1)h(x14) h(x23)h(x5)\\n\u03b8 +-\\nsorted scores\\nFigure 9.4 An e', 'xample ROC curve and illustrated threshold. Varying the value\\nof \u03b8 from one extreme to the other generates points on the curve.\\nPoints are generated along the curve by varying a threshold value \u03b8 as i', 'n the\\nright panel of \ufb01gure 9.4, from higher values to lower ones. The threshold is used to\\ndetermine the label of any point x (positive or negative) based on sgn( h(x) \u2212 \u03b8).\\nAt one extreme, all points', ' are predicted as negative; thus, the false positive rate is\\nz e r o ,b u tt h et r u ep o s i t i v er a t ei sz e r oa sw e l l .T h i sg i v e st h e\ufb01 r s tp o i n t( 0, 0) of\\nthe plot. At the othe', 'r extreme, all points are predicted as positive; thus, both the\\ntrue and the false positive rates are equal to one, which gives the point (1 , 1). In\\nt h ei d e a lc a s e ,a sa l r e a d yd i s c u s', ' s e d ,t h eA U Cv a l u ei so n e ,a n d ,w i t ht h ee x c e p t i o n\\nof (0,0), the curve coincides with a horizontal line reaching (1, 1).\\n9.6 Preference-based setting\\nThis section examines a di\ufb00', 'erent setting for the problem of learning to rank: the\\npreference-based setting.I nt h i ss e t t i n g ,t h eo b j e c t i v ei st or a n ka sa c c u r a t e l ya s\\npossible any test subsetX \u2286X , typ', 'ically a \ufb01nite set that we refer to as a\ufb01nite query\\nsubset. This is close to the query-based scenario of search engines or information\\nextraction systems and the terminology stems from the fact that X', ' could be a\\nset of items needed to rank in response to a particular query. The advantage of\\nthis setting over the score-based setting is that here the learning algorithm is not\\nrequired to return a li', 'near ordering of all points of X, which may be impossible\\nto achieve faultlessly in accordance with a general possibly non-transitive pairwise\\npreference labeling. Supplying a correct linear ordering ', 'for a query subset is more\\nlikely to be achievable exactly or at least with a better approximation.\\nThe preference-based setting consists of two stages. In the \ufb01rst stage, a sample of\\nlabeled pairs S,', ' exactly as in the score-based setting, is used to learn a preference\\nfunction h : X\u00d7 X \u21a6\u2192 [0,1], that is, a function that assigns a higher value to a\\npair (u, v)w h e nu is preferred to v or is to be', ' ranked higher than v, and smaller9.6 Preference-based setting 227\\nvalues in the opposite case. This preference function can be obtained as the output\\nof a standard classi\ufb01cation algorithm trained on ', 'S. A crucial di\ufb00erence with the\\nscore-based setting is that, in general, the preference function h is not required to\\ninduce a linear ordering. The relation it induces may be non-transitive; thus, we\\n', 'may have, for example, h(u, v)= h(v,w )= h(w,u ) = 1 for three distinct points u,\\nv,a n dw.\\nIn the second stage, given a query subsetX \u2208X , the preference functionh is used\\nto determine a ranking of X', '.H o wc a nh be used to generate an accurate ranking?\\nThis will be the main focus of this section. The computational complexity of the\\nalgorithm determining the ranking is also crucial. Here, we will ', 'measure its running\\ntime complexity in terms of the number of calls to h.\\nWhen the preference function is obtained as the output of a binary classi\ufb01cation\\nalgorithm, the preference-based setting can b', 'e viewed as a reduction of ranking to\\nclassi\ufb01cation: the second stage speci\ufb01es how a ranking is obtained from a classi\ufb01er\u2019s\\noutput.\\n9.6.1 Second-stage ranking problem\\nThe ranking problem of the second', ' stage is modeled as follows. We assume that a\\npreference function h is given. From the point of view of this stage, the way the\\nfunction h has been determined is immaterial, it can be viewed as a bla', 'ck box. As\\nalready discussed, h is not assumed to be transitive. But, we will assume that it is\\npairwise consistent,t h a ti sh(u, v)+ h(v,u )=1 ,f o ra l lu, v \u2208X .\\nLet D be an unknown distribution a', 'ccording to which pairs ( X,\u03c3\\n\u2217)a r ed r a w n\\nwhere X \u2286X is a query subset and \u03c3\u2217 a target ranking or permutation of X,\\nthat is, a bijective function from X to {1,..., |X|}. Thus, we consider a stoch', 'astic\\nscenario, and \u03c3\u2217 is a random variable. The objective of a second-stage algorithmA\\nconsists of using the preference function h to return an accurate ranking A(X)f o r\\nany query subset X. The algo', 'rithm may be deterministic, in which case A(X)i s\\nuniquely determined from X o ri tm a yb er a n d o m i z e d ,i nw h i c hc a s ew ed e n o t eb y\\ns the randomization seed it may depend on.\\nThe foll', 'owing loss function L can be used to measure the disagreement between\\nar a n k i n g\u03c3 and a desired one \u03c3\u2217 over a set X of n \u2265 1e l e m e n t s :\\nL(\u03c3, \u03c3\u2217)= 2\\nn(n \u2212 1)\\n\u2211\\nu\u0338=v\\n1\u03c3(u)<\u03c3(v)1\u03c3\u2217(v)<\u03c3\u2217(u), (9', '.22)\\nwhere the sum runs over all pairs (u, v)w i t hu and v distinct elements ofX.A l lt h e\\nresults presented in the following hold for a broader set of loss functions described\\nlater. Abusing the no', 'tation, we also de\ufb01ne the loss of the preference functionh with228 Ranking\\nrespect to a ranking \u03c3\u2217 of a set X of n \u2265 1e l e m e n t sb y\\nL(h, \u03c3\u2217)= 2\\nn(n \u2212 1)\\n\u2211\\nu\u0338=v\\nh(u, v)1\u03c3\u2217(v)<\u03c3\u2217(u). (9.23)\\nThe exp', 'ected loss for a deterministic algorithm A is thus E(X,\u03c3\u2217)\u223cD[L(A(X),\u03c3 \u2217)].\\nThe regret of algorithm A is then de\ufb01ned as the di\ufb00erence between its loss and that\\nof the best \ufb01xed global ranking. This can', ' be written as follows:\\nReg(A)= E\\n(X,\u03c3\u2217)\u223cD\\n[L(A(X),\u03c3 \u2217)] \u2212 min\\n\u03c3\u2032\\nE\\n(X,\u03c3\u2217)\u223cD\\n[L(\u03c3\u2032\\n|X,\u03c3 \u2217)], (9.24)\\nwhere \u03c3\u2032\\n|X denotes the ranking induced onX by a global ranking\u03c3\u2032 of X. Similarly,\\nwe de\ufb01ne the regr', 'et of the preference function as follows\\nReg(h)= E\\n(X,\u03c3\u2217)\u223cD\\n[L(h|X,\u03c3 \u2217)] \u2212 min\\nh\u2032\\nE\\n(X,\u03c3\u2217)\u223cD\\n[L(h\u2032\\n|X,\u03c3 \u2217)], (9.25)\\nwhere h|X denotes the restriction of h to X \u00d7 X,a n ds i m i l a r l yw i t hh\u2032.T h ', 'er e g r e t\\nresults presented in this section hold assuming the followingpairwise independence\\non irrelevant alternatives property:\\nE\\n\u03c3\u2217|X1\\n[1\u03c3\u2217(v)<\u03c3\u2217(u)]= E\\n\u03c3\u2217|X2\\n[1\u03c3\u2217(v)<\u03c3\u2217(u)], (9.26)\\nfor any u, v', ' \u2208X and any two sets X1 and X2 containing u and v.2 Similar\\nregret de\ufb01nitions can be given for a randomized algorithm additionally taking the\\nexpectation over s.\\nClearly, the quality of the ranking ou', 'tput by the second-stage algorithm inti-\\nmately depends on that of the preference functionh. In the next sections, we discuss\\nboth a deterministic and a randomized second-stage algorithm for which the', ' regret\\ncan be upper bounded in terms of the regret of the preference function.\\n2. More generally, they hold without that assumption using the following weaker notions\\nof regret:\\nReg\u2032(A)= E\\n(X,\u03c3\u2217)\u223cD\\n[', 'L(A(X),\u03c3 \u2217)] \u2212 E\\nX\\nh\\nmin\\n\u03c3\u2032\\nE\\n\u03c3\u2217|X\\n[L(\u03c3\u2032,\u03c3 \u2217)]\\ni\\n(9.27)\\nReg\u2032(h)= E\\n(X,\u03c3\u2217)\u223cD\\n[L(h|X,\u03c3 \u2217)] \u2212 E\\nX\\nh\\nmin\\nh\u2032\\nE\\n\u03c3\u2217|X\\n[L(h\u2032,\u03c3 \u2217)]\\ni\\n, (9.28)\\nwhere \u03c3\u2032 denotes a ranking of X, h\u2032 a preference function de\ufb01ned o', 'ver X \u00d7 X,a n d\u03c3\u2217|X\\nthe random variable \u03c3\u2217 conditioned on X.9.6 Preference-based setting 229\\n9.6.2 Deterministic algorithm\\nA natural deterministic algorithm for the second-stage is based on thesort-by', '-degree\\nalgorithm. This consists of ranking each element ofX based on the number of other\\nelements it is preferred to according to the preference functionh.L e tAsort-by-degree\\ndenote this algorithm. ', 'In the bipartite setting, the following bounds can be proven\\nfor the expected loss of this algorithm and its regret:\\nE\\nX,\u03c3\u2217\\n[L(Asort-by-degree(X),\u03c3 \u2217)] \u2264 2E\\nX,\u03c3\u2217\\n[L(h, \u03c3\u2217)] (9.29)\\nReg(Asort-by-degree(', 'X)) \u2264 2R e g(h) . (9.30)\\nThese results show that the sort-by-degree algorithm can achieve an accurate\\nranking when the loss or the regret of the preference function h is small. They\\nalso bound the ran', 'king loss or regret of the algorithm in terms of the classi\ufb01cation\\nloss or regret of h, which can be viewed as a guarantee for the reduction of ranking\\nto classi\ufb01cation using the sort-by-degree algori', 'thm.\\nNevertheless, in some cases, the guarantee given by these results is weak or unin-\\nformative owing to the presence of the factor of two. Consider the case of a binary\\nclassi\ufb01er h with an error ra', 'te of just 25 percent, which is quite reasonable in many\\napplications. Assume that the Bayes error is close to zero for the classi\ufb01cation prob-\\nlem and, similarly, that for the ranking problem the reg', 'ret and loss approximately\\ncoincide. Then, using the bound in (9.29) guarantees a worst-case pairwise mis-\\nranking error of at most 50 percent for the ranking algorithm, which is the pairwise\\nmisranki', 'ng error of random ranking.\\nFurthermore, the running time complexity of the algorithm quadratic, that is in\\n\u03a9(|X|\\n2)o faq u e r ys e tX, since it requires calling the preference function for every\\npai', 'r (u, v)w i t hu and v in X.\\nAs shown by the following theorem, no deterministic algorithm can improve upon\\nthe factor of two appearing in the regret guarantee of the sort-by-degree algorithm.\\nTheorem', ' 9.3 Lower bound for deterministic algorithms\\nFor any deterministic algorithm A, there is a bipartite distribution for which\\nReg(A) \u2265 2R e g(h). (9.31)\\nProof C o n s i d e rt h es i m p l ec a s ew h ', 'e r eX = X = {u, v, w} and where the\\npreference function induces a cycle as illustrated by \ufb01gure 9.5a. An arrow from\\nu to v indicates that v is preferred to u according to h. The proof is based on an\\n', 'adversarial choice of the target \u03c3\\n\u2217.\\nWithout loss of generality, either A returns the ranking u, v, w(\ufb01gure 9.5b) or\\nw,v,u (\ufb01gure 9.5c). In the \ufb01rst case, let \u03c3\u2217 be de\ufb01ned by the labeling indicated i', 'n\\nthe \ufb01gure. In that case, we have L(h, \u03c3\u2217)=1 /3, since u is preferred to w according230 Ranking\\nu\\nv w\\nh\\nu, v w\\n+- +-\\nw,v u\\n(a) (b) (c)\\nFigure 9.5 Illustration of the proof of theorem 9.3.\\nto h while ', 'w is labeled positively and u negatively. The loss of the algorithm is\\nL(A,\u03c3 \u2217)=2 /3, since both u and v are ranked higher than the positively labeledw\\nby the algorithm. Similarly, \u03c3\u2217 can be de\ufb01ned as', ' in \ufb01gure 9.5c in the second case,\\nand we \ufb01nd again that L(h, \u03c3\u2217)=1 /3a n d L(A,\u03c3 \u2217)=2 /3. This concludes the\\nproof.\\nThe theorem suggests that randomization is necessary in order to achieve a better\\ng', 'uarantee. In the next section, we present a randomized algorithm that bene\ufb01ts\\nboth from better guarantees and a better time complexity.\\n9.6.3 Randomized algorithm\\nThe general idea of the algorithm des', 'cribed in this section is to use a straightforward\\nextension of the randomized QuickSort algorithm in the second stage. Unlike in\\nthe standard version of QuickSort , here the comparison function is ba', 'sed on the\\npreference function, which in general is not transitive. Nevertheless, it can be shown\\nhere, too, that the expected time complexity of the algorithm is inO(n log n)w h e n\\na p p l i e dt oa', ' na r r a yo fs i z en.\\nThe algorithm works as follows, as illustrated by \ufb01gure 9.6. At each recursive\\nstep, a pivot element u is selected uniformly at random from X.F o re a c hv \u0338= u, v\\nis placed on', ' the left ofu with probability h(v,u ) and to its right with the remaining\\nprobability h(u, v). The algorithm proceeds recursively with the array to the left of\\nu and the one to its right and returns ', 'the concatenation of the permutation returned\\nby the left recursion, u, and the permutation returned by the right recursion.\\nLet A\\nQuickSort denote this algorithm. In the bipartite setting, the follow', 'ing\\nguarantees can be proven:\\nE\\nX,\u03c3\u2217,s\\n[L(AQuickSort(X,s ),\u03c3 \u2217)] = E\\nX,\u03c3\u2217\\n[L(h, \u03c3\u2217)] (9.32)\\nReg(AQuickSort) \u2264 Reg(h) . (9.33)\\nThus, here, the factor of two of the bounds in the deterministic case has ', 'vanished,9.6 Preference-based setting 231\\nleft recursion right recursion\\nrandom \\npivot\\nu\\nv\\nh(v,u ) h(u, v)\\nFigure 9.6 Illustration of randomized QuickSort based on a preference functionh\\n(not necessar', 'ily transitive).\\nwhich is substantially more favorable. Furthermore, the guarantee for the loss is\\nan equality. Moreover, the expected time complexity of the algorithm is only in\\nO(n log n), and, if o', 'nly the top k items are needed to be ranked, as in many\\napplications, the time complexity is reduced to O(n + k log k).\\nFor the QuickSort algorithm, the following guarantee can also be proven in the\\nc', 'ase of general ranking setting (not necessarily bipartite setting):\\nE\\nX,\u03c3\u2217,s\\n[L(AQuickSort(X,s ),\u03c3 \u2217)] \u2264 2E\\nX,\u03c3\u2217\\n[L(h, \u03c3\u2217)]. (9.34)\\n9.6.4 Extension to other loss functions\\nAll of the results just pres', 'ented hold for a broader class of loss functionsL\u03c9 de\ufb01ned\\nin terms of a weight function or emphasis function \u03c9. L\u03c9 is similar to (9.22), but\\nmeasures the weighted disagreement between a ranking\u03c3 a n d', 'ad e s i r e do n e\u03c3\u2217 over\\nas e tX of n \u2265 1e l e m e n t sa sf o l l o w s :\\nL\u03c9(\u03c3, \u03c3\u2217)= 2\\nn(n \u2212 1)\\n\u2211\\nu\u0338=v\\n\u03c9(\u03c3\u2217(v),\u03c3 \u2217(u)) 1\u03c3(u)<\u03c3(v) 1\u03c3\u2217(v)<\u03c3\u2217(u), (9.35)\\nwhere the sum runs over all pairs ( u, v)w i t', ' hu and v distinct elements of X,a n d\\nwhere \u03c9 is a symmetric function whose properties are described below. Thus, the loss\\ncounts the number of pairwise misrankings of \u03c3 with respect to \u03c3\u2217, each weig', 'hted\\nby \u03c9.F u n c t i o n\u03c9 is assumed to satisfy the following three natural axioms:\\nsymmetry: \u03c9(i, j)= \u03c9(j, i) for all i, j;\\nmonotonicity: \u03c9(i, j) \u2264 \u03c9(i, k)i fe i t h e ri<j<k or i>j>k ;\\ntriangle ine', 'quality: \u03c9(i, j) \u2264 \u03c9(i, k)+ \u03c9(k,j ).232 Ranking\\nThe motivation for this last property stems from the following: if correctly ordering\\nitems in positions (i, k)a n d(k,j ) is not of great importance, t', 'hen the same should\\nhold for items in positions (i, j).\\nUsing di\ufb00erent functions \u03c9, the family of functions L\u03c9 can cover several familiar\\nand important losses. Here are some examples. Setting \u03c9(i, j) ', '= 1 for all i \u0338= j\\nyields the unweighted pairwise misranking measure. For a \ufb01xed integer k \u2265 1,\\nthe function \u03c9 de\ufb01ned by \u03c9(i, j)=1 ((i\u2264k)\u2228(j\u2264k))\u2227(i\u0338=j) for all ( i, j) can be used\\nto emphasize ranking', ' at the top k elements. Misranking of pairs with at least\\none element ranked among the top k is penalized by this function. This can be\\nof interest in applications such as information extraction or se', 'arch engines where\\nthe ranking of the top documents matters more. For this emphasis function, all\\nelements ranked below k are in a tie. Any tie relation can be encoded using \u03c9.\\nFinally, in a bipartite', ' ranking scenario with m\\n+ positive and m\u2212 negative points\\nand m+ + m\u2212 = n, choosing \u03c9(i, j)= n(n\u22121)\\n2m\u2212 m+ yields the standard loss function\\ncoinciding with 1 \u2212 AUC.\\n9.7 Discussion\\nThe objective func', 'tion for the ranking problems discussed in this chapter were\\nall based on pairwise misranking. Other ranking criteria have been introduced in\\ninformation retrieval and used to derive alternative ranki', 'ng algorithms. Here, we\\nbrie\ufb02y present several of these criteria.\\nPrecision, precision@n, average precision, recall. All of these criteria assume that\\npoints are partitioned into two classes (positive', 's and negatives), as in the bipar-\\ntite ranking setting. Precision is the fraction of positively predicted points that\\nare in fact positive. Whereas precision takes into account all positive predictio', 'ns,\\nprecision@n only considers the top n predictions. For example, precision@5 consid-\\ners only the top 5 positively predicted points.Average precisioninvolves computing\\nprecision@n for each value ofn', ', and averaging across these values. Each precision@n\\ncomputation can be interpreted as computing precision for a \ufb01xed value of recall,\\nor the fraction of positive points that are predicted to be posi', 'tive (recall coincides\\nwith the notion of true positive rate).\\nDCG, NDCG. These criteria assume the existence of relevance scores associated\\nwith the points to be ranked, e.g., given a web search quer', 'y, each website returned\\nby a search engine has an associated relevance score. Moreover, these criteria\\nmeasure the extent to which points with large relevance scores appear at or near the\\nbeginning o', 'f a ranking. De\ufb01ne ( c\\ni)i\u2208N as a prede\ufb01ned sequence of non-increasing\\nand non-negative discount factors, e.g., ci = log(i)\u22121. Then, given a ranking of m\\npoints and de\ufb01ning ri as the relevance score o', 'f the ith point in this ranking, the9.8 Chapter notes 233\\ndiscounted cumulative gain (DCG)is de\ufb01ned as DCG = \u2211m\\ni=1 ciri. Note that DCG\\nis an increasing function of m. In contrast, the normalized disc', 'ounted cumulative\\ngain (NDCG) normalizes the DCG across values of m by dividing the DCG by the\\nIDCG, or the ideal DCG that would result from an optimal ordering of the points.\\n9.8 Chapter notes\\nThe pr', 'oblem of learning to rank is distinct from the purely algorithmic one of rank\\naggregation, which, as shown by Dwork, Kumar, Naor, and Sivakumar [2001], is\\nNP-hard even for k = 4 rankings. The Rademach', 'er complexity and margin-based\\ngeneralization bounds for pairwise ranking given in theorem 9.1 and corollary 5.1 are\\nnovel. Margin bounds based on covering numbers were also given by Rudin, Cortes,\\nMo', 'hri, and Schapire [2005]. Other learning bounds in the score-based setting of\\nranking, including VC-dimension and stability-based learning bounds, have been\\ngiven by Agarwal and Niyogi [2005], Agarwal', ' et al. [2005] and Cortes et al. [2007b].\\nThe ranking algorithm based on SVMs presented in section 9.3 has been used and\\ndiscussed by several researchers. One early and speci\ufb01c discussion of its use c', 'an be\\nfound in Joachims [2002]. The fact that the algorithm is simply a special instance of\\nSVMs seems not to be clearly stated in the literature. The theoretical justi\ufb01cation\\npresented here for its u', 'se in ranking is novel.\\nRankBoost was introduced by Freund et al. [2003]. The version of the algorithm\\npresented here is the coordinate descent RankBoost from Rudin et al. [2005].\\nRankBoost in general', ' does not achieve a maximum margin and may not increase\\nthe margin at each iteration. A Smooth Margin ranking algorithm [Rudin et al.,\\n2005] based on a modi\ufb01ed version of the objective function of Ran', 'kBoost can be\\nshown to increase the smooth margin at every iteration, but the comparison of\\nits empirical performance with that of RankBoost has not been reported. For the\\nempirical ranking quality of', ' AdaBoost and the connections between AdaBoost and\\nRankBoost in the bipartite, setting see Cortes and Mohri [2003] and Rudin et al.\\n[2005].\\nThe Receiver Operating Characteristics (ROC) curves were ori', 'ginally developed\\nin signal detection theory [Egan, 1975] in connection with radio signals during\\nWorld War II. They also had applications to psychophysics [Green and Swets, 1966]\\nand have been used s', 'ince then in a variety of other applications, in particular for\\nmedical decision making. The area under an ROC curve (AUC) is equivalent to the\\nWilcoxon-Mann-Whitney statistic [Hanley and McNeil, 1982', '] and is closely related\\nto the Gini index [Breiman et al., 1984] (see also chapter 8). For a statistical analysis\\nof the AUC and con\ufb01dence intervals depending on the error rate, see Cortes and\\nMohri ', '[2003, 2005]. The deterministic algorithm in the preference-based setting234 Ranking\\ndiscussed in this chapter was presented and analyzed by Balcan et al. [2008]. The\\nrandomized algorithm as well as m', 'uch of the results presented in section 9.6 are\\ndue to Ailon and Mohri [2008].\\nA somewhat related problem of ordinal regression has been studied by some\\nauthors [McCullagh, 1980, McCullagh and Nelder,', ' 1983, Herbrich et al., 2000] which\\nconsists of predicting the correct label of each item out of a \ufb01nite set, as in multi-\\nclass classi\ufb01cation, with the additional assumption of an ordering among the ', 'labels.\\nThis problem is distinct, however, from the pairwise ranking problem discussed in\\nthis chapter.\\nThe DCG ranking criterion was introduced by J\u00a8arvelin and Kek\u00a8al\u00a8ainen [2000],\\nand has been used', ' and discussed in a number of subsequent studies, in particular\\nCossock and Zhang [2008] who consider a subset ranking problem formulated in\\nterms of DCG, for which they consider a regression-based so', 'lution.\\n9.9 Exercises\\n9.1 Uniform margin-bound for ranking. Use theorem 9.1 to derive a margin-based\\nlearning bound for ranking that holds uniformly for all \u03c1> 0 (see similar binary\\nclassi\ufb01cation boun', 'ds of theorem 4.5 and exercise 4.2).\\n9.2 On-line ranking. Give an on-line version of the SVM-based ranking algorithm\\npresented in section 9.3.\\n9.3 Empirical margin loss of RankBoost. Derive an upper b', 'ound on the empirical\\npairwise ranking margin loss of RankBoost similar to that of theorem 6.3 for\\nAdaBoost.\\n9.4 Margin maximization and RankBoost. Give an example showing that Rank-\\nBoost does not ac', 'hieve the maximum margin, as in the case of AdaBoost.\\n9.5 RankPerceptron. Adapt the Perceptron algorithm to derive a pairwise ranking\\nalgorithm based on a linear scoring function. Assume that the trai', 'ning sample is\\nlinear separable for pairwise ranking. Give an upper bound on the number of updates\\nmade by the algorithm in terms of the ranking margin.\\n9.6 Margin-maximization ranking. Give a linear ', 'programming (LP) algorithm re-\\nturning a linear hypothesis for pairwise ranking based on margin maximization.\\n9.7 Bipartite ranking. Suppose that we use a binary classi\ufb01er for ranking in the9.9 Exerci', 'ses 235\\nbipartite setting. Prove that if the error of the binary classi\ufb01er is\u03f5, then that of the\\nranking it induces is also at most \u03f5. Show that the converse does not hold.\\n9.8 Multipartite ranking. C', 'onsider the ranking scenario in a k-partite setting\\nwhere X is partitioned into k subsets X1,..., Xk with k \u2265 1. The bipartite case\\n(k = 2) is already speci\ufb01cally examined in the chapter. Give a preci', 'se formulation\\nof the problem in terms of k distributions. Does RankBoost admit an e\ufb03cient\\nimplementation in this case? Give the pseudocode of the algorithm.\\n9.9 Deviation bound for the AUC. Let h be ', 'a \ufb01xed scoring function used to rank\\nt h ep o i n t so fX. Use Hoe\ufb00ding\u2019s bound to show that with high probability the AUC\\nof h for a \ufb01nite sample is close to its average.\\n9.10 k-partite weight functi', 'on. Show how the weight function \u03c9 c a nb ed e \ufb01 n e ds o\\nthat L\u03c9 encodes the natural loss function associated to ak-partite ranking scenario.10 Regression\\nThis chapter discusses in depth the learning', ' problem of regression,w h i c hc o n s i s t s\\nof using data to predict, as closely as possible, the correct real-valued labels of the\\npoints or items considered. Regression is a common task in machi', 'ne learning with a\\nvariety of applications, which justi\ufb01es the speci\ufb01c chapter we reserve to its analysis.\\nThe learning guarantees presented in the previous sections focused largely on\\nclassi\ufb01cation p', 'roblems. Here we present generalization bounds for regression, both\\nfor \ufb01nite and in\ufb01nite hypothesis sets. Several of these learning bounds are based on\\nthe familiar notion of Rademacher complexity, w', 'hich is useful for characterizing\\nthe complexity of hypothesis sets in regression as well. Others are based on a\\ncombinatorial notion of complexity tailored to regression that we will introduce,\\npseud', 'o-dimension,w h i c hc a nb ev i e w e da sa ne x t e n s i o no ft h eV C - d i m e n s i o nt o\\nregression. We describe a general technique for reducing regression problems to\\nclassi\ufb01cation and deri', 'ving generalization bounds based on the notion of pseudo-\\ndimension. We present and analyze several regression algorithms, including linear\\nregression, kernel ridge regression , support-vector regress', 'ion, Lasso,a n ds e v e r a l\\non-line versions of these algorithms. We discuss in detail the properties of these\\nalgorithms, including the corresponding learning guarantees.\\n10.1 The problem of regres', 'sion\\nWe \ufb01rst introduce the learning problem of regression. Let X denote the input space\\nand Y a measurable subset of R.W ed e n o t eb yD an unknown distribution over X\\naccording to which input points', ' are drawn and by f : X\u2192 Y the target labeling\\nfunction. This corresponds to a deterministic learning scenario that we adopt to\\nsimplify the presentation. As discussed in section 2.4.1, the determinis', 'tic scenario\\ncan be straightforwardly extended to a stochastic one where we have instead a\\ndistribution over the pairs (x, y) \u2208X\u00d7Y .\\nAs in all supervised learning problems, the learner receives a labe', 'led sample\\nS =\\n(\\n(x1,y1),..., (xm,y m)\\n\u23a1\\n\u2208 (X\u00d7 Y )m with x1,...,x m drawn i.i.d. according\\nto D,a n d yi = f(xi) for all i \u2208 [1,m]. Since the labels are real numbers, it is238 Regression\\nnot reasonabl', 'e to hope that the learner could predict precisely the correct label.\\nInstead, we can require that his predictions be close to the correct ones. This is\\nthe key di\ufb00erence between regression and classi', '\ufb01cation: in regression, the measure\\nof error is based on the magnitude of the di\ufb00erence between the real-valued label\\npredicted and the true or correct one, and not based on the equality or inequality', '\\nof these two values.\\nWe denote by L: Y\u00d7Y\u2192 R\\n+ the loss function used to measure the magnitude\\nof error. The most common loss function used in regression is the squared loss L2\\nde\ufb01ned by L(y,y \u2032)= |y\u2032', ' \u2212 y|2 for all y,y \u2032 \u2208Y , or, more generally, anLp loss de\ufb01ned\\nby L(y,y \u2032)= |y\u2032 \u2212 y|p,f o rs o m ep \u2265 1 and all y,y \u2032 \u2208Y .\\nGiven a hypothesis set H of functions mapping X to Y, the regression problem\\n', 'consists of using the labeled sample S to \ufb01nd a hypothesis h \u2208 H with small\\nexpected loss or generalization error R(h) with respect to the target f:\\nR(h)= E\\nx\u223cD\\n[\\nL\\n(\\nh(x),f (x)\\n\u23a1]\\n. (10.1)\\nAs in the ', 'previous chapters, the empirical loss or error ofh \u2208 H is denoted by \u02c6R(h)\\nand de\ufb01ned by\\n\u02c6R(h)= 1\\nm\\nm\u2211\\ni=1\\nL\\n(\\nh(xi),y i\\n\u23a1\\n. (10.2)\\nIn the common case where L is the squared loss, this represents the ', 'mean squared\\nerror of h on the sample S.\\nW h e nt h el o s sf u n c t i o nL is bounded by some M> 0, that is L(y\u2032,y ) \u2264 M for all\\ny,y \u2032 \u2208Y or, more strictly,L(h(x),f (x)) \u2264 M for all h \u2208 H and x \u2208X ,', ' the problem\\nis referred to as a bounded regression problem. Much of the theoretical results\\npresented in the following sections are based on that assumption. The analysis of\\nunbounded regression prob', 'lemsis technically more elaborate and typically requires\\nsome other types of assumptions.\\n10.2 Generalization bounds\\nThis section presents learning guarantees for bounded regression problems. We start', '\\nw i t ht h es i m p l ec a s eo fa\ufb01 n i t eh y p o t h e s i ss e t .\\n10.2.1 Finite hypothesis sets\\nIn the case of a \ufb01nite hypothesis, we can derive a generalization bound for regression\\nby a straigh', 'tforward application of Hoe\ufb00ding\u2019s inequality and the union bound.10.2 Generalization bounds 239\\nTheorem 10.1\\nLet L be a bounded loss function. Assume that the hypothesis setH is \ufb01nite. Then,\\nfor any ', '\u03b4> 0, with probability at least 1 \u2212 \u03b4, the following inequality holds for all\\nh \u2208 H:\\nR(h) \u2264 \u02c6R(h)+ M\\n\u221a\\nlog |H| +l o g1\\n\u03b4\\n2m .\\nProof By Hoe\ufb00ding\u2019s inequality, since L takes values in [0,M ], for any h ', '\u2208 H,\\nthe following holds:\\nPr\\n[\\nR(h) \u2212 \u02c6R(h) >\u03f5\\n]\\n\u2264 e\u2212 2m\u03f52\\nM 2 .\\nT h u s ,b yt h eu n i o nb o u n d ,w ec a nw r i t e\\nPr\\n[\\n\u2203h \u2208 H : R(h) \u2212 \u02c6R(h) >\u03f5\\n]\\n\u2264\\n\u2211\\nh\u2208H\\nPr\\n[\\nR(h) \u2212 \u02c6R(h) >\u03f5\\n]\\n\u2264| H|e\u2212 2m\u03f52\\nM 2 ', '.\\nSetting the right-hand side to be equal to\u03b4yields the statement of the theorem.\\nWith the same assumptions and using the same proof, a two-sided bound can be\\nderived: with probability at least 1 \u2212 \u03b4,', ' for all h \u2208 H,\\n|R(h) \u2212 \u02c6R(h)|\u2264 M\\n\u221a\\nlog |H| +l o g2\\n\u03b4\\n2m .\\nThese learning bounds are similar to those derived for classi\ufb01cation. In fact, they\\ncoincide with the classi\ufb01cation bounds given in the incon', 'sistent case when M =1 .\\nThus, all the remarks made in that context apply identically here. In particular,\\na larger sample size m guarantees better generalization; the bound increases as a\\nfunction of', ' log |H| and suggests selecting, for the same empirical error, a smaller\\nhypothesis set. This is an instance of Occam\u2019s razor principle for regression. In the\\nnext sections, we present other instances', ' of this principle for the general case of\\nin\ufb01nite hypothesis sets using the notions of Rademacher complexity and pseudo-\\ndimension.\\n10.2.2 Rademacher complexity bounds\\nHere, we show how the Rademache', 'r complexity bounds of theorem 3.1 can be\\nused to derive generalization bounds for regression in the case of the family of L\\np\\nloss functions. We \ufb01rst show an upper bound for the Rademacher complexity', ' of a\\nrelevant family of functions.240 Regression\\nTheorem 10.2 Rademacher complexity of Lp loss functions\\nLet p \u2265 1 and Hp = {x \u21a6\u2192| h(x) \u2212 f(x)|p : h \u2208 H}. Assume that |h(x) \u2212 f(x)|\u2264 M\\nfor all x \u2208X an', 'd h \u2208 H.T h e n ,f o ra n ys a m p l eS of size m, the following inequality\\nholds:\\n\u02c6RS(Hp) \u2264 pMp\u22121 \u02c6RS(H) .\\nProof Let \u03c6p : x \u21a6\u2192| x|p, then, Hp c a nb er e w r i t t e na sHp = {\u03c6p \u25e6h: h \u2208 H\u2032},\\nwhere H', '\u2032 = {x \u21a6\u2192 h(x) \u2212 f(x): h \u2208 H}.S i n c e\u03c6p is pMp\u22121-Lipschitz over [\u2212M,M ],\\nwe can apply Talagrand\u2019s lemma (lemma 4.2):\\n\u02c6RS(Hp) \u2264 pMp\u22121 \u02c6RS(H\u2032) . (10.3)\\nNow, \u02c6RS(H\u2032) can be expressed as follows:\\n\u02c6RS(H\u2032', ')= 1\\nm E\\n\u03c3\\n[\\nsup\\nh\u2208H\\nm\u2211\\ni=1\\n(\\n\u03c3ih(xi)+ \u03c3if(xi)\\n\u23a1]\\n= 1\\nm E\\n\u03c3\\n[\\nsup\\nh\u2208H\\nm\u2211\\ni=1\\n\u03c3ih(xi)\\n]\\n+E\\n\u03c3\\n[ m\u2211\\ni=1\\n\u03c3if(xi)\\n]\\n= \u02c6RS(H) ,\\nsince E\u03c3\\n[ \u2211m\\ni=1 \u03c3if(xi)\\n]\\n= \u2211m\\ni=1 E\u03c3 [\u03c3i]f(xi)=0 .\\nCombining this result wi', 'th the general Rademacher complexity learning bound\\nof theorem 3.1 yields directly the following Rademacher complexity bounds for\\nregression with L\\np losses.\\nTheorem 10.3 Rademacher complexity regress', 'ion bounds\\nLet p \u2265 1 and assume that \u2225h \u2212 f \u2225\u221e \u2264 M for all h \u2208 H.T h e n ,f o ra n y\u03b4> 0,w i t h\\nprobability at least1 \u2212 \u03b4over a sample S of size m, each of the following inequalities\\nholds for all h ', '\u2208 H:\\nE\\n[\u23d0\u23d0h(x) \u2212 f(x)\\n\u23d0\u23d0p]\\n\u2264 1\\nm\\nm\u2211\\ni=1\\n\u23d0\u23d0h(xi) \u2212 f(xi)\\n\u23d0\u23d0p\\n+2 pMp\u22121Rm(H)+ Mp\\n\u221a\\nlog 1\\n\u03b4\\n2m\\nE\\n[\u23d0\u23d0h(x) \u2212 f(x)\\n\u23d0\u23d0p]\\n\u2264 1\\nm\\nm\u2211\\ni=1\\n\u23d0\u23d0h(xi) \u2212 f(xi)\\n\u23d0\u23d0p\\n+2 pMp\u22121 \u02c6RS(H)+3 Mp\\n\u221a\\nlog 2\\n\u03b4\\n2m .\\nAs in the case of ', 'classi\ufb01cation, these generalization bounds suggest a trade-o\ufb00\\nb e t w e e nr e d u c i n gt h ee m p i r i c a le r r o r ,w h i c hm a yr e q u i r em o r ec o m p l e xh y p o t h e s i s\\nsets, and ', 'controlling the Rademacher complexity of H, which may increase the\\nempirical error. An important bene\ufb01t of the last learning bound is that it is data-\\ndependent. This can lead to more accurate learnin', 'g guarantees. The upper bounds\\non R\\nm(H)o r RS(H) for kernel-based hypotheses (theorem 5.5) can be used directly10.2 Generalization bounds 241\\nx1 x2\\nt2\\nt1\\nFigure 10.1 Illustration of the shattering of', ' a set of two points {x1,x 2} with\\nwitnesses t1 and t2.\\nhere to derive generalization bounds in terms of the trace of the kernel matrix or\\nthe maximum diagonal entry.\\n10.2.3 Pseudo-dimension bounds\\nAs', ' previously discussed in the case of classi\ufb01cation, it is sometimes computationally\\nhard to estimate the empirical Rademacher complexity of a hypothesis set. In\\nchapter 3, we introduce other measures ', 'of the complexity of a hypothesis set\\nsuch as the VC-dimension, which are purely combinatorial and typically easier\\nto compute or upper bound. However, the notion of shattering or that of VC-\\ndimensio', 'n introduced for binary classi\ufb01cation are not readily applicable to real-\\nvalued hypothesis classes.\\nWe \ufb01rst introduce a new notion ofshattering for families of real-valued functions.\\nAs in previous c', 'hapters, we will use the notation G for a family of functions,\\nwhenever we intend to later interpret it (at least in some cases) as the family of loss\\nfunctions associated to some hypothesis set H: G ', '= {x \u21a6\u2192 L(h(x),f (x)): h \u2208 H}.\\nDe\ufb01nition 10.1 Shattering\\nLet G be a family of functions from X to R.As e t {x\\n1,...,x m}\u2286X is said to be\\nshattered by G if there exist t1,...,t m \u2208 R such that,\\n\u23d0\u23d0\u23d0\\n\u23d0\u23d0\u23d0', '\\n\u23d0\u23d0\\n\u23a7\\n\u23aa\u23aa\\n\u23a8\\n\u23aa\u23aa\\n\u23a9\\n\u23a1\\n\u23a2\u23a2\u23a3\\nsgn\\n(\\ng(x\\n1) \u2212 t1\\n\u23a1\\n..\\n.\\nsgn\\n(\\ng(x\\nm) \u2212 tm\\n\u23a1\\n\u23a4\\n\u23a5\u23a5\u23a6: g \u2208 G\\n\u23ab\\n\u23aa\u23aa\\n\u23ac\\n\u23aa\u23aa\\n\u23ad\\n\u23d0\u23d0\u23d0\u23d0\\n\u23d0\u23d0\u23d0\u23d0\\n=2\\nm .\\nWhen they exist, the threshold values t1,...,t m are said to witness the shattering.\\nThus, ', '{x1,...,x m} is shattered if for some witnesses t1,...,t m, the family of\\nfunctions G is rich enough to contain a function going above a subset A of the242 Regression\\nLoss\\n-2 -1 0 1 2\\n0.0\\n0.5\\n1.0\\n1.5\\n', 'L(h(x),f (x))\\nt\\nI(L(h(x),f (x)) >t )\\nx\\nFigure 10.2 Af u n c t i o ng: x \u21a6\u2192 L(h(x),f (x)) (in blue) de\ufb01ned as the loss of some\\n\ufb01xed hypothesis h \u2208 H, and its thresholded version x \u21a6\u2192 1L(h(x),f (x))>t (', 'in red) with\\nrespect to the threshold t (in yellow).\\nset of points I = {(xi,t i): i \u2208 [1,m]} and below the others (I \u2212 A), for any choice\\nof the subset A. Figure 10.1 illustrates this shattering in a ', 'simple case. The notion\\nof shattering naturally leads to the following de\ufb01nition.\\nDe\ufb01nition 10.2 Pseudo-dimension\\nLet G be a family of functions mapping from X to R. Then, the pseudo-dimension\\nof G, d', 'enoted by Pdim(G), is the size of the largest set shattered by G.\\nBy de\ufb01nition of the shattering just introduced, the notion of pseudo-dimension of\\na family of real-valued functions G coincides with t', 'hat of the VC-dimension of the\\ncorresponding thresholded functions mapping X to {0,1}:\\nPdim(G)=V C d i m\\n({\\n(x, t) \u21a6\u2192 1(g(x)\u2212t)>0 : g \u2208 G\\n}\u23a1\\n. (10.4)\\nFigure 10.2 illustrates this interpretation. In vi', 'ew of this interpretation, the follow-\\ning two results follow directly the properties of the VC-dimension.\\nTheorem 10.4\\nThe pseudo-dimension of hyperplanes in RN is given by\\nPdim({x \u21a6\u2192 w \u00b7 x + b: w \u2208 ', 'RN ,b \u2208 R})= N +1 .\\nTheorem 10.5\\nThe pseudo-dimension of a vector space of real-valued functions H is equal to the\\ndimension of the vector space:\\nPdim(H)=d i m (H) .\\nThe following theorem gives a gene', 'ralization bound for bounded regression10.2 Generalization bounds 243\\nin terms of the pseudo-dimension of a family of loss function G = {x \u21a6\u2192\\nL(h(x),f (x)): h \u2208 H} associated to a hypothesis set H. Th', 'e key technique to\\nderive these bounds consists of reducing the problem to that of classi\ufb01cation by\\nmaking use of the following general identity for the expectation of a random variable\\nX:\\nE[X]= \u2212\\n\u222b\\n0', '\\n\u2212\u221e\\nPr[X<t ]dt +\\n\u222b +\u221e\\n0\\nPr[X>t ]dt , (10.5)\\nwhich holds by de\ufb01nition of the Lebesgue integral. In particular, for any distribution\\nD and any non-negative measurable function f, we can write\\nE\\nx\u223cD\\n[f(x', ')] =\\n\u222b \u221e\\n0\\nPr\\nx\u223cD\\n[f(x) >t ]dt . (10.6)\\nTheorem 10.6\\nLet H be a family of real-valued functions and letG = {x \u21a6\u2192 L(h(x),f (x)): h \u2208 H}\\nbe the family of loss functions associated toH. Assume that Pdim(', 'G)= d and that\\nthe loss function L is bounded by M.T h e n ,f o ra n y\u03b4> 0, with probability at least\\n1 \u2212 \u03b4 over the choice of a sample of size m, the following inequality holds for all\\nh \u2208 H:\\nR(h) \u2264 ', '\u02c6R(h)+ M\\n\u221a\\n2d log em\\nd\\nm + M\\n\u221a\\nlog 1\\n\u03b4\\n2m . (10.7)\\nProof Let S be a sample of size m drawn i.i.d. according to D and let \u02c6D denote\\nthe empirical distribution de\ufb01ned by S. For any h \u2208 H and t \u2265 0, we d', 'enote by\\nc(h, t) the classi\ufb01er de\ufb01ned by c(h, t): x \u21a6\u2192 1L(h(x),f(x))>t.T h ee r r o ro fc(h, t)c a n\\nbe de\ufb01ned by\\nR(c(h, t)) = Pr\\nx\u223cD\\n[c(h, t)(x) = 1] = Pr\\nx\u223cD\\n[L(h(x),f (x)) >t ],\\nand, similarly, its', ' empirical error is \u02c6R(c(h, t)) = Prx\u223c bD[L(h(x),f (x)) >t ].\\nNow, in view of the identity (10.6) and the fact that the loss functionL is bounded244 Regression\\nby M,w ec a nw r i t e :\\n|R(h) \u2212 \u02c6R(h)| ', '=\\n\u23d0\u23d0\\n\u23d0 E\\nx\u223cD\\n[L(h(x),f (x))] \u2212 E\\nx\u223c bD\\n[L(h(x),f (x))]\\n\u23d0\u23d0\\n\u23d0\\n=\\n\u23d0\u23d0\\n\u23d0\\n\u23d0\\n\u23d0\\n\u222b\\nM\\n0\\n(\\nPr\\nx\u2208D\\n[L(h(x),f (x)) >t ] \u2212 Pr\\nx\u2208 bD\\n[L(h(x),f (x)) >t ]\\n\u23a1\\ndt\\n\u23d0\u23d0\\n\u23d0\\n\u23d0\\n\u23d0\\n\u2264 M sup\\nt\u2208[0,M]\\n\u23d0\u23d0\\n\u23d0\u23d0 Pr\\nx\u2208D\\n[L(h(x),f (x)) >t ] ', '\u2212 Pr\\nx\u2208 bD\\n[L(h(x),f (x)) >t ]\\n\u23d0\u23d0\u23d0\u23d0\\n= M sup\\nt\u2208[0,M]\\n\u23d0\u23d0\u23d0R(c(h, t)) \u2212 \u02c6R(c(h, t))\\n\u23d0\u23d0\u23d0 .\\nThis implies the following inequality:\\nPr\\n[\\n|R(h) \u2212 \u02c6R(h)| >\u03f5\\n]\\n\u2264 Pr\\n[\\nsup\\nh\u2208H\\nt\u2208[0,M]\\n\u23d0\u23d0\u23d0R(c(h, t)) \u2212 \u02c6R(c(h, t))', '\\n\u23d0\u23d0\u23d0 > \u03f5\\nM\\n]\\n.\\nThe right-hand side can be bounded using a standard generalization bound for clas-\\ns i \ufb01 c a t i o n( c o r o l l a r y3 . 4 )i nt e r m so ft h eV C - d i m e n s i o no ft h ef a m i ', 'l yo fh y p o t h e s e s\\n{c(h, t): h \u2208 H,t \u2208 [0,M ]}, which, by de\ufb01nition of the pseudo-dimension, is pre-\\ncisely Pdim(G)= d. The resulting bound coincides with (10.7).\\nThe notion of pseudo-dimension', ' is suited to the analysis of regression as demon-\\nstrated by the previous theorem; however, it is not a scale-sensitive notion. There\\nexists an alternative complexity measure, thefat-shattering dimen', 'sion,t h a ti ss c a l e -\\nsensitive and that can be viewed as a natural extension of the pseudo-dimension.\\nIts de\ufb01nition is based on the notion of \u03b3-shattering.\\nDe\ufb01nition 10.3 \u03b3-shattering\\nLet G be a', ' family of functions from X to R and let \u03b3> 0.As e t {x\\n1,...,x m}\u2286X\\nis said to be \u03b3-shattered by G if there exist t1,...,t m \u2208 R such that for all\\ny \u2208{ \u22121,+1}m,t h e r ee x i s t sg \u2208 G such that:\\n\u2200i', ' \u2208 [1,m],y i(g(xi) \u2212 ti) \u2265 \u03b3.\\nThus, {x1,...,x m} is \u03b3-shattered if for some witnesses t1,...,t m, the family of\\nfunctions G is rich enough to contain a function going at least \u03b3 above a subset A\\nof th', 'e set of pointsI = {(xi,t i): i \u2208 [1,m]} and at least \u03b3 below the others (I \u2212 A),\\nfor any choice of the subset A.\\nDe\ufb01nition 10.4 \u03b3-fat-dimension\\nThe \u03b3-fat-dimension of G, fat\u03b3(G), is the size of the l', 'argest set that is \u03b3-shattered\\nby G.\\nFiner generalization bounds than those based on the pseudo-dimension can be10.3 Regression algorithms 245\\n\u03a6(x)\\ny\\nFigure 10.3 For N =1 , linear regression consists ', 'of \ufb01nding the line of best \ufb01t,\\nmeasured in terms of the squared loss.\\nderived in terms of the \u03b3-fat-dimension. However, the resulting learning bounds,\\nare not more informative than those based on the ', 'Rademacher complexity, which\\nis also a scale-sensitive complexity measure. Thus, we will not detail an analysis\\nbased on the \u03b3-fat-dimension.\\n10.3 Regression algorithms\\nT h er e s u l t so ft h ep r e', ' v i o u ss e c t i o n ss h o wt h a t ,f o rt h es a m ee m p i r i c a le r r o r ,\\nhypothesis sets with smaller complexity measured in terms of the Rademacher\\ncomplexity or in terms of pseudo-dime', 'nsion bene\ufb01t from better generalization\\nguarantees. One family of functions with relatively small complexity is that of linear\\nhypotheses. In this section, we describe and analyze several algorithms b', 'ased on\\nthat hypothesis set:linear regression, kernel ridge regression(KRR), support vector\\nregression (SVR), and Lasso. These algorithms, in particular the last three, are\\nextensively used in practic', 'e and often lead to state-of-the-art performance results.\\n10.3.1 Linear regression\\nWe start with the simplest algorithm for regression known aslinear regression.L e t\\n\u03a6 : X\u2192 R\\nN b eaf e a t u r em a p', ' p i n gf r o mt h ei n p u ts p a c eX to RN and consider the\\nfamily of linear hypotheses\\nH = {x \u21a6\u2192 w \u00b7 \u03a6(x)+ b: w \u2208 RN ,b \u2208 R} . (10.8)\\nLinear regression consists of seeking a hypothesis in H with t', 'he smallest empirical\\nmean squared error. Thus, for a sample S =\\n(\\n(x1,y1),..., (xm,y m)\\n\u23a1\\n\u2208 (X\u00d7 Y )m,\\nthe following is the corresponding optimization problem:\\nmin\\nw,b\\n1\\nm\\nm\u2211\\ni=1\\n(w \u00b7 \u03a6(xi)+ b \u2212 yi)2 ', '. (10.9)246 Regression\\nFigure 10.3 illustrates the algorithm in the simple case whereN = 1. The optimiza-\\ntion problem admits the simpler formulation:\\nmin\\nW\\nF(W)= 1\\nm \u2225X\u22a4W \u2212 Y\u22252, (10.10)\\nusing the not', 'ationX =\\n[\u03a6(x1) ... \u03a6(xm)\\n1 ... 1\\n]\\n, W =\\n[ w1..\\n.\\nwN\\n1\\n]\\nand Y =\\n[ y1..\\n.\\nym\\n]\\n. The objective\\nfunction F is convex, by composition of the convex function u \u21a6\u2192\u2225 u\u22252 with the\\na\ufb03ne function W \u21a6\u2192 X\u22a4W \u2212 ', 'Y, and it is di\ufb00erentiable. Thus, F admits a global\\nminimum at W if and only if \u2207F(W) = 0, that is if and only if\\n2\\nmX(X\u22a4W \u2212 Y)=0 \u21d4 XX\u22a4W = XY . (10.11)\\nWhen XX\u22a4 is invertible, this equation admits a u', 'nique solution. Otherwise, the\\nequation admits a family of solutions that can be given in terms of the pseudo-inverse\\nof matrix XX\\n\u22a4 (see appendix A) byW =( XX\u22a4)\u2020 XY +(I \u2212 (XX\u22a4)\u2020 (XX\u22a4))W0,\\nwhere W0 is', ' an arbitrary matrix in RN \u00d7N . Among these, the solution W =\\n(XX\u22a4)\u2020 XY is the one with the minimal norm and is often preferred for that reason.\\nThus, we will write the solutions as\\nW =\\n{\\n(XX\u22a4)\u22121XY if', ' XX\u22a4 is invertible,\\n(XX\u22a4)\u2020 XY otherwise.\\n(10.12)\\nThe matrix XX\u22a4 can be computed in O(mN2). The cost of its inversion or that of\\ncomputing its pseudo-inverse is in O(N3).1 Finally, the multiplication w', 'ith X and\\nY takes O(mN2). Therefore, the overall complexity of computing the solution W\\nis in O(mN2 + N3). Thus, when the dimension of the feature space N is not too\\nlarge, the solution can be compute', 'd e\ufb03ciently.\\nWhile linear regression is simple and admits a straightforward implementation,\\nit does not bene\ufb01t from a strong generalization guarantee, since it is limited to\\nminimizing the empirical e', 'rror without controlling the norm of the weight vector\\nand without any other regularization. Its performance is also typically poor in most\\napplications. The next sections describe algorithms with bot', 'h better theoretical\\nguarantees and improved performance in practice.\\n1. In the analysis of the computational complexity of the algorithms discussed in this\\nchapter, the cubic-time complexity of matri', 'x inversion can be replaced by a more favorable\\ncomplexity O(N\\n2+\u03c9 ), with \u03c9 = .376 using asymptotically faster matrix inversion methods\\nsuch as that of Coppersmith and Winograd.10.3 Regression algori', 'thms 247\\n10.3.2 Kernel ridge regression\\nWe \ufb01rst present a learning guarantee for regression with bounded linear hypotheses\\nin a feature space de\ufb01ned by a PDS kernel. This will provide a strong theoret', 'ical\\nsupport for the kernel ridge regression algorithm presented in this section. The\\nlearning bounds of this section are given for the squared loss. Thus, in particular,\\nthe generalization error of a', ' hypothesis h is de\ufb01ned by R(h)=E\\n[\\n(h(x) \u2212 f(x))\\n2]\\nwhen the target function is f.\\nTheorem 10.7\\nLet K : X\u00d7 X \u2192 R be a PDS kernel, \u03a6: X\u2192 H a feature mapping associated to\\nK,a n dH = {x \u21a6\u2192 w \u00b7 \u03a6(x): \u2225w', '\u2225H \u2264 \u039b}. Assume that there exists r> 0 such that\\nK(x, x) \u2264 r2 and |f(x)|\u2264 \u039br for all x \u2208X .T h e n ,f o ra n y\u03b4> 0,w i t hp r o b a b i l i t y\\nat least 1 \u2212 \u03b4, each of the following inequalities holds', ' for all h \u2208 H:\\nR(h) \u2264 \u02c6R(h)+ 8r2\u039b2\\n\u221am\\n\u239b\\n\u239d1+ 1\\n2\\n\u221a\\nlog 1\\n\u03b4\\n2\\n\u239e\\n\u23a0 (10.13)\\nR(h) \u2264 \u02c6R(h)+ 8r2\u039b2\\n\u221am\\n\u239b\\n\u239d\\n\u221a\\nTr[K]\\nmr2 + 3\\n4\\n\u221a\\nlog 2\\n\u03b4\\n2\\n\u239e\\n\u23a0 . (10.14)\\nProof For all x \u2208X ,w eh a v e|w \u00b7 \u03a6(x)|\u2264 \u039b\u2225\u03a6(x)\u2225\u2264 \u039br,t ', 'h u s ,f o ra l lx \u2208X and\\nh \u2208 H, |h(x) \u2212 f(x)|\u2264 2\u039br. By the bound on the empirical Rademacher complexity\\nof kernel-based hypotheses (theorem 5.5), the following holds for any sample S of\\nsize m:\\n\u02c6RS(H', ') \u2264 \u039b\\n\u221a\\nTr[K]\\nm \u2264\\n\u221a\\nr2\u039b2\\nm ,\\nwhich implies that Rm(H) \u2264\\n\u221a\\nr2\u039b2\\nm . Plugging in this inequality in the \ufb01rst bound\\nof theorem 10.3 with M =2 \u039br gives\\nR(h) \u2264 \u02c6R(h)+4 M Rm(H)+ M2\\n\u221a\\nlog 1\\n\u03b4\\n2m = \u02c6R(h)+ 8r2', '\u039b2\\n\u221am\\n(\\n1+ 1\\n2\\n\u221a\\nlog 1\\n\u03b4\\n2\\n\u23a1\\n.\\nThe second generalization bound is shown in a similar way by using the second\\nbound of theorem 10.3.\\nThe \ufb01rst bound of the theorem just presented has the form R(h) \u2264 \u02c6R(', 'h)+ \u03bb\u039b2,\\nwith \u03bb = 8r2\\n\u221am\\n(\\n1+ 1\\n2\\n\u221a\\nlog 1\\n\u03b4\\n2\\n\u23a1\\n= O( 1\u221am ). Kernel ridge regression is de\ufb01ned by the\\nminimization of an objective function that has precisely this form and thus is directly248 Regressi', 'on\\nmotivated by the theoretical analysis just presented:\\nmin\\nw\\nF(w)= \u03bb\u2225w\u22252 +\\nm\u2211\\ni=1\\n(w \u00b7 \u03a6(xi) \u2212 yi)2 . (10.15)\\nHere, \u03bb is a positive parameter determining the trade-o\ufb00 between the regularization\\nterm', ' \u2225w\u22252 and the empirical mean squared error. The objective function di\ufb00ers from\\nthat of linear regression only by the \ufb01rst term, which controls the norm ofw.A si n\\nthe case of linear regression, the pr', 'oblem can be rewritten in a more compact form\\nas\\nmin\\nW\\nF(W)= \u03bb\u2225W\u22252 + \u2225X\u22a4W \u2212 Y\u22252, (10.16)\\nwhere X \u2208 RN \u00d7m is the matrix formed by the feature vectors,X =[ \u03a6(x1) ... \u03a6(xm) ],\\nW = w,a n d Y =( y1,...,y m', ')\u22a4.H e r et o o ,F is convex, by the convexity of\\nw \u21a6\u2192\u2225 w\u22252 and that of the sum of two convex functions, and is di\ufb00erentiable.\\nThus F admits a global minimum at W if and only if\\n\u2207F(W)=0 \u21d4 (XX\u22a4 + \u03bbI)W ', '= XY \u21d4 W =( XX\u22a4 + \u03bbI)\u22121XY. (10.17)\\nNote that the matrix XX\u22a4 + \u03bbI is always invertible, since its eigenvalues are the\\nsum of the non-negative eigenvalues of the symmetric positive semide\ufb01nite matrix\\nXX', '\\n\u22a4 and \u03bb> 0. Thus, kernel ridge regression admits a closed-form solution.\\nAn alternative formulation of the optimization problem for kernel ridge regression\\nequivalent to (10.15) is\\nmin\\nw\\nm\u2211\\ni=1\\n(w \u00b7 ', '\u03a6(xi) \u2212 yi)2 subject to: \u2225w\u22252 \u2264 \u039b2.\\nThis makes the connection with the bounded linear hypothesis set of theorem 10.7\\neven more evident. Using slack variables \u03bei, for all i \u2208 [1,m], the problem can be\\n', 'equivalently written as\\nmin\\nw\\nm\u2211\\ni=1\\n\u03be2\\ni subject to: (\u2225w\u22252 \u2264 \u039b2) \u2227\\n(\\n\u2200i \u2208 [1,m],\u03be i = yi \u2212 w \u00b7 \u03a6(xi)\\n\u23a1\\n.\\nThis is a convex optimization problem with di\ufb00erentiable objective function and\\nconstraints. T', 'o derive the equivalent dual problem, we introduce the LagrangianL,\\nwhich is de\ufb01ned for all \u03be,w, \u03b1\u2032,a n d\u03bb \u2265 0b y\\nL(\u03be,w,\u03b1\u2032,\u03bb)=\\nm\u2211\\ni=1\\n\u03be2\\ni +\\nm\u2211\\ni=1\\n\u03b1\u2032\\ni(yi \u2212 \u03bei \u2212 w \u00b7 \u03a6(xi)) +\u03bb(\u2225w\u22252 \u2212 \u039b2) .10.3 Regres', 'sion algorithms 249\\nThe KKT conditions lead to the following equalities:\\n\u2207wL = \u2212\\nm\u2211\\ni=1\\n\u03b1\u2032\\ni\u03a6(xi)+2 \u03bbw =0 = \u21d2 w = 1\\n2\u03bb\\nm\u2211\\ni=1\\n\u03b1\u2032\\ni\u03a6(xi)\\n\u2207\u03bei L =2 \u03bei \u2212 \u03b1\u2032\\ni =0 = \u21d2 \u03bei = \u03b1\u2032\\ni/2\\n\u2200i \u2208 [1,m],\u03b1 \u2032\\ni(yi \u2212 \u03bei \u2212', ' w \u00b7 \u03a6(xi)) = 0\\n\u03bb(\u2225w\u22252 \u2212 \u039b2)=0 .\\nPlugging in the expressions of w and \u03beisi nt h a to fL gives\\nL =\\nm\u2211\\ni=1\\n\u03b1\u20322\\ni\\n4 +\\nm\u2211\\ni=1\\n\u03b1\u2032\\niyi \u2212\\nm\u2211\\ni=1\\n\u03b1\u2032\\ni\\n2\\n2 \u2212 1\\n2\u03bb\\nm\u2211\\ni,j=1\\n\u03b1\u2032\\ni\u03b1\u2032\\nj\u03a6(xi)\u22a4\u03a6(xj)\\n+ \u03bb\\n( 1\\n4\u03bb2 \u2225\\nm\u2211\\n', 'i=1\\n\u03b1\u2032\\ni\u03a6(xi)\u22252 \u2212 \u039b2\\n\u23a1\\n= \u2212 1\\n4\\nm\u2211\\ni=1\\n\u03b1\u20322\\ni +\\nm\u2211\\ni=1\\n\u03b1\u2032\\niyi \u2212 1\\n4\u03bb\\nm\u2211\\ni,j=1\\n\u03b1\u2032\\ni\u03b1\u2032\\nj\u03a6(xi)\u22a4\u03a6(xj) \u2212 \u03bb\u039b2\\n= \u2212\u03bb\\nm\u2211\\ni=1\\n\u03b12\\ni +2\\nm\u2211\\ni=1\\n\u03b1iyi \u2212\\nm\u2211\\ni,j=1\\n\u03b1i\u03b1j\u03a6(xi)\u22a4\u03a6(xj) \u2212 \u03bb\u039b2,\\nwith \u03b1\u2032\\ni =2 \u03bb\u03b1i. Thus, the equi', 'valent dual optimization problem for KRR can be\\nwritten as follows:\\nmax\\n\u03b1 \u2208Rm\\n\u2212\u03bb\u03b1\u22a4\u03b1 +2 \u03b1\u22a4Y \u2212 \u03b1\u22a4(X\u22a4X)\u03b1 , (10.18)\\nor, more compactly, as\\nmax\\n\u03b1 \u2208Rm\\nG(\u03b1)= \u2212\u03b1\u22a4(K + \u03bbI)\u03b1 +2 \u03b1\u22a4Y , (10.19)\\nwhere K = X\u22a4X is th', 'e kernel matrix associated to the training sample. The\\nobjective function G is concave and di\ufb00erentiable. The optimal solution is obtained\\nby di\ufb00erentiating the function and setting it to zero:\\n\u2207G(\u03b1)=', '0 \u21d0\u21d2 2(K + \u03bbI)\u03b1 =2 Y \u21d0\u21d2 \u03b1 =( K + \u03bbI)\u22121Y . (10.20)\\nNote that (K+\u03bbI) is invertible, since its eigenvalues are the sum of the eigenvalues\\nof the SPSD matrixK and \u03bb> 0. Thus, as in the primal case, the du', 'al optimization\\nproblem admits a closed-form solution. By the \ufb01rst KKT equation, w can be\\ndetermined from \u03b1 by\\nw =\\nm\u2211\\ni=1\\n\u03b1i\u03a6(xi)= X\u03b1 = X(K + \u03bbI)\u22121Y. (10.21)250 Regression\\nThe hypothesis h solution ca', 'n be given as follows in terms of \u03b1:\\n\u2200x \u2208X ,h (x)= w \u00b7 \u03a6(x)=\\nm\u2211\\ni=1\\n\u03b1iK(xi,x) . (10.22)\\nNote that the form of the solution, h = \u2211m\\ni=1 \u03b1iK(xi, \u00b7), could be immediately\\npredicted using the Representer ', 'theorem, since the objective function minimized by\\nKRR falls within the general framework of theorem 5.4. This also could show thatw\\nc o u l db ew r i t t e na sw = X\u03b1. This fact, combined with the fo', 'llowing simple lemma,\\ncan be used to determine \u03b1 in a straightforward manner, without the intermediate\\nderivation of the dual problem.\\nLemma 10.1\\nThe following identity holds for any matrix X:\\n(XX\\n\u22a4 +', ' \u03bbI)\u22121X = X(X\u22a4X + \u03bbI)\u22121 .\\nProof Observe that (XX\u22a4 +\u03bbI)X = X(X\u22a4X+\u03bbI). Left-multiplying by (XX\u22a4 +\\n\u03bbI)\u22121 this equality and right-multiplying it by (X\u22a4X + \u03bbI)\u22121 yields the statement\\nof the lemma.\\nNow, usi', 'ng this lemma, the primal solution of w c a nb er e w r i t t e na sf o l l o w s :\\nw =( XX\u22a4 + \u03bbI)\u22121XY = X(X\u22a4X + \u03bbI)\u22121Y = X(K + \u03bbI)\u22121Y.\\nComparing with w = X\u03b1 gives immediately \u03b1 =( K + \u03bbI)\u22121Y.\\nOur pre', 'sentation of the KRR algorithm was given for linear hypotheses with no\\no\ufb00set, that is we implicitly assumed b = 0. It is common to use this formulation\\nand to extend it to the general case by augmenti', 'ng the feature vector\u03a6(x)w i t ha n\\ne x t r ac o m p o n e n te q u a lt oo n ef o ra l lx \u2208X and the weight vector w with an extra\\ncomponent b \u2208 R. For the augmented feature vector \u03a6\u2032(x) \u2208 RN+1 and w', 'eight\\nvector w\u2032 \u2208 RN+1,w eh a v ew\u2032 \u00b7\u03a6\u2032(x)= w \u00b7\u03a6(x)+ b. Nevertheless, this formulation\\ndoes not coincide with the general KRR algorithm where a solution of the form\\nx \u21a6\u2192 w \u00b7\u03a6(x)+ b is sought. This is ', 'because for the general KRR, the regularization\\nterm is \u03bb\u2225w\u2225, while for the extension just described it is \u03bb\u2225w\\n\u2032\u2225.\\nIn both the primal and dual cases, KRR admits a closed-form solution. Table 10.1\\ngive', 's the time complexity of the algorithm for computing the solution and the one\\nfor determining the prediction value of a point in both cases. In the primal case,\\ndetermining the solutionw requires comp', 'uting matrixXX\\n\u22a4,w h i c ht a k e sO(mN2),\\nthe inversion of (XX\u22a4 + \u03bbI), which is in O(N3), and multiplication with X,w h i c h\\nis in O(mN2). Prediction requires computing the inner product ofw with a ', 'feature\\nvector of the same dimension that can be achieved inO(N). The dual solution \ufb01rst\\nrequires computing the kernel matrix K.L e t\u03ba be the maximum cost of computing10.3 Regression algorithms 251\\nSo', 'lution Prediction\\nPrimal O(mN2 + N3) O(N)\\nDual O(\u03bam2 + m3) O(\u03bam)\\nT able 10.1 Comparison of the running-time complexity of KRR for computing\\nthe solution or the prediction value of a point in both the ', 'primal and the dual\\ncase. \u03ba denotes the time complexity of computing a kernel value; for polynomial\\nand Gaussian kernels, \u03ba = O(N).\\nK(x, x\u2032) for all pairs (x, x\u2032) \u2208X\u00d7X . Then, K can be computed in O(\u03ba', 'm2). The\\ninversion of matrix K + \u03bbI can be achieved in O(m3) and multiplication with Y\\ntakes O(m2). Prediction requires computing the vector ( K(x1,x),...,K (xm,x))\u22a4\\nfor some x \u2208X ,w h i c hr e q u i ', 'r e sO(\u03bam), and the inner product with \u03b1, which is in\\nO(m).\\nThus, in both cases, the main step for computing the solution is a matrix inversion,\\nwhich takes O(N3) in the primal case,O(m3) in the dual ', 'case. When the dimension\\nof the feature space is relatively small, solving the primal problem is advantageous,\\nwhile for high-dimensional spaces and medium-sized training sets, solving the dual\\nis pre', 'ferable. Note that for relatively large matrices, the space complexity could also\\nbe an issue: the size of relatively large matrices could be prohibitive for memory\\nstorage and the use of external mem', 'ory could signi\ufb01cantly a\ufb00ect the running time\\nof the algorithm.\\nFor sparse matrices, there exist several techniques for faster computations of the\\nmatrix inversion. This can be useful in the primal ca', 'se where the features can be\\nrelatively sparse. On the other hand, the kernel matrix K is typically dense; thus,\\nthere is less hope for bene\ufb01ting from such techniques in the dual case. In such cases,\\n', 'or, more generally, to deal with the time and space complexity issues arising when\\nm and N are large, approximation methods using low-rank approximations via the\\nNystr\u00a8om method or the partial Cholesk', 'y decomposition can be used very e\ufb00ectively.\\nThe KRR algorithm admits several advantages: it bene\ufb01ts from favorable theo-\\nretical guarantees since it can be derived directly from the generalization bo', 'und we\\npresented; it admits a closed-form solution, which can make the analysis of many\\nof its properties convenient; and it can be used with PDS kernels, which extends its\\nuse to non-linear regressio', 'n solutions and more general features spaces. KRR also\\nadmits favorable stability properties that we discuss in chapter 11.\\nThe algorithm can be generalized to learning a mapping from X to R\\np, p> 1.\\n', 'T h i sc a nb ed o n eb yf o r m u l a t i n gt h ep r o b l e ma sp independent regression problems,\\neach consisting of predicting one of the p target components. Remarkably, the\\ncomputation of the s', 'olution for this generalized algorithm requires only a single252 Regression\\n\u03a6(x)\\ny\\n\u03f5\\nw\u00b7\u03a6(x)+b\\nFigure 10.4 SVR attempts to \ufb01t a \u201ctube\u201d with width \u03f5 to the data. Training data\\nwithin the \u201cepsilon tube\u201d ', '(blue points) incur no loss.\\nmatrix inversion, e.g., (K + \u03bbI)\u22121 in the dual case, regardless of the value of p.\\nOne drawback of the KRR algorithm, in addition to the computational issues for\\ndetermini', 'ng the solution for relatively large matrices, is the fact that the solution it\\nreturns is typically not sparse. The next two sections present two sparse algorithms\\nf o rl i n e a rr e g r e s s i o n', ' .\\n10.3.3 Support vector regression\\nIn this section, we present the support vector regression (SVR) algorithm, which\\nis inspired by the SVM algorithm presented for classi\ufb01cation in chapter 4. The\\nmain', ' idea of the algorithm consists of \ufb01tting a tube of width \u03f5> 0t ot h ed a t a ,a s\\nillustrated by \ufb01gure 10.4. As in binary classi\ufb01cation, this de\ufb01nes two sets of points:\\nthose falling inside the tube,', ' which are \u03f5-close to the function predicted and thus\\nnot penalized, and those falling outside, which are penalized based on their distance\\nto the predicted function, in a way that is similar to the p', 'enalization used by SVMs\\nin classi\ufb01cation.\\nUsing a hypothesis set H of linear functions: H = {x \u21a6\u2192 w \u00b7 \u03a6(x)+ b: w \u2208\\nR\\nN ,b \u2208 R},w h e r e\u03a6 is the feature mapping corresponding some PDS kernel K,\\nthe o', 'ptimization problem for SVR can be written as follows:\\nmin\\nw,b\\n1\\n2 \u2225w\u22252 + C\\nm\u2211\\ni=1\\n\u23d0\u23d0yi \u2212 (w \u00b7 \u03a6(xi)+ b)\\n\u23d0\u23d0\\n\u03f5 , (10.23)\\nwhere |\u00b7| \u03f5 denotes the \u03f5-insensitive loss:\\n\u2200y,y \u2032 \u2208Y , |y\u2032 \u2212 y|\u03f5 =m a x ( 0, |y', '\u2032 \u2212 y|\u2212 \u03f5). (10.24)\\nThe use of this loss function leads to sparse solutions with a relatively small\\nnumber of support vectors. Using slack variables \u03bei \u2265 0a n d \u03be\u2032\\ni \u2265 0, i \u2208 [1,m],10.3 Regression alg', 'orithms 253\\nthe optimization problem can be equivalently written as\\nmin\\nw,b,\u03be,\u03be\u2032\\n1\\n2 \u2225w\u22252 + C\\nm\u2211\\ni=1\\n(\u03bei + \u03be\u2032\\ni) (10.25)\\nsubject to (w \u00b7 \u03a6(xi)+ b) \u2212 yi \u2264 \u03f5 + \u03bei\\nyi \u2212 (w \u00b7 \u03a6(xi)+ b) \u2264 \u03f5 + \u03be\u2032\\ni\\n\u03bei \u2265 0,\u03be', '\u2032\\ni \u2265 0, \u2200i \u2208 [1,m].\\nThis is a convex quadratic program (QP) with a\ufb03ne constraints. Introducing the\\nLagrangian and applying the KKT conditions leads to the following equivalent dual\\nproblem in terms o', 'f the kernel matrix K:\\nmax\\n\u03b1 ,\u03b1 \u2032\\n\u2212 \u03f5(\u03b1\u2032 + \u03b1)\u22a41 +( \u03b1\u2032 \u2212 \u03b1)\u22a4y \u2212 1\\n2(\u03b1\u2032 \u2212 \u03b1)\u22a4K(\u03b1\u2032 \u2212 \u03b1) (10.26)\\nsubject to: (0 \u2264 \u03b1 \u2264 C) \u2227 (0 \u2264 \u03b1\u2032 \u2264 C) \u2227 ((\u03b1\u2032 \u2212 \u03b1)\u22a41 =0 ).\\nAny PDS kernelK can be used with SVR, which exte', 'nds the algorithm to non-linear\\nregression solutions. Problem (10.26) is a convex QP similar to the dual problem\\nof SVMs and can be solved using similar optimization techniques. The solutions \u03b1\\nand \u03b1\u2032', ' de\ufb01ne the hypothesis h returned by SVR as follows:\\n\u2200x \u2208X ,h (x)=\\nm\u2211\\ni=1\\n(\u03b1\u2032\\ni \u2212 \u03b1i)K(xi,x)+ b, (10.27)\\nwhere the o\ufb00set b c a nb eo b t a i n e df r o map o i n txj with 0 <\u03b1 j <C by\\nb = \u2212\\nm\u2211\\ni=1\\n(\u03b1\u2032\\n', 'i \u2212 \u03b1i)K(xi,xj)+ yj + \u03f5, (10.28)\\nor from a point xj with 0 <\u03b1 \u2032\\nj <C via\\nb = \u2212\\nm\u2211\\ni=1\\n(\u03b1\u2032\\ni \u2212 \u03b1i)K(xi,xj)+ yj \u2212 \u03f5. (10.29)\\nBy the complementarity conditions, for all i \u2208 [1,m], the following equalitie', 's hold:\\n\u03b1i\\n(\\n(w \u00b7 \u03a6(xi)+ b) \u2212 yi \u2212 \u03f5 \u2212 \u03bei\\n\u23a1\\n=0\\n\u03b1\u2032\\ni\\n(\\n(w \u00b7 \u03a6(xi)+ b) \u2212 yi + \u03f5 + \u03be\u2032\\ni\\n\u23a1\\n=0 .\\nThus, if \u03b1i \u0338=0o r \u03b1\u2032\\ni \u0338=0 ,t h a ti si fxi is a support vector, then, either (w \u00b7\u03a6(xi)+\\nb) \u2212 yi \u2212 \u03f5 = \u03bei h', 'olds or yi \u2212 (w \u00b7\u03a6(xi)+ b) \u2212 \u03f5 = \u03be\u2032\\ni. This shows that support vectors\\npoints lying outside the \u03f5-tube. Of course, at most one of \u03b1i or \u03b1\u2032\\ni is non-zero for\\nany point xi: the hypothesis either overest', 'imates or underestimates the true label254 Regression\\nby more than \u03f5. For the points within the \u03f5-tube, we have \u03b1j = \u03b1\u2032\\nj =0 ;t h u s ,\\nthese points do not contribute to the de\ufb01nition of the hypothesi', 's returned by SVR.\\nThus, when the number of points inside the tube is relatively large, the hypothesis\\nreturned by SVR is relatively sparse. The choice of the parameter \u03f5 determines a\\ntrade-o\ufb00 between', ' sparsity and accuracy: larger \u03f5 values provide sparser solutions,\\nsince more points can fall within the \u03f5-tube, but may ignore too many key points\\nfor determining an accurate solution.\\nThe following ', 'generalization bounds hold for the \u03f5-insensitive loss and kernel-\\nbased hypotheses and thus for the SVR algorithm. We denote byD the distribution\\naccording to which sample points are drawn and by \u02c6D t', 'he empirical distribution\\nde\ufb01ned by a training sample of size m.\\nTheorem 10.8\\nLet K : X\u00d7 X \u2192 R be a PDS kernel, let\u03a6: X\u2192 H be a feature mapping associated\\nto K and let H = {x \u21a6\u2192 w \u00b7 \u03a6(x): \u2225w\u2225\\nH \u2264 \u039b}. ', 'Assume that there exists r> 0 such\\nthat K(x, x) \u2264 r2 and |f(x)|\u2264 \u039br for all x \u2208X .F i x\u03f5> 0.T h e n ,f o ra n y\u03b4> 0,\\nwith probability at least1 \u2212 \u03b4, each of the following inequalities holds for all h ', '\u2208 H,\\nE\\nx\u223cD\\n[|h(x) \u2212 f(x)|\u03f5] \u2264 E\\nx\u223c bD\\n[|h(x) \u2212 f(x)|\u03f5]+ 2r\u039b\u221am\\n(\\n1+\\n\u221a\\nlog 1\\n\u03b4\\n2\\n\u23a1\\nE\\nx\u223cD\\n[|h(x) \u2212 f(x)|\u03f5] \u2264 E\\nx\u223c bD\\n[|h(x) \u2212 f(x)|\u03f5]+ 2r\u039b\u221am\\n(\u221a\\nTr[K]\\nmr2 +3\\n\u221a\\nlog 2\\n\u03b4\\n2\\n\u23a1\\n.\\nProof Let H\u03f5 = {x \u21a6\u2192| h(x)\u2212 f(', 'x)|\u03f5 : h \u2208 H} and let H\u2032 = {x \u21a6\u2192 h(x)\u2212 f(x): h \u2208\\nH}. Note that the function \u03a6\u03f5 : x \u21a6\u2192| x|\u03f5 is 1-Lipschitz. Thus, by Talagrand\u2019s lemma\\n(lemma 4.2), we have \u02c6RS(H\u03f5) \u2264 \u02c6RS(H\u2032). By the proof of theorem 10', '.2, the equality\\n\u02c6RS(H\u2032)= \u02c6RS(H) holds, thus \u02c6RS(H\u03f5) \u2264 \u02c6RS(H).\\nAs in the proof of theorem 10.7, for allx \u2208X and h \u2208 H,w eh a v e|h(x) \u2212 f(x)|\u2264\\n2\u039br and Rm(H) \u2264\\n\u221a\\nr2\u039b2\\nm . By the general Rademacher comp', 'lexity learning bound\\nof theorem 3.1, for any \u03b4> 0, with with probability at least 1 \u2212 \u03b4, the following\\nlearning bound holds with M =2 \u039br:\\nE[|h(x) \u2212 f(x)|\u03f5] \u2264 \u02c6E[|h(x) \u2212 f(x)|\u03f5]+2 Rm(H)+ M\\n\u221a\\nlog 1\\n\u03b4\\n2', 'm .\\nUsing Rm(H) \u2264\\n\u221a\\nr2\u039b2\\nm yields the \ufb01rst statement of the theorem. The second\\nstatement is shown in a similar way.\\nThese results provide strong theoretical guarantees for the SVR algorithm. Note,\\nho', 'wever, that the theorem does not provide guarantees for the expected loss of the\\nhypotheses in terms of the squared loss. For 0 <\u03f5< 1/4, the inequality |x|\\n2 \u2264| x|\u03f510.3 Regression algorithms 255\\nholds', ' for all x in [\u2212\u03b7\u2032\\n\u03f5, \u2212\u03b7\u03f5] \u222a [\u03b7\u03f5,\u03b7\u2032\\n\u03f5]w i t h\u03b7\u03f5 = 1\u2212 \u221a1\u22124\u03f5\\n2 and \u03b7\u2032\\n\u03f5 = 1+\u221a1\u22124\u03f5\\n2 .F o r\\nsmall values of \u03f5, \u03b7\u03f5 \u2248 0a n d\u03b7\u2032\\n\u03f5 \u2248 1, thus, if M =2 r\u03bb \u2264 1, then, the squared loss\\ncan be upper bounded by th', 'e\u03f5-insensitive loss for almost all values of (h(x) \u2212 f(x))\\nin [\u22121,1] and the theorem can be used to derive a useful generalization bound for\\nthe squared loss.\\nMore generally, if the objective is to ac', 'hieve a small squared loss, then, SVR\\ncan be modi\ufb01ed by using the quadratic \u03f5-insensitive loss, that is the square of the\\n\u03f5-insensitive loss, which also leads to a convex QP. We will refer byquadratic', ' SVR\\nto this version of the algorithm. Introducing the Lagrangian and applying the KKT\\nconditions leads to the following equivalent dual optimization problem for quadratic\\nSVR in terms of the kernel m', 'atrix K:\\nmax\\n\u03b1 ,\u03b1 \u2032\\n\u2212 \u03f5(\u03b1\u2032 + \u03b1)\u22a41 +( \u03b1\u2032 \u2212 \u03b1)\u22a4y \u2212 1\\n2(\u03b1\u2032 \u2212 \u03b1)\u22a4\\n(\\nK + 1\\nCI\\n\u23a1\\n(\u03b1\u2032 \u2212 \u03b1)\\n(10.30)\\nsubject to: (\u03b1 \u2265 0) \u2227 (\u03b1 \u2265 0) \u2227 (\u03b1\u2032 \u2212 \u03b1)\u22a41 =0 ).\\nAny PDS kernelK can be used with quadratic SVR, which exten', 'ds the algorithm to\\nnon-linear regression solutions. Problem (10.30) is a convex QP similar to the dual\\nproblem of SVMs in the separable case and can be solved using similar optimization\\ntechniques. T', 'he solutions \u03b1 and \u03b1\\n\u2032 de\ufb01ne the hypothesis h returned by SVR as\\nfollows:\\nh(x)=\\nm\u2211\\ni=1\\n(\u03b1\u2032\\ni \u2212 \u03b1i)K(xi,x)+ b, (10.31)\\nwhere the o\ufb00set b can be obtained from a point xj with 0 <\u03b1 j <C or 0 <\u03b1 \u2032\\nj <C\\nex', 'actly as in the case of SVR with (non-quadratic) \u03f5-insensitive loss. Note that for\\n\u03f5 = 0, the quadratic SVR algorithm coincides with KRR as can be seen from the\\ndual optimization problem (the addition', 'al constraint (\u03b1\u2032 \u2212 \u03b1)\u22a41 = 0 appears here\\ndue to use of an o\ufb00set b). The following generalization bound holds for quadratic\\nSVR. It can be shown in a way that is similar to the proof of theorem 10.8 u', 'sing\\nthe fact that the quadratic \u03f5-insensitive function x \u21a6\u2192| x|2\\n\u03f5 is 2-Lipschitz.\\nTheorem 10.9\\nLet K : X\u00d7 X \u2192 R be a PDS kernel, \u03a6: X\u2192 H a feature mapping associated to\\nK,a n dH = {x \u21a6\u2192 w \u00b7 \u03a6(x): \u2225w', '\u2225H \u2264 \u039b}. Assume that there exists r> 0 such that\\nK(x, x) \u2264 r2 and |f(x)|\u2264 \u039br for all x \u2208X .F i x\u03f5> 0.T h e n ,f o ra n y\u03b4> 0,w i t h256 Regression\\n-4 -2 0 2 4\\n0\\n2\\n4\\n6\\n8loss\\nx\\nx \u21a6\u2192 max(0, |x|\u2212 \u03f5)2\\nquad', 'ratic \u03b5-insensitive\\nHuber\\nx \u21a6\u2192\\n{\\nx2 if |x|\u2264 c\\n2c|x|\u2212 c2 otherwise.\\n\u03b5-insensitive\\nx \u21a6\u2192 max(0, |x|\u2212 \u03f5)\\nFigure 10.5 Alternative loss functions that can be used in conjunction with SVR.\\nprobability at lea', 'st 1 \u2212 \u03b4, each of the following inequalities holds for all h \u2208 H:\\nE\\nx\u223cD\\n[|h(x) \u2212 f(x)|2\\n\u03f5] \u2264 E\\nx\u223c bD\\n[|h(x) \u2212 f(x)|2\\n\u03f5]+ 8r2\u039b2\\n\u221am\\n\u239b\\n\u239d1+ 1\\n2\\n\u221a\\nlog 1\\n\u03b4\\n2\\n\u239e\\n\u23a0\\nE\\nx\u223cD\\n[|h(x) \u2212 f(x)|2\\n\u03f5] \u2264 E\\nx\u223c bD\\n[|h(x) \u2212 ', 'f(x)|2\\n\u03f5]+ 8r2\u039b2\\n\u221am\\n\u239b\\n\u239d\\n\u221a\\nTr[K]\\nmr2 + 3\\n4\\n\u221a\\nlog 2\\n\u03b4\\n2\\n\u239e\\n\u23a0 .\\nThis theorem provides a strong justi\ufb01cation for the quadratic SVR algorithm. Alter-\\nnative convex loss functions can be used to de\ufb01ne regres', 'sion algorithms, in particular\\nthe Huber loss (see \ufb01gure 10.5), which penalizes smaller errors quadratically and\\nlarger ones only linearly.\\nSVR admits several advantages: the algorithm is based on sol', 'id theoretical\\nguarantees, the solution returned is sparse, and it allows a natural use of PDS\\nkernels, which extend the algorithm to non-linear regression solutions. SVR also\\nadmits favorable stabili', 'ty properties that we discuss in chapter 11. However, one\\ndrawback of the algorithm is that it requires the selection of two parameters, C\\nand \u03f5. These can be selected via cross-validation, as in the ', 'case of SVMs, but this\\nrequires a relatively larger validation set. Some heuristics are often used to guide\\nthe search for their values: C is searched near the maximum value of the labels in\\nthe absen', 'ce of an o\ufb00set (b = 0) and for a normalized kernel, and\u03f5 is chosen close to\\nthe average di\ufb00erence of the labels. As already discussed, the value of\u03f5 determines\\nthe number of support vectors and the sp', 'arsity of the solution. Another drawback of\\nSVR is that, as in the case of SVMs or KRR, it may be computationally expensive\\nwhen dealing with large training sets. One e\ufb00ective solution in such cases, ', 'as for\\nKRR, consists of approximating the kernel matrix using low-rank approximations\\nvia the Nystr\u00a8om method or the partial Cholesky decomposition. In the next section,10.3 Regression algorithms 257\\n', 'we discuss an alternative sparse algorithm for regression.\\n10.3.4 Lasso\\nUnlike the KRR and SVR algorithms, the Lasso (least absolute shrinkage and\\nselection operator) algorithm does not admit a natura', 'l use of PDS kernels. Thus,\\nhere, we assume that the input space X is a subset of R\\nN and consider a family of\\nlinear hypotheses H = {x \u21a6\u2192 w \u00b7 x + b: w \u2208 RN ,b \u2208 R}.\\nLet S =\\n(\\n(x1,y1),..., (xm,y m)\\n\u23a1\\n', '\u2208 (X\u00d7 Y )m be a labeled training sample.\\nLasso is based on the minimization of the empirical squared error on S with a\\nregularization term depending on the norm of the weight vector, as in the case of', '\\nthe ridge regression, but using the L\\n1 n o r mi n s t e a do ft h eL2 norm and without\\nsquaring the norm:\\nmin\\nw,b\\nF(w,b )= \u03bb\u2225w\u22251 +\\nm\u2211\\ni=1\\n(w \u00b7 xi + b \u2212 yi)2 . (10.32)\\nHere \u03bb denotes a positive param', 'eter as for ridge regression. This is a convex\\noptimization problem, since \u2225\u00b7\u22251 is convex as with all norms and since the empirical\\nerror term is convex, as already discussed for linear regression. Th', 'e optimization\\nfor Lasso can be written equivalently as\\nmin\\nw,b\\nm\u2211\\ni=1\\n(w \u00b7 xi + b \u2212 yi)2 subject to: \u2225w\u22251 \u2264 \u039b1, (10.33)\\nwhere \u039b1 is a positive parameter.\\nThe key property of Lasso as in the case of o', 'ther algorithms using the L1\\nnorm constraint is that it leads to a sparse solution w, that is one with few\\nnon-zero components. Figure 10.6 illustrates the di\ufb00erence between the L1 and L2\\nregularizati', 'ons in dimension two. The objective function of (10.33) is a quadratic\\nfunction, thus its contours are ellipsoids, as illustrated by the \ufb01gure (in blue). The\\nareas corresponding to L1 and L2 balls of ', 'a \ufb01xed radius \u039b 1 are also shown in the\\nleft and right panel (in red). The Lasso solution is the point of intersection of the\\ncontours with the L1 ball. As can be seen form the \ufb01gure, this can typical', 'ly occur\\nat a corner of the L1 ball where some coordinates are zero. In contrast, the ridge\\nregression solution is at the point of intersection of the contours and the L2 ball,\\nw h e r en o n eo ft h ', 'ec o o r d i n a t e si st y p i c a l l yz e r o .\\nThe following results show that Lasso also bene\ufb01ts from strong theoretical guar-\\nantees. We \ufb01rst give a general upper bound on the empirical Rademac', 'her complexity\\nof L\\n1 norm-constrained linear hypotheses .\\nTheorem 10.10 Rademacher complexity of linear hypotheses with bounded258 Regression\\nL1 regularization L2 regularization\\nFigure 10.6 Compariso', 'n of the Lasso and ridge regression solutions.\\nL1 norm\\nLet X\u2286 RN and let S =\\n(\\n(x1,y1),..., (xm,y m)\\n\u23a1\\n\u2208 (X\u00d7 Y )m be a sample of\\nsize m. Assume that for all i \u2208 [1,m], \u2225xi\u2225\u221e \u2264 r\u221e for some r\u221e > 0,a n d', 'l e t\\nH = {x \u2208X \u21a6\u2192 w \u00b7 x: \u2225w\u22251 \u2264 \u039b1}. Then, the empirical Rademacher complexity of\\nH can be bounded as follows:\\n\u02c6RS(H) \u2264\\n\u221a\\n2r2\u221e \u039b2\\n1 log(2N)\\nm . (10.34)\\nProof For any i \u2208 [1,m] we denote by xij the jt', 'h component of xi.\\n\u02c6RS(H)= 1\\nm E\\n\u03c3\\n[\\nsup\\n\u2225w\u22251\u2264\u039b1\\nm\u2211\\ni=1\\n\u03c3iw \u00b7 xi\\n]\\n= \u039b1\\nm E\\n\u03c3\\n[\\ued79\\ued79\\ued79\\nm\u2211\\ni=1\\n\u03c3ixi\\n\\ued79\\ued79\\ued79\\n\u221e\\n]\\n(by de\ufb01nition of the dual norm)\\n= \u039b1\\nm E\\n\u03c3\\n[\\nmax\\nj\u2208[1,N]\\n\u23d0\u23d0\\n\u23d0\\n\u23d0\\n\u23d0\\nm\u2211\\ni=1\\n\u03c3ixij\\n\u23d0\u23d0\\n\u23d0\\n\u23d0\\n\u23d0\\n]\\n(by de\ufb01', 'nition of \u2225\u00b7\u2225\\n\u221e )\\n= \u039b1\\nm E\\n\u03c3\\n[\\nmax\\nj\u2208[1,N]\\nmax\\ns\u2208{\u22121,+1}\\ns\\nm\u2211\\ni=1\\n\u03c3ixij\\n]\\n(by de\ufb01nition of \u2225\u00b7\u2225 \u221e )\\n= \u039b1\\nm E\\n\u03c3\\n[\\nsup\\nz\u2208A\\nm\u2211\\ni=1\\n\u03c3izi\\n]\\n,\\nwhere A denotes the set of N vectors {s(x1j,...,x mj)\u22a4 : j \u2208 [1,', 'N ],s \u2208{ \u22121, +1}}.\\nFor any z \u2208 A,w eh a v e\u2225z\u22252 \u2264\\n\u221a\\nmr2\u221e = r\u221e\\n\u221am. Thus, by Massart\u2019s lemma10.3 Regression algorithms 259\\n(theorem 3.3), sinceA contains at most 2N elements, the following inequality ho', 'lds:\\n\u02c6RS(H) \u2264 \u039b1r\u221e\\n\u221am\\n\u221a\\n2l o g( 2N)\\nm = r\u221e \u039b1\\n\u221a\\n2l o g( 2N)\\nm ,\\nwhich concludes the proof.\\nNote that dependence of the bound on the dimension N is only logarithmic, which\\nsuggests that using very high', '-dimensional feature spaces does not signi\ufb01cantly a\ufb00ect\\ngeneralization.\\nUsing the Rademacher complexity bound just proven and the general result of\\ntheorem 10.3, the following generalization bound can', ' be shown to hold for the\\nhypothesis set used by Lasso, using the squared loss.\\nTheorem 10.11\\nLet X\u2286 R\\nN and H = {x \u2208X \u21a6\u2192 w \u00b7 x: \u2225w\u22251 \u2264 \u039b1}.A s s u m et h a tt h e r ee x i s t s\\nr\u221e > 0 such for all x', ' \u2208X , \u2225x\u2225\u221e \u2264 r\u221e and |f(x)|\u2264 \u039b1r\u221e .T h e n ,f o ra n y\u03b4> 0,\\nwith probability at least1 \u2212 \u03b4, each of the following inequalities holds for all h \u2208 H:\\nR(h) \u2264 \u02c6R(h)+ 8r2\\n\u221e \u039b2\\n1\u221am\\n\u239b\\n\u239d\u221a\\nlog(2N)+ 1\\n2\\n\u221a\\nlog 1\\n', '\u03b4\\n2\\n\u239e\\n\u23a0 . (10.35)\\nProof For allx \u2208X ,b yH \u00a8older\u2019s inequality, we have|w\u00b7x|\u2264\u2225 w\u22251\u2225x\u2225\u221e \u2264 \u039b1r\u221e ,\\nthus, for all h \u2208 H, |h(x) \u2212 f(x)|\u2264 2r\u221e \u039b1. Plugging in the inequality of\\ntheorem 10.10 in the bound of t', 'heorem 10.3 with M =2 r\u221e \u039b1 gives\\nR(h) \u2264 \u02c6R(h)+8 r2\\n\u221e \u039b2\\n1\\n\u221a\\n2l o g( 2N)\\nm +( 2r\u221e \u039b1)2\\n\u221a\\nlog 1\\n\u03b4\\n2m ,\\nwhich can be simpli\ufb01ed and written as (10.35).\\nAs in the case of ridge regression, we observe that', ' the objective function minimized\\nby Lasso has the same form as the right-hand side of this generalization bound.\\nThere exist a variety of di\ufb00erent methods for solving the optimization problem of\\nLass', 'o, including an e\ufb03cient algorithm (Lars) for computing the entireregularization\\npath of solutions, that is, the Lasso solutions for all values of the regularization\\nparameter \u03bb, and other on-line solu', 'tions that apply more generally to optimization\\nproblems with an L1 norm constraint.\\nHere, we show that the Lasso problems (10.32) or (10.33) are equivalent to a\\nquadratic program (QP), and therefore ', 'that any QP solver can be used to compute\\nthe solution. Observe that any weight vector w c a nb ew r i t t e na sw = w+ \u2212 w\u2212 ,\\nwith w+ \u2265 0, w\u2212 \u2265 0, and w+\\nj =0o r w\u2212\\nj =0f o ra n y j \u2208 [1,N ], which i', 'mplies\\n\u2225w\u22251 = \u2211N\\nj=1 w+\\nj + w\u2212\\nj . This can be done by de\ufb01ning the jth component of w+ as\\nwj if wj \u2265 0, 0 otherwise, and similarly thejth component of w\u2212 as \u2212wj if wj \u2264 0,260 Regression\\n0 otherwise, f', 'or any j \u2208 [1,N ]. With the replacementw = w+ \u2212 w\u2212 ,w i t hw+ \u2265 0,\\nw\u2212 \u2265 0, and \u2225w\u22251 = \u2211N\\nj=1 w+\\nj + w\u2212\\nj , the Lasso problem (10.32) becomes\\nmin\\nw+\u22650,w\u2212 \u22650,b\\n\u03bb\\nN\u2211\\nj=1\\n(w+\\nj + w\u2212\\nj )+\\nm\u2211\\ni=1\\n(\\n(w+ \u2212 w\u2212', ' ) \u00b7 xi + b \u2212 yi\\n\u23a12\\n. (10.36)\\nConversely, a solution w = w+ \u2212 w\u2212 of (10.36) veri\ufb01es the condition w+\\nj =0o r\\nw\u2212\\nj =0f o ra n yj \u2208 [1,N ], thus wj = w+\\nj when wj \u2265 0a n dwj = \u2212w\u2212\\nj when wj \u2264 0.\\nThis is', ' because if \u03b4j =m i n (w+\\nj ,w \u2212\\nj ) > 0f o rs o m ej \u2208 [1,N ], replacing w+\\nj with\\n(w+\\nj \u2212 \u03b4j)a n dw\u2212\\nj with (w\u2212\\nj \u2212 \u03b4j)w o u l dn o ta \ufb00 e c tw+\\nj \u2212 w\u2212\\nj =( w+\\nj \u2212 \u03b4) \u2212 (w\u2212\\nj \u2212 \u03b4),\\nbut would reduce ', 'the term ( w+\\nj + w\u2212\\nj ) in the objective function by 2 \u03b4j > 0a n d\\nprovide a better solution. In view of this analysis, problems (10.32) and (10.36)\\nadmit the same optimal solution and are equivalent', '. Problem (10.36) is a QP since\\nthe objective function is quadratic in w\\n+, w\u2212 ,a n db, and since the constraints are\\na\ufb03ne. With this formulation, the problem can be straightforwardly shown to admit\\na', ' natural online algorithmic solution (exercise 10.10).\\n2\\nThus, Lasso has several advantages: it bene\ufb01ts from strong theoretical guarantees\\nand returns a sparse solution, which is advantageous when the', 're are accurate\\nsolutions based on few features. The sparsity of the solution is also computationally\\nattractive; sparse feature representations of the weight vector can be used to make\\nthe inner prod', 'uct with a new vector more e\ufb03cient. The algorithm\u2019s sparsity can also\\nbe used for feature selection. The main drawback of the algorithm is that it does not\\nadmit a natural use of PDS kernels and thus ', 'an extension to non-linear regression,\\nunlike KRR and SVR. One solution is then to use empirical kernel maps, as discussed\\nin chapter 5. Also, Lasso\u2019s solution does not admit a closed-form solution. T', 'his is\\nnot a critical property from the optimization point of view but one that can make\\nsome mathematical analyses very convenient.\\n10.3.5 Group norm regression algorithms\\nOther types of regularizati', 'on aside from the L\\n1 or L2 norm can be used to de\ufb01ne\\nregression algorithms. For instance, in some situations, the feature space may be\\nnaturally partitioned into subsets, and it may be desirable to \ufb01', 'nd a sparse solution\\nthat selects or omits entire subsets of features. A natural norm in this setting is\\nthe group or mixed norm L\\n2,1, which is a combination of the L1 and L2 norms.\\nImagine that we p', 'artition w \u2208 RN as w1,..., wk,w h e r ewj \u2208 RNj for 1 \u2264 j \u2264 k\\nand \u2211\\nj Nj = N, and de\ufb01ne W =( w\u22a4\\n1 ,..., w\u22a4\\nk )\u22a4.T h e nt h eL2,1 norm of W is\\n2. The technique we described to avoid absolute values in ', 'the objective function can be\\nused similarly in other optimization problems.10.3 Regression algorithms 261\\nWidrowHoff(w0)\\n1 w1 \u2190 w0 \u22bf typically w0 = 0\\n2 for t \u2190 1 to T do\\n3 Receive(xt)\\n4 \u02c6yt \u2190 wt \u00b7 xt', '\\n5 Receive(yt)\\n6 wt+1 \u2190 wt +2 \u03b7(wt \u00b7 xt \u2212 yt)xt \u22bf learning rate \u03b7> 0.\\n7 return wT+1\\nFigure 10.7 The Widrow-Ho\ufb00 algorithm.\\nde\ufb01ned as\\n\u2225W\u22252,1 =\\nk\u2211\\nj=1\\n\u2225wj \u2225 .\\nCombining the L2,1 norm with the empirical m', 'ean squared error leads to theGroup\\nLasso formulation. More generally, an Lq,p group norm regularization can be used\\nfor q,p \u2265 1 (see appendix A for the de\ufb01nition of group norms).\\n10.3.6 On-line regre', 'ssion algorithms\\nThe regression algorithms presented in the previous sections admit natural on-\\nline versions. Here, we brie\ufb02y present two examples of these algorithms. These\\nalgorithms are particular', 'ly useful for applications to very large data sets for which\\na batch solution can be computationally too costly to derive and more generally in\\nall of the on-line learning settings discussed in chapte', 'r 7.\\nOur \ufb01rst example is known as the Widrow-Ho\ufb00 algorithm and coincides with\\nthe application of stochastic gradient descent techniques to the linear regression\\nobjective function. Figure 10.7 gives t', 'he pseudocode of the algorithm. A similar\\nalgorithm can be derived by applying the stochastic gradient technique to ridge\\nregression. At each round, the weight vector is augmented with a quantity that', '\\ndepends on the prediction error (w\\nt \u00b7 xt \u2212 yt).\\nOur second example is an online version of the SVR algorithm, which is obtained\\nby application of stochastic gradient descent to the dual objective fu', 'nction of SVR.\\nFigure 10.8 gives the pseudocode of the algorithm for an arbitrary PDS kernel K\\nin the absence of any o\ufb00set (b = 0). Another on-line regression algorithm is given262 Regression\\nOnLineDu', 'alSVR()\\n1 \u03b1 \u2190 0\\n2 \u03b1\u2032 \u2190 0\\n3 for t \u2190 1 to T do\\n4 Receive(xt)\\n5 \u02c6yt \u2190 \u2211T\\ns=1(\u03b1\u2032\\ns \u2212 \u03b1s)K(xs,xt)\\n6 Receive(yt)\\n7 \u03b1\u2032\\nt+1 \u2190 \u03b1\u2032\\nt + min(max(\u03b7(yt \u2212 \u02c6yt \u2212 \u03f5), \u2212\u03b1\u2032\\nt),C \u2212 \u03b1\u2032\\nt)\\n8 \u03b1t+1 \u2190 \u03b1t + min(max(\u03b7(\u02c6yt \u2212 yt ', '\u2212 \u03f5), \u2212\u03b1t),C \u2212 \u03b1t)\\n9 return \u2211T\\nt=1 \u03b1tK(xt, \u00b7)\\nFigure 10.8 An on-line version of dual SVR.\\nby exercise 10.10 for Lasso.\\n10.4 Chapter notes\\nThe generalization bounds presented in this chapter are for bo', 'unded regression\\nproblems. When {x \u21a6\u2192 L(h(x),f (x)): h \u2208 H}, the family of losses of the hypotheses,\\nis not bounded, a single function can take arbitrarily large values with arbitrarily\\nsmall probabil', 'ities. This is the main issue for deriving uniform convergence bounds\\nfor unbounded losses. This problem can be avoided either by assuming the existence\\nof an envelope, that is a single non-negative f', 'unction with a \ufb01nite expectation lying\\nabove the absolute value of the loss of every function in the hypothesis set [Dudley,\\n1984, Pollard, 1984, Dudley, 1987, Pollard, 1989, Haussler, 1992], or by as', 'suming\\nthat some moment of the loss functions is bounded [Vapnik, 1998, 2006]. Cortes,\\nMansour, and Mohri [2010a] give two-sided generalization bounds for unbounded\\nlosses with \ufb01nite second moments. T', 'he one-sided version of their bounds coincides\\nwith that of Vapnik [1998, 2006] modulo a constant factor, but the proofs given by\\nVapnik in both books seem to be incorrect.\\nThe Rademacher complexity b', 'ounds given for regression in this chapter (theo-\\nrem 10.2) are novel. The notion of pseudo-dimension is due to Pollard [1984]. Its\\nequivalent de\ufb01nition in terms of VC-dimension is discussed by Vapnik', ' [2000]. The\\nnotion of fat-shattering was introduced by Kearns and Schapire [1990]. The linear\\nregression algorithm is a classical algorithm in statistics that dates back at least to10.5 Exercises 263', '\\nthe nineteenth century. The ridge regression algorithm is due to Hoerl and Kennard\\n[1970]. Its kernelized version (KRR) was introduced and discussed by Saunders,\\nGammerman, and Vovk [1998]. An extens', 'ion of KRR to outputs in R\\np with p> 1\\nwith possible constraints on the regression is presented and analyzed by Cortes,\\nMohri, and Weston [2007c]. The support vector regression (SVR) algorithm is dis-', '\\ncussed in Vapnik [2000]. Lasso was introduced by Tibshirani [1996]. The LARS\\nalgorithm for solving its optimization problem was later presented by Efron et al.\\n[2004]. The Widrow-Ho\ufb00 on-line algorith', 'm is due to Widrow and Ho\ufb00 [1988]. The\\ndual on-line SVR algorithm was \ufb01rst introduced and analyzed by Vijayakumar and\\nWu [1999]. The kernel stability analysis of exercise 9.3 is from Cortes et al. [20', '10b].\\nFor large-scale problems where a straightforward batch optimization of a primal or\\ndual objective function is intractable, general iterative stochastic gradient descent\\nmethods similar to those ', 'presented in section 10.3.6, or quasi-Newton methods\\nsuch as the limited-memory BFGS (Broyden-Fletcher-Goldfard-Shanno) algorithm\\n[Nocedal, 1980] can be practical alternatives in practice.\\nIn addition', ' to the linear regression algorithms presented in this chapter and their\\nkernel-based non-linear extensions, there exist many other algorithms for regression,\\nincluding decision trees for regression (', 'see chapter 8), boosting trees for regression,\\nand arti\ufb01cial neural networks.\\n10.5 Exercises\\n10.1 Pseudo-dimension and monotonic functions.\\nAssume that \u03c6 is a strictly monotonic function and let \u03c6 \u25e6H ', 'be the family of\\nfunctions de\ufb01ned by \u03c6 \u25e6H = {\u03c6(h(\u00b7)) : h \u2208 H},w h e r eH is some set of real-valued\\nfunctions. Show that Pdim(\u03c6 \u25e6H)=P d i m (H).\\n10.2 Pseudo-dimension of linear functions. Let H be the', ' set of all linear functions\\nin dimension d, i.e. h(x)= w\u22a4x for some w \u2208 Rd. Show that Pdim(H)= d.\\n10.3 Linear regression.\\n(a) What condition is required on the data X in order to guarantee thatXX\u22a4\\nis', ' invertible?\\n(b) Assume the problem is under-determined. Then, we can choose a solution\\nw such that the equality X\u22a4w = X\u22a4(XX\u22a4)\u2020 Xy (which can be shown to\\nequal X\u2020 Xy) holds. One particular choice that', ' satis\ufb01es this equality is w\u2217 =\\n(XX\u22a4)\u2020 Xy. However, this is not the unique solution. As a function of w\u2217,\\ncharacterize all choices of w that satisfy X\u22a4w = X\u2020 Xy (Hint: use the fact264 Regression\\nthat ', 'XX\u2020 [X = X).\\n10.4 Perturbed kernels. Suppose two di\ufb00erent kernel matrices,K and K\u2032,a r eu s e dt o\\ntrain two kernel ridge regression hypothesis with the same regularization parameter\\n\u03bb. In this proble', 'm, we will show that the di\ufb00erence in the optimal dual variables,\\n\u03b1 and \u03b1\\n\u2032 respectively, is bounded by a quantity that depends on \u2225K\u2032 \u2212 K\u22252.\\n(a) Show \u03b1\u2032 \u2212 \u03b1 =\\n(\\n(K\u2032 + \u03bbI)\u22121(K\u2032 \u2212 K)(K + \u03bbI)\u22121\u23a1\\ny.( Hin', 't:S h o wt h a tf o r\\nany invertible matrix M, M\u20321 \u2212 M1 = \u2212M\u2032\u22121(M\u2032 \u2212 M)M\u22121.)\\n(b) Assuming \u2200y \u2208Y , |y|\u2264 M, show that\\n\u2225\u03b1\u2032 \u2212 \u03b1\u2225\u2264\\n\u221amM \u2225K\u2032 \u2212 K\u22252\\n\u03bb2 .\\n10.5 Huber loss. Derive the primal and dual optimizatio', 'n problem used to solve the\\nSVR problem with the Huber loss:\\nLc(\u03bei)=\\n{\\n1\\n2 \u03be2\\ni , if |\u03bei|\u2264 c\\nc\u03bei \u2212 1\\n2 c2, otherwise\\n,\\nwhere \u03bei = w \u00b7 \u03a6(xi)+ b \u2212 yi.\\n10.6 SVR and squared loss. Assuming that 2 r\u039b \u2264 1, ', 'use theorem 10.8 to derive a\\ngeneralization bound for the squared loss.\\n10.7 SVR dual formulations. Give a detailed and carefully justi\ufb01ed derivation of\\nthe dual formulations of the SVR algorithm both', ' for the \u03f5-insensitive loss and the\\nquadratic \u03f5-insensitive loss.\\n10.8 Optimal kernel matrix. Suppose in addition to optimizing the dual variables\\n\u03b1 \u2208 Rm, as in (10.19), we also wish to optimize over ', 'the entries of the PDS kernel\\nmatrix K \u2208 Rm\u00d7m.\\nmin\\nK\u2ab00\\nmax\\n\u03b1\\n\u2212\u03bb\u03b1\u22a4\u03b1 \u2212 \u03b1\u22a4K\u03b1 +2 \u03b1\u22a4y , s.t. \u2225K\u22252 \u2264 1\\n(a) What is the closed-form solution for the optimal K for the joint optimiza-\\ntion?\\n(b) Optimizing ove', 'r the choice of kernel matrix will provide a better value of\\nthe objective function. Explain, however, why the resulting kernel matrix is not\\nuseful in practice.10.5 Exercises 265\\nOnLineLasso(w+\\n0 ,w\u2212', '\\n0 )\\n1 w+\\n1 \u2190 w+\\n0 \u22bf w+\\n0 \u2265 0\\n2 w\u2212\\n1 \u2190 w\u2212\\n0 \u22bf w\u2212\\n0 \u2265 0\\n3 for t \u2190 1 to T do\\n4 Receive(xt,y t)\\n5 for j \u2190 1 to N do\\n6 w+\\nt+1j \u2190 max\\n(\\n0,w +\\ntj \u2212 \u03b7\\n[\\n\u03bb \u2212\\n[\\nyt \u2212 (w+\\nt \u2212 w\u2212\\nt ) \u00b7 xt\\n]\\nxtj\\n]\u23a1\\n7 w\u2212\\nt+1j \u2190 ma', 'x\\n(\\n0,w \u2212\\ntj \u2212 \u03b7\\n[\\n\u03bb+\\n[\\nyt \u2212 (w+\\nt \u2212 w\u2212\\nt ) \u00b7 xt\\n]\\nxtj\\n]\u23a1\\n8 return w+\\nT+1 \u2212 w\u2212\\nT+1\\nFigure 10.9 On-line algorithm for Lasso.\\n10.9 Leave-one-out error. In general, the computation of the leave-one-out e', 'rror\\ncan be very costly since, for a sample of size m, it requires training the algorithm\\nm times. The objective of this problem is to show that, remarkably, in the case\\nof kernel ridge regression, th', 'e leave-one-out error can be computed e\ufb03ciently by\\ntraining the algorithm only once.\\nLet S =( (x1,y1),..., (xm,y m)) denote a training sample of size m and for any\\ni \u2208 [1,m], let Si denote the sample ', 'of size m \u2212 1 obtained from S by removing\\n(xi,y i): Si = S \u2212{ (xi,y i)}. For any sampleT,l e thT denote a hypothesis obtained\\nby training T. By de\ufb01nition (see de\ufb01nition 4.1), for the squared loss, the', ' leave-one-\\nout error with respect to S is de\ufb01ned by\\n\u02c6RLOO(KRR) = 1\\nm\\nm\u2211\\ni=1\\n(hSi (xi) \u2212 yi)2 .\\n(a) Let S\u2032\\ni =( (x1,y1),..., (xi,h Si (yi)),..., (xm,y m)). Show that hSi = hS\u2032\\ni\\n.\\n(b) De\ufb01ne yi = y \u2212 y', 'iei + hSi (xi)ei, that is the vector of labels with the\\nith component replaced with hSi (xi). Prove that for KRR hSi (xi)= y\u22a4\\ni (K +\\n\u03bbI)\u22121Kei.\\n(c) Prove that the leave-one-out error admits the followi', 'ng simple expression\\nin terms of hS:\\n\u02c6RLOO(KRR) = 1\\nm\\nm\u2211\\ni=1\\n[ hS(xi) \u2212 yi\\ne\u22a4\\ni (K + \u03bbI)\u22121Kei\\n]2\\n. (10.37)266 Regression\\n(d) Suppose that the diagonal entries of matrixM =( K+\u03bbI)\u22121K are all equal\\nto \u03b3', '. How do the empirical error \u02c6R of the algorithm and the leave-one-out error\\n\u02c6RLOO relate? Is there any value of \u03b3 for which the two errors coincide?\\n10.10 On-line Lasso. Use the formulation (10.36) o', 'f the optimization problem of\\nLasso and stochastic gradient descent (see section 7.3.1) to show that the problem\\ncan be solved using the on-line algorithm of \ufb01gure 10.9.\\n10.11 On-line quadratic SVR. D', 'erive an on-line algorithm for the quadratic SVR\\nalgorithm (provide the full pseudocode).11 Algorithmic Stability\\nIn chapters 2\u20134 and several subsequent chapters, we presented a variety of general-\\niz', 'ation bounds based on di\ufb00erent measures of the complexity of the hypothesis set\\nH used for learning, including the Rademacher complexity, the growth function,\\nand the VC-dimension. These bounds ignore', ' the speci\ufb01c algorithm used, that is,\\nthey hold for any algorithm using H as a hypothesis set.\\nOne may ask if an analysis of the properties of a speci\ufb01c algorithm could lead\\nto \ufb01ner guarantees. Such a', 'n algorithm-dependent analysis could have the bene\ufb01t\\nof a more informative guarantee. On the other hand, it could be inapplicable to\\no t h e ra l g o r i t h m su s i n gt h es a m eh y p o t h e s i ', 'ss e t .A l t e r n a t i v e l y ,a sw es h a l ls e ei n\\nthis chapter, a more general property of the learning algorithm could be used to\\nincorporate algorithm-speci\ufb01c properties while extending the', ' applicability of the\\nanalysis to other learning algorithms with similar properties.\\nThis chapter uses the property of algorithmic stability to derive algorithm-\\ndependent learning guarantees. We \ufb01rst', ' present a generalization bound for any\\nalgorithm that is su\ufb03ciently stable. Then, we show that the wide class of kernel-\\nbased regularization algorithms enjoys this property and derive a general uppe', 'r\\nbound on their stability coe\ufb03cient. Finally, we illustrate the application of these\\nresults to the analysis of several algorithms both in the regression and classi\ufb01cation\\nsettings, including kernel ', 'ridge regression (KRR), SVR, and SVMs.\\n11.1 De\ufb01nitions\\nWe start by introducing the notation and de\ufb01nitions relevant to our analysis of\\nalgorithmic stability. We denote by z a labeled example ( x, y) \u2208', 'X\u00d7 Y .T h e\\nhypotheses h we consider map X to a set Y\\n\u2032 sometimes di\ufb00erent from Y.I n\\nparticular, for classi\ufb01cation, we may have Y = {\u22121, +1} while the hypothesis h\\nlearned takes values in R. The loss', ' functions L we consider are therefore de\ufb01ned\\nover Y\u2032 \u00d7Y ,w i t hY\u2032 = Y in most cases. For a loss function L: Y\u2032 \u00d7Y \u2192 R+,w e\\ndenote the loss of a hypothesis h at point z by Lz(h)= L(h(x),y ). We denot', 'e by\\nD the distribution according to which samples are drawn and byH the hypothesis268 Algorithmic Stability\\nset. The empirical error or loss of h \u2208 H on a sample S =( z1,...,z m)a n di t s\\ngeneraliza', 'tion error are de\ufb01ned, respectively, by\\n\u02c6R(h)= 1\\nm\\nm\u2211\\ni=1\\nLzi (h)a n d R(h)= E\\nz\u223cD\\n[Lz(h)].\\nG i v e na na l g o r i t h mA,w ed e n o t eb yhS the hypothesis hS \u2208 H returned by A when\\ntrained on sampl', 'e S.W ew i l ls a yt h a tt h el o s sf u n c t i o nL is bounded by M \u2265 0i f\\nfor all h \u2208 H and z \u2208X\u00d7Y , Lz(h) \u2264 M. For the results presented this chapter, a\\nweaker condition su\ufb03ces, namely that Lz(hS', ') \u2264 M for all hypotheses hS returned\\nby the algorithm A considered.\\nWe are now able to de\ufb01ne the notion of uniform stability, the algorithmic property\\nu s e di nt h ea n a l y s e so ft h i sc h a p t', ' e r .\\nDe\ufb01nition 11.1 Uniform stability\\nLet S and S\u2032 be any two training samples that di\ufb00er by a single point. Then, a\\nlearning algorithm A is uniformly \u03b2-stable if the hypotheses it returns when trai', 'ned\\non any such samples S and S\u2032 satisfy\\n\u2200z \u2208Z , |Lz(hS) \u2212 Lz(hS\u2032 )|\u2264 \u03b2.\\nThe smallest such \u03b2 satisfying this inequality is called the stability coe\ufb03cient of A.\\nIn other words, whenA is trained on two ', 'similar training sets, the losses incurred by\\nthe corresponding hypotheses returned byA should not di\ufb00er by more than\u03b2. Note\\nthat a uniformly\u03b2-stable algorithm is often referred to as being\u03b2-stable or', ' even just\\nstable (for some unspeci\ufb01ed \u03b2). In general, the coe\ufb03cient \u03b2 depends on the sample\\nsize m. We will see in section 11.2 that\u03b2 = o(1/\u221am) is necessary for the convergence\\nof the stability-based', ' learning bounds presented in this chapter. In section 11.3, we\\nwill show that a more favorable condition holds, that is, \u03b2 = O(1/m), for a wide\\nfamily of algorithms.\\n11.2 Stability-based generalizati', 'on guarantee\\nIn this section, we show that exponential bounds can be derived for the generaliza-\\ntion error of stable learning algorithms. The main result is presented in theorem 11.1.\\nTheorem 11.1\\nAs', 'sume that the loss function L is bounded by M \u2265 0.L e tA be a \u03b2-stable learning\\nalgorithm and letS be a sample ofm points drawn i.i.d. according to distributionD.11.2 Stability-based generalization gu', 'arantee 269\\nThen, with probability at least 1 \u2212 \u03b4over the sample S drawn, the following holds:\\nR(hS) \u2264 \u02c6R(hS)+ \u03b2 +( 2m\u03b2 + M)\\n\u221a\\nlog 1\\n\u03b4\\n2m .\\nProof The proof is based on the application of McDiarmid\u2019s i', 'nequality (theo-\\nrem D.3) to the function \u03a6 de\ufb01ned for all samples S by \u03a6(S)= R(hS) \u2212 \u02c6R(hS). Let\\nS\u2032 be another sample of size m with points drawn i.i.d. according to D that di\ufb00ers\\nfrom S by exactly o', 'ne point. We denote that point by zm in S, z\u2032\\nm in S\u2032, i.e.,\\nS =( z1,...,z m\u22121,z m)a n d S\u2032 =( z1,...,z m\u22121,z \u2032\\nm).\\nBy de\ufb01nition of \u03a6, the following inequality holds:\\n|\u03a6(S\u2032) \u2212 \u03a6(S)|\u2264| R(hS\u2032 ) \u2212 R(hS)|', ' + | \u02c6R(hS\u2032 ) \u2212 \u02c6R(hS)|. (11.1)\\nWe bound each of these two terms separately. By the\u03b2-stability of A,w eh a v e\\n|R(hS) \u2212 R(hS\u2032 )| = | E\\nz\\n[Lz(hS)] \u2212 E\\nz\\n[Lz(hS\u2032 )]|\u2264 E\\nz\\n[|Lz(hS) \u2212 Lz(hS\u2032 )|] \u2264 \u03b2.\\nUsin', 'g the boundedness of L along with \u03b2-stability of A,w ea l s oh a v e\\n| \u02c6R(hS) \u2212 \u02c6R(hS\u2032 )| = 1\\nm\\n\u23d0\u23d0\u23d0\u23d0\\n\u23d0\\n(\\nm\u22121\u2211\\ni=1\\nLzi (hS) \u2212 Lzi (hS\u2032 )\\n\u23a1\\n+ Lzm (hS) \u2212 Lz\u2032\\nm (hS\u2032 )\\n\u23d0\u23d0\u23d0\u23d0\\n\u23d0\\n\u2264 1\\nm\\n[( m\u22121\u2211\\ni=1\\n|Lzi (hS) \u2212', ' Lzi (hS\u2032 )|\\n\u23a1\\n+ |Lzm (hS) \u2212 Lz\u2032\\nm (hS\u2032 )|\\n]\\n\u2264 m \u2212 1\\nm \u03b2 + M\\nm \u2264 \u03b2 + M\\nm .\\nThus, in view of (11.1), \u03a6 satis\ufb01es the condition |\u03a6(S) \u2212 \u03a6(S\u2032)|\u2264 2\u03b2 + M\\nm .B y\\napplying McDiarmid\u2019s inequality to \u03a6( S), we ', 'can bound the deviation of \u03a6 from\\nits mean as\\nPr\\n[\\n\u03a6(S) \u2265 \u03f5 +E\\nS\\n[\u03a6(S)]\\n]\\n\u2264 exp\\n( \u22122m\u03f52\\n(2m\u03b2 + M)2\\n\u23a1\\n,\\nor, equivalently, with probability 1\u2212 \u03b4,\\n\u03a6(S) <\u03f5 +E\\nS\\n[\u03a6(S)], (11.2)\\nwhere \u03b4=e x p\\n(\\n\u22122m\u03f52\\n(2m\u03b2+M', ')2\\n\u23a1\\n. If we solve for\u03f5 in this expression for\u03b4, plug into (11.2)270 Algorithmic Stability\\nand rearrange terms, then, with probability 1 \u2212 \u03b4,w eh a v e\\n\u03a6(S) \u2264 E\\nS\u223cDm\\n[\u03a6(S) ]+( 2m\u03b2 + M)\\n\u221a\\nlog 1\\n\u03b4\\n2m . ', '(11.3)\\nWe now bound the expectation term, \ufb01rst noting that by linearity of expectation\\nES[\u03a6(S)] = ES[R(hS)] \u2212 ES[ \u02c6R(hS)]. By de\ufb01nition of the generalization error,\\nE\\nS\u223cDm\\n[R(hS)] = E\\nS\u223cDm\\n[\\nE\\nz\u223cD\\n[Lz', '(hS)]\\n]\\n=E\\nS,z\u223cDm+1\\n[Lz(hS)]. (11.4)\\nBy the linearity of expectation,\\nE\\nS\u223cDm\\n[ \u02c6R(hS)] = 1\\nm\\nm\u2211\\ni=1\\nE\\nS\u223cDm\\n[Lzi (hS)] = E\\nS\u223cDm\\n[Lz1 (hS)], (11.5)\\nwhere the second equality follows from the fact that t', 'hezi are drawn i.i.d. and thus\\nthe expectations ES\u223cDm [Lzi (hS)], i \u2208 [1,m], are all equal. The last expression in\\n(11.5) is the expected loss of a hypothesis on one of its training points. We can\\nrew', 'rite it as E S\u223cDm [Lz1 (hS) ]=E S,z\u223cDm+1 [Lz(hS\u2032 )], where S\u2032 is a sample of m\\npoints containing z extracted from the m +1p o i n t sf o r m e db yS and z.T h u s ,i n\\nview of (11.4) and by the \u03b2-stab', 'ility of A, it follows that\\n| E\\nS\u223cDm\\n[\u03a6(S)]| =\\n\u23d0\u23d0 E\\nS,z\u223cDm+1\\n[Lz(hS)] \u2212 E\\nS,z\u223cDm+1\\n[Lz(hS\u2032 )]\\n\u23d0\u23d0\\n\u2264 E\\nS,z\u223cDm+1\\n[\\n|Lz(hS) \u2212 Lz(hS\u2032 )|\\n]\\n\u2264 E\\nS,z\u223cDm+1\\n[\u03b2]= \u03b2.\\nWe can thus replace ES[\u03a6(S)] by \u03b2 in (11.3), ', 'which completes the proof.\\nThe bound of the theorem converges for (m\u03b2)/\u221am = o(1), that is \u03b2 = o(1/\u221am). In\\nparticular, when the stability coe\ufb03cient\u03b2 is inO(1/m), the theorem guarantees that\\nR(hS)\u2212 \u02c6R(h', 'S)= O(1/\u221am) with high probability. In the next section, we show that\\nkernel-based regularization algorithms precisely admit this property under some\\ngeneral assumptions.\\n11.3 Stability of kernel-based', ' regularization algorithms\\nLet K be a positive de\ufb01nite symmetric kernel, H the reproducing kernel Hilbert\\nspace associated to K,a n d \u2225\u00b7\u2225 K the norm induced by K in H.Ak e r n e l - b a s e d\\nregulari', 'zation algorithm is de\ufb01ned by the minimization over H of an objective\\nfunction FS based on a training sample S =( z1,...,z m) and de\ufb01ned for all h \u2208 H11.3 Stability of kernel-based regularization algo', 'rithms 271\\nx y\\n}\\nF(y)\\nBF (y||x)\\nF(x)+( y \u2212 x) \u00b7\u2207 F(x)\\nFigure 11.1 Illustration of the quantity measured by the Bregman divergence\\nde\ufb01ned based on a convex and di\ufb00erentiable function F. The divergence ', 'measures\\nthe distance between F(y) and the hyperplane tangent to the curve at point x.\\nby:\\nFS(h)= \u02c6RS(h)+ \u03bb\u2225h\u22252\\nK. (11.6)\\nIn this equation, \u02c6RS(h)= 1\\nm\\n\u2211m\\ni=1 Lzi (h) is the empirical error of hypothe', 'sish with\\nrespect to a loss functionL and \u03bb \u2265 0 a trade-o\ufb00 parameter balancing the emphasis\\non the empirical error versus the regularization term \u2225h\u22252\\nK. The hypothesis set H\\nis the subset of H formed', ' by the hypotheses possibly returned by the algorithm.\\nAlgorithms such as KRR, SVR and SVMs all fall under this general model.\\nWe \ufb01rst introduce some de\ufb01nitions and tools needed for a general proof of', ' an\\nupper bound on the stability coe\ufb03cient of kernel-based regularization algorithms.\\nOur analysis will assume that the loss function L i sc o n v e xa n dt h a ti tf u r t h e r\\nveri\ufb01es the following', ' Lipschitz-like smoothness condition.\\nDe\ufb01nition 11.2 \u03c3-admissibility\\nA loss function L is \u03c3-admissible with respect to the hypothesis class H if there\\nexists \u03c3 \u2208 R\\n+ such that for any two hypothesesh,', ' h\u2032 \u2208 H and for all (x, y) \u2208X\u00d7 Y ,\\n|L(h\u2032(x),y ) \u2212 L(h(x),y )|\u2264 \u03c3|h\u2032(x) \u2212 h(x)|. (11.7)\\nThis assumption holds for the quadratic loss and most other loss functions where\\nthe hypothesis set and the set o', 'f output labels are bounded by some M \u2208 R+:\\n\u2200h \u2208 H, \u2200x \u2208X , |h(x)|\u2264 M and \u2200y \u2208Y , |y|\u2264 M.\\nWe will use the notion of Bregman divergence, BF which can be de\ufb01ned for any\\nconvex and di\ufb00erentiable function', 'F : H \u2192 R as follows: for all f,g \u2208 H,\\nBF (f \u2225g)= F(f) \u2212 F(g) \u2212\u27e8 f \u2212 g, \u2207F(g)\u27e9 .\\nFigure 11.1 illustrates the geometric interpretation of the Bregman divergence. We\\ngeneralize this de\ufb01nition to cover t', 'he case of convex but non-di\ufb00erentiable loss272 Algorithmic Stability\\n}\\n\u03b4F(h)\\nF(h)\\nFigure 11.2 Illustration of the notion of sub-gradient: elements of the subgradient\\nset \u2202F(h) are shown in red at poi', 'nt h, for the function F s h o w ni nb l u e .\\nfunctions F by using the notion of subgradient. For a convex function F : H \u2192 R,\\nwe denote by \u2202F(h) the subgradient of F at h, which is de\ufb01ned as follows', ':\\n\u2202F(h)= {g \u2208 H: \u2200h\u2032 \u2208 H,F (h\u2032) \u2212 F(h) \u2265\u27e8 h\u2032 \u2212 h, g\u27e9}.\\nThus, \u2202F(h)i st h es e to fv e c t o r sg de\ufb01ning a hyperplane supporting function F at\\npoint h (see \ufb01gure 11.2). \u2202F(h)c o i n c i d e sw i t h\u2207F', '(h)w h e nF is di\ufb00erentiable ath,\\ni.e. \u2202F(h)= {\u2207F(h)}. Note that at a point h where F is minimal, 0 is an element\\nof \u2202F(h). Furthermore, the subgradient is additive, that is, for two convex function\\nF', '1 and F2, \u2202(F1 + F2)(h)= {g1 + g2 : g1 \u2208 \u2202F1(h),g2 \u2208 \u2202F2(h)}. For any h \u2208 H,\\nwe \ufb01x \u03b4F(h) to be an (arbitrary) element of \u2202F(h). For any such choice of \u03b4F,w e\\ncan de\ufb01ne the generalized Bregman divergen', 'ceassociated to F by:\\n\u2200h\u2032,h \u2208 H,B F (h\u2032\u2225h)= F(h\u2032) \u2212 F(h) \u2212\u27e8 h\u2032 \u2212 h, \u03b4F(h)\u27e9 . (11.8)\\nNote that by de\ufb01nition of the subgradient, BF (h\u2032\u2225h) \u2265 0 for all h\u2032,h \u2208 H.\\nStarting from (11.6), we can now de\ufb01ne th', 'e generalized Bregman divergence\\nof FS.L e t N denote the convex function h \u2192\u2225 h\u22252\\nK.S i n c eN is di\ufb00erentiable,\\n\u03b4N(h)= \u2207N(h) for all h \u2208 H,a n d \u03b4N and thus BN is uniquely de\ufb01ned. To\\nmake the de\ufb01nit', 'ion of the Bregman divergences for FS and \u02c6RS compatible so that\\nBFS = B bRS\\n+\u03bbBN ,w ed e \ufb01 n e\u03b4\u02c6RS in terms of \u03b4FS by: \u03b4\u02c6RS(h)= \u03b4FS(h) \u2212 \u03bb\u2207N(h)\\nfor all h \u2208 H.F u r t h e r m o r e ,w ec h o o s e\u03b4FS(', 'h)t ob e0f o ra n yp o i n th where FS is\\nminimal and let \u03b4FS(h) be an arbitrary element of \u2202FS(h) for all other h \u2208 H.W e\\nproceed in a similar way to de\ufb01ne the Bregman divergences forFS\u2032 and \u02c6RS\u2032 so ', 'that\\nBFS\u2032 = B bRS\u2032 + \u03bbBN .\\nWe will use the notion of generalized Bregman divergence for the proof of the fol-\\nlowing general upper bound on the stability coe\ufb03cient of kernel-based regularization\\nalgor', 'ithms.11.3 Stability of kernel-based regularization algorithms 273\\nProposition 11.1\\nLet K be a positive de\ufb01nite symmetric kernel such that for all x \u2208X , K(x, x) \u2264 r2\\nfor some r \u2208 R+ and let L be a co', 'nvex and \u03c3-admissible loss function. Then, the\\nkernel-based regularization algorithm de\ufb01ned by the minimization (11.6) is \u03b2-stable\\nwith the following upper bound on \u03b2:\\n\u03b2 \u2264 \u03c32r2\\nm\u03bb .\\nProof Let h be a m', 'inimizer ofFS and h\u2032 a minimizer ofFS\u2032 ,w h e r es a m p l e sS and\\nS\u2032 di\ufb00er exactly by one point,zm in S and z\u2032\\nm in S\u2032. Since the generalized Bregman\\ndivergence is non-negative and since BFS = B bRS', '\\n+ \u03bbBN and BFS\u2032 = B bRS\u2032 + \u03bbBN ,\\nwe can write\\nBFS (h\u2032\u2225h)+ BFS\u2032 (h\u2225h\u2032) \u2265 \u03bb\\n(\\nBN (h\u2032\u2225h)+ BN (h\u2225h\u2032)\\n\u23a1\\n.\\nObserve that BN (h\u2032\u2225h)+ BN (h\u2225h\u2032)= \u2212\u27e8 h\u2032 \u2212 h, 2h\u27e9\u2212\u27e8 h \u2212 h\u2032, 2h\u2032\u27e9 =2 \u2225h\u2032 \u2212 h\u22252\\nK.\\nLet \u0394 h denote h\u2032 ', '\u2212 h, then we can write\\n2\u03bb||\u0394 h\u22252\\nK\\n\u2264 BFS (h\u2032||h)+ BFS\u2032 (h||h\u2032)\\n= FS(h\u2032) \u2212 FS(h) \u2212\u27e8 h\u2032 \u2212 h, \u03b4FS(h)\u27e9 + FS\u2032 (h) \u2212 FS\u2032 (h\u2032) \u2212\u27e8 h \u2212 h\u2032,\u03b4FS\u2032 (h\u2032)\u27e9\\n= FS(h\u2032) \u2212 FS(h)+ FS\u2032 (h) \u2212 FS\u2032 (h\u2032)\\n= \u02c6RS(h\u2032) \u2212 \u02c6RS(h)+ \u02c6R', 'S\u2032 (h) \u2212 \u02c6RS\u2032 (h\u2032).\\nThe second equality follows from the de\ufb01nition of h\u2032 and h as minimizers and our\\nchoice of the subgradients for minimal points which together imply \u03b4FS\u2032 (h\u2032)=0\\nand \u03b4FS(h) = 0. The ', 'last equality follows from the de\ufb01nitions ofFS and FS\u2032 .N e x t ,\\nwe express the resulting inequality in terms of the loss function L and use the fact\\nthat S and S\u2032 di\ufb00er by only one point along with ', 'the \u03c3-admissibility of L to get\\n2\u03bb\u2225\u0394 h\u22252\\nK \u2264 1\\nm[Lzm (h\u2032) \u2212 Lzm (h)+ Lz\u2032m (h) \u2212 Lz\u2032m (h\u2032)]\\n\u2264 \u03c3\\nm[|\u0394 h(xm)| + |\u0394 h(x\u2032\\nm)|]. (11.9)\\nBy the reproducing kernel property and the Cauchy-Schwarz inequality ,', ' for all\\nx \u2208X ,\\n\u0394 h(x)= \u27e8\u0394 h, K(x, \u00b7)\u27e9\u2264\u2225 \u0394 h\u2225K \u2225K(x, \u00b7)\u2225K =\\n\u221a\\nK(x, x)\u2225\u0394 h\u2225K \u2264 r\u2225\u0394 h\u2225K.\\nIn view of (11.9), this implies \u2225\u0394 h\u2225K \u2264 \u03c3r\\n\u03bbm .B yt h e\u03c3-admissibility of L and the\\nreproducing property, the fo', 'llowing holds:\\n\u2200z \u2208 X \u00d7 Y, |Lz(h\u2032) \u2212 Lz(h)|\u2264 \u03c3|\u0394 h(x)|\u2264 r\u03c3\u2225\u0394 h\u2225K,274 Algorithmic Stability\\nwhich gives\\n\u2200z \u2208 X \u00d7 Y, |Lz(h\u2032) \u2212 Lz(h)|\u2264 \u03c32r2\\nm\u03bb ,\\nand concludes the proof.\\nThus, under the assumptions of t', 'he proposition, for a \ufb01xed\u03bb, the stability coe\ufb03cient\\nof kernel-based regularization algorithms is in O(1/m).\\n11.3.1 Application to regression algorithms: SVR and KRR\\nHere, we analyze more speci\ufb01cally ', 'two widely used regression algorithms, Support\\nVector Regression (SVR) and Kernel Ridge Regression (KRR), which are both\\nspecial instances of the family of kernel-based regularization algorithms.\\nSVR ', 'is based on the \u03f5-insensitive loss L\\n\u03f5 de\ufb01ned for all ( y,y \u2032) \u2208Y\u00d7Y by:\\nL\u03f5(y\u2032,y )=\\n{\\n0i f |y\u2032 \u2212 y|\u2264 \u03f5;\\n|y\u2032 \u2212 y|\u2212 \u03f5 otherwise.\\n(11.10)\\nWe now present a stability-based bound for SVR assuming that L\u03f5 is', ' bounded for\\nthe hypotheses returned by SVR (which, as we shall later see in lemma 11.1, is\\ni n d e e dt h ec a s ew h e nt h el a b e ls e tY is bounded).\\nCorollary 11.1 Stability-based learning boun', 'd for SVR\\nAssume that K(x, x) \u2264 r\\n2 for all x \u2208X for some r \u2265 0 and that L\u03f5 is bounded\\nby M \u2265 0.L e t hS denote the hypothesis returned by SVR when trained on an\\ni.i.d. sample S of size m.T h e n ,f o', ' ra n y\u03b4> 0, the following inequality holds with\\nprobability at least 1 \u2212 \u03b4:\\nR(hS) \u2264 \u02c6R(hS)+ r2\\nm\u03bb +\\n(2r2\\n\u03bb + M\\n\u23a1\\n\u221a\\nlog 1\\n\u03b4\\n2m .\\nProof We \ufb01rst show that L\u03f5(\u00b7)= L\u03f5(\u00b7,y )i s1 - L i p s c h i t zf o ra n', ' yy \u2208Y . For any\\ny\u2032,y \u2032\u2032 \u2208Y , we must consider four cases. First, if |y\u2032 \u2212 y|\u2264 \u03f5 and |y\u2032\u2032 \u2212 y|\u2264 \u03f5,\\nthen |L\u03f5(y\u2032\u2032) \u2212 L\u03f5(y\u2032)| =0 .S e c o n d ,i f|y\u2032 \u2212 y| >\u03f5 and |y\u2032\u2032 \u2212 y| >\u03f5 ,t h e n\\n|L\u03f5(y\u2032\u2032) \u2212 L\u03f5(y\u2032)| ', '= ||y\u2032\u2032 \u2212 y|\u2212| y\u2032 \u2212 y| |\u2264| y\u2032\u2032 \u2212 y\u2032|, by the triangle inequality.\\nThird, if |y\u2032 \u2212 y|\u2264 \u03f5 and |y\u2032\u2032 \u2212 y| >\u03f5 ,t h e n|L\u03f5(y\u2032\u2032) \u2212 L\u03f5(y\u2032)| = ||y\u2032\u2032 \u2212 y|\u2212 \u03f5| =\\n|y\u2032\u2032 \u2212 y|\u2212 \u03f5 \u2264| y\u2032\u2032 \u2212 y|\u2212| y\u2032 \u2212 y|\u2264| y\u2032\u2032 \u2212 y\u2032|.F ', 'o u r t h ,i f|y\u2032\u2032 \u2212 y|\u2264 \u03f5 and |y\u2032 \u2212 y| >\u03f5 ,\\nby symmetry the same inequality is obtained as in the previous case.\\nThus, in all cases,|L\u03f5(y\u2032\u2032,y )\u2212L\u03f5(y\u2032,y )|\u2264| y\u2032\u2032 \u2212y\u2032|. This implies in particular that\\n', 'L\u03f5 is \u03c3-admissible with \u03c3 = 1 for any hypothesis setH. By proposition 11.1, under\\nthe assumptions made, SVR is \u03b2-stable with \u03b2 \u2264 r2\\nm\u03bb . Plugging this expression into\\nthe bound of theorem 11.1 yields ', 'the result.11.3 Stability of kernel-based regularization algorithms 275\\nWe next present a stability-based bound for KRR, which is based on the square\\nloss L2 de\ufb01ned for all y\u2032,y \u2208Y by:\\nL2(y\u2032,y )=( y\u2032 ', '\u2212 y)2. (11.11)\\nAs in the SVR setting, we assume in our analysis that L2 is bounded for the\\nhypotheses returned by KRR (which, as we shall later see again in lemma 11.1,\\nis indeed the case when the lab', 'el set Y is bounded).\\nCorollary 11.2 Stability-based learning bound for KRR\\nAssume that K(x, x) \u2264 r2 for all x \u2208X for some r \u2265 0 and that L2 is bounded\\nby M \u2265 0.L e t hS denote the hypothesis returned', ' by KRR when trained on an\\ni.i.d. sample S of size m.T h e n ,f o ra n y\u03b4> 0, the following inequality holds with\\nprobability at least 1 \u2212 \u03b4:\\nR(hS) \u2264 \u02c6R(hS)+ 4Mr2\\n\u03bbm +\\n(8Mr2\\n\u03bb + M\\n\u23a1\\n\u221a\\nlog 1\\n\u03b4\\n2m .\\nPro', 'of For any (x, y) \u2208X\u00d7Y and h, h\u2032 \u2208 H,\\n|L2(h\u2032(x),y ) \u2212 L2(h(x),y )| =\\n\u23d0\u23d0(h\u2032(x) \u2212 y)2 \u2212 (h(x) \u2212 y)2\u23d0\u23d0\\n=\\n\u23d0\u23d0\\n\u23d0\\n[\\nh\\n\u2032(x) \u2212 h(x)][(h\u2032(x) \u2212 y)+( h(x) \u2212 y)\\n]\u23d0\u23d0\\n\u23d0\\n\u2264 (|h\\n\u2032(x) \u2212 y| + |h(x) \u2212 y|)|h(x) \u2212 h\u2032(x)|\\n\u2264 ', '2\\n\u221a\\nM |h(x) \u2212 h\u2032(x)|,\\nw h e r ew eu s e dt h eM-boundedness of the loss. Thus, L2 is \u03c3-admissible with\\n\u03c3 =2\\n\u221a\\nM. Therefore, by proposition 11.1, KRR is\u03b2-stable with\u03b2 \u2264 4r2M\\nm\u03bb . Plugging\\nthis expressi', 'on into the bound of theorem 11.1 yields the result.\\nThe previous two corollaries assumed bounded loss functions. We now present a\\nlemma that implies in particular that the loss functions used by SVR ', 'and KRR are\\nbounded when the label set is bounded.\\nLemma 11.1\\nAssume that K(x, x) \u2264 r\\n2 for all x \u2208X for some r \u2265 0 and that for all y \u2208 Y ,\\nL(0,y ) \u2264 B for some B \u2265 0. Then, the hypothesis hS returne', 'd by a kernel-based\\nregularization algorithm trained on a sampleS is bounded as follows:\\n\u2200x \u2208 X, |hS(x)|\u2264 r\\n\u221a\\nB/\u03bb.\\nProof By the reproducing kernel property and the Cauchy-Schwarz inequality ,\\nwe can w', 'rite\\n\u2200x \u2208 X, |hS(x)| = \u27e8hS,K (x, \u00b7)\u27e9\u2264\u2225 hS \u2225K\\n\u221a\\nK(x, x) \u2264 r\u2225hS \u2225K. (11.12)276 Algorithmic Stability\\nThe minimization (11.6) is over H, which includes 0. Thus, by de\ufb01nition of FS and\\nhS, the following i', 'nequality holds:\\nFS(hS) \u2264 FS(0) = 1\\nm\\nm\u2211\\ni=1\\nL(0,y i) \u2264 B.\\nSince the loss L is non-negative, we have\u03bb\u2225hS \u22252\\nK \u2264 FS(hS)a n dt h u s\u03bb\u2225hS \u22252\\nK \u2264 B.\\nCombining this inequality with (11.12) yields the resul', 't.\\n11.3.2 Application to classi\ufb01cation algorithms: SVMs\\nThis section presents a generalization bound for SVMs, when using the standard\\nhinge loss de\ufb01ned for all y \u2208Y = {\u22121, +1} and y\u2032 \u2208 R by\\nLhinge(y\u2032', ',y )=\\n{\\n0i f 1 \u2212 yy\u2032 \u2264 0;\\n1 \u2212 yy\u2032 otherwise.\\n(11.13)\\nCorollary 11.3 Stability-based learning bound for SVMs\\nAssume that K(x, x) \u2264 r2 for all x \u2208X for some r \u2265 0.L e thS denote the hypothesis\\nreturned ', 'by SVMs when trained on an i.i.d. sampleS of size m.T h e n ,f o ra n y\u03b4> 0,\\nthe following inequality holds with probability at least 1 \u2212 \u03b4:\\nR(hS) \u2264 \u02c6R(hS)+ r2\\nm\u03bb +\\n(2r2\\n\u03bb + r\u221a\\n\u03bb\\n+1\\n\u23a1\\n\u221a\\nlog 1\\n\u03b4\\n2m .\\nP', 'roof It is straightforward to verify that Lhinge(\u00b7,y ) is 1-Lipschitz for any y \u2208Y\\nand therefore that it is \u03c3-admissible with \u03c3 =1 .T h e r e f o r e ,b yp r o p o s i t i o n1 1 . 1 ,\\nSVMs is\u03b2-stable', ' with\u03b2 \u2264 r2\\nm\u03bb .S i n c e|Lhinge(0,y )|\u2264 1 for anyy \u2208Y , by lemma 11.1,\\n\u2200x \u2208X , |hS(x)|\u2264 r/\\n\u221a\\n\u03bb. Thus, for any sample S and any x \u2208X and y \u2208Y ,t h e\\nloss is bounded as follows: Lhinge(hS(x),y ) \u2264 r/\\n\u221a', '\\n\u03bb+ 1. Plugging this value of M\\nand the one found for \u03b2 into the bound of theorem 11.1 yields the result.\\nSince the hinge loss upper bounds the binary loss, the bound of the corollary 11.3\\nalso applie', 's to the generalization error of hS measured in terms of the standard\\nbinary loss used in classi\ufb01cation.\\n11.3.3 Discussion\\nNote that the learning bounds presented for kernel-based regularization algor', 'ithms\\nare of the form R(hS) \u2212 \u02c6R(hS) \u2264 O\\n( 1\\n\u03bb\u221am\\n\u23a1\\n. Thus, these bounds are informative\\nonly when \u03bb \u226b 1/\u221am. The regularization parameter \u03bb i saf u n c t i o no ft h es a m p l e\\nsize m: for larger val', 'ues of m, it is expected to be smaller, decreasing the emphasis\\non regularization. The magnitude of \u03bb a\ufb00ects the norm of the linear hypotheses11.4 Chapter notes 277\\nused for prediction, with a larger ', 'value of\u03bb implying a smaller hypothesis norm. In\\nthis sense, \u03bb is a measure of the complexity of the hypothesis set and the condition\\nrequired for \u03bb can be interpreted as stating that a less complex h', 'ypothesis set\\nguarantees better generalization.\\nNote also that our analysis of stability in this chapter assumed a \ufb01xed \u03bb:t h e\\nregularization parameter is assumed to be invariant to the change of one', ' point of\\nthe training sample. While this is a mild assumption, it may not hold in general.\\n11.4 Chapter notes\\nThe notion of algorithmic stability was \ufb01rst used by Devroye, Rogers and Wagner\\n[Rogers a', 'nd Wagner, 1978, Devroye and Wagner, 1979a,b] for thek-nearest neighbor\\nalgorithm and other k-local rules. Kearns and Ron [1999] later gave a formal de\ufb01ni-\\ntion of stability and used it to provide an ', 'analysis of the leave-one-out error. Much\\nof the material presented in this chapter is based on Bousquet and Elissee\ufb00 [2002].\\nOur proof of proposition 11.1 is novel and generalizes the results of Bous', 'quet and\\nElissee\ufb00 [2002] to the case of non-di\ufb00erentiable convex losses. Moreover, stability-\\nbased generalization bounds have been extended to ranking algorithms [Agarwal\\nand Niyogi, 2005, Cortes et ', 'al., 2007b], as well as to the non-i.i.d. scenario of sta-\\ntionary \u03a6- and \u03b2-mixing processes [Mohri and Rostamizadeh, 2010], and to the\\ntransductive setting [Cortes et al., 2008a]. Additionally, exerc', 'ise 11.5 is based on\\nCortes et al. [2010b], which introduces and analyzes stability with respect to the\\nchoice of the kernel function or kernel matrix.\\nNote that while, as shown in this chapter, unifo', 'rm stability is su\ufb03cient for\\nderiving generalization bounds, it is not a necessary condition. Some algorithms may\\ngeneralize well in the supervised learning scenario but may not be uniformly stable,\\nf', 'or example, the Lasso algorithm [Xu et al., 2008]. Shalev-Shwartz et al. [2009]\\nhave used the notion of stability to provide necessary and su\ufb03cient conditions for a\\ntechnical condition of learnability', ' related to PAC-learning, even in general scenarios\\nwhere learning is possible only by using non-ERM rules.\\n11.5 Exercises\\n11.1 Tighter stability bounds\\n(a) Assuming the conditions of theorem 11.1 hol', 'd, can one hope to guarantee\\na generalization with slack better than O(1/\u221am) even if the algorithm is very\\nstable, i.e. \u03b2 \u2192 0?278 Algorithmic Stability\\n(b) Can you show an O(1/m) generalization guaran', 'tee if L is bounded by\\nC/\u221am (a very strong condition)? If so, how stable does the learning algorithm\\nneed to be?\\n11.2 Quadratic hinge loss stability. Let L denote the quadratic hinge loss function\\nde\ufb01', 'ned for all y \u2208{ +1, \u22121} and y\u2032 \u2208 R by\\nL(y\u2032,y )=\\n{\\n0i f 1 \u2212 y\u2032y \u2264 0;\\n(1 \u2212 y\u2032y)2 otherwise.\\nAssume that L(h(x),y )i sb o u n d e db yM,1 \u2264 M< \u221e , for all h \u2208 H, x \u2208X ,a n d\\ny \u2208{ +1, \u22121}, which also imp', 'lies a bound on|h(x)| for all h \u2208 H and x \u2208X .D e r i v e\\na stability-based generalization bound for SVMs with the quadratic hinge loss.\\n11.3 Stability of linear regression.\\n(a) How does the stability', ' bound in corollary 11.2 for ridge regression (i.e.\\nkernel ridge regression with a linear kernel) behave as \u03bb \u2192 0?\\n(b) Can you show a stability bound for linear regression (i.e. ridge regression\\nwith ', '\u03bb = 0)? If not, show a counter-example.\\n11.4 Kernel stability. Suppose an approximation of the kernel matrix K, denoted\\nK\u2032, is used to train the hypothesish\u2032 (and leth denote the non-approximate hypot', 'h-\\nesis). At test time, no approximation is made, so if we letkx =\\n[\\nK(x, x1),...,K (x, xm)\\n]\u22a4\\nwe can writeh(x)= \u03b1\u22a4kx and h\u2032(x)= \u03b1\u2032\u22a4kx.S h o wt h a ti f\u2200x, x\u2032 \u2208X ,K (x, x\u2032) \u2264 r\\nthen\\n|h\u2032(x) \u2212 h(x)|\u2264 rm', 'M\\n\u03bb2 \u2225K\u2032 \u2212 K\u22252 .\\n(Hint: Use exercise 9.3)\\n11.5 Stability of relative-entropy regularization.\\n(a) Consider an algorithm that selects a distribution g over a hypothesis class\\nwhich is parameterized by \u03b8', ' \u2208 \u0398. Given a point z =( x, y) the expected loss is\\nde\ufb01ned as\\nH(g,z )=\\n\u222b\\n\u0398\\nL(h\u03b8(x),y )g(\u03b8) d\u03b8 ,\\nwith respect to a base loss function L. Assuming the loss function L is\\nbounded by M, show that the expe', 'cted loss H is M-admissible, i.e. show\\n|H(g,z ) \u2212 H(g\u2032,z )|\u2264 M\\n\u222b\\n\u0398 |g(\u03b8) \u2212 g\u2032(\u03b8)| d\u03b8.11.5 Exercises 279\\n(b) Consider an algorithm that minimizes the entropy regularized objective\\nover the choice of di', 'stribution g:\\nFS(g)= 1\\nm\\nm\u2211\\ni=1\\nH(g,z i)\\n\\ued19 \\ued18\\ued17 \\ued1a\\nbRS(g)\\n+\u03bbK(g,f 0) .\\nHere, K is the Kullback-Leibler divergence (or relative entropy) between two\\ndistributions,\\nK(g,f 0)=\\n\u222b\\n\u0398\\ng(\u03b8)l o gg(\u03b8)\\nf0(\u03b8) d\u03b8 , (', '11.14)\\nand f0 is some \ufb01xed distribution. Show that such an algorithm is stable by\\nperforming the following steps:\\ni. First use the fact 1\\n2 (\\n\u222b\\n\u0398 |g(\u03b8) \u2212 g\u2032(\u03b8)|d\u03b8)2 \u2264 K(g,g \u2032) (Pinsker\u2019s inequal-\\nity)', ', to show\\n(\u222b\\n\u0398\\n|gS(\u03b8) \u2212 gS\u2032 (\u03b8)| d\u03b8\\n\u23a12\\n\u2264 BK(.,f0)(g\u2225g\u2032)+ BK(.,f0)(g\u2032\u2225g) .\\nii. Next, let g be the minimizer of FS and g\u2032 the minimizer of FS\u2032 ,w h e r e\\nS and S\u2032 di\ufb00er only at the index m. Show that\\nBK', '(.,f0)(g\u2225g\u2032)+ BK(.,f0)(g\u2032\u2225g)\\n\u2264 1\\nm\u03bb\\n\u23d0\u23d0H(g\u2032,zm) \u2212 H(g,z m)+ H(g,z \u2032\\nm) \u2212 H(g\u2032,z \u2032\\nm)\\n\u23d0\u23d0\\n\u2264 2M\\nm\u03bb\\n\u222b\\n\u0398\\n|g(\u03b8) \u2212 g\u2032(\u03b8)|d\u03b8 .\\niii. Finally, combine the results above to show that the entropy regularized\\nalgor', 'ithm is 2M 2\\nm\u03bb -stable.12 Dimensionality Reduction\\nIn settings where the data has a large number of features, it is often desirable\\nto reduce its dimension, or to \ufb01nd a lower-dimensional representati', 'on preserving\\nsome of its properties. The key arguments for dimensionality reduction (or manifold\\nlearning) techniques are:\\nComputational: to compress the initial data as a preprocessing step to speed', ' up\\nsubsequent operations on the data.\\nVisualization: to visualize the data for exploratory analysis by mapping the input\\ndata into two- or three-dimensional spaces.\\nFeature extraction: to hopefully g', 'enerate a smaller and more e\ufb00ective or useful\\nset of features.\\nThe bene\ufb01ts of dimensionality reduction are often illustrated via simulated data,\\nsuch as the Swiss roll dataset. In this example, the in', 'put data, depicted in \ufb01g-\\nure 12.1a, is three-dimensional, but it lies on a two-dimensional manifold that\\nis \u201cunfolded\u201d in two-dimensional space as shown in \ufb01gure 12.1b. It is important\\nto note, howev', 'er, that exact low-dimensional manifolds are rarely encountered in\\npractice. Hence, this idealized example is more useful to illustrate the concept of\\ndimensionality reduction than to verify the e\ufb00ect', 'iveness of dimensionality reduction\\nalgorithms.\\nDimensionality reduction can be formalized as follows. Consider a sample S =\\n(x\\n1,...,x m), a feature mapping \u03a6 : X\u2192 RN and the data matrix X \u2208 RN \u00d7m\\nde', '\ufb01ned as (\u03a6(x1),..., \u03a6(xm)). The ith data point is represented byxi = \u03a6(xi), or\\nthe ith column of X,w h i c hi sa nN-dimensional vector. Dimensionality reduction\\ntechniques broadly aim to \ufb01nd, for k \u226a ', 'N,a k-dimensional representation of the\\ndata, Y \u2208 Rk\u00d7m, that is in some way faithful to the original representation X.\\nIn this chapter we will discuss various techniques that address this problem.\\nWe ', '\ufb01rst present the most commonly used dimensionality reduction technique called\\nprincipal component analysis(PCA). We then introduce a kernelized version of PCA\\n(KPCA) and show the connection between KP', 'CA and manifold learning algorithms.\\nWe conclude with a presentation of the Johnson-Lindenstrauss lemma, a classical\\ntheoretical result that has inspired a variety of dimensionality reduction methods2', '82 Dimensionality Reduction\\n(a) (b)\\nFigure 12.1 The \u201cSwiss roll\u201d dataset. (a) high-dimensional representation. (b)\\nlower-dimensional representation.\\nbased on the concept of random projections. The dis', 'cussion in this chapter relies\\non basic matrix properties that are reviewed in appendix A.\\n12.1 Principal Component Analysis\\nFix k \u2208 [1,N ]a n dl e tX be a mean-centered data matrix, that is, \u2211m\\ni=1 x', 'i = 0.\\nDe\ufb01ne Pk as the set of N-dimensional rank- k orthogonal projection matrices.\\nPCA consists of projecting the N-dimensional input data onto the k-dimensional\\nlinear subspace that minimizes recons', 'truction error, that is the sum of the squared\\nL2-distances between the original data and the projected data. Thus, the PCA\\nalgorithm is completely de\ufb01ned by the orthogonal projection matrix solution ', 'P\u2217 of\\nthe following minimization problem:\\nmin\\nP\u2208Pk\\n\u2225PX \u2212 X\u22252\\nF . (12.1)\\nThe following theorem shows that PCA coincides with the projection of each\\ndata point onto the k top singular vectors of the sam', 'ple covariance matrix, i.e.,\\nC = 1\\nmXX\u22a4 for the mean-centered data matrix X. Figure 12.2 illustrates the\\nbasic intuition behind PCA, showing how two-dimensional data points with highly\\ncorrelated feat', 'ures can be more succinctly represented with a one-dimensional\\nrepresentation that captures most of the variance in the data.\\nTheorem 12.1\\nLet P\\n\u2217 \u2208P k be the PCA solution, i.e., the orthogonal projec', 'tion matrix solution of\\n(12.1).T h e n ,P\u2217 = UkU\u22a4\\nk ,w h e r eUk \u2208 RN \u00d7k is the matrix formed by the top k\\nsingular vectors of C = 1\\nmXX\u22a4, the sample covariance matrix corresponding toX.12.2 Kernel Pr', 'incipal Component Analysis (KPCA) 283\\nMoreover, the associatedk-dimensional representation ofX is given by Y = U\u22a4\\nk X.\\nProof Let P = P\u22a4 be an orthogonal projection matrix. By the de\ufb01nition of\\nthe Frob', 'enius norm, the linearity of the trace operator and the fact that P is\\nidempotent, i.e., P2 = P,w eo b s e r v et h a t\\n\u2225PX \u2212 X\u22252\\nF =T r [ (PX \u2212 X)\u22a4(PX \u2212 X)] = Tr[X\u22a4P2X \u2212 2X\u22a4PX + X\u22a4X]\\n= \u2212 Tr[X\u22a4PX]+T r', ' [X\u22a4X] .\\nSince Tr[X\u22a4X] is a constant with respect to P,w eh a v e\\nmin\\nP\u2208Pk\\n\u2225PX \u2212 X\u22252\\nF =m a x\\nP\u2208Pk\\nTr[X\u22a4PX] . (12.2)\\nBy de\ufb01nition of orthogonal projections in Pk, P = UU\u22a4 for some U \u2208 RN \u00d7k\\ncontaining', ' orthogonal columns. Using the invariance of the trace operator under\\ncyclic permutations and the orthogonality of the columns of U,w eh a v e\\nTr[X\u22a4PX]= U\u22a4XX\u22a4U =\\nk\u2211\\ni=1\\nu\u22a4\\ni XX\u22a4ui ,\\nwhere ui is the it', 'h column ofU. By the Rayleigh quotient (section A.2.3), it is clear\\nthat the largestk singular vectors ofXX\u22a4 maximize the rightmost sum above. Since\\nXX\u22a4 and C di\ufb00er only by a scaling factor, they have', ' the same singular vectors,\\nand thus Uk maximizes this sum, which proves the \ufb01rst statement of the theorem.\\nFinally, since PX = UkU\u22a4\\nk X, Y = U\u22a4\\nk X is a k-dimensional representation of X\\nwith Uk as t', 'he basis vectors.\\nBy de\ufb01nition of the covariance matrix, the top singular vectors of C are the\\ndirections of maximal variance in the data, and the associated singular values\\nare equal to these varianc', 'es. Hence, PCA can also be viewed as projecting onto\\nthe subspace of maximal variance. Under this interpretation, the \ufb01rst principal\\ncomponent is derived from projection onto the direction of maximal ', 'variance, given\\nby the top singular vector ofC. Similarly, theith principal component, for 1\u2264 i \u2264 k,\\nis derived from projection onto the ith direction of maximal variance, subject to\\northogonality con', 'straints to the previous i \u2212 1 directions of maximal variance (see\\nexercise 12.1 for more details).\\n12.2 Kernel Principal Component Analysis (KPCA)\\nIn the previous section, we presented the PCA algori', 'thm, which involved projecting\\nonto the singular vectors of the sample covariance matrix C.I nt h i ss e c t i o n ,w e284 Dimensionality Reduction\\n7 8 9 10 11 12 1340\\n41\\n42\\n43\\n44\\n45\\n46\\nUS shoe size\\nE', 'uropean shoe size\\n\u22125 0 5\u22126\\n\u22124\\n\u22122\\n0\\n2\\n4\\n6\\nUS shoe size (mean centered)\\nEuropean shoe size (mean centered)\\n(a) (b)\\nFigure 12.2 Example of PCA. (a) Two-dimensional data points with features cap-\\nturing s', 'hoe size measured with di\ufb00erent units. (b) One-dimensional representation\\n(blue squares) that captures the most variance in the data, generated by projecting\\nonto largest principal component (red line', ') of the mean-centered data points.\\npresent a kernelized version of PCA, called KPCA. In the KPCA setting, \u03a6 is\\na feature mapping to an arbitrary RKHS (not necessarily to R\\nN )a n dw ew o r k\\nexclusiv', 'ely with a kernel function K corresponding to the inner product in this\\nRKHS. The KPCA algorithm can thus be de\ufb01ned as a generalization of PCA in\\nw h i c ht h ei n p u td a t ai sp r o j e c t e do n ', 't ot h et o pp r i n c i p l ec o m p o n e n t si nt h i sR K H S .\\nWe will show the relationship between PCA and KPCA by drawing upon the deep\\nconnections among the SVDs of X, C and K. We then illus', 'trate how various\\nmanifold learning algorithms can be interpreted as special instances of KPCA.\\nLet K be a PDS kernel de\ufb01ned over X\u00d7 X and de\ufb01ne the kernel matrix as K =\\nX\\n\u22a4X.S i n c eX admits the fol', 'lowing singular value decomposition: X = U\u03a3V\u22a4, C\\nand K c a nb er e w r i t t e na sf o l l o w s :\\nC = 1\\nmU\u039bU\u22a4 K = V\u039bV\u22a4 , (12.3)\\nwhere \u039b = \u03a32 is the diagonal matrix of the singular values of mC and U ', 'is the\\nmatrix of the singular vectors of C (and mC).\\nStarting with the SVD of X, note that right multiplying byV\u03a3\u22121 and using the\\nrelationship between \u039b and \u03a3 yields U = XV\u039b\u22121/2. Thus, the singular ve', 'ctoru of\\nC associated to the singular value \u03bb/m coincides with Xv\u221a\\n\u03bb ,w h e r ev is the singular\\nvector of K associated to \u03bb. Now \ufb01x an arbitrary feature vector x = \u03a6(x)f o r\\nx \u2208X . Then, following th', 'e expression for Y in theorem 12.1, the one-dimensional12.3 KPCA and manifold learning 285\\nrepresentation of x derived by projection onto Pu = uu\u22a4 is de\ufb01ned by\\nx\u22a4u = x\u22a4 Xv\u221a\\n\u03bb\\n= k\u22a4\\nx v\u221a\\n\u03bb\\n, (12.4)\\nwher', 'e kx =( K(x1,x),...,K (xm,x))\u22a4.I f x is one of the data points, i.e.,x = xi for\\n1 \u2264 i \u2264 m,t h e nkx is the ith column of K and (12.4) can be simpli\ufb01ed as follows:\\nx\u22a4u = k\u22a4\\nx v\u221a\\n\u03bb\\n= \u03bbvi\u221a\\n\u03bb\\n=\\n\u221a\\n\u03bbvi , (1', '2.5)\\nwhere vi is theith component ofv. More generally, the PCA solution of theorem 12.1\\ncan be fully de\ufb01ned by the top k singular vectors of K, v1,..., vk,a n dt h e\\ncorresponding singular values. Thi', 's alternative derivation of the PCA solution in\\nterms of K precisely de\ufb01nes the KPCA solution, providing a generalization of PCA\\nvia the use of PDS kernels (see chapter 5 for more details on kernel me', 'thods).\\n12.3 KPCA and manifold learning\\nSeveral manifold learning techniques have been proposed as non-linear methods for\\ndimensionality reduction. These algorithms implicitly assume that high-dimensi', 'onal\\ndata lie on or near a low-dimensional non-linear manifold embedded in the input\\nspace. They aim to learn this manifold structure by \ufb01nding a low-dimensional\\nspace that in some way preserves the l', 'ocal structure of high-dimensional input\\ndata. For instance, the Isomap algorithm aims to preserve approximate geodesic\\ndistances, or distances along the manifold, between all pairs of data points. Ot', 'her\\nalgorithms, such as Laplacian eigenmaps and locally linear embedding, focus only\\non preserving local neighborhood relationships in the high-dimensional space. We\\nwill next describe these classical', ' manifold learning algorithms and then interpret\\nthem as speci\ufb01c instances of KPCA.\\n12.3.1 Isomap\\nIsomap aims to extract a low-dimensional data representation that best preserves\\nall pairwise distance', 's between input points, as measured by their geodesic distances\\nalong the underlying manifold. It approximates geodesic distance assuming thatL\\n2\\ndistance provides good approximations for nearby point', 's, and for faraway points\\nit estimates distance as a series of hops between neighboring points. The Isomap\\nalgorithm works as follows:\\n1. Find the t nearest neighbors for each data point based on L\\n2 ', 'distance and\\nconstruct an undirected neighborhood graph, denoted by G,w i t hp o i n t sa sn o d e s286 Dimensionality Reduction\\nand links between neighbors as edges.\\n2. Compute the approximate geodes', 'ic distances, \u0394 ij, between all pairs of nodes\\n(i, j) by computing all-pairs shortest distances in G using, for instance, the Floyd-\\nWarshall algorithm.\\n3. Convert the squared distance matrix into am\u00d7', 'm similarity matrix by performing\\ndouble centering, i.e., compute KIso = \u22121\\n2H\u0394H, where \u0394 is the squared distance\\nmatrix, H = Im \u2212 1\\nm11\u22a4 is the centering matrix, Im is the m \u00d7 m identity matrix\\nand 1', ' is a column vector of all ones (for more details on double centering see\\nexercise 12.2).\\n4. Find the optimal k-dimensional representation, Y = {yi}n\\ni=1,s u c ht h a tY =\\nargminY\u2032\\n\u2211\\ni,j\\n(\\n\u2225y\u2032\\ni \u2212 y\u2032\\n', 'j \u22252\\n2 \u2212 \u03942\\nij\\n\u23a1\\n. The solution is given by,\\nY =( \u03a3Iso,k)1/2U\u22a4\\nIso,k (12.6)\\nwhere \u03a3Iso,k is the diagonal matrix of the top k singular values of KIso and UIso,k\\nare the associated singular vectors.\\nKIs', 'o can naturally be viewed as a kernel matrix, thus providing a simple connection\\nbetween Isomap and KPCA. Note, however, that this interpretation is valid only\\nwhen K\\nIso is in fact positive semide\ufb01ni', 'te, which is indeed the case in the continuum\\nlimit for a smooth manifold.\\n12.3.2 Laplacian eigenmaps\\nThe Laplacian eigenmaps algorithm aims to \ufb01nd a low-dimensional representation\\nthat best preserves', ' neighborhood relations as measured by a weight matrixW.T h e\\nalgorithm works as follows:\\n1. Find t nearest neighbors for each point.\\n2. Construct W,as p a r s e ,s y m m e t r i cm \u00d7 m matrix, where ', 'W\\nij =e x p\\n(\\n\u2212\u2225 xi \u2212\\nxj \u22252\\n2/\u03c32\u23a1\\nif (xi,xj) are neighbors, 0 otherwise, and \u03c3 is a scaling parameter.\\n3. Construct the diagonal matrix D, such that Dii = \u2211\\nj Wij.\\n4. Find the k-dimensional representa', 'tion by minimizing the weighted distance\\nbetween neighbors as,\\nY =a r g m i n\\nY\u2032\\n\u2211\\ni,j\\nWij \u2225y\u2032\\ni \u2212 y\u2032\\nj \u22252\\n2\\n. (12.7)\\nThis objective function penalizes nearby inputs for being mapped to faraway\\noutput', 's, with \u201cnearness\u201d measured by the weight matrix W. The solution to the\\nminimization in (12.7) is Y = U\u22a4\\nL,k,w h e r eL = D \u2212 W is the graph Laplacian\\nand U\u22a4\\nL,k are the bottom k singular vectors of L', ', excluding the last singular vector12.3 KPCA and manifold learning 287\\ncorresponding to the singular value 0 (assuming that the underlying neighborhood\\ngraph is connected).\\nThe solution to (12.7) can', ' also be interpreted as \ufb01nding the largest singular\\nvectors ofL\u2020 , the pseudo-inverse ofL.D e \ufb01 n i n gKL = L\u2020 we can thus view Laplacian\\nE i g e n m a p sa sa ni n s t a n c eo fK P C Ai nw h i c ht ', 'h eo u t p u td i m e n s i o n sa r en o r m a l i z e d\\nto have unit variance, which corresponds to setting \u03bb = 1 in (12.5). Moreover, it\\ncan be shown that K\\nL is the kernel matrix associated with t', 'he commute times of\\ndi\ufb00usion on the underlying neighborhood graph, where the commute time between\\nnodes i and j in a graph is the expected time taken for a random walk to start at\\nnode i,r e a c hn o ', 'd ej and then return to i.\\n12.3.3 Locally linear embedding (LLE)\\nThe Locally linear embedding (LLE) algorithm also aims to \ufb01nd a low-dimensional\\nrepresentation that preserves neighborhood relations as', ' measured by a weight\\nmatrix W. The algorithm works as follows:\\n1. Find t nearest neighbors for each point.\\n2. Construct W,as p a r s e ,s y m m e t r i cm\u00d7m matrix, whose ith row sums to one and\\ncont', 'ains the linear coe\ufb03cients that optimally reconstruct x\\ni from its t neighbors.\\nMore speci\ufb01cally, if we assume that the ith row of W sums to one, then the\\nreconstruction error is\\n(\\nxi \u2212\\n\u2211\\nj\u2208Ni\\nWijxj\\n\u23a1', '2\\n=\\n( \u2211\\nj\u2208Ni\\nWij(xi \u2212 xj)\\n\u23a12\\n=\\n\u2211\\nj,k\u2208Ni\\nWijWikC\u2032\\njk (12.8)\\nwhere Ni is the set of indices of the neighbors of pointxi and C\u2032\\njk =( xi \u2212xj)\u22a4(xi \u2212\\nxk) the local covariance matrix. Minimizing this expres', 'sion with the constraint\u2211\\nj Wij = 1 gives the solution\\nWij =\\n\u2211\\nk(C\u2032\u22121)jk\u2211\\nst(C\u2032\u22121)st\\n. (12.9)\\nNote that the solution can be equivalently obtained by \ufb01rst solving the system of\\nlinear equations \u2211\\nj C\u2032\\n', 'kjWij =1 ,f o r k \u2208N i, and then normalizing so that the\\nweights sum to one.\\n3. Find the k-dimensional representation that best obeys neighborhood relations as\\nspeci\ufb01ed by W, i.e.,\\nY =a r g m i n\\nY\u2032\\n\u2211', '\\ni\\n(\\ny\u2032\\ni \u2212\\n\u2211\\nj\\nWijy\u2032\\nj\\n\u23a12\\n. (12.10)288 Dimensionality Reduction\\nThe solution to the minimization in (12.10) isY = U\u22a4\\nM,k,w h e r eM =( I\u2212W\u22a4)(I\u2212\\nW\u22a4)a n dU\u22a4\\nM,k are the bottom k singular vectors ofM, e', 'xcluding the last singular\\nvector corresponding to the singular value 0.\\nAs discussed in exercise 12.5, LLE coincides with KPCA used with a particular\\nkernel matrix KLLE whereby the output dimensions ', 'are normalized to have unit\\nvariance (as in the case of Laplacian Eigenmaps).\\n12.4 Johnson-Lindenstrauss lemma\\nThe Johnson-Lindenstrauss lemma is a fundamental result in dimensionality reduc-\\ntion tha', 't states that any m points in high-dimensional space can be mapped to a\\nmuch lower dimension, k \u2265 O(\\nlog m\\n\u03f52 ), without distorting pairwise distance between\\nany two points by more than a factor of (1', ' \u00b1 \u03f5). In fact, such a mapping can be\\nfound in randomized polynomial time by projecting the high-dimensional points\\nonto randomly chosen k-dimensional linear subspaces. The Johnson-Lindenstrauss\\nlemma', ' is formally presented in lemma 12.3. The proof of this lemma hinges on\\nlemma 12.1 and lemma 12.2, and it is an example of the \u201cprobabilistic method\u201d,\\nin which probabilistic arguments lead to a determ', 'inistic statement. Moreover, as\\nwe will see, the Johnson-Lindenstrauss lemma follows by showing that the squared\\nlength of a random vector is sharply concentrated around its mean when the vector\\nis pr', 'ojected onto a k-dimensional random subspace.\\nFirst, we prove the following property of the \u03c7\\n2-squared distribution (see de\ufb01ni-\\ntion C.6 in appendix), which will be used in lemma 12.2.\\nLemma 12.1\\nLet', ' Q be a random variable following a \u03c72-squared distribution with k degrees of\\nfreedom. Then, for any 0 <\u03f5< 1/2, the following inequality holds:\\nPr[(1 \u2212 \u03f5)k \u2264 Q \u2264 (1 +\u03f5)k] \u2265 1 \u2212 2e\u2212(\u03f52\u2212\u03f53)k/4 . (12.11)', '\\nProof By Markov\u2019s inequality, we can write\\nPr[Q \u2265 (1 +\u03f5)k]=P r [ e x p (\u03bbQ) \u2265 exp(\u03bb(1 +\u03f5)k)] \u2264 E[exp(\u03bbQ)]\\nexp(\u03bb(1 +\u03f5)k)\\n= (1 \u2212 2\u03bb)\u2212k/2\\nexp(\u03bb(1 +\u03f5)k) ,\\nwhere we used for the \ufb01nal equality the expressi', 'on of the moment-generating\\nfunction of a \u03c72-squared distribution, E[exp(\u03bbQ)], for \u03bb< 1/2 (equation C.14).\\nChoosing \u03bb = \u03f5\\n2(1+\u03f5) < 1/2, which minimizes the right-hand side of the \ufb01nal12.4 Johnson-Lind', 'enstrauss lemma 289\\nequality, and using the identity 1 +\u03f5 \u2264 exp(\u03f5 \u2212 (\u03f52 \u2212 \u03f53)/2) yield\\nPr[Q \u2265 (1 +\u03f5)k] \u2264\\n( 1+ \u03f5\\nexp(\u03f5)\\n\u23a1k/2\\n\u2264\\n(exp\\n(\\n\u03f5 \u2212 \u03f52\u2212\u03f53\\n2\\n\u23a1\\nexp(\u03f5)\\n\u23a1k/2\\n=e x p\\n(\\n\u2212 k\\n4(\u03f52 \u2212 \u03f53)\\n\u23a1\\n.\\nThe statement', ' of the lemma follows by using similar techniques to bound Pr[ Q \u2264\\n(1 \u2212 \u03f5)k] and by applying the union bound.\\nLemma 12.2\\nLet x \u2208 RN,d e \ufb01 n ek<N and assume that entries in A \u2208 Rk\u00d7N are sampled\\nindepen', 'dently from the standard normal distribution, N(0, 1).T h e n ,f o ra n y0 <\\n\u03f5< 1/2,\\nPr\\n[\\n(1 \u2212 \u03f5)\u2225x\u22252 \u2264\u2225 1\u221a\\nk\\nAx\u22252 \u2264 (1 +\u03f5)\u2225x\u22252\\n]\\n\u2265 1 \u2212 2e\u2212(\u03f52\u2212\u03f53)k/4 . (12.12)\\nProof Let \u02c6x = Ax and observe that\\nE[\u02c6x2', '\\nj ]=E\\n[( N\u2211\\ni=1\\nAjixi\\n\u23a12]\\n=E\\n[ N\u2211\\ni=1\\nA2\\njix2\\ni\\n]\\n=\\nN\u2211\\ni=1\\nx2\\ni = \u2225x\u22252 .\\nThe second and third equalities follow from the independence and unit variance,\\nrespectively, of the Aij.N o w ,d e \ufb01 n eTj = ', '\u02c6xj/\u2225x\u2225 and note that the Tjsa r e\\nindependent standard normal random variables since the Aij are i.i.d. standard\\nnormal random variables and E[ \u02c6x2\\nj ]= \u2225x\u22252. Thus, the variable Q de\ufb01ned by\\nQ = \u2211k\\nj=', '1 T2\\nj follows a \u03c72-squared distribution with k degrees of freedom and\\nwe have\\nPr\\n[\\n(1 \u2212 \u03f5)\u2225x\u22252 \u2264 \u2225\u02c6x\u22252\\nk \u2264 (1 +\u03f5)\u2225x\u22252\\n]\\n=P r\\n[\\n(1 \u2212 \u03f5)k \u2264\\nk\u2211\\nj=1\\nT2\\nj \u2264 (1 +\u03f5)k\\n]\\n=P r\\n[\\n(1 \u2212 \u03f5)k \u2264 Q \u2264 (1 +\u03f5)k\\n]\\n\u2265 1 \u2212', ' 2e\u2212(\u03f52\u2212\u03f53)k/4 ,\\nwhere the \ufb01nal inequality holds by lemma 12.1, thus proving the statement of the\\nlemma.\\nLemma 12.3 Johnson-Lindenstrauss\\nFor any 0 <\u03f5< 1/2 and any integer m> 4,l e tk = 20 logm\\n\u03f52 . T', 'hen for any set V of\\nm points in RN, there exists a map f : RN \u2192 Rk such that for all u,v \u2208 V ,\\n(1 \u2212 \u03f5)\u2225u \u2212 v\u22252 \u2264\u2225 f(u) \u2212 f(v)\u22252 \u2264 (1 +\u03f5)\u2225u \u2212 v\u22252. (12.13)\\nProof Let f = 1\u221a\\nk A where k<N and entries in', ' A \u2208 Rk\u00d7N are sampled290 Dimensionality Reduction\\nindependently from the standard normal distribution, N(0, 1). For \ufb01xed u,v \u2208 V ,\\nwe can apply lemma 12.2, with x = u \u2212 v, to lower bound the success p', 'robability\\nby 1 \u2212 2e\u2212(\u03f52\u2212\u03f53)k/4. Applying the union bound over the O(m2)p a i r si nV , setting\\nk = 20\\n\u03f52 log m and upper bounding \u03f5 by 1/2, we have\\nPr[success] \u2265 1 \u2212 2m2e\u2212(\u03f52\u2212\u03f53)k/4 =1 \u2212 2m5\u03f5\u22123 > 1 \u2212', ' 2m\u22121/2 > 0 .\\nSince the success probability is strictly greater than zero, a map that satis\ufb01es the\\ndesired conditions must exist, thus proving the statement of the lemma.\\n12.5 Chapter notes\\nPCA was in', 'troduced in the early 1900s by Pearson [1901]. KPCA was introduced\\nroughly a century later, and our presentation of KPCA is a more concise derivation\\nof results given by Mika et al. [1999]. Isomap and', ' LLE were pioneering works on\\nnon-linear dimensionality reduction introduced byTenenbaum et al. [2000], Roweis\\nand Saul [2000]. Isomap itself is a generalization of a standard linear dimensionality\\nre', 'duction technique called Multidimensional Scaling [Cox and Cox, 2000]. Isomap\\nand LLE led to the development of several related algorithms for manifold learning,\\ne.g., Laplacian Eigenmaps and Maximum ', 'Variance Unfolding [Belkin and Niyogi,\\n2001, Weinberger and Saul, 2006]. As shown in this chapter, classical manifold\\nlearning algorithms are special instances of KPCA [Ham et al., 2004]. The Johnson-', '\\nLindenstrauss lemma was introduced by Johnson and Lindenstrauss [1984], though\\nour proof of the lemma follows Vempala [2004]. Other simpli\ufb01ed proofs of this lemma\\nhave also been presented, including ', 'Dasgupta and Gupta [2003].\\n12.6 Exercises\\n12.1 PCA and maximal variance. Let X be an uncentered data matrix and let\\n\u00afx = 1\\nm\\n\u2211\\ni xi b et h es a m p l em e a no ft h ec o l u m n so fX.\\n(a) Show that t', 'he variance of one-dimensional projections of the data onto an\\narbitrary vector u equals u\u22a4Cu,w h e r eC = 1\\nm\\n\u2211\\ni(xi \u2212 \u00afx)(xi \u2212 \u00afx)\u22a4 is the\\nsample covariance matrix.\\n(b) Show that PCA with k = 1 proj', 'ects the data onto the direction (i.e.,\\nu\u22a4u = 1) of maximal variance.\\n12.2 Double centering. In this problem we will prove the correctness of the double12.6 Exercises 291\\ncentering step in Isomap when', ' working with Euclidean distances. De\ufb01neX and \u00afx as\\nin exercise 12.1, and de\ufb01ne X\u2217 as the centered version ofX,t h a ti s ,l e tx\u2217\\ni = xi \u2212 \u00afx\\nbe the ith column of X\u2217.L e tK = X\u22a4X,a n dl e tD denote t', 'he Euclidean distance\\nmatrix, i.e., Dij = \u2225xi \u2212 xj \u2225.\\n(a) Show that Kij = 1\\n2 (Kii + Kjj + D2\\nij).\\n(b) Show that K\u2217 = X\u2217\u22a4X\u2217 = K \u2212 1\\nmK11\u22a4 \u2212 1\\nm11\u22a4K + 1\\nm2 11\u22a4K11\u22a4.\\n(c) Using the results from (a) and (', 'b) show that\\nK\u2217\\nij = \u2212 1\\n2\\n[\\nD2\\nij \u2212 1\\nm\\nm\u2211\\nk=1\\nD2\\nik \u2212 1\\nm\\nm\u2211\\nk=1\\nD2\\nkj + \u00afD\\n]\\n,\\nwhere \u00afD = 1\\nm2\\n\u2211\\nu\\n\u2211\\nv D2\\nu,v is the mean of the m2 entries in D.\\n(d) Show that K\u2217 = \u22121\\n2HDH.\\n12.3 Laplacian eigenmap', 's. Assume k = 1 and we seek a one-dimensional represen-\\ntation y. Show that (12.7) is equivalent to y =a r g m i ny\u2032 y\u2032\u22a4Ly\u2032,w h e r eL is the\\ngraph Laplacian.\\n12.4 Nystr\u00a8om method. De\ufb01ne the following', ' block representation of a kernel matrix:\\nK =\\n[\\nWK \u22a4\\n21\\nK21 K22\\n]\\nand C =\\n[\\nW\\nK21\\n]\\n.\\nThe Nystr\u00a8om method uses W \u2208 Rl\u00d7l and C \u2208 Rm\u00d7l to generate the approximation\\n\u02dcK = CW\u2020 C\u22a4 \u2248 K.\\n(a) Show that W is S', 'PSD and that \u2225K \u2212 \u02dcK\u2225F = \u2225K22 \u2212 K21W\u2020 K\u22a4\\n21\u2225F .\\n(b) Let K = X\u22a4X for some X \u2208 RN \u00d7m,a n dl e tX\u2032 \u2208 RN \u00d7l be the \ufb01rst\\nl columns of X. Show that \u02dcK = X\u22a4PUX\u2032 X,w h e r ePUX\u2032 is the orthogonal\\nprojection o', 'nto the span of the left singular vectors of X\u2032.\\n(c) Is \u02dcK SPSD?\\n(d) If rank(K)=r a n k (W)= r \u226a m,s h o wt h a t\u02dcK = K. Note: this statement\\nholds whenever rank(K)=r a n k (W), but is of interest mai', 'nly in the low-rank\\nsetting.\\n(e) If m = 20M andK is a dense matrix, how much space is required to storeK\\ni fe a c he n t r yi ss t o r e da sad o u b l e ?H o wm u c hs p a c ei sr e q u i r e db yt h', ' eN y s t r \u00a8om\\nmethod if l = 10K?292 Dimensionality Reduction\\n12.5 Expression for KLLE. Show the connection between LLE and KPCA by\\nderiving the expression for KLLE.\\n12.6 Random projection, PCA, and ', 'nearest neighbors.\\n(a) Download the MNIST test set of handwritten digits at:\\nhttp://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz.\\nCreate a data matrix X \u2208 RN \u00d7m from the \ufb01rst m =2 ,000 instance', 's of this\\ndataset (the dimension of each instance should be N = 784).\\n(b) Find the ten nearest neighbors for each point in X, that is, compute Ni,10\\nfor 1 \u2264 i \u2264 m,w h e r eNi,t denotes the set of the ', 't nearest neighbors for the ith\\ndatapoint and nearest neighbors are de\ufb01ned with respect to the L2 norm. Also\\ncompute Ni,50 for all i.\\n(c) Generate \u02dcX = AX,w h e r eA \u2208 Rk\u00d7N , k = 100 and entries of A ', 'are\\nsampled independently from the standard normal distribution. Find the ten\\nnearest neighbors for each point in \u02dcX, that is, compute \u02dcNi,10 for 1 \u2264 i \u2264 m.\\n(d) Report the quality of approximation by ', 'computing score10 = 1\\nm\\n\u2211m\\ni=1 |Ni,10\u2229\\n\u02dcNi,10|. Similarly, compute score50 = 1\\nm\\n\u2211m\\ni=1 |Ni,50 \u2229 \u02dcNi,10|.\\n(e) Generate two plots that show score 10 and score50 as functions of k (i.e.,\\nperform steps (', 'c) and (d) for k = {1,10, 50, 100,250, 500}). Provide a one- or\\ntwo-sentence explanation of these plots.\\n(f) Generate similar plots as in (e) using PCA (with various values ofk)t og e n -\\nerate \u02dcX and', ' subsequently compute nearest neighbors. Are the nearest neighbor\\napproximations generated via PCA better or worse than those generated via\\nrandom projections? Explain why.13 Learning Automata and Lan', 'guages\\nThis chapter presents an introduction to the problem of learning languages. This\\nis a classical problem explored since the early days of formal language theory and\\ncomputer science, and there i', 's a very large body of literature dealing with related\\nmathematical questions. In this chapter, we present a brief introduction to this\\nproblem and concentrate speci\ufb01cally on the question of learning ', '\ufb01nite automata,\\nwhich, by itself, has been a topic investigated in multiple forms by thousands of\\ntechnical papers. We will examine two broad frameworks for learning automata,\\nand for each, we will pr', 'esent an algorithm. In particular, we describe an algorithm\\nfor learning automata in which the learner has access to several types of query, and\\nwe discuss an algorithm for identifying a sub-class of ', 'the family of automata in the\\nlimit.\\n13.1 Introduction\\nLearning languages is one of the earliest problems discussed in linguistics and\\ncomputer science. It has been prompted by the remarkable faculty ', 'of humans to\\nlearn natural languages. Humans are capable of uttering well-formed new sentences\\nat an early age, after having been exposed only to \ufb01nitely many sentences. Moreover,\\neven at an early age', ', they can make accurate judgments of grammaticality for new\\nsentences.\\nIn computer science, the problem of learning languages is directly related to that\\nof learning the representation of the computa', 'tional device generating a language.\\nThus, for example, learning regular languages is equivalent to learning \ufb01nite au-\\ntomata, or learning context-free languages or context-free grammars is equivalent', '\\nto learning pushdown automata.\\nThere are several reasons for examining speci\ufb01cally the problem of learning\\n\ufb01nite automata. Automata provide natural modeling representations in a variety\\nof di\ufb00erent d', 'omains including systems, networking, image processing, text and\\nspeech processing, logic and many others. Automata can also serve as simple or\\ne\ufb03cient approximations for more complex devices. For exa', 'mple, in natural language294 Learning Automata and Languages\\n0\\nb\\n1a\\n3\\na\\n2a\\n\u03b5\\nb\\nb\\n0\\nb\\n1a 2a 3b\\na\\nb\\n(a) (b)\\nFigure 13.1 (a) A graphical representation of a \ufb01nite automaton. (b) Equivalent\\n(minimal) dete', 'rministic automaton.\\nprocessing, they can be used to approximate context-free languages. When it is\\npossible, learning automata is often e\ufb03cient, though, as we shall see, the problem\\nis hard in a numb', 'er of natural scenarios. Thus, learning more complex devices or\\nlanguages is even harder.\\nWe consider two general learning frameworks: the model ofe\ufb03cient exact learning\\nand the model of identi\ufb01cation', ' in the limit . For each of these models, we brie\ufb02y\\ndiscuss the problem of learning automata and describe an algorithm.\\nWe \ufb01rst give a brief review of some basic automata de\ufb01nitions and algorithms,\\nth', 'en discuss the problem of e\ufb03cient exact learning of automata and that of the\\nidenti\ufb01cation in the limit.\\n13.2 Finite automata\\nWe will denote by \u03a3 a \ufb01nite alphabet. The length of a string x \u2208 \u03a3\u2217 over t', 'hat\\nalphabet is denoted by |x|.T h eempty string is denoted by \u03f5,t h u s|\u03f5| = 0. For any\\nstring x = x1 \u00b7\u00b7\u00b7 xk \u2208 \u03a3\u2217 of length k \u2265 0, we denote by x[j]= x1 \u00b7\u00b7\u00b7 xj its pre\ufb01x of\\nlength j \u2264 k and de\ufb01ne x[0', '] as \u03f5.\\nFinite automata are labeled directed graphs equipped with initial and \ufb01nal states.\\nThe following gives a formal de\ufb01nition of these devices.\\nDe\ufb01nition 13.1 Finite automata\\nA \ufb01nite automaton A i', 's a 5-tuple (\u03a3,Q ,I,F,E ) where \u03a3 is a \ufb01nite alphabet, Q a\\n\ufb01nite set of states, I \u2286 Q a set of initial states, F \u2286 Q a set of \ufb01nal states, and\\nE \u2286 Q \u00d7 (\u03a3 \u222a{ \u03f5}) \u00d7 Q a \ufb01nite set of transitions.\\nFigure ', '13.1a shows a simple example of a \ufb01nite automaton. States are represented\\nby circles. A bold circle indicates an initial state, a double circle a \ufb01nal state. Each\\ntransition is represented by an arrow', ' from its origin state to its destination state\\nwith its label in \u03a3 \u222a{ \u03f5}.\\nA path from an initial state to a \ufb01nal state is said to be an accepting path.A n13.3 E\ufb03cient exact learning 295\\nautomaton is ', 'said to be trim if all of its states are accessible from an initial state\\nand admit a path to a \ufb01nal state, that is, if all of its states lie on an accepting\\npath. A string x \u2208 \u03a3\\n\u2217 is accepted by an a', 'utomaton A i\ufb00 x labels an accepting path.\\nFor convenience, we will say that x \u2208 \u03a3\u2217 is rejected by A when it is not accepted.\\nThe set of all strings accepted by A de\ufb01nes the language accepted by A deno', 'ted by\\nL(A). The class of languages accepted by \ufb01nite automata coincides with the family\\nof regular languages, that is, languages that can be described byregular expressions.\\nAny \ufb01nite automaton admit', 's an equivalent automaton with no \u03f5-transition,t h a t\\nis, no transition labeled with the empty string: there exists a general \u03f5-removal\\nalgorithm that takes as input an automaton and returns an equiv', 'alent automaton\\nwith no \u03f5-transition.\\nAn automaton with no\u03f5-transition is said to bedeterministic if it admits a unique\\ninitial state and if no two transitions sharing the same label leave any given s', 'tate.\\nA deterministic \ufb01nite automaton is often referred to by the acronym DFA, while\\nthe acronym NFA is used for arbitrary automata, that is, non-deterministic \ufb01nite\\nautomata. Any NFA admits an equiva', 'lent DFA: there exists a general (exponential-\\ntime) determinization algorithm that takes as input an NFA with no \u03f5-transition\\nand returns an equivalent DFA. Thus, the class of languages accepted by D', 'FAs\\ncoincides with that of the languages accepted by NFAs, that is regular languages.\\nFor any string x \u2208 \u03a3\\n\u2217 and DFA A,w ed e n o t eb yA(x) the state reached in A when\\nreading x from its unique initi', 'al state.\\nA DFA is said to be minimal if it admits no equivalent deterministic automaton\\nwith a smaller number of states. There exists a general minimization algorithm\\ntaking as input a deterministic ', 'automaton and returning a minimal one that runs\\nin O(|E| log |Q|). When the input DFA is acyclic, that is when it admits no path\\nforming a cycle, it can be minimized in linear timeO(|Q|+ |E|). Figure ', '13.1b shows\\nthe minimal DFA equivalent to the NFA of \ufb01gure 13.1a.\\n13.3 E\ufb03cient exact learning\\nIn the e\ufb03cient exact learning framework, the problem consists of identifying a\\ntarget concept c from a \ufb01ni', 'te set of examples in time polynomial in the size of the\\nrepresentation of the concept and in an upper bound on the size of the representation\\nof an example. Unlike the PAC-learning framework, in this', ' model, there is no\\nstochastic assumption, instances are not assumed to be drawn according to some\\nunknown distribution. Furthermore, the objective is to identify the target concept\\nexactly, without a', 'ny approximation. A concept class C is said to be e\ufb03ciently\\nexactly learnable if there is an algorithm for e\ufb03cient exact learning of any c \u2208 C.\\nWe will consider two di\ufb00erent scenarios within the frame', 'work of e\ufb03ciently exact296 Learning Automata and Languages\\nlearning: a passive and an active learning scenario. The passive learning scenario is\\nsimilar to the standard supervised learning scenario di', 'scussed in previous chapters\\nbut without any stochastic assumption: the learning algorithm passively receives\\ndata instances as in the PAC model and returns a hypothesis, but here, instances\\nare not a', 'ssumed to be drawn from any distribution. In the active learning scenario,\\nthe learner actively participates in the selection of the training samples by using\\nvarious types of queries that we will des', 'cribe. In both cases, we will focus more\\nspeci\ufb01cally on the problem of learning automata.\\n13.3.1 Passive learning\\nThe problem of learning \ufb01nite automata in this scenario is known as the minimum\\nconsis', 'tent DFA learning problem . It can be formulated as follows: the learner\\nreceives a \ufb01nite sample S =( (x\\n1,y1),..., (xm,y m)) with xi \u2208 \u03a3\u2217 and yi \u2208{ \u22121, +1}\\nfor any i \u2208 [1,m]. If yi =+ 1 ,t h e nxi is', ' an accepted string, otherwise it is rejected.\\nThe problem consists of using this sample to learn the smallest DFA A consistent\\nwith S, that is the automaton with the smallest number of states that ac', 'cepts the\\nstrings of S with label +1 and rejects those with label \u22121. Note that seeking the\\nsmallest DFA consistent withS can be viewed as following Occam\u2019s razor principle.\\nThe problem just described', ' is distinct from the standard minimization of DFAs. A\\nminimal DFA accepting exactly the strings ofS labeled positively may not have the\\nsmallest number of states: in general there may be DFAs with fe', 'wer states accepting\\na superset of these strings and rejecting the negatively labeled sample strings.\\nFor example, in the simple case S =( ( a,+1), (b, \u22121)), a minimal deterministic\\nautomaton acceptin', 'g the unique positively labeled stringa or the unique negatively\\nlabeled string b admits two states. However, the deterministic automaton accepting\\nthe language a\u2217 accepts a and rejects b and has only', ' one state.\\nPassive learning of \ufb01nite automata turns out to be a computationally hard\\nproblem. The following theorems present several negative results known for this\\nproblem.\\nTheorem 13.1\\nThe problem ', 'of \ufb01nding the smallest deterministic automaton consistent with a set\\nof accepted or rejected strings is NP-complete.\\nHardness results are known even for a polynomial approximation, as stated by the\\nfo', 'llowing theorem.\\nTheorem 13.2\\nIf P \u0338= NP, then, no polynomial-time algorithm can be guaranteed to \ufb01nd a DFA\\nconsistent with a set of accepted or rejected strings of size smaller than a polynomial\\nfunc', 'tion of the smallest consistent DFA, even when the alphabet is reduced to just13.3 E\ufb03cient exact learning 297\\ntwo elements.\\nOther strong negative results are known for passive learning of \ufb01nite automa', 'ta\\nunder various cryptographic assumptions.\\nThese negative results for passive learning invite us to consider alternative\\nlearning scenarios for \ufb01nite automata. The next section describes a scenario l', 'eading\\nto more positive results where the learner can actively participate in the data\\nselection process using various types of queries.\\n13.3.2 Learning with queries\\nThe model of learning with queries', ' corresponds to that of a (minimal) teacher or\\noracle and an active learner. In this model, the learner can make the following two\\nt y p e so fq u e r i e st ow h i c ha no r a c l er e s p o n d s :\\n', 'membership queries: the learner requests the target label f(x) \u2208{ \u22121, +1} of an\\ninstance x and receives that label;\\nequivalence queries: the learner conjectures hypothesish; he receives the response\\ny', 'es if h = f, a counter-example otherwise.\\nWe will say that a concept class C is e\ufb03ciently exactly learnable with membership\\nand equivalence queries when it is e\ufb03ciently exactly learnable within this m', 'odel.\\nThis model is not realistic, since no such oracle is typically available in practice.\\nNevertheless, it provides a natural framework, which, as we shall see, leads to\\npositive results. Note also ', 'that for this model to be signi\ufb01cant, equivalence must be\\ncomputationally testable. This would not be the case for some concept classes such\\nas that of context-free grammars, for example, for which th', 'e equivalence problem is\\nundecidable. In fact, equivalence must be further e\ufb03ciently testable, otherwise the\\nresponse to the learner cannot be supplied in a reasonable amount of time.\\n1\\nE\ufb03cient exact ', 'learning within this model of learning with queries implies the\\nfollowing variant of PAC-learning: we will say that a concept class C is PAC-\\nlearnable with membership queries if it is PAC-learnable b', 'y an algorithm that has\\naccess to a polynomial number of membership queries.\\nTheorem 13.3\\nLet C be a concept class that is e\ufb03ciently exactly learnable with membership and\\nequivalence queries, then C i', 's PAC-learnable using membership queries.\\n1. For a human oracle, answering membership queries may also become very hard in some\\ncases when the queries are near the class boundaries. This may also make', ' the model\\ndi\ufb03cult to adopt in practice.298 Learning Automata and Languages\\nProof Let A be an algorithm for e\ufb03ciently exactly learning C using membership\\nand equivalence queries. Fix \u03f5, \u03b4 >0. We repla', 'ce in the execution of A for learning\\ntarget c \u2208 C, each equivalence query by a test of the current hypothesis on a\\npolynomial number of labeled examples. Let D be the distribution according to\\nwhich ', 'points are drawn. To simulate the tth equivalence query, we draw mt =\\n1\\n\u03f5 (log 1\\n\u03b4 + t log 2) points i.i.d. according to D to test the current hypothesis ht.I f\\nht is consistent with all of these poin', 'ts, then the algorithm stops and returns ht.\\nOtherwise, one of the points drawn does not belong toht, which provides a counter-\\nexample.\\nSince A learns c exactly, it makes at most T equivalence querie', 's, where T is\\npolynomial in the size of the representation of the target concept and in an upper\\nbound on the size of the representation of an example. Thus, if no equivalence\\nquery is positively resp', 'onded by the simulation, the algorithm will terminate afterT\\nequivalence queries and return the correct conceptc. Otherwise, the algorithm stops\\nat the \ufb01rst equivalence query positively responded by t', 'he simulation. The hypothesis\\nit returns is not an \u03f5-approximation only if the equivalence query stopping the\\nalgorithm is incorrectly responded positively. By the union bound, since for any\\n\ufb01xed t \u2208 ', '[1,T ], Pr[R(h\\nt) >\u03f5 ] \u2264 (1 \u2212 \u03f5)mt , the probability that for some t \u2208 [1,T ],\\nR(ht) >\u03f5 can be bounded as follows:\\nPr[\u2203t \u2208 [1,T ]: R(ht) >\u03f5 ] \u2264\\nT\u2211\\ni=1\\nPr[R(ht) >\u03f5 ]\\n\u2264\\nT\u2211\\ni=1\\n(1 \u2212 \u03f5)mt \u2264\\nT\u2211\\ni=1\\ne\u2212mt\u03f5 \u2264', '\\nT\u2211\\ni=1\\n\u03b4\\n2t \u2264\\n+\u221e\u2211\\ni=1\\n\u03b4\\n2t = \u03b4.\\nThus, with probability at least 1 \u2212 \u03b4, the hypothesis returned by the algorithm is\\nan \u03f5-approximation. Finally, the maximum number of points drawn is \u2211T\\nt=1 mt =\\n1\\n\u03f5 (', 'T log 1\\n\u03b4 + T(T+1)\\n2 log 2), which is polynomial in 1 /\u03f5,1 /\u03b4,a n d T. Since the rest\\nof the computational cost of A is also polynomial by assumption, this proves the\\nPAC-learning of C.\\n13.3.3 Learnin', 'g automata with queries\\nIn this section, we describe an algorithm for e\ufb03cient exact learning of DFAs with\\nmembership and equivalence queries. We will denote by A the target DFA and by\\n\u02c6A the DFA that ', 'is the current hypothesis of the algorithm. For the discussion of\\nthe algorithm, we assume without loss of generality that A is a minimal DFA.\\nThe algorithm uses two sets of strings, U and V . U is a ', 'set of access strings:\\nreading an access string u \u2208 U from the initial state ofA leads to a state A(u). The\\nalgorithm ensures that the states A(u), u \u2208 U,a r ea l ld i s t i n c t .T od os o ,i tu s e', ' sa13.3 E\ufb03cient exact learning 299\\na ba\\nb\\n/gid1\\n/gid1 \u03b5\\na\\nbb\\nb\\nbaa\\na\\nb\\n0\\na\\n1b\\nb\\n2a\\nb\\n3a\\na\\nb\\n(a) (b) (c)\\nFigure 13.2 (a) Classi\ufb01cation tree T,w i t hU = {\u03f5, b, ba} and V = {\u03f5, a}.( b )C u r r e n t\\naut', 'omaton bA constructed using T. (c) Target automaton A.\\nset V of distinguishing strings.S i n c eA is minimal, for two distinct states q and q\u2032\\nof A, there must exist at least one string that leads to ', 'a \ufb01nal state from q and not\\nfrom q\u2032, or vice versa. That string helps distinguish q and q\u2032.T h es e to fs t r i n g sV\\nhelp distinguish any pair of access strings in U. They de\ufb01ne in fact a partition ', 'of\\nall strings of \u03a3\u2217.\\nThe objective of the algorithm is to \ufb01nd at each iteration a new access string\\ndistinguished from all previous ones, ultimately obtaining a number of access strings\\nequal to the ', 'number of states of A. It can then identify each state A(u)o f A with\\ni t sa c c e s ss t r i n gu. To \ufb01nd the destination state of the transition labeled witha \u2208 \u03a3\\nleaving state u, it su\ufb03ces to deter', 'mine, using the partition induced byV the access\\nstring u\\n\u2032 that belongs to the same equivalence class asua.T h e\ufb01 n a l i t yo fe a c hs t a t e\\ncan be determined in a similar way.\\nBoth sets U and V ', 'are maintained by the algorithm via a binary decision treeT\\nsimilar to those presented in chapter 8. Figure 13.2a shows an example. T de\ufb01nes\\nthe partition of all strings induced by the distinguishing ', 'stringsV .T h el e a v e so fT\\nare each labeled with a distinct u \u2208 U and its internal nodes with a string v \u2208 V .\\nT h ed e c i s i o nt r e eq u e s t i o nd e \ufb01 n e db yv \u2208 V , given a string x \u2208 \u03a3\u2217', ',i sw h e t h e rxv\\nis accepted by A, which is determined via a membership query. If accepted, x is\\nassigned to right sub-tree, otherwise to the left sub-tree, and the same is applied\\nrecursively with', ' the sub-trees until a leaf is reached. We denote byT(x)t h el a b e lo f\\nthe leaf reached. For example, for the treeT of \ufb01gure 13.2a and target automaton A\\nof \ufb01gure 13.2c, T(baa)= b since baa is not ', 'accepted by A (root question) and baaa\\nis (question at node a). At its initialization step, the algorithm ensures that the\\nroot node is labeled with \u03f5, which is convenient to check the \ufb01nality of the ', 'strings.\\nThe tentative hypothesis DFA \u02c6A can be constructed fromT as follows. We denote\\nby ConstructAutomaton() the corresponding function. A distinct state \u02c6A(u)i s\\ncreated for each leaf u \u2208 V . The ', '\ufb01nality of a state \u02c6A(u) is determined based on\\nthe sub-tree of the root node that u belongs to: \u02c6A(u)i sm a d e\ufb01 n a li \ufb00u belongs300 Learning Automata and Languages\\nQueryLearnAutomata()\\n1 t \u2190 Member', 'shipQuery(\u03f5)\\n2 T \u2190 T0\\n3 \u02c6A \u2190 A0\\n4 while (EquivalenceQuery( \u02c6A) \u0338= true) do\\n5 x \u2190 CounterExample()\\n6 if (T = T0) then\\n7 T \u2190 T1 \u22bf nil replaced with x.\\n8 else j \u2190 argmink A(x[k]) \u0338\u2261T \u02c6A(x[k])\\n9 Split( \u02c6A', '(x[j \u2212 1]))\\n10 \u02c6A \u2190 ConstructAutomaton(T)\\n11 return \u02c6A\\nFigure 13.3 Algorithm for learning automata with membership and equivalence\\nqueries. A0 is a single-state automaton with self-loops labeled with ', 'all a \u2208 \u03a3.T h a t\\nstate is initial. It is \ufb01nal i\ufb00 t = true. T0 is a tree with root node labeled with \u03f5 and\\ntwo leaves, one labeled with \u03f5, the other with nil. the right leaf is labeled with \u03f5\\nlabels i', '\ufb00 t = true. T1 i st h et r e eo b t a i n e df r o mT0 by replacing nil with x.\\nto the right sub-tree that is i\ufb00 u = \u03f5u is accepted by A. The destination of the\\ntransition labeled with a \u2208 \u03a3 leaving s', 'tate \u02c6A(u) is the state \u02c6A(v)w h e r ev = T(ua).\\nFigure 13.2b shows the DFA \u02c6A constructed from the decision tree of \ufb01gure 13.2a.\\nFor convenience, for anyx \u2208 \u03a3\u2217, we denote byU( \u02c6A(x)) the access strin', 'g identifying\\nstate \u02c6A(x).\\nFigure 13.3 shows the pseudocode of the algorithm. The initialization steps at\\nlines 1\u20133 construct a tree T with a single internal node labeled with \u03f5 and one leaf\\nstring la', 'beled with \u03f5, the other left undetermined and labeled with nil.T h e ya l s o\\nde\ufb01ne a tentative DFA \u02c6A with a single state with self-loops labeled with all elements\\nof the alphabet. That single state ', 'is an initial state. It is made a \ufb01nal state only if\\n\u03f5 is accepted by the target DFA A, which is determined via the membership query\\nof line 1.\\nAt each iteration of the loop of lines 4\u201311, an equivale', 'nce query is used. If\u02c6A is not\\nequivalent to A, then a counter-example stringx is received (line 5). IfT is the tree\\nconstructed in the initialization step, then the leaf labeled withnil is replaced w', 'ith\\nx (lines 6\u20137). Otherwise, since x is a counter-example, states A(x)a n d \u02c6A(x)h a v ea\\ndi\ufb00erent \ufb01nality; thus, the string x de\ufb01ning A(x) and the access string U( \u02c6A(x)) are13.3 E\ufb03cient exact learn', 'ing 301\\nv\u2032\\nT(x[j \u2212 1])\\nu\u2032\\nx[j \u2212 1]\\nxjv\\nv\u2032\\nT(x[j \u2212 1]) u\u2032\\nFigure 13.4 Illustration of the splitting procedure Split( bA(x[j \u2212 1])).\\nassigned to di\ufb00erent equivalence classes by T. Thus, there exists a s', 'mallest j such\\nthat A(x[j]) and \u02c6A(x[j]) are not equivalent, that is, such that the pre\ufb01x x[j]o f x\\nand the access stringU( \u02c6A(x[j])) are assigned to di\ufb00erent leaves byT. j cannot be 0\\nsince the initi', 'alization ensures that \u02c6A(\u03f5) is an initial state and has the same \ufb01nality\\nas the initial state A(\u03f5)o f A. The equivalence of A(x[j]) and \u02c6A(x[j]) is tested by\\nchecking the equality of T(x[j]) and T(U(', ' \u02c6A(x[j]))), which can be both determined\\nusing the tree T and membership queries (line 8).\\nNow, by de\ufb01nition, A(x[j \u2212 1]) and \u02c6A(x[j \u2212 1]) are equivalent, that is T assigns\\nx[j \u22121] to the leaf labele', 'd withU( \u02c6A(x[j \u22121])). But, x[j \u22121] and U( \u02c6A(x[j \u22121])) must\\nbe distinguished since A(x[j \u2212 1]) and \u02c6A(x[j \u2212 1]) admit transitions labeled with\\nt h es a m el a b e lxj to two non-equivalent states. Le', 't v be a distinguishing string for\\nA(x[j]) and \u02c6A(x[j]). v can be obtained as the least common ancestor of the leaves\\nlabeled with x[j]a n d U( \u02c6A(x[j])). To distinguish x[j \u2212 1] and U( \u02c6A(x[j \u2212 1])),', ' it\\nsu\ufb03ces to split the leaf of T labeled with T(x[j \u2212 1]) to create an internal node xjv\\ndominating a leaf labeled with x[j \u2212 1] and another one labeled with T(x[j \u2212 1])\\n(line 9). Figure 13.4 illustr', 'ates this construction. Thus, this provides a new access\\nstring x[j \u2212 1] which, by construction, is distinguished from U( \u02c6A(x[j \u2212 1])) and all\\nother access strings.\\nThus, the number of access strings', ' (or states of \u02c6A) increases by one at each\\ni t e r a t i o no ft h el o o p .W h e ni tr e a c h e st h en u m b e ro fs t a t e so fA, all states of A\\nare of the form A(u) for a distinct u \u2208 U. A an', 'd \u02c6A have then the same number\\nof states and in fact A = \u02c6A. Indeed, let (A(u),a ,A(u\u2032)) be a transition in A,t h e n\\nby de\ufb01nition the equality A(ua)= A(u\u2032) holds. The tree T de\ufb01nes a partition\\nof all', ' strings in terms of their distinguishing strings in A. Since in A, ua and u\u2032\\nlead to the same state, they are assigned to the same leaf by T, that is, the leaf\\nlabeled with u\u2032. The destination of the', ' transition from \u02c6A(u)w i t hl a b e la is found\\nby ConstructAutomaton() by determining the leaf in T assigned to ua,t h a t\\nis, u\u2032. Thus, by construction, the same transition ( \u02c6A(u),a , \u02c6A(u\u2032)) is c', 'reated in \u02c6A.\\nAlso, a state A(u)o f A is \ufb01nal i\ufb00 u accepted by A that is i\ufb00 u is assigned to the\\nright sub-tree of the root node by T, which is the criterion determining the \ufb01nality\\nof \u02c6A(u). Thus, th', 'e automata A and \u02c6A coincide.302 Learning Automata and Languages\\nA\\n0\\na\\n1b\\nb\\n2a\\nb\\n3a\\nb\\na\\nT \u02c6A counter-example x\\n\u03b5\\n\u03b5 NIL\\n\u03b5\\na\\nb\\nx = b\\n\u03b5\\n\u03b5 b\\n\u03b5\\na\\nb\\nb\\na\\nb x = baa\\n\u03b5\\na b\\n\u03b5 ba\\n\u03b5\\na\\nbb\\nb\\nbaa\\na\\nb\\nx = baaa\\n\u03b5\\na a\\n', '\u03b5 ba b baa\\n\u03b5\\na\\nbb\\nb\\nbaa\\nb\\nbaaa\\nb\\na\\nFigure 13.5 Illustration of the execution of Algorithm QueryLearnAutomata()\\nfor the target automaton A. Each line shows the current decision tree T and the\\ntentative', ' DFA bA constructed using T.W h e nbA is not equivalent to A, the learner\\nreceives a counter-example x i n d i c a t e di nt h et h i r dc o l u m n .\\nThe following is the analysis of the running-time', ' complexity of the algorithm. At\\neach iteration, one new distinguished access string is found associated to a distinct\\nstate of A, thus, at most|A| states are created. For each counter-examplex, at mo', 'st\\n|x| tree operations are performed. Constructing\u02c6A requires O(|\u03a3||A|) tree operations.\\nThe cost of a tree operation is O(|A|) since it consists of at most |A| membership\\nqueries. Thus, the overall c', 'omplexity of the algorithm is inO(|\u03a3||A|2 +n|A|), where\\nn is the maximum length of a counter-example. Note that this analysis assumes\\nthat equivalence and membership queries are made in constant time.', '\\nOur analysis shows the following result.13.4 Identi\ufb01cation in the limit 303\\nTheorem 13.4 Learning DFAs with queries\\nThe class of all DFAs is e\ufb03ciently exactly learnable using membership and equiva-\\nl', 'ence queries.\\nFigure 13.5 illustrates a full execution of the algorithm in a speci\ufb01c case.\\nIn the next section, we examine a di\ufb00erent learning scenario for automata.\\n13.4 Identi\ufb01cation in the limit\\nIn', ' the identi\ufb01cation in the limit framework , the problem consists of identifying a\\ntarget concept c exactly after receiving a \ufb01nite set of examples. A class of languages\\nis said to be identi\ufb01able in th', 'e limit if there exists an algorithm that identi\ufb01es\\nany language L in that class after examining a \ufb01nite number of examples and its\\nhypothesis remains unchanged thereafter.\\nThis framework is perhaps l', 'ess realistic from a computational point of view since\\nit requires no upper bound on the number of instances or the e\ufb03ciency of the\\nalgorithm. Nevertheless, it has been argued by some to be similar to', ' the scenario\\nof humans learning languages. In this framework as well, negative results hold for\\nthe general problem of learning DFAs.\\nTheorem 13.5\\nDeterministic automata are not identi\ufb01able in the li', 'mit from positive examples.\\nSome sub-classes of \ufb01nite automata can however be successfully identi\ufb01ed in the\\nlimit. Most algorithms for inference of automata are based on a state-partitioning\\nparadigm.', ' They start with an initial DFA, typically a tree accepting the \ufb01nite set\\nof sample strings available and the trivial partition: each block is reduced to one\\nstate of the tree. At each iteration, they', ' merge partition blocks while preserving\\nsome congruence property. The iteration ends when no other merging is possible.\\nThe \ufb01nal partition de\ufb01nes the automaton inferred as follows. Thus, the choice o', 'f\\nthe congruence fully determines the algorithm and a variety of di\ufb00erent algorithms\\ncan be de\ufb01ned by varying that choice. A state-splitting paradigm can be similarly\\nde\ufb01ned starting from the single-s', 'tate automaton accepting \u03a3\\n\u2217. In this section, we\\npresent an algorithm for learning reversible automata, which is a special instance\\nof the general state-partitioning algorithmic paradigm just describ', 'ed.\\nLet A =( \u03a3,Q ,I,F,E )b eaD F Aa n dl e t\u03c0 be a partition of Q.T h eD F Ad e \ufb01 n e d\\nby the partition \u03c0 is called the automaton quotient of A and \u03c0. It is denoted by304 Learning Automata and Langua', 'ges\\nA/\u03c0 and de\ufb01ned as follows: A/\u03c0 =( \u03a3,\u03c0,I \u03c0,F\u03c0,E \u03c0)w i t h\\nI\u03c0 = {B \u2208 \u03c0: I \u2229 B \u0338= \u2205}\\nF\u03c0 = {B \u2208 \u03c0: F \u2229 B \u0338= \u2205}\\nE\u03c0 = {(B,a,B \u2032): \u2203(q,a,q \u2032) \u2208 E | q \u2208 B,q \u2032 \u2208 B\u2032,B \u2208 \u03c0,B \u2032 \u2208 \u03c0}.\\nLet S be a \ufb01nite set of ', 'strings and let Pref(S) denote the set of pre\ufb01xes of all strings\\nof S.A pre\ufb01x-tree automaton accepting exactly the set of strings S is a particular\\nDFA denoted by PT (S)=( \u03a3 , Pref(S), {\u03f5},S ,ES)w h e', ' r e\u03a3i st h es e to fa l p h a b e t\\nsymbols used in S and ES de\ufb01ned as follows:\\nES = {(x, a, xa): x \u2208 Pref(S),x a \u2208 Pref(S)}.\\nFigure 13.7a shows the pre\ufb01x-tree automaton of a particular set of string', 's S.\\n13.4.1 Learning reversible automata\\nIn this section, we show that the sub-class of reversible automata or reversible\\nlanguages can be identi\ufb01ed in the limit.\\nGiven a DFA A,w ed e \ufb01 n ei t srevers', 'e A\\nR as the automaton derived from A by\\nmaking the initial state \ufb01nal, the \ufb01nal states initial, and by reversing the direction of\\nevery transition. The language accepted by the reverse ofA is precise', 'ly the language\\nof the reverse (or mirror image) of the strings accepted by A.\\nDe\ufb01nition 13.2 Reversible automata\\nA \ufb01nite automaton A is said to be reversible i\ufb00 bothA and A\\nR are deterministic. A\\nlan', 'guage L is said to be reversible if it is the language accepted by some reversible\\nautomaton.\\nSome direct consequences of this de\ufb01nition are that a reversible automaton A has\\na unique \ufb01nal state and t', 'hat its reverse AR is also reversible. Note also that a trim\\nreversible automaton A is minimal. Indeed, if states q and q\u2032 in A are equivalent,\\nthen, they admit a common string x leading both from q a', 'nd from q\u2032 to a \ufb01nal\\nstate. But, by the reverse determinism of A, reading the reverse of x from the \ufb01nal\\nstate must lead to a unique state, which implies that q = q\u2032.\\nFor any u \u2208 \u03a3\u2217 and any language L', ' \u2286 \u03a3\u2217,l e tS u \ufb00L(u) denote the set of all\\npossible su\ufb03xes in L for u:\\nSu\ufb00L(u)= {v \u2208 \u03a3\u2217: uv \u2208 L}. (13.1)\\nSu\ufb00L(u) is also often denoted by u\u22121L.O b s e r v et h a ti fL is a reversible language13.4 Ide', 'nti\ufb01cation in the limit 305\\nL, then the following implication holds for any two strings u, u\u2032 \u2208 \u03a3\u2217:\\nSu\ufb00L(u) \u2229 Su\ufb00L(u\u2032) \u0338= \u2205 =\u21d2 Su\ufb00L(u)=S u \ufb00L(u\u2032). (13.2)\\nIndeed, let A be a reversible automaton accept', 'ing L.L e t q be the state of A\\nreached from the initial state when reading u and q\u2032 the one reached reading u\u2032.I f\\nv \u2208 Su\ufb00L(u) \u2229 Su\ufb00L(u\u2032), then v can be read both from q and q\u2032 to reach the \ufb01nal\\nstat', 'e. Since AR is deterministic, reading back the reverse of v from the \ufb01nal state\\nmust lead to a unique state, therefore q = q\u2032,t h a ti sS u \ufb00L(u)=S u \ufb00L(u\u2032).\\nLet A =( \u03a3 ,Q ,{i0}, {f0},E ) be a reversi', 'ble automaton accepting a reversible\\nlanguage L. We de\ufb01ne a set of strings SL as follows:\\nSL = {d[q]f[q]: q \u2208 Q}\u222a{ d[q],a ,f[q\u2032]: q,q \u2032 \u2208 Q, a \u2208 \u03a3} ,\\nwhere d[q] is a string of minimum length fromi0 to', ' q,a n df[q] a string of minimum\\nlength from q to f0. As shown by the following proposition, SL characterizes the\\nlanguage L in the sense that any reversible language containingSL must contain L.\\nProp', 'osition 13.1\\nLet L be a reversible language. Then,L is the smallest reversible language containing\\nSL.\\nProof Let L\u2032 be a reversible language containing SL and let x = x1 \u00b7\u00b7\u00b7 xn be\\nas t r i n ga c c e ', 'p t e db yL,w i t hxk \u2208 \u03a3f o r k \u2208 [1,n ]a n d n \u2265 1. For convenience,\\nwe also de\ufb01ne x0 as \u03f5.L e t( q0,x1,q1) \u00b7\u00b7\u00b7 (qn\u22121,xn,q n) be the accepting path in\\nA labeled with x. We show by recurrence that Su', '\ufb00 L\u2032 (x0 \u00b7\u00b7\u00b7 xk)=S u \ufb00 L\u2032 (d[qk])\\nfor all k \u2208 [0,n ]. Since d[q0]= d[i0]= \u03f5, this clearly holds for k =0 .N o w\\nassume that Su\ufb00 L\u2032 (x0 \u00b7\u00b7\u00b7 xk)=S u \ufb00 L\u2032 (d[qk]) for some k \u2208 [0,n \u2212 1]. This im-\\nplies i', 'mmediately that Su\ufb00 L\u2032 (x0 \u00b7\u00b7\u00b7 xkxk+1)=S u \ufb00 L\u2032 (d[qk]xk+1). By de\ufb01nition,\\nSL contains both d[qk+1]f[qk+1]a n d d[qk]xk+1f[qk+1]. Since L\u2032 includes SL,t h e\\nsame holds for L\u2032.T h u s ,f[qk+1] belongs ', 'to SuffL\u2032 (d[qk+1) \u2229 SuffL\u2032 (d[qk]xk+1).\\nIn view of (13.2), this implies that Su\ufb00 L\u2032 (d[qk]xk+1)=S u \ufb00L\u2032 (d[qk+1]). Thus, we\\nhave Su\ufb00L\u2032 (x0 \u00b7\u00b7\u00b7 xkxk+1)=S u \ufb00 L\u2032 (d[qk+1]). This shows that Su\ufb00 L\u2032 (x0 \u00b7', '\u00b7\u00b7 xk)=\\nSu\ufb00L\u2032 (d[qk]) holds for all k \u2208 [0,n ], in particular, for k = n. Note that since\\nqn = f0,w eh a v ef[qn]= \u03f5,t h e r e f o r ed[qn]= d[qn]f[qn]i si n S \u2286 L\u2032,w h i c h\\nimplies that Su\ufb00L\u2032 (d[qn]', ') contains \u03f5 and thus that Su\ufb00L\u2032 (x0 \u00b7\u00b7\u00b7 xk) contains \u03f5.T h i s\\nis equivalent to x = x0 \u00b7\u00b7\u00b7 xk \u2208 L\u2032.\\nFigure 13.6 shows the pseudocode of an algorithm for inferring a reversible\\nautomaton from a sample', ' S of m strings x1,...,x m. The algorithm starts by\\ncreating a pre\ufb01x-tree automaton A for S (line 1) and then iteratively de\ufb01nes a\\npartition \u03c0 of the states of A, starting with the trivial partition \u03c0', '0 with one block\\nper state (line 2). The automaton returned is the quotient ofA and the \ufb01nal partition306 Learning Automata and Languages\\nLearnReversibleAutomata(S =( x1,...,x m))\\n1 A =( \u03a3,Q ,{i0},F,E', ' ) \u2190 PT (S)\\n2 \u03c0 \u2190 \u03c00 \u22bf trivial partition.\\n3 list \u2190{ (f,f \u2032): f \u2032 \u2208 F } \u22bff arbitrarily chosen in F.\\n4 while list \u0338= \u2205do\\n5 Remove(list, (q1,q2))\\n6 if B(q1,\u03c0) \u0338= B(q2,\u03c0) then\\n7 B1 \u2190 B(q1,\u03c0)\\n8 B2 \u2190 B(q2,\u03c0', ')\\n9 for all a \u2208 \u03a3 do\\n10 if (succ(B1,a) \u0338= \u2205) \u2227 (succ(B2,a) \u0338= \u2205) then\\n11 Add(list,(succ(B1,a),s u c c(B2,a)))\\n12 if (pred(B1,a) \u0338= \u2205\u2227 (pred(B1,a) \u0338= \u2205) then\\n13 Add(list,(pred(B1,a),p re d(B2,a)))\\n14 U', 'pdate(succ, pred, B1,B2)\\n15 \u03c0 \u2190 Merge(\u03c0,B1,B2)\\n16 return A/\u03c0\\nFigure 13.6 Algorithm for learning reversible automata from a set of positive\\nstrings S.\\n\u03c0 de\ufb01ned.\\nThe algorithm maintains a list list of p', 'airs of states whose corresponding blocks\\nare to be merged, starting with all pairs of \ufb01nal states ( f,f \u2032) for an arbitrarily\\nchosen \ufb01nal state f \u2208 F (line 3). We denote byB(q,\u03c0) the block containing', 'q based\\non the partition \u03c0.\\nFor each block B and alphabet symbol a \u2208 \u03a3, the algorithm also maintains a\\nsuccessor succ(B,a ), that is, a state that can be reached by reading a from a state\\nof B; succ(B', ',a )= \u2205if no such state exists. It maintains similarly the predecessor\\npred(B,a ), which is a state that admits a transition labeled with a leading to a\\nstate in B; pred(B,a )= \u2205if no such state exist', 's.\\nThen, while list is not empty, a pair is removed from list and processed as\\nfollows. If the pair ( q1,q \u2032\\n1) has not been already merged, the pairs formed by the\\nsuccessors and predecessors of B1 =', ' B(q1,\u03c0)a n dB2 = B(q2,\u03c0) are added to list\\n(lines 10\u201313). Before merging blocks B1 and B2 i n t oan e wb l o c kB\u2032 that de\ufb01nes13.4 Identi\ufb01cation in the limit 307\\n0 1a\\n10\\nb\\n2a\\n5\\nb\\n3a 4a\\n6a\\n8\\nb\\n7b\\n9a\\n1', '1a\\n14\\nb\\n12b 13a\\n{5, 11}\\n{1, 3, 8, 12}\\nb\\n{6, 10}\\na\\n{0, 2, 4, 7, \\n9, 13, 14}\\na b\\nb\\na\\na\\nb\\n(a) (b)\\nFigure 13.7 Example of inference of a reversible automaton. (a) Pre\ufb01x-treePT (S)\\nrepresenting S =( \u03f5, aa,', ' bb, aaaa, abab, abba, baba). (b) Automaton bA returned by\\nLearnReversibleAutomata() for the input S. A double-direction arrow represents\\ntwo transitions with the same label with opposite directions. ', 'The language accepted\\nby\\nbA i st h a to fs t r i n g sw i t ha ne v e nn u m b e ro fasa n dbs.\\na new partition \u03c0 (line 15), the successor and predecessor values for the new block\\nB\u2032 are de\ufb01ned as fol', 'lows (line 14). For each symbol a \u2208 \u03a3, succ(B\u2032,a)= \u2205 if\\nsucc(B1,a)= succ(B2,a)= \u2205, otherwise succ(B\u2032,a)i ss e tt oo n eo fsucc(B1,a)i fi t\\nis non-empty,succ(B2,a) otherwise. The predecessor values are', ' de\ufb01ned in a similar\\nway. Figure 13.7 illustrates the application of the algorithm in the case of a sample\\nwith m =7s t r i n g s .\\nProposition 13.2\\nLet S be a \ufb01nite set of strings and let A = PT (S) ', 'be the pre\ufb01x-tree automaton de-\\n\ufb01ned from S. Then, the \ufb01nal partition de\ufb01ned by LearnReversibleAutomata()\\nused with input S is the \ufb01nest partition \u03c0 for which A/\u03c0 is reversible.\\nProof Let T be the num', 'ber of iterations of the algorithm for the input sampleS.\\nWe denote by\u03c0t the partition de\ufb01ned by the algorithm after t \u2265 1 iterations of the\\nloop, with \u03c0T the \ufb01nal partition.\\nA/\u03c0T is a reversible auto', 'maton since all \ufb01nal states are guaranteed to be merged\\ninto the same block as a consequence of the initialization step of line 3 and, for\\nany block B, by de\ufb01nition of the algorithm, states reachable ', 'by a \u2208 \u03a3f r o mB are\\ncontained in the same block, and similarly for those admitting a transition labeled\\nwith a to a state of B.\\nLet \u03c0\\n\u2032 be a partition of the states of A for which A/\u03c0\u2032 is reversible.', ' We show\\nby recurrence that \u03c0T re\ufb01nes \u03c0\u2032. Clearly, the trivial partition\u03c00 re\ufb01nes \u03c0\u2032.A s s u m e\\nthat \u03c0s re\ufb01nes \u03c0\u2032 for all s \u2264 t. \u03c0t+1 is obtained from \u03c0 by merging two blocks\\nB(q1,\u03c0t)a n d B(q2,\u03c0t). ', 'Since \u03c0t re\ufb01nes \u03c0\u2032,w em u s th a v eB(q1,\u03c0t) \u2286 B(q1,\u03c0 \u2032)\\nand B(q2,\u03c0t) \u2286 B(q2,\u03c0 \u2032). To show that \u03c0t+1 re\ufb01nes \u03c0\u2032,i ts u \ufb03 c e st op r o v et h a t308 Learning Automata and Languages\\nB(q1,\u03c0 \u2032)= B(q2,\u03c0 \u2032)', '.\\nA reversible automaton has only one \ufb01nal state, therefore, for the partition \u03c0\u2032,\\na l l\ufb01 n a ls t a t e so fA must be placed in the same block. Thus, if the pair ( q1,q2)\\nprocessed at the ( t + 1)th ', 'iteration is a pair of \ufb01nal states placed in list at the\\ninitialization step (line 3), then we must have B(q1,\u03c0 \u2032)= B(q2,\u03c0 \u2032). Otherwise,\\n(q1,q2) was placed in list as a pair of successor or predecess', 'or states of two states\\nq\u2032\\n1 and q\u2032\\n2 merged at a previous iteration s \u2264 t.S i n c e\u03c0s re\ufb01nes \u03c0\u2032, q\u2032\\n1 and q\u2032\\n2 are in\\nt h es a m eb l o c ko f\u03c0\u2032 and since A/\u03c0\u2032 is reversible, q1 and q2 must also be i', 'n the same\\nblock as successors or predecessors of the same block for the same labela \u2208 \u03a3, thus\\nB(q1,\u03c0 \u2032)= B(q2,\u03c0 \u2032).\\nTheorem 13.6\\nLet S be a \ufb01nite set of strings and let A be the automaton returned by', '\\nLearnReversibleAutomata() when used with inputS.T h e n ,L(A) is the small-\\nest reversible language containing S.\\nProof Let L be a reversible language containing S,a n dl e tA\u2032 be a reversible\\nautoma', 'ton with L(A\u2032)= L. Since every string of S is accepted by A\u2032,a n y\\nu \u2208 Pref(S) can be read from the initial state of A\u2032 to reach some state q(u)\\nof A\u2032. Consider the automaton A\u2032\u2032 derived from A\u2032 by ke', 'eping only states of the\\nform q(u) and transitions between such states. A\u2032\u2032 has the unique \ufb01nal state of A\u2032\\nsince q(u)i s\ufb01 n a lf o ru \u2208 S, and it has the initial state of A\u2032,s i n c e\u03f5 is a pre\ufb01x\\nof ', 'strings of S.F u r t h e r m o r e ,A\u2032\u2032 directly inherits from A\u2032 the property of being\\ndeterministic and reverse deterministic. Thus, A\u2032\u2032 is reversible.\\nThe states of A\u2032\u2032 de\ufb01ne a partition of Pref( S', '): u, v \u2208 Pref(S) are in the same\\nblock i\ufb00 q(u)= q(v). Since by de\ufb01nition of the pre\ufb01x-tree PT (S), its states\\ncan be identi\ufb01ed with Pref( S), the states of A\u2032\u2032 also de\ufb01ne a partition \u03c0\u2032 of the\\nstates', ' of PT (S)a n dt h u sA\u2032\u2032 = PT (S)/\u03c0\u2032. By proposition 13.2, the partition\\n\u03c0 de\ufb01ned by algorithm LearnReversibleAutomata() run with input S is the\\n\ufb01nest such that PT (S)/\u03c0 is reversible. Therefore, we ', 'must have L(PT (S)/\u03c0) \u2286\\nL(PT (S)/\u03c0\u2032)= L(A\u2032\u2032). Since A\u2032\u2032 is a sub-automaton of A\u2032, L contains L(A\u2032\u2032)a n d\\ntherefore L(PT (S)/\u03c0)= L(A), which concludes the proof.\\nFor the following theorem, a positive p', 'resentation of a language L is an in\ufb01nite\\nsequence (xn)n\u2208N such that {xn : n \u2208 N} = L. Thus, in particular, for any x \u2208 L\\nthere exists n \u2208 N such that x = xn. An algorithm identi\ufb01es L in the limit fro', 'm a\\npositive presentation if there exists N \u2208 N such that for n \u2265 N t h eh y p o t h e s i si t\\nreturns is L.\\nTheorem 13.7 Identi\ufb01cation in the limit of reversible languages\\nLet L be a reversible lang', 'uage, then algorithm LearnReversibleAutomata()\\nidenti\ufb01es L in the limit from a positive presentation.13.5 Chapter notes 309\\nProof Let L be a reversible language. By proposition 13.1, L admits a \ufb01nite\\n', 'characteristic sample SL.L e t( xn)n\u2208N be a positive presentation of L and let\\nXn denote the union of the \ufb01rst n elements of the sequence. Since SL is \ufb01nite,\\nthere exists N \u2265 1 such that SL \u2286 XN . By ', 'theorem 13.6, for any n \u2265 N,\\nLearnReversibleAutomata( )r u no nt h e\ufb01 n i t es a m p l eXn returns the smallest\\nreversible languageL\u2032 containing Xn a fortioriSL,w h i c h ,b yd e \ufb01 n i t i o no fSL,i ', 'm p l i e s\\nthat L\u2032 = L.\\nThe main operations needed for the implementation of the algorithm for learning\\nreversible automata are the standard find and union to determine the block a\\nstate belongs to a', 'nd to merge two blocks into a single one. Using a disjoint-set\\ndata structure for these operations, the time complexity of the algorithm can be\\nshown to be in O(n\u03b1(n)), where n denotes the sum of the ', 'lengths of all strings\\nin the input sample S and \u03b1(n) the inverse of the Ackermann function, which is\\nessentially constant (\u03b1(n) \u2264 4f o rn \u2264 1080).\\n13.5 Chapter notes\\nFor an overview of \ufb01nite automata', ' and some related recent results, see Hopcroft\\nand Ullman [1979] or the more recent Handbook chapter by Perrin [1990], as well\\nas the series of books by M. Lothaire [Lothaire, 1982, 1990, 2005].\\nTheor', 'em 13.1, stating that the problem of \ufb01nding a minimum consistent DFA is\\nNP-hard, is due to Gold [1978]. This result was later extended by Angluin [1978].\\nPitt and Warmuth [1993] further strengthened t', 'hese results by showing that even an\\napproximation within a polynomial function of the size of the smallest automaton\\nis NP-hard (theorem 13.2). Their hardness results apply also to the case where\\npre', 'diction is made using NFAs. Kearns and Valiant [1994] presented hardness results\\nof a di\ufb00erent nature relying on cryptographic assumptions. Their results imply that\\nno polynomial-time algorithm can le', 'arn consistent NFAs polynomial in the size of\\nthe smallest DFA from a \ufb01nite sample of accepted and rejected strings if any of\\nthe generally accepted cryptographic assumptions holds: if factoring Blum ', 'integers\\nis hard; or if the RSA public key cryptosystem is secure; or if deciding quadratic\\nresiduosity is hard.\\nOn the positive side, Trakhtenbrot and Barzdin [1973] showed that the smallest\\n\ufb01nite au', 'tomaton consistent with the input data can be learned exactly from a\\nuniform complete sample, whose size is exponential in the size of the automaton.\\nThe worst-case complexity of their algorithm is ex', 'ponential, but a better average-\\ncase complexity can be obtained assuming that the topology and the labeling are\\nselected randomly [Trakhtenbrot and Barzdin, 1973] or even that the topology is\\nselecte', 'd adversarially [Freund et al., 1993].310 Learning Automata and Languages\\nCortes, Kontorovich, and Mohri [2007a] study an approach to the problem of\\nlearning automata based on linear separation in som', 'e appropriate high-dimensional\\nfeature space; see also Kontorovich et al. [2006, 2008]. The mapping of strings to\\nthat feature space can be de\ufb01ned implicitly using the rational kernels presented in\\nch', 'apter 5, which are themselves de\ufb01ned via weighted automata and transducers.\\nThe model of learning with queries was introduced by Angluin [1978], who also\\nproved that \ufb01nite automata can be learned in t', 'ime polynomial in the size of\\nthe minimal automaton and that of the longest counter-example. Bergadano and\\nVarricchio [1995] further extended this result to the problem of learning weighted\\nautomata d', 'e\ufb01ned over any \ufb01eld. Using the relationship between the size of a minimal\\nweighted automaton over a \ufb01eld and the rank of the corresponding Hankel matrix,\\nthe learnability of many other concepts classe', 's such as disjoint DNF can be shown\\n[Beimel et al., 2000]. Our description of an e\ufb03cient implementation of the algorithm\\nof Angluin [1982] using decision trees is adapted from Kearns and Vazirani [199', '4].\\nThe model of identi\ufb01cation in the limit of automata was introduced and analyzed\\nby Gold [1967]. Deterministic \ufb01nite automata were shown not to be identi\ufb01able in\\nthe limit from positive examples [G', 'old, 1967]. But, positive results were given for\\nthe identi\ufb01cation in the limit of a number of sub-classes, such as the family of k-\\nreversible languages Angluin [1982] considered in this chapter. Pos', 'itive results also\\nhold for learning subsequential transducers Oncina et al. [1993]. Some restricted\\nclasses of probabilistic automata such as acyclic probabilistic automata were also\\nshown by Ron et ', 'al. [1995] to be e\ufb03ciently learnable.\\nThere is a vast literature dealing with the problem of learning automata. In\\nparticular, positive results have been shown for a variety of sub-families of \ufb01nite\\na', 'utomata in the scenario of learning with queries and learning scenarios of di\ufb00erent\\nkinds have been introduced and analyzed for this problem. The results presented in\\nthis chapter should therefore be ', 'viewed only as an introduction to that material.\\n13.6 Exercises\\n13.1 Minimal DFA. Show that a minimal DFA A also has the minimal number\\nof transitions among all other DFAs equivalent to A. Prove that ', 'a language L is\\nregular i\ufb00 Q = {Su\ufb00L(u): u \u2208 \u03a3\u2217} is \ufb01nite. Show that the number of states of a\\nminimal DFA A with L(A)= L is precisely the cardinality of Q.\\n13.2 VC-dimension of \ufb01nite automata.\\n(a) Wh', 'at is the VC-dimension of the family of all \ufb01nite automata? What does\\nthat imply for PAC-learning of \ufb01nite automata? Does this result change if we13.6 Exercises 311\\nrestrict ourselves to learning acyc', 'lic automata (automata with no cycles)?\\n(b) Show that the VC-dimension of the family of DFAs with at most n states\\nis bounded by O(|\u03a3|n log n).\\n13.3 PAC learning with membership queries. Give an examp', 'le of a concept classC\\nthat is e\ufb03ciently PAC-learnable with membership queries but that is not e\ufb03ciently\\nexactly learnable.\\n13.4 Learning monotone DNF formulae with queries. Show that the class of mon', 'o-\\ntone DNF formulae overn variables is e\ufb03ciently exactly learnable using membership\\nand equivalence queries. (Hint:a prime implicant t of a formula f is a product of\\nliterals such that t implies f bu', 't no proper sub-term of t implies f. Use the fact\\nthat for monotone DNF, the number of prime implicants is at the most the number\\nof terms of the formula.)\\n13.5 Learning with unreliable query response', 's. Consider the problem where the\\nlearner must \ufb01nd an integer x selected by the oracle within [1 ,n ], where n \u2265 1i s\\ngiven. To do so, the learner can ask questions of the form ( x \u2264 m?) or (x>m ?)\\nfo', 'r m \u2208 [1,n ] .T h eo r a c l er e s p o n d st ot h e s eq u e s t i o n sb u tm a yg i v ea ni n c o r r e c t\\nresponse to k questions. How many questions should the learner ask to determine\\nx?( Hint', ': observe that the learner can repeat each question 2 k +1t i m e sa n du s e\\nthe majority vote.)\\n13.6 Algorithm for learning reversible languages. What is the DFA A returned\\nby the algorithm for lear', 'ning reversible languages when applied to the sample\\nS = {ab, aaabb, aabbb, aabbbb}?S u p p o s ew ea d dan e ws t r i n gt ot h es a m p l e ,s a y\\nx = abab.H o ws h o u l dA be updated to compute th', 'e result of the algorithm for\\nS \u222a{x}? More generally, describe a method for updating the result of the algorithm\\nincrementally.\\n13.7 k-reversible languages. A \ufb01nite automaton A\\n\u2032 is said to be k-deter', 'ministic if\\nit is deterministic modulo a lookahead k: if two distinct states p and q are both\\ninitial, or are both reached from another state r by reading a \u2208 \u03a3, then no string\\nu of length k can be re', 'ad in A\u2032 both from p and q. A \ufb01nite automaton A is said to\\nbe k-reversible if it is deterministic and if AR is k-deterministic. A language L is\\nk-reversible if it is accepted by some k-reversible auto', 'maton.\\n(a) Prove that L is k-reversible i\ufb00 for any strings u, u\u2032,v \u2208 \u03a3\u2217 with |v| = k,\\nSu\ufb00L(uv) \u2229 Su\ufb00L(u\u2032v) \u0338= \u2205 =\u21d2 Su\ufb00L(uv)=S u \ufb00L(u\u2032v).312 Learning Automata and Languages\\n(b) Show that a k-reversible', ' language admits a characteristic language.\\n(c) Show that the following de\ufb01nes an algorithm for learning k-reversible\\nautomata. Proceed as in the algorithm for learning reversible automata but\\nwith th', 'e following merging rule instead: merge blocks B\\n1 and B2 if they can\\nbe reached by the same string u of length k from some other block and if B1\\nand B2 are both \ufb01nal or have a common successor.14 Rei', 'nforcement Learning\\nThis chapter presents an introduction to reinforcement learning, a rich area of\\nmachine learning with connections to control theory, optimization, and cognitive\\nsciences. Reinforce', 'ment learning is the study of planing and learning in a scenario\\nwhere a learner actively interacts with the environment to achieve a certain goal.\\nThis active interaction justi\ufb01es the terminology ofa', 'gent used to refer to the learner.\\nThe achievement of the agent\u2019s goal is typically measured by the reward he receives\\nfrom the environment and which he seeks to maximize.\\nWe \ufb01rst introduce the genera', 'l scenario of reinforcement learning and then intro-\\nduce the model of Markov decision processes (MDPs), which is widely adopted in\\nthis area, as well as essential concepts such as that ofpolicy or po', 'licy value related\\nto this model. The rest of the chapter presents several algorithms for the planning\\nproblem, which corresponds to the case where the environment model is known to\\nthe agent, and the', 'n a series of learning algorithms for the more general case of an\\nunknown model.\\n14.1 Learning scenario\\nThe general scenario of reinforcement learning is illustrated by \ufb01gure 14.1. Unlike\\nthe supervis', 'ed learning scenario considered in previous chapters, here, the learner\\ndoes not passively receive a labeled data set. Instead, he collects information\\nthrough a course of actions by interacting with ', 'the environment.I nr e s p o n s et o\\nan action, the learner or agent, receives two types of information: his current state\\nin the environment, and a real-valued reward, which is speci\ufb01c to the task a', 'nd its\\ncorresponding goal.\\nThere are several di\ufb00erences between the learning scenario of reinforcement\\nlearning and that of supervised learning examined in most of the previous chapters.\\nUnlike the su', 'pervised learning scenario, in reinforcement learning there is no \ufb01xed\\ndistribution according to which instances are drawn; the choice of a policy de\ufb01nes\\nt h ed i s t r i b u t i o n .I nf a c t ,s l ', 'i g h tc h a n g e st ot h ep o l i c ym a yh a v ed r a m a t i ce \ufb00 e c t so n\\nthe rewards received. Furthermore, in general, the environment may not be \ufb01xed314 Reinforcement Learning\\nEnvironment\\nAg', 'ent\\naction\\nstate\\nreward\\nFigure 14.1 Representation of the general scenario of reinforcement learning.\\nand could vary as a result of the actions selected by the agent. This may be a more\\nrealistic mode', 'l for some learning problems than the standard supervised learning.\\nThe objective of the agent is to maximize his reward and thus to determine\\nthe best course of actions, or policy, to achieve that ob', 'jective. However, the\\ninformation he receives from the environment is only the immediate reward related\\nto the action just taken. No future or long-term reward feedback is provided by\\nthe environment.', ' An important aspect of reinforcement learning is to take into\\nconsideration delayed rewards or penalties. The agent is faced with the dilemma\\nbetween exploring unknown states and actions to gain more', ' information about the\\nenvironment and the rewards, and exploiting the information already collected to\\noptimize his reward. This is known as the exploration versus exploitation trade-\\no\ufb00 inherent in ', 'reinforcement learning. Note that within this scenario, training and\\ntesting phases are intermixed.\\nTwo main settings can be distinguished here: the case where the environment\\nmodel is known to the ag', 'ent, in which case his objective of maximizing the reward\\nreceived is reduced to a planning problem, and the case where the environment\\nmodel is unknown, in which case he faces a learning problem. In ', 'the latter case,\\nthe agent must learn from the state and reward information gathered to both\\ngain information about the environment and determine the best action policy. This\\nchapter presents algorith', 'mic solutions for both of these settings.\\n14.2 Markov decision process model\\nWe \ufb01rst introduce the model of Markov decision processes (MDPs), a model of the\\nenvironment and interactions with the envir', 'onment widely adopted in reinforcement\\nlearning. An MDP is a Markovian process de\ufb01ned as follows.\\nDe\ufb01nition 14.1 MDPs\\nA Markov decision process (MDP) is de\ufb01ned by:14.3 Policy 315\\nas e to f states S, p', 'ossibly in\ufb01nite.\\na start state or initial state s0 \u2208 S.\\nas e to f actions A, possibly in\ufb01nite.\\na transition probability Pr[s\u2032|s, a]: distribution over destination statess\u2032 = \u03b4(s, a).\\na reward probabil', 'ity Pr[r\u2032|s, a]: distribution over rewards returnedr\u2032 = r(s, a).\\nThe model is Markovian because the transition and reward probabilities depend\\nonly on the current state s and not the entire history of', ' states and actions taken.\\nThis de\ufb01nition of MDP can be further generalized to the case of non-discrete state\\na n da c t i o ns e t s .\\nIn a discrete-time model, actions are taken at a set ofdecision ', 'epochs {0,...,T },\\nand this is the model we will adopt in what follows. This model can also be\\nstraightforwardly generalized to a continuous-time one where actions are taken at\\narbitrary points in tim', 'e.\\nWhen T is \ufb01nite, the MDP is said to have a \ufb01nite horizon . Independently of the\\n\ufb01niteness of the time horizon, an MDP is said to be \ufb01nite when both S and A are\\n\ufb01nite sets. Here, we are considering ', 'the general case where the reward r(s, a)a t\\nstate s when taking action a is a random variable. However, in many cases, the\\nreward is assumed to be a deterministic function of the pair of the state an', 'd action\\npair (s, a).\\nFigure 14.2 illustrates the model corresponding to an MDP. At time t \u2208 [0,T ]\\nthe state observed by the agent iss\\nt and he takes action at \u2208 A. The state reached\\nis st+1 (with pr', 'obability Pr[ st+1|at,st]) and the reward received rt+1 \u2208 R (with\\nprobability Pr[rt+1|at,st]).\\nMany real-world tasks can be represented by MDPs. Figure 14.3 gives the example\\nof a simple MDP for a rob', 'ot picking up balls on a tennis court.\\n14.3 Policy\\nThe main problem for an agent in an MDP environment is to determine the action\\nto take at each state, that is, an action policy.\\n14.3.1 De\ufb01nition\\nDe\ufb01', 'nition 14.2 Policy\\nA policy is a mapping \u03c0: S \u2192 A.\\nMore precisely, this is the de\ufb01nition of a stationary policy since the choice of the\\naction does not depend on the time. More generally, we could de\ufb01', 'ne anon-stationary316 Reinforcement Learning\\nst st+1 st+2\\nat/rt+1 at+1/rt+2\\nFigure 14.2 Illustration of the states and transitions of an MDP at di\ufb00erent times.\\npolicy as a sequence of mappings\u03c0t : S \u2192', ' A indexed by t. In particular, in the \ufb01nite\\nhorizon case, typically a non-stationary policy is necessary.\\nThe agent\u2019s objective is to \ufb01nd a policy that maximizes his expected (reward)\\nreturn. The ret', 'urn he receives following a policy\u03c0 along a speci\ufb01c sequence of states\\nst,...,s T is de\ufb01ned as follows:\\n\ufb01nite horizon ( T< \u221e ): \u2211T \u2212t\\n\u03c4=0 r(st+\u03c4,\u03c0(st+\u03c4)).\\nin\ufb01nite horizon ( T = \u221e ): \u2211T \u2212t\\n\u03c4=0 \u03b3\u03c4r(st+\u03c4', ',\u03c0(st+\u03c4)), where \u03b3 \u2208 [0, 1) is a constant\\nfactor less than one used to discount future rewards.\\nNote that the return is a single scalar summarizing a possibly in\ufb01nite sequence\\nof immediate rewards. In', ' the discounted case, early rewards are viewed as more\\nvaluable than later ones.\\nThis leads to the following de\ufb01nition of the value of a policy at each state.\\n14.3.2 Policy value\\nDe\ufb01nition 14.3 Policy', ' value\\nThe value V\\n\u03c0(s) of a policy \u03c0 at state s \u2208 S is de\ufb01ned as the expected reward\\nreturned when starting at s and following policy \u03c0:\\n\ufb01nite horizon: V\u03c0(s)=E\\n[\u2211T \u2212t\\n\u03c4=0 r(st+\u03c4,\u03c0(st+\u03c4))|st = s\\n]\\n;\\ni', 'n\ufb01nite discounted horizon: V\u03c0(s)=E\\n[\u2211T \u2212t\\n\u03c4=0 \u03b3\u03c4r(st+\u03c4,\u03c0(st+\u03c4))|st = s\\n]\\n;\\nwhere the expectations are over the random selection of the statesst and the reward\\nvalues rt+1. An in\ufb01nite undiscounted hori', 'zon is also often considered based on the\\nlimit of the average reward, when it exists.\\nAs we shall see later, there exists a policy that is optimal for any start state. In view\\nof the de\ufb01nition of the', ' policy values, seeking the optimal policy can be equivalently\\nformulated as determining a policy with maximum value at all states.\\n14.3.3 Policy evaluation\\nThe value of a policy at state s can be exp', 'ressed in terms of its values at other\\nstates, forming a system of linear equations.14.3 Policy 317\\nstart  search/[.1, R1]\\nother\\nsearch/[.9, R1]  carry/[.5, R3]\\ncarry/[.5, -1] pickup/[1, R2]\\nFigure 14', '.3 Example of a simple MDP for a robot picking up balls on a tennis\\ncourt. The set of actions is A = {search,carry,pickup } and the set of states reduced\\nto S = {start, other}. Each transition is labe', 'led with the action followed by the\\nprobability of the transition probability and the reward received after taking that\\naction. R1, R2,a n dR3 are real numbers indicating the reward associated to each', '\\ntransition (case of deterministic reward).\\nProposition 14.1 Bellman equation\\nThe values V\u03c0(s) of policy \u03c0 at states s \u2208 S for an in\ufb01nite horizon MDP obey the\\nfollowing system of linear equations:\\n\u2200s ', '\u2208 S, V \u03c0(s)=E [ r(s, \u03c0(s)] +\u03b3\\n\u2211\\ns\u2032\\nPr[s\u2032|s, \u03c0(s)]V\u03c0(s\u2032). (14.1)\\nProof We can decompose the expression of the policy value as a sum of the \ufb01rst\\nterm and the rest of the terms:\\nV\u03c0(s)=E\\n[T \u2212t\u2211\\n\u03c4=0\\n\u03b3\u03c4r(st', '+\u03c4,\u03c0(st+\u03c4)) | st = s\\n]\\n=E [r(s, \u03c0(s)] +\u03b3E\\n[T \u2212t\u2211\\n\u03c4=0\\n\u03b3\u03c4r(st+1+\u03c4,\u03c0(st+1+\u03c4)) | st = s\\n]\\n=E [r(s, \u03c0(s)] +\u03b3E[V\u03c0(\u03b4(s, \u03c0(s)))],\\nsince we can recognize the expression of V\u03c0(\u03b4(s, \u03c0(s))) in the expectation of ', 'the\\nsecond line.\\nThe Bellman equations can be rewritten as\\nV = R + \u03b3PV, (14.2)\\nusing the following notation: P denotes the transition probability matrix de\ufb01ned\\nby Ps,s\u2032 =P r [s\u2032|s, \u03c0(s)] for all s, s\u2032', ' \u2208 S; V is the value column matrix whose sth\\ncomponent is Vs = V\u03c0(s); and R the reward column matrix whose sth component\\nis Rs =E [r(s, \u03c0(s)]. V is typically the unknown variable in the Bellman equati', 'ons\\nand is determined by solving for it. The following theorem shows that for a \ufb01nite318 Reinforcement Learning\\nMDP this system of linear equations admits a unique solution.\\nTheorem 14.1\\nFor a \ufb01nite M', 'DP, Bellman\u2019s equation admits a unique solution given by\\nV0 =( I \u2212 \u03b3P)\u22121R. (14.3)\\nProof The Bellman equation (14.2) can be equivalently written as\\n(I \u2212 \u03b3P)V = R.\\nThus, to prove the theorem it su\ufb03ces t', 'o show that (I \u2212 \u03b3P) is invertible. To do so,\\nnote that the norm in\ufb01nity of P can be computed using its stochasticity properties:\\n\u2225P\u2225\u221e =m a x\\ns\\n\u2211\\ns\u2032\\n|Pss\u2032 | =m a x\\ns\\n\u2211\\ns\u2032\\nPr[s\u2032|s, \u03c0(s)] = 1.\\nThis impl', 'ies that \u2225\u03b3P\u2225\u221e = \u03b3< 1. The eigenvalues of P are thus all less than one,\\nand (I \u2212 \u03b3P) is invertible.\\nThus, for a \ufb01nite MDP, when the transition probability matrix P and the reward\\nexpectations R are kn', 'own, the value of policy \u03c0 at all states can be determined by\\ninverting a matrix.\\n14.3.4 Optimal policy\\nThe objective of the agent can be reformulated as that of seeking the optimal policy\\nde\ufb01ned as f', 'ollows.\\nDe\ufb01nition 14.4 Optimal policy\\nAp o l i c y\u03c0\\n\u2217 is optimal if it has maximal value for all states s \u2208 S.\\nThus, by de\ufb01nition, for any s \u2208 S, V\u03c0\u2217(s)=m a x\u03c0 V\u03c0(s). We will use the shorter\\nnotation ', 'V \u2217 instead of V\u03c0\u2217. V \u2217(s) is the maximal cumulative reward the agent can\\nexpect to receive when starting at state s.\\nDe\ufb01nition 14.5 State-action value function\\nThe optimal state-action value function', ' Q\u2217 is de\ufb01ned for all (s, a) \u2208 S \u00d7 A as the\\nexpected return for taking actiona \u2208 A at state s \u2208 S and then following the optimal\\npolicy:\\nQ\u2217(s, a)=E [ r(s, a)] +\u03b3\\n\u2211\\ns\u2032 \u2208S\\nPr[s\u2032 | s, a]V \u2217(s\u2032). (14.4)14', '.4 Planning algorithms 319\\nIt is not hard to see then that the optimal policy values are related to Q\u2217 via\\n\u2200s \u2208 S, V \u2217(s)=m a x\\na\u2208A\\nQ\u2217(s, a). (14.5)\\nIndeed, by de\ufb01nition, V \u2217(s) \u2264 maxa\u2208A Q\u2217(s, a) for ', 'all s \u2208 S.I ff o rs o m es we had\\nV \u2217(s) < maxa\u2208A Q\u2217(s, a), then then maximizing action would de\ufb01ne a better policy.\\nObserve also that, by de\ufb01nition of the optimal policy, we have\\n\u2200s \u2208 S, \u03c0 \u2217(s) = arg', 'max\\na\u2208A\\nQ\u2217(s, a). (14.6)\\nThus, the knowledge of the state-value function Q\u2217 is su\ufb03cient for the agent\\nto determine the optimal policy, without any direct knowledge of the reward or\\ntransition probabil', 'ities. Replacing Q\u2217 by its de\ufb01nition in (14.5) gives the following\\nsystem of equations for the optimal policy values V \u2217(s):\\nV \u2217(s)=m a x\\na\u2208A\\n{\\nE[r(s, a)] +\u03b3\\n\u2211\\ns\u2032 \u2208S\\nPr[s\u2032|s, a]V \u2217(s\u2032)\\n}\\n, (14.7)\\nalso', ' known as Bellman equations. Note that this new system of equations is not\\nlinear due to the presence of the max operator. It is distinct from the previous linear\\nsystem we de\ufb01ned under the same name ', 'in (14.1) and (14.2).\\n14.4 Planning algorithms\\nIn this section, we assume that the environment model is known. That is, the\\ntransition probability Pr[s\\n\u2032|s, a]a n dt h ee x p e c t e dr e w a r dE [r(', 's, a)] for all s, s\u2032 \u2208 S\\nand a \u2208 A are assumed to be given. The problem of \ufb01nding the optimal policy then\\ndoes not require learning the parameters of the environment model or estimating\\nother quantiti', 'es helpful in determining the best course of actions, it is purely a\\nplanning problem.\\nThis section discusses three algorithms for this planning problem: the value\\niteration algorithm, the policy iter', 'ation algorithm, and a linear programming\\nf o r m u l a t i o no ft h ep r o b l e m .\\n14.4.1 Value iteration\\nThe value iteration algorithm seeks to determine the optimal policy values V\\n\u2217(s)\\nat each ', 'state s \u2208 S, and thereby the optimal policy. The algorithm is based on\\nthe Bellman equations (14.7). As already indicated, these equations do not form\\na system of linear equations and require a di\ufb00ere', 'nt technique to determine the\\nsolution. The main idea behind the design of the algorithm is to use an iterative320 Reinforcement Learning\\nV alueIteration(V0)\\n1 V \u2190 V0 \u22bf V0 arbitrary value\\n2 while \u2225V \u2212', ' \u03a6(V)\u2225\u2265 (1\u2212\u03b3)\u03f5\\n\u03b3 do\\n3 V \u2190 \u03a6(V)\\n4 return \u03a6(V)\\nFigure 14.4 Value iteration algorithm.\\nmethod to solve them: the new values of V (s) are determined using the Bellman\\ne q u a t i o n sa n dt h ec u r r e ', 'n tv a l u e s .T h i sp r o c e s si sr e p e a t e du n t i lac o n v e r g e n c e\\ncondition is met.\\nFor a vector V in R\\n|S|,w ed e n o t eb yV (s)i t ssth coordinate, for any s \u2208 S.L e t\\n\u03a6 : R|S| ', '\u2192 R|S| be the mapping de\ufb01ned based on Bellman\u2019s equations (14.7):\\n\u2200s \u2208 S, [\u03a6(V)](s)=m a x\\na\u2208A\\n{\\nE[r(s, a)] +\u03b3\\n\u2211\\ns\u2032\u2208S\\nPr[s\u2032|s, a]V (s\u2032)\\n}\\n. (14.8)\\nThe maximizing actions a \u2208 A in these equations de\ufb01ne ', 'an action to take at each\\nstate s \u2208 S, that is a policy \u03c0. We can thus rewrite these equations in matrix terms\\nas follows:\\n\u03a6(V)=m a x\\n\u03c0\\n{R\u03c0 + \u03b3P\u03c0V}, (14.9)\\nwhere P\u03c0 is the transition probability matri', 'x de\ufb01ned by ( P\u03c0)ss\u2032 =P r [s\u2032|s, \u03c0(s)]\\nfor all s, s\u2032 \u2208 S,a n dR\u03c0 the reward vector de\ufb01ned by ( R\u03c0)s =E [r(s, \u03c0(s)], for all\\ns \u2208 S.\\nThe algorithm is directly based on (14.9). The pseudocode is given ab', 'ove. Starting\\nfrom an arbitrary policy value vector V0 \u2208 R|S|, the algorithm iteratively applies\\n\u03a6 to the current V to obtain a new policy value vector until \u2225V \u2212 \u03a6(V)\u2225 <\\n(1\u2212\u03b3)\u03f5\\n\u03b3 ,w h e r e\u03f5> 0 is a ', 'desired approximation. The following theorem proves the\\nconvergence of the algorithm to the optimal policy values.\\nTheorem 14.2\\nFor any initial value V0, the sequence de\ufb01ned by Vn+1 = \u03a6(Vn) converges ', 'to V\u2217.\\nProof We \ufb01rst show that \u03a6 is \u03b3-Lipschitz for the \u2225\u00b7\u2225 \u221e .1 For any s \u2208 S and\\n1. A \u03b2-Lipschitz function with \u03b2< 1i sa l s oc a l l e d\u03b2-contracting.I na complete metric\\nspace, that is a metric sp', 'ace where any Cauchy sequence converges to a point of that14.4 Planning algorithms 321\\nV \u2208 R|S|,l e ta\u2217(s) be the maximizing action de\ufb01ning \u03a6(V)(s) in (14.8). Then, for\\nany s \u2208 S and any U \u2208 R|S|,\\n\u03a6(V', ')(s) \u2212 \u03a6(U)(s) \u2264 \u03a6(V)(s) \u2212\\n(\\nE[r(s, a\u2217(s))] +\u03b3\\n\u2211\\ns\u2032 \u2208S\\nPr[s\u2032 | s, a\u2217(s)]U(s\u2032)\\n\u23a1\\n= \u03b3\\n\u2211\\ns\u2032\u2208S\\nPr[s\u2032|s, a\u2217(s)][V(s\u2032) \u2212 U(s\u2032)]\\n\u2264 \u03b3\\n\u2211\\ns\u2032\u2208S\\nPr[s\u2032|s, a\u2217(s)]\u2225V \u2212 U\u2225\u221e = \u03b3\u2225V \u2212 U\u2225\u221e .\\nProceeding similarly with \u03a6(U', ')(s) \u2212 \u03a6(V)(s), we obtain \u03a6(U)(s) \u2212 \u03a6(V)(s) \u2264\\n\u03b3\u2225V \u2212 U\u2225\u221e .T h u s ,|\u03a6(V)(s) \u2212 \u03a6(U)(s)|\u2264 \u03b3\u2225V \u2212 U\u2225\u221e for all s, which implies\\n\u2225\u03a6(V) \u2212 \u03a6(U)\u2225\u221e \u2264 \u03b3\u2225V \u2212 U\u2225\u221e ,\\nthat is the \u03b3-Lipschitz property of \u03a6. Now, by Be', 'llman equations (14.7), V\u2217 =\\n\u03a6(V\u2217), thus for any n \u2208 N,\\n\u2225V\u2217 \u2212 Vn+1\u2225\u221e = \u2225\u03a6(V\u2217) \u2212 \u03a6(Vn)\u2225\u221e \u2264 \u03b3\u2225V\u2217 \u2212 Vn\u2225\u221e \u2264 \u03b3n+1\u2225V\u2217 \u2212 V0\u2225\u221e ,\\nwhich proves the convergence of the sequence to V\u2217 since \u03b3 \u2208 (0, 1).\\nThe \u03f5-opti', 'mality of the value returned by the algorithm can be shown as follows.\\nBy the triangle inequality and the \u03b3-Lipschitz property of \u03a6, for any n \u2208 N,\\n\u2225V\u2217 \u2212 Vn+1\u2225\u221e \u2264\u2225 V\u2217 \u2212 \u03a6(Vn+1)\u2225\u221e + \u2225\u03a6(Vn+1) \u2212 Vn+1\u2225\u221e\\n=', ' \u2225\u03a6(V\u2217) \u2212 \u03a6(Vn+1)\u2225\u221e + \u2225\u03a6(Vn+1) \u2212 \u03a6(Vn)\u2225\u221e\\n\u2264 \u03b3\u2225V\u2217 \u2212 Vn+1\u2225\u221e + \u03b3\u2225Vn+1 \u2212 Vn\u2225\u221e .\\nThus, if Vn+1 is the policy value returned by the algorithm, we have\\n\u2225V\u2217 \u2212 Vn+1\u2225\u221e \u2264 \u03b3\\n1 \u2212 \u03b3\u2225Vn+1 \u2212 Vn\u2225\u221e \u2264 \u03f5.\\nThe convergence ', 'of the algorithm is inO(log 1\\n\u03f5 ) number of iterations. Indeed, observe\\nthat\\n\u2225Vn+1 \u2212Vn\u2225\u221e = \u2225\u03a6(Vn)\u2212\u03a6(Vn\u22121)\u2225\u221e \u2264 \u03b3\u2225Vn \u2212Vn\u22121\u2225\u221e \u2264 \u03b3n\u2225\u03a6(V0)\u2212V0\u2225\u221e .\\nThus, if n is the largest integer such that (1\u2212\u03b3)\u03f5\\n\u03b3 \u2264\u2225 Vn+', '1 \u2212 Vn\u2225\u221e ,i tm u s tv e r i f y\\nspace, a \u03b2-contracting function f admits a \ufb01xed point:a n ys e q u e n c e(f(xn))n\u2208N converges\\nto some x with f(x)= x. RN , N \u2265 1, or, more generally, any \ufb01nite-dimensi', 'onal vector\\nspace, is a complete metric space.322 Reinforcement Learning\\n1\\na/[3/4, 2]\\n2\\na/[1/4, 2]\\nb/[1, 2]\\nd/[1, 3]\\nc/[1, 2]\\nFigure 14.5 Example of MDP with two states. The state set is reduced to\\nS ', '= {1, 2} and the action set to A = {a, b, c, d}. Only transitions with non-zero\\nprobabilities are represented. Each transition is labeled with the action taken\\nfollowed by a pair [p, r] after a slash ', 'separator, where p is the probability of the\\ntransition and r the expected reward for taking that transition.\\n(1\u2212\u03b3)\u03f5\\n\u03b3 \u2264 \u03b3n\u2225\u03a6(V0) \u2212 V0\u2225\u221e and therefore n \u2264 O\\n(\\nlog 1\\n\u03f5\\n\u23a1\\n.2\\nFigure 14.5 shows a simple e', 'xample of MDP with two states. The iterated values\\nof these states calculated by the algorithm for that MDP are given by\\nVn+1(1) = max\\n{\\n2+ \u03b3\\n(3\\n4Vn(1) + 1\\n4Vn(2)\\n\u23a1\\n, 2+ \u03b3Vn(2)\\n}\\nVn+1(2) = max\\n{\\n3+ \u03b3V', 'n(1),2+ \u03b3Vn(2)\\n}\\n.\\nFor V0(1) = \u22121, V0(2) = 1, and \u03b3 =1 /2, we obtain V1(1) = V1(2) = 5 /2.\\nThus, both states seem to have the same policy value initially. However, by the \ufb01fth\\niteration, V5(1) = 4.531', '25, V5(2) = 5.15625 and the algorithm quickly converges\\nto the optimal values V\u2217(1) = 14/3a n dV\u2217(2) = 16/3 showing that state 2 has a\\nhigher optimal value.\\n14.4.2 Policy iteration\\nAn alternative algo', 'rithm for determining the best policy consists of using policy\\nevaluations, which can be achieved via a matrix inversion, as shown by theorem 14.1.\\nThe pseudocode of the algorithm known as policy iter', 'ation algorithm is given in\\n\ufb01gure 14.6. Starting with an arbitrary action policy \u03c00, the algorithm repeatedly\\ncomputes the value of the current policy \u03c0 via that matrix inversion and greedily\\nselects ', 'the new policy as the one maximizing the right-hand side of the Bellman\\nequations (14.9).\\nThe following theorem proves the convergence of the policy iteration algorithm.\\nTheorem 14.3\\n2. Here, the O-no', 'tation hides the dependency on the discount factor \u03b3. As a function of\\n\u03b3, the running time is not polynomial.14.4 Planning algorithms 323\\nPolicyIteration(\u03c00)\\n1 \u03c0 \u2190 \u03c00 \u22bf\u03c00 arbitrary policy\\n2 \u03c0\u2032 \u2190 nil\\n3', ' while (\u03c0 \u0338= \u03c0\u2032) do\\n4 V \u2190 V\u03c0 \u22bf policy evaluation: solve (I \u2212 \u03b3P\u03c0)V = R\u03c0.\\n5 \u03c0\u2032 \u2190 \u03c0\\n6 \u03c0 \u2190 argmax\u03c0 {R\u03c0 + \u03b3P\u03c0V} \u22bf greedy policy improvement.\\n7 return \u03c0\\nFigure 14.6 Policy iteration algorithm.\\nLet (Vn)n\u2208N ', 'be the sequence of policy values computed by the algorithm, then, for\\nany n \u2208 N, the following inequalities hold:\\nVn \u2264 Vn+1 \u2264 V\u2217. (14.10)\\nProof Let \u03c0n+1 be the policy improvement at the nth iteration ', 'of the algorithm.\\nWe \ufb01rst show that ( I \u2212 \u03b3P\u03c0n+1 )\u22121 preserves ordering, that is, for any column\\nmatrices X and Y in R|S|,i f( Y \u2212 X) \u2265 0,t h e n(I \u2212 \u03b3P\u03c0n+1 )\u22121(Y \u2212 X) \u2265 0.\\nAs shown in the proof of th', 'eorem 14.1, \u2225\u03b3P\u2225\u221e = \u03b3< 1. Since the radius of\\nconvergence of the power series (1\u2212 x)\u22121 is one, we can use its expansion and write\\n(I \u2212 \u03b3P\u03c0n+1 )\u22121 =\\n\u221e\u2211\\nk=0\\n(\u03b3P\u03c0n+1 )k.\\nThus, if Z =( Y \u2212 X) \u2265 0,t h e n(', 'I \u2212 \u03b3P\u03c0n+1 )\u22121Z = \u2211\u221e\\nk=0(\u03b3P\u03c0n+1 )kZ \u2265 0,s i n c e\\nthe entries of matrix P\u03c0n+1 and its powers are all non-negative as well as those of\\nZ.\\nNow, by de\ufb01nition of \u03c0n+1,w eh a v e\\nR\u03c0n+1 + \u03b3P\u03c0n+1 Vn \u2265 R\u03c0n + ', '\u03b3P\u03c0n Vn = Vn,\\nwhich shows thatR\u03c0n+1 \u2265 (I\u2212\u03b3P\u03c0n+1 )Vn.S i n c e(I\u2212\u03b3P\u03c0n+1 )\u22121 preserves ordering,\\nthis implies that Vn+1 =( I \u2212 \u03b3P\u03c0n+1 )\u22121R\u03c0n+1 \u2265 Vn, which concludes the proof\\nof the theorem.\\nNote that t', 'wo consecutive policy values can be equal only at the last iteration of\\nthe algorithm. The total number of possible policies is |A||S|,t h u st h i sc o n s t i t u t e s\\na straightforward upper bound', ' on the maximal number of iterations. Better upper324 Reinforcement Learning\\nbounds of the form O\\n(|A||S|\\n|S|\\n\u23a1\\nare known for this algorithm.\\nFor the simple MDP shown by \ufb01gure 14.5, let the initial po', 'licy \u03c00 be de\ufb01ned by\\n\u03c00(1) = b, \u03c00(2) = c. Then, the system of linear equations for evaluating this policy\\nis\\n{\\nV\u03c00 (1) = 1 +\u03b3V\u03c00 (2)\\nV\u03c00 (2) = 2 +\u03b3V\u03c00 (2),\\nwhich gives V\u03c00 (1) = 1+\u03b3\\n1\u2212\u03b3 and V\u03c00 (2) =', ' 2\\n1\u2212\u03b3 .\\nTheorem 14.4\\nLet (Un)n\u2208N be the sequence of policy values generated by the value iteration\\nalgorithm, and (Vn)n\u2208N the one generated by the policy iteration algorithm. If\\nU0 = V0,t h e n ,\\n\u2200n ', '\u2208 N, Un \u2264 Vn \u2264 V\u2217. (14.11)\\nProof We \ufb01rst show that the function \u03a6 previously introduced is monotonic. Let\\nU and V be such thatU \u2264 V and let\u03c0 be the policy such that\u03a6(U)= R\u03c0+\u03b3P\u03c0U.\\nThen,\\n\u03a6(U) \u2264 R\u03c0 + \u03b3P\u03c0', 'V \u2264 max\\n\u03c0\u2032\\n{R\u03c0\u2032 + \u03b3P\u03c0\u2032 V} = \u03a6(V).\\nThe proof is by induction on n. Assume that Un \u2264 Vn,t h e nb yt h em o n o t o n i c i t y\\nof \u03a6,w eh a v e\\nUn+1 = \u03a6(Un) \u2264 \u03a6(Vn)=m a x\\n\u03c0\\n{R\u03c0 + \u03b3P\u03c0Vn}.\\nLet \u03c0n+1 be the ', 'maximizing policy, that is,\u03c0n+1 =a r g m a x\u03c0 {R\u03c0 +\u03b3P\u03c0Vn}. Then,\\n\u03a6(Vn)= R\u03c0n+1 + \u03b3P\u03c0n+1 Vn \u2264 R\u03c0n+1 + \u03b3P\u03c0n+1 Vn+1 = Vn+1,\\nand thus Un+1 \u2264 Vn+1.\\nThe theorem shows that the policy iteration algorithm conv', 'erges in a smaller\\nnumber of iterations than the value iteration algorithm due to the optimal policy.\\nBut, each iteration of the policy iteration algorithm requires computing a policy\\nvalue, that is, ', 'solving a system of linear equations, which is more expensive to\\ncompute that an iteration of the value iteration algorithm.\\n14.4.3 Linear programming\\nAn alternative formulation of the optimization pr', 'oblem de\ufb01ned by the Bellman\\nequations (14.7) is via linear programming (LP), that is an optimization prob-14.5 Learning algorithms 325\\nlem with a linear objective function and linear constraints. LPs ', 'admit (weakly)\\npolynomial-time algorithmic solutions. There exist a variety of di\ufb00erent methods\\nfor solving relative large LPs in practice, using the simplex method, interior-point\\nmethods, or a varie', 'ty of special-purpose solutions. All of these methods could be\\napplied in this context.\\nBy de\ufb01nition, the equations (14.7) are each based on a maximization. These\\nmaximizations are equivalent to seeki', 'ng to minimize all elements of {V (s): s \u2208 S}\\nunder the constraints V (s) \u2265 E[r(s, a)] + \u03b3\u2211\\ns\u2032\u2208S Pr[s\u2032|s, a]V (s\u2032), (s \u2208 S). Thus,\\nthis can be written as the following LP for any set of \ufb01xed positive ', 'weights\u03b1(s) > 0,\\n(s \u2208 S):\\nmin\\nV\\n\u2211\\ns\u2208S\\n\u03b1(s)V (s) (14.12)\\nsubject to \u2200s \u2208 S, \u2200a \u2208 A, V(s) \u2265 E[r(s, a)] +\u03b3\\n\u2211\\ns\u2032 \u2208S\\nPr[s\u2032|s, a]V (s\u2032),\\nwhere \u03b1 > 0 is the vector with the sth component equal to \u03b1(s).3 To m', 'ake each\\ncoe\ufb03cient \u03b1(s) interpretable as a probability, we can further add the constraints that\u2211\\ns\u2208S \u03b1(s)=1 .T h en u m b e ro fr o w so ft h i sL Pi s|S||A| and its number of columns\\n|S|. The complex', 'ity of the solution techniques for LPs is typically more favorable in\\nterms of the number of rows than the number of columns. This motivates a solution\\nbased on the equivalent dual formulation of this', ' LP which can be written as\\nmax\\nx\\n\u2211\\ns\u2208S,a\u2208A\\nE[r(s, a)]x(s, a) (14.13)\\nsubject to \u2200s \u2208 S,\\n\u2211\\na\u2208A\\nx(s\u2032,a)= \u03b1(s\u2032)+ \u03b3\\n\u2211\\ns\u2208S,a\u2208A\\nPr[s\u2032|s, a] x(s\u2032,a)\\n\u2200s \u2208 S, \u2200a \u2208 A, x(s, a) \u2265 0,\\nand for which the number of ', 'rows is only |S| and the number of columns |S||A|.\\nHere x(s, a) can be interpreted as the probability of being in state s and taking\\naction a.\\n14.5 Learning algorithms\\nThis section considers the more ', 'general scenario where the environment model of\\nan MDP, that is the transition and reward probabilities , is unknown. This matches\\n3. Let us emphasize that the LP is only in terms of the variables V (', 's), as indicated by\\nthe subscript of the minimization operator, and not in terms of V (s)a n d\u03b1(s).326 Reinforcement Learning\\nmany realistic applications of reinforcement learning where, for example, ', 'a robot is\\nplaced in an environment that it needs to explore in order to reach a speci\ufb01c goal.\\nHow can an agent determine the best policy in this context? Since the environment\\nmodels are not known, h', 'e may seek to learn them by estimating transition or reward\\nprobabilities. To do so, as in the standard case of supervised learning, the agent\\nneeds some amount of training information. In the context', ' of reinforcement learning\\nwith MDPs, the training information is the sequence of immediate rewards the agent\\nreceives based on the actions he has taken.\\nThere are two main learning approaches that ca', 'n be adopted. One known as the\\nmodel-free approach consists of learning an action policy directly. Another one, a\\nmodel-based approach, consists of \ufb01rst learning the environment model, and then\\nuse th', 'at to learn a policy. The Q-learning algorithm we present for this problem is\\nwidely adopted in reinforcement learning and belongs to the family of model-free\\napproaches.\\nThe estimation and algorithmi', 'c methods adopted for learning in reinforcement\\nlearning are closely related to the concepts and techniques in stochastic approxi-\\nmation. Thus, we start by introducing several useful results of this ', '\ufb01eld that will\\nbe needed for the proofs of convergence of the reinforcement learning algorithms\\npresented.\\n14.5.1 Stochastic approximation\\nStochastic approximation methods are iterative algorithms for', ' solving optimization\\nproblems whose objective function is de\ufb01ned as the expectation of some random\\nvariable, or to \ufb01nd the \ufb01xed point of a function H that is accessible only through\\nnoisy observation', 's. These are precisely the type of optimization problems found in\\nreinforcement learning. For example, for the Q-learning algorithm we will describe,\\nthe optimal state-action value function Q\\n\u2217 is the', ' \ufb01xed point of some function H\\nthat is de\ufb01ned as an expectation and thus not directly accessible.\\nWe start with a basic result whose proof and related algorithm show the \ufb02avor\\nof more complex ones fou', 'nd in stochastic approximation. The theorem is a gener-\\nalization of a result known as the strong law of large numbers. It shows that under\\nsome conditions on the coe\ufb03cients, an iterative sequence of ', 'estimates\u03bcm converges\\nalmost surely (a.s.) to the mean of a bounded random variable.\\nTheorem 14.5 Mean estimation\\nLet X be a random variable taking values in[0, 1] and let x0,...,x m be i.i.d. values\\n', 'of X. De\ufb01ne the sequence (\u03bcm)m\u2208N by\\n\u03bcm+1 =( 1 \u2212 \u03b1m)\u03bcm + \u03b1mxm, (14.14)14.5 Learning algorithms 327\\nwith \u03bc0 = x0, \u03b1m \u2208 [0,1], \u2211\\nm\u22650 \u03b1m =+ \u221e and \u2211\\nm\u22650 \u03b12\\nm < +\u221e .T h e n ,\\n\u03bcm\\na.s\\n\u2212\u2212\u2192 E[X]. (14.15)\\nProof ', 'We give the proof of the L2 convergence. The a.s. convergence is shown\\nlater for a more general theorem. By the independence assumption, for m \u2265 0,\\nVar[\u03bcm+1]=( 1 \u2212 \u03b1m)2 Var[\u03bcm]+ \u03b12\\nm Var[xm] \u2264 (1 \u2212 \u03b1m', ') Var[\u03bcm]+ \u03b12\\nm. (14.16)\\nLet \u03f5> 0 and suppose that there existsN \u2208 N such that for allm \u2265 N, Var[\u03bcm] \u2265 \u03f5.\\nThen, for m \u2265 N,\\nVar[\u03bcm+1] \u2264 Var[\u03bcm] \u2212 \u03b1m Var[\u03bcm]+ \u03b12\\nm \u2264 Var[\u03bcm] \u2212 \u03b1m\u03f5 + \u03b12\\nm,\\nwhich implies,', ' by reapplying this inequality, that\\nVar[\u03bcm+N ] \u2264 Var[\u03bcN ] \u2212 \u03f5\\nm+N\u2211\\nn=N\\n\u03b1n +\\nm+N\u2211\\nn=N\\n\u03b12\\nn\\n\\ued19 \\ued18\\ued17 \\ued1a\\n\u2192\u2212\u221e when m\u2192\u221e\\n,\\ncontradicting Var[\u03bcm+N ] \u2265 0. Thus, this contradicts the existence of such an integer\\nN', '. Therefore, for all N \u2208 N,t h e r ee x i s t sm0 \u2265 N such that Var[\u03bcm0 ] \u2264 \u03f5.\\nChoose N large enough so that for all m \u2265 N, the inequality \u03b1m \u2264 \u03f5 holds. This\\nis possible since the sequence (\u03b12\\nm)m\u2208N a', 'nd thus (\u03b1m)m\u2208N converges to zero in view\\nof \u2211\\nm\u22650 \u03b12\\nm < +\u221e . We will show by induction that for anym \u2265 m0, Var[\u03bcm] \u2264 \u03f5,\\nwhich implies the statement of the theorem.\\nAssume that Var[ \u03bcm] \u2264 \u03f5 for some ', 'm \u2265 m0. Then, using this assumption,\\ninequality 14.16, and the fact that \u03b1m \u2264 \u03f5, the following inequality holds:\\nVar[\u03bcm+1] \u2264 (1 \u2212 \u03b1m)\u03f5 + \u03f5\u03b1m = \u03f5.\\nThus, this proves that limm\u2192 +\u221e Var[\u03bcm] = 0, that is t', 'he L2 convergence of \u03bcm to\\nE[X].\\nNote that the hypotheses of the theorem related to the sequence (\u03b1m)m\u2208N hold in\\nparticular when \u03b1m = 1\\nm . The special case of the theorem with this choice of \u03b1m\\ncoinc', 'ides with the strong law of large numbers. This result has tight connections\\nwith the general problem of stochastic optimization.\\nStochastic optimization is the general problem of \ufb01nding the solution ', 'to the\\nequation\\nx = H(x),\\nwhere x \u2208 RN ,w h e n328 Reinforcement Learning\\nH(x) cannot be computed, for example, because H is not accessible or because\\nthe cost of its computation is prohibitive;\\nbut a', 'n i.i.d. sample of m noisy observations H(xi)+ wi are available, i \u2208 [1,m],\\nwhere the noise random variable w has expectation zero: E[w]= 0.\\nThis problem arises in a variety of di\ufb00erent contexts and a', 'pplications. As we shall\\nsee, it is directly related to the learning problem for MDPs.\\nOne general idea for solving this problem is to use an iterative method and de\ufb01ne\\na sequence (xt)t\u2208N in a way sim', 'ilar to what is suggested by theorem 14.5:\\nxt+1 =( 1 \u2212 \u03b1t)xt + \u03b1t[H(xt)+ wt] (14.17)\\n= xt + \u03b1t[H(xt)+ wt \u2212 xt], (14.18)\\nwhere (\u03b1t)t\u2208N follow conditions similar to those assumed in theorem 14.5. More\\ng', 'enerally, we consider sequences de\ufb01ned via\\nxt+1 = xt + \u03b1tD(xt,wt), (14.19)\\nwhere D is a function mapping RN \u00d7 RN to RN . There are many di\ufb00erent theorems\\nguaranteeing the convergence of this sequence ', 'under various assumptions. We will\\npresent one of the most general forms of such theorems, which relies on the following\\ngeneral result.\\nTheorem 14.6 Supermartingale convergence\\nLet (X\\nt)t\u2208N, (Yt)t\u2208N,', 'a n d (Zt)t\u2208N be sequences of non-negative random variables\\nsuch that \u2211\u221e\\nt=0 Yt < \u221e .L e t Ft denote all the information for t\u2032 \u2264 t: Ft =\\n{(Xt\u2032 )t\u2032 \u2264t, (Yt\u2032 )t\u2032\u2264t, (Zt\u2032 )t\u2032 \u2264t}.T h e n ,i fE\\n[\\nXt+1\\n\u23d0\u23d0', 'Ft\\n]\\n\u2264 Xt + Yt \u2212 Zt, the following\\nholds:\\nXt converges to a limit (with probability one).\\n\u2211\u221e\\nt=0 Zt < \u221e .\\nThe following is one of the most general forms of such theorems.\\nTheorem 14.7\\nLet D be a funct', 'ion mapping RN \u00d7 RN to RN, (xt)t\u2208N and (wt)t\u2208N two sequences\\nin RN,a n d(\u03b1t)t\u2208N a sequence of real numbers with xt+1 = xt + \u03b1tD(xt,wt).L e t\\nFt denote the entire history for t\u2032 \u2264 t, that is: Ft = {(xt', '\u2032 )t\u2032 \u2264t, (wt\u2032 )t\u2032 \u2264t, (\u03b1t\u2032 )t\u2032 \u2264t}.\\nLet \u03a8 denote x \u2192 1\\n2 \u2225x \u2212 x\u2217\u22252\\n2 for some x\u2217 \u2208 RN and assume that D and (\u03b1)t\u2208N\\nverify the following conditions:\\n\u2203K1,K2 \u2208 R:E\\n[\\n\u2225D(xt,wt)\u22252\\n2\\n\u23d0\u23d0 Ft\\n]\\n\u2264 K1 + K2 \u03a8(xt', ');\\n\u2203c \u2265 0: \u2207\u03a8(xt)\u22a4 E\\n[\\nD(xt,wt)\\n\u23d0\u23d0 Ft\\n]\\n\u2264\u2212 c\u03a8(xt);14.5 Learning algorithms 329\\n\u03b1t > 0, \u2211\u221e\\nt=0 \u03b1t = \u221e , \u2211\u221e\\nt=0 \u03b12\\nt < \u221e .\\nThen, the sequence xt converges almost surely to x\u2217:\\nxt\\na.s\\n\u2212\u2212\u2192 x\u2217. (14.20)\\nPro', 'of Since function \u03a8 is quadratic, a Taylor expansion gives\\n\u03a8(xt+1)=\u03a8 ( xt)+ \u2207\u03a8(xt)\u22a4(xt+1 \u2212 xt)+ 1\\n2(xt+1 \u2212 xt)\u22a4\u22072\u03a8(xt)(xt+1 \u2212 xt).\\nThus,\\nE\\n[\\n\u03a8(xt+1)\\n\u23d0\u23d0Ft\\n]\\n=\u03a8 (xt)+ \u03b1t\u2207\u03a8(xt)\u22a4 E\\n[\\nD(xt,wt)\\n\u23d0\u23d0Ft\\n]\\n+ \u03b12\\n', 't\\n2 E\\n[\\n\u2225D(xt,wt)\u22252\u23d0\u23d0Ft\\n]\\n\u2264 \u03a8(xt) \u2212 \u03b1tc\u03a8(xt)+ \u03b12\\nt\\n2 (K1 + K2\u03a8(xt))\\n=\u03a8 (xt)+ \u03b12\\nt K1\\n2 \u2212\\n(\\n\u03b1tc \u2212 \u03b12\\nt K2\\n2\\n\u23a1\\n\u03a8(xt).\\nSince by assumption the series\u2211\u221e\\nt=0 \u03b12\\nt is convergent, (\u03b12\\nt )t and thus (\u03b1t)t con', 'verges\\nto zero. Therefore, for t su\ufb03ciently large, the term\\n(\\n\u03b1tc \u2212 \u03b12\\nt K2\\n2\\n\u23a1\\n\u03a8(xt) has the\\nsign of \u03b1tc\u03a8(xt) and is non-negative, since \u03b1t > 0, \u03a8( xt) \u2265 0, and c> 0.\\nThus, by the supermartingale con', 'vergence theorem 14.6, \u03a8( xt) converges and\u2211\u221e\\nt=0\\n(\\n\u03b1tc \u2212 \u03b12\\nt K2\\n2\\n\u23a1\\n\u03a8(xt) < \u221e . Since \u03a8(xt) converges and \u2211\u221e\\nt=0 \u03b12\\nt < \u221e ,w eh a v e\u2211\u221e\\nt=0\\n\u03b12\\nt K2\\n2 \u03a8(xt) < \u221e .B u t ,s i n c e\u2211\u221e\\nt=0 \u03b1t = \u221e , if th', 'e limit of \u03a8(xt) were non-zero,\\nwe would have \u2211\u221e\\nt=0 \u03b1tc\u03a8(xt)= \u221e . This implies that the limit of \u03a8( xt) is zero,\\nthat is limt\u2192\u221e \u2225xt \u2212 x\u2217\u22252 \u2192 0, which implies xt\\na.s\\n\u2212\u2212\u2192 x\u2217.\\nThe following is another r', 'elated result for which we do not present the full proof.\\nTheorem 14.8\\nLet H be a function mapping RN to RN,a n d(xt)t\u2208N, (wt)t\u2208N,a n d(\u03b1t)t\u2208N be three\\nsequences in RN with\\n\u2200s \u2208 [1,N ], xt+1(s)= xt(s)', '+ \u03b1t(s)\\n[\\nH(xt)(s) \u2212 xt(s)+ wt(s)\\n]\\n.\\nLet Ft denote the entire history fort\u2032 \u2264 t,t h a ti s :Ft = {(xt\u2032 )t\u2032 \u2264t,(wt\u2032 )t\u2032\u2264t, (\u03b1t\u2032 )t\u2032 \u2264t}\\nand assume that the following conditions are met:\\n\u2203K1,K2 \u2208 R:E\\n[', '\\nw2\\nt (s)\\n\u23d0\u23d0 Ft\\n]\\n\u2264 K1 + K2 \u2225xt\u22252 for some norm \u2225\u00b7\u2225 ;\\nE\\n[\\nwt\\n\u23d0\u23d0 Ft\\n]\\n=0 ;\\n\u2200s \u2208 [1,N ], \u2211\u221e\\nt=0 \u03b1t = \u221e , \u2211\u221e\\nt=0 \u03b12\\nt < \u221e ;a n d\\nH is a \u2225\u00b7\u2225 \u221e -contraction with \ufb01xed point x\u2217.330 Reinforcement Learning\\nTh', 'en, the sequence xt converges almost surely to x\u2217:\\nxt\\na.s\\n\u2212\u2212\u2192 x\u2217. (14.21)\\nThe next sections present several learning algorithms for MDPs with an unknown\\nmodel.\\n14.5.2 TD(0) algorithm\\nThis section pres', 'ents an algorithm, TD(0) algorithm, for evaluating a policy in the\\ncase where the environment model is unknown. The algorithm is based on Bellman\u2019s\\nlinear equations giving the value of a policy \u03c0 (see', ' proposition 14.1):\\nV\\n\u03c0(s)=E [ r(s, \u03c0(s)] +\u03b3\\n\u2211\\ns\u2032\\nPr[s\u2032|s, \u03c0(s)]V\u03c0(s\u2032)\\n=E\\ns\u2032\\n[\\nr(s, \u03c0(s)) +\u03b3V\u03c0(s\u2032)|s\\n]\\n.\\nHowever, here the probability distribution according to which this last expectation\\nis de\ufb01ned i', 's not known. Instead, the TD(0) algorithm consists of\\nsampling a new state s\u2032;a n d\\nupdating the policy values according to the following, which justi\ufb01es the name of\\nthe algorithm:\\nV (s) \u2190 (1 \u2212 \u03b1)V (s', ')+ \u03b1[r(s, \u03c0(s)) +\u03b3V(s\u2032)]\\n= V (s)+ \u03b1[r(s, \u03c0(s)) +\u03b3V(s\u2032) \u2212 V (s)\\ued19 \\ued18\\ued17 \\ued1a\\ntemporal di\ufb00erence of V values\\n]. (14.22)\\nHere, the parameter \u03b1 is a function of the number of visits to the state s.\\nThe pseudocod', 'e of the algorithm is given above. The algorithm starts with an\\narbitrary policy value vector V0. An initial state is returned by SelectState at\\nthe beginning of each epoch. Within each epoch, the ite', 'ration continues until a\\n\ufb01nal state is found. Within each iteration, action \u03c0(s) is taken from the current\\nstate s following policy \u03c0. The new state s\\n\u2032 reached and the reward r\u2032 received are\\nobserved', '. The policy value of state s is then updated according to the rule (14.22)\\nand current state set to be s\u2032.\\nThe convergence of the algorithm can be proven using theorem 14.8. We will give\\ninstead the ', 'full proof of the convergence of the Q-learning algorithm, for which that\\nof TD(0) can be viewed as a special case.14.5 Learning algorithms 331\\nTD(0)()\\n1 V \u2190 V0 \u22bf initialization.\\n2 for t \u2190 0 to T do\\n3', ' s \u2190 SelectState()\\n4 for each step of epoch t do\\n5 r\u2032 \u2190 Reward(s, \u03c0(s))\\n6 s\u2032 \u2190 NextState(\u03c0,s)\\n7 V (s) \u2190 (1 \u2212 \u03b1)V (s)+ \u03b1[r\u2032 + \u03b3V(s\u2032)]\\n8 s \u2190 s\u2032\\n9 return V\\n14.5.3 Q-learning algorithm\\nThis section presen', 'ts an algorithm for estimating the optimal state-action value\\nfunction Q\u2217 in the case of an unknown model. Note that the optimal policy or policy\\nvalue can be straightforwardly derived from Q\u2217 via: \u03c0\u2217', '(s) = argmax a\u2208A Q\u2217(s, a)\\nand V \u2217(s)=m a x a\u2208A Q\u2217(s, a). To simplify the presentation, we will assume a\\ndeterministic reward function.\\nThe Q-learning algorithm is based on the equations giving the opt', 'imal state-\\naction value function Q\u2217 (14.4):\\nQ\u2217(s, a)=E [ r(s, a)] +\u03b3\\n\u2211\\ns\u2032\u2208S\\nPr[s\u2032 | s, a]V \u2217(s\u2032)\\n=E\\ns\u2032\\n[r(s, a)+ \u03b3max\\na\u2208A\\nQ\u2217(s, a)].\\nAs for the policy values in the previous section, the distribution', ' model is not known.\\nThus, the Q-learning algorithm consists of the following main steps:\\nsampling a new state s\u2032;a n d\\nupdating the policy values according to the following:\\nQ(s, a) \u2190 \u03b1Q(s, a)+( 1 \u2212 ', '\u03b1)[r(s, a)+ \u03b3max\\na\u2032 \u2208A\\nQ(s\u2032,a \u2032)]. (14.23)\\nwhere the parameter \u03b1 is a function of the number of visits to the state s.\\nThe algorithm can be viewed as a stochastic formulation of the value iteration\\nal', 'gorithm presented in the previous section. The pseudocode is given above. Within332 Reinforcement Learning\\nQ-Learning(\u03c0)\\n1 Q \u2190 Q0 \u22bf initialization, e.g., Q0 =0 .\\n2 for t \u2190 0 to T do\\n3 s \u2190 SelectState(', ')\\n4 for each step of epoch t do\\n5 a \u2190 SelectAction(\u03c0,s) \u22bf policy \u03c0 derived from Q,e . g . ,\u03f5-greedy.\\n6 r\u2032 \u2190 Reward(s, a)\\n7 s\u2032 \u2190 NextState(s, a)\\n8 Q(s, a) \u2190 Q(s, a)+ \u03b1\\n[\\nr\u2032 + \u03b3maxa\u2032 Q(s\u2032,a \u2032) \u2212 Q(s, a)', '\\n]\\n9 s \u2190 s\u2032\\n10 return Q\\neach epoch, an action is selected from the current state s using a policy \u03c0 derived\\nfrom Q. The choice of the policy \u03c0 is arbitrary so long as it guarantees that every\\npair (s,', ' a) is visited in\ufb01nitely many times. The reward received and the state s\u2032\\nobserved are then used to update Q following (14.23).\\nTheorem 14.9\\nConsider a \ufb01nite MDP. Assume that for all s \u2208 S and a \u2208 A, ', '\u2211\u221e\\nt=0 \u03b1t(s, a)= \u221e ,\\nand \u2211\u221e\\nt=0 \u03b12\\nt (s, a) < \u221e with \u03b1t(s, a) \u2208 [0, 1]. Then, the Q-learning algorithm\\nconverges to the optimal value Q\u2217 (with probability one).\\nNote that the conditions on \u03b1t(s, a) im', 'pose that each state-action pair is visited\\nin\ufb01nitely many times.\\nProof Let (Qt(s, a))t\u22650 denote the sequence of state-action value functions at\\n(s, a) \u2208 S \u00d7 A generated by the algorithm. By de\ufb01nition', ' of the Q-learning updates,\\nQt+1(st,at)= Qt(st,at)+ \u03b1\\n[\\nr(st,at)+ \u03b3max\\na\u2032\\nQt(st+1,a \u2032) \u2212 Qt(st,at)\\n]\\n.\\nThis can be rewritten as the following for all s \u2208 S and a \u2208 A:\\nQt+1(s, a)= Qt(s, a)+ \u03b1t(s, a)\\n[\\n', 'r(s, a)+ \u03b3 E\\ns\u2032 \u223cPr[\u00b7|s,a]\\n[\\nmax\\na\u2032\\nQt(s\u2032,a \u2032)\\n]\\n\u2212 Qt(s, a)\\n]\\n+ \u03b3\u03b1t(s, a)\\n[\\nmax\\na\u2032\\nQt(s\u2032,a \u2032) \u2212 E\\ns\u2032 \u223cPr[\u00b7|s,a]\\n[\\nmax\\na\u2032\\nQt(s\u2032,a \u2032)\\n]]\\n, (14.24)\\nif we de\ufb01ne \u03b1t(s, a)a s0i f( s, a) \u0338=( st,at)a n d \u03b1t(s', 't,at) otherwise. Now, let Qt14.5 Learning algorithms 333\\ndenote the vector with components Qt(s, a), wt the vector whose s\u2032th is\\nwt(s\u2032)=m a x\\na\u2032\\nQt(s\u2032,a \u2032) \u2212 E\\ns\u2032 \u223cPr[\u00b7|s,a]\\n[\\nmax\\na\u2032\\nQt(s\u2032,a \u2032)\\n]\\n,\\nan', 'd H(Qt) the vector with components H(Qt)(x, a)d e \ufb01 n e db y\\nH(Qt)(x, a)= r(s, a)+ \u03b3 E\\ns\u2032 \u223cPr[\u00b7|s,a]\\n[\\nmax\\na\u2032\\nQt(s\u2032,a \u2032)\\n]\\n.\\nThen, in view of (14.24),\\n\u2200(s, a) \u2208 S \u00d7A, Qt+1(s, a)= Qt(s, a)+ \u03b1t(s, a)\\n[\\n', 'H(Qt)(s, a) \u2212 Qt(s, a)+ \u03b3wt(s)\\n]\\n.\\nWe now show that the hypotheses of theorem 14.8 hold for Qt and wt, which will\\nimply the convergence of Qt to Q\u2217. The conditions on \u03b1t hold by assumption. By\\nde\ufb01niti', 'on of wt,E [wt\\n\u23d0\u23d0 Ft] = 0. Also, for any s\u2032 \u2208 S,\\n|wt(s\u2032)|\u2264 max\\na\u2032\\n|Qt(s\u2032,a \u2032)| +\\n\u23d0\u23d0\u23d0\u23d0 E\\ns\u2032 \u223cPr[\u00b7|s,a]\\n[\\nmax\\na\u2032\\nQt(s\u2032,a \u2032)\\n]\u23d0\u23d0\u23d0\u23d0\\n\u2264 2m a x\\ns\u2032\\n| max\\na\u2032\\nQt(s\u2032,a \u2032)| =2 \u2225Qt\u2225\u221e .\\nThus, E\\n[\\nw2\\nt (s)\\n\u23d0\u23d0 Ft\\n]\\n\u2264', ' 4\u2225Qt\u22252\\n\u221e . Finally, H is a \u03b3-contraction for \u2225\u00b7\u2225 \u221e since for\\nany Q\u2032\\n1,Q\u2032\u2032\\n2 \u2208 R|S|\u00d7|A|,a n d(s, a) \u2208 S \u00d7 A, we can write\\n|H(Q2)(x, a) \u2212 H(Q\u2032\\n1)(x, a)| =\\n\u23d0\u23d0\\n\u23d0\\n\u23d0\u03b3 E\\ns\u2032\u223cPr[\u00b7|s,a]\\n[\\nmax\\na\u2032\\nQ2(s\u2032,a \u2032) \u2212 m', 'ax\\na\u2032\\nQ1(s\u2032,a \u2032)\\n]\u23d0\u23d0\\n\u23d0\\n\u23d0\\n\u2264 \u03b3 E\\ns\u2032 \u223cPr[\u00b7|s,a]\\n[\u23d0\u23d0\u23d0max\\na\u2032\\nQ2(s\u2032,a \u2032) \u2212 max\\na\u2032\\nQ1(s\u2032,a \u2032)\\n\u23d0\u23d0\u23d0\\n]\\n\u2264 \u03b3 E\\ns\u2032 \u223cPr[\u00b7|s,a]\\nmax\\na\u2032\\n[|Q2(s\u2032,a \u2032) \u2212 Q1(s\u2032,a \u2032)|]\\n\u2264 \u03b3max\\ns\u2032\\nmax\\na\u2032\\n[|Q2(s\u2032,a \u2032) \u2212 Q1(s\u2032,a \u2032)|]\\n= \u03b3\u2225Q\u2032\u2032', '\\n2 \u2212 Q\u2032\\n1\u2225\u221e .\\nSince H is a contraction, it admits a \ufb01xed point Q\u2217: H(Q\u2217)= Q\u2217.\\nT h ec h o i c eo ft h ep o l i c y\u03c0 according to which an action a is selected (line 5) is not\\nspeci\ufb01ed by the algorithm ', 'and, as already indicated, the theorem guarantees the\\nconvergence of the algorithm for an arbitrary policy so long as it ensures that every\\npair (s, a) is visited in\ufb01nitely many times. In practice, se', 'veral natural choices are\\nconsidered for \u03c0. One possible choice is the policy determined by the state-action\\nvalue at timet, Qt. Thus, the action selected from states is argmaxa\u2208A Qt(s, a). But\\nthis c', 'hoice typically does not guarantee that all actions are taken or that all states\\nare visited. Instead, a standard choice in reinforcement learning is the so-called \u03f5-\\ngreedy policy, which consists of ', 'selecting with probability (1 \u2212 \u03f5) the greedy action334 Reinforcement Learning\\nfrom state s, that is, argmaxa\u2208A Qt(s, a), and with probability \u03f5 a random action\\nfrom s,f o rs o m e\u03f5 \u2208 (0, 1). Another ', 'possible choice is the so-called Boltzmann\\nexploration, which, given the current state-action value Q,e p o c ht \u2208 [0,T ], and\\ncurrent state s, consists of selecting action a with the following probab', 'ility:\\npt(a|s, Q)= e\\nQ(s,a)\\n\u03c4t\\n\u2211\\na\u2032\u2208A e\\nQ(s,a\u2032 )\\n\u03c4t\\n,\\nwhere \u03c4t is the temperature. \u03c4t must be de\ufb01ned so that \u03c4t \u2192 0a s t \u2192\u221e ,w h i c h\\nensures that for large values of t, the greedy action based on Q ', 'is selected. This is\\nnatural, since as t increases, we can expect Q to be close to the optimal function.\\nOn the other hand, \u03c4t must be chosen so that it does not tend to 0 too fast to\\nensure that all ', 'actions are visited in\ufb01nitely often. It can be chosen, for instance, as\\n1/ log(n\\nt(s)), where nt(s) is the number of times s has been visited up to epoch t.\\nReinforcement learning algorithms include t', 'wo components: a learning policy,\\nwhich determines the action to take, and an update rule, which de\ufb01nes the new\\nestimate of the optimal value function. For an o\ufb00-policy algorithm, the update\\nrule does', ' not necessarily depend on the learning policy. Q-learning is an o\ufb00-policy\\nalgorithm since its update rule (line 8 of the pseudocode) is based on the max\\noperator and the comparison of all possible ac', 'tions a\\n\u2032,t h u si td o e sn o td e p e n do n\\nthe policy \u03c0. In contrast, the algorithm presented in the next section, SARSA, is\\nan on-policy algorithm.\\n14.5.4 SARSA\\nSARSA is also an algorithm for est', 'imating the optimal state-value function in the\\ncase of an unknown model. The pseudocode is given in \ufb01gure 14.7. The algorithm\\nis in fact very similar to Q-learning, except that its update rule (line ', '9 of the\\npseudocode) is based on the actiona\\n\u2032 selected by the learning policy. Thus, SARSA\\nis an on-policy algorithm, and its convergence therefore crucially depends on the\\nlearning policy. In partic', 'ular, the convergence of the algorithm requires, in addition\\nto all actions being selected in\ufb01nitely often, that the learning policy becomes greedy\\nin the limit. The proof of the convergence of the al', 'gorithm is nevertheless close to\\nthat of Q-learning.\\nThe name of the algorithm derives from the sequence of instructions de\ufb01ning\\nsuccessively s, a, r\\n\u2032, s\u2032,a n d a\u2032, and the fact that the update to th', 'e function Q\\ndepends on the quintuple (s, a, r\u2032,s \u2032,a).14.5 Learning algorithms 335\\nSARSA(\u03c0)\\n1 Q \u2190 Q0 \u22bf initialization, e.g., Q0 =0 .\\n2 for t \u2190 0 to T do\\n3 s \u2190 SelectState()\\n4 a \u2190 SelectAction(\u03c0(Q),s)', ' \u22bf policy \u03c0 derived from Q,e . g . ,\u03f5-greedy.\\n5 for each step of epoch t do\\n6 r\u2032 \u2190 Reward(s, a)\\n7 s\u2032 \u2190 NextState(s, a)\\n8 a\u2032 \u2190 SelectAction(\u03c0(Q),s \u2032) \u22bf policy \u03c0 derived from Q,e . g . ,\u03f5-greedy.\\n9 Q(s,', ' a) \u2190 Q(s, a)+ \u03b1t(s, a)\\n[\\nr\u2032 + \u03b3Q(s\u2032,a \u2032) \u2212 Q(s, a)\\n]\\n10 s \u2190 s\u2032\\n11 a \u2190 a\u2032\\n12 return Q\\nFigure 14.7 The SARSA algorithm.\\n14.5.5 TD( \u03bb) algorithm\\nBoth TD(0) and Q-learning algorithms are only based on im', 'mediate rewards. The\\nidea of TD(\u03bb) consists instead of using multiple steps ahead. Thus, forn> 1s t e p s ,\\nwe would have the update\\nV (s) \u2190 V (s)+ \u03b1(Rn\\nt \u2212 V (s)),\\nwhere Rn\\nt is de\ufb01ned by\\nRn\\nt = rt+1', ' + \u03b3rt+2 + ... + \u03b3n\u22121rt+n + \u03b3nV (st+n).\\nHow should n be chosen? Instead of selecting a speci\ufb01c n,T D (\u03bb) is based on a\\ngeometric distribution over all rewardsRn\\nt , that is, it usesR\u03bb\\nt =( 1\u2212 \u03bb) \u2211\u221e\\nn=', '0 \u03bbnRn\\nt\\ninstead of Rn\\nt where \u03bb \u2208 [0, 1]. Thus, the main update becomes\\nV (s) \u2190 V (s)+ \u03b1(R\u03bb\\nt \u2212 V (s)).\\nThe pseudocode of the algorithm is given above. For\u03bb = 0, the algorithm coincides\\nwith TD(0). \u03bb', ' = 1 corresponds to the total future reward.\\nIn the previous sections, we presented learning algorithms for an agent navigating336 Reinforcement Learning\\nTD(\u03bb)()\\n1 V \u2190 V0 \u22bf initialization.\\n2 e \u2190 0\\n3 f', 'or t \u2190 0 to T do\\n4 s \u2190 SelectState()\\n5 for each step of epoch t do\\n6 s\u2032 \u2190 NextState(\u03c0,s)\\n7 \u03b4\u2190 r(s, \u03c0(s)) +\u03bbV(s\u2032) \u2212 V (s)\\n8 e(s) \u2190 \u03bbe(s)+1\\n9 for u \u2208 S do\\n10 if u \u0338= s then\\n11 e(u) \u2190 \u03b3\u03bbe(u)\\n12 V (u) \u2190 V', ' (u)+ \u03b1\u03b4e(u)\\n13 s \u2190 s\u2032\\n14 return V\\nin an unknown environment. The scenario faced in many practical applications is\\nmore challenging; often, the information the agent receives about the environment\\nis ', 'uncertain or unreliable. Such problems can be modeled as partially observable\\nMarkov decision processes (POMDPs). POMDPs are de\ufb01ned by augmenting the\\nde\ufb01nition of MDPs with an observation probability ', 'distribution depending on the\\naction taken, the state reached, and the observation. The presentation of their model\\nand solution techniques are beyond the scope of this material.\\n14.5.6 Large state sp', 'ace\\nIn some cases in practice, the number of states or actions to consider for the\\nenvironment may be very large. For example, the number of states in the game\\nof backgammon is estimated to be over 10', '\\n20.T h u s ,t h ea l g o r i t h m sp r e s e n t e di n\\nthe previous section can become computationally impractical for such applications.\\nMore importantly, generalization becomes extremely di\ufb03cult.', '\\nS u p p o s ew ew i s ht oe s t i m a t et h ep o l i c yv a l u eV\\n\u03c0(s)a te a c hs t a t es using\\nexperience obtained using policy \u03c0. To cope with the case of large state spaces,\\nwe can map each sta', 'te of the environment to RN v i aam a p p i n g\u03a6 : S \u2192 RN ,w i t h14.6 Chapter notes 337\\nN relatively small ( N \u2248 200 has been used for backgammon) and approximate\\nV\u03c0(s) by a function fw(s) parameteri', 'zed by some vector w. For example, fw could\\nbe a linear function de\ufb01ned by fw(s)= w \u00b7\u03a6(s) for alls \u2208 S, or some more complex\\nnon-linear function of w. The problem then consists of approximating V\u03c0 wit', 'h fw\\nand can be formulated as a regression problem. Note, however, that the empirical\\ndata available is not i.i.d.\\nSuppose that at each time stept the agent receives the exact policy valueV\u03c0(st).\\nThen', ', if the family of functions fw is di\ufb00erentiable, a gradient descent method\\napplied to the empirical squared loss can be used to sequentially update the weight\\nvector w via:\\nwt+1 = wt \u2212 \u03b1\u2207wt\\n1\\n2[V\u03c0(st', ') \u2212 fwt (st)]2 = wt + \u03b1[V\u03c0(st) \u2212 fwt (st)]\u2207wt fwt (st).\\nIt is worth mentioning, however, that for large action spaces, there are simple cases\\nwhere the methods used do not converge and instead cycle.\\n', '14.6 Chapter notes\\nReinforcement learning is an important area of machine learning with a large body\\nof literature. This chapter presents only a brief introduction to this area. For a\\nmore detailed st', 'udy, the reader could consult the book of Sutton and Barto [1998],\\nwhose mathematical content is short, or those of Puterman [1994] and Bertsekas\\n[1987], which discuss in more depth several aspects, a', 's well as the more recent book\\nof Szepesv\u00b4ari [2010]. The Ph.D. theses of Singh [1993] and Littman [1996] are also\\nexcellent sources.\\nSome foundational work on MDPs and the introduction of the tempora', 'l di\ufb00erence\\n(TD) methods are due to Sutton [1984]. Q-learning was introduced and analyzed\\nby Watkins [1989], though it can be viewed as a special instance of TD methods.\\nThe \ufb01rst proof of the converge', 'nce of Q-learning was given by Watkins and Dayan\\n[1992].\\nMany of the techniques used in reinforcement learning are closely related to those\\nof stochastic approximation which originated with the work o', 'f Robbins and Monro\\n[1951], followed by a series of results including Dvoretzky [1956], Schmetterer [1960],\\nKiefer and Wolfowitz [1952], and Kushner and Clark [1978]. For a recent survey of\\nstochastic', ' approximation, including a discussion of powerful proof techniques based\\non ODE (ordinary di\ufb00erential equations), see Kushner [2010] and the references\\ntherein. The connection with stochastic approxi', 'mation was emphasized by Tsitsiklis\\n[1994] and Jaakkola et al. [1994], who gave a related proof of the convergence of\\nQ-learning. For the convergence rate of Q-learning, consult Even-Dar and Mansour\\n[', '2003]. For recent results on the convergence of the policy iteration algorithm, see Ye338 Reinforcement Learning\\n[2011], which shows that the algorithm is strongly polynomial for a \ufb01xed discount\\nfacto', 'r.\\nReinforcement learning has been successfully applied to a variety of problems\\nincluding robot control, board games such as backgammon in which Tesauro\u2019s TD-\\nGammon reached the level of a strong mas', 'ter [Tesauro, 1995] (see also chapter\\n11 of Sutton and Barto [1998]), chess, elevator scheduling problems [Crites and\\nBarto, 1996], telecommunications, inventory management, dynamic radio channel\\nassi', 'gnment [Singh and Bertsekas, 1997], and a number of other problems (see\\nchapter 1 of Puterman [1994]).Conclusion\\nWe described a large variety of machine learning algorithms and techniques and\\ndiscusse', 'd their theoretical foundations as well as their use and applications. While\\nthis is not a fully comprehensive presentation, it should nevertheless o\ufb00er the reader\\nsome idea of the breadth of the \ufb01eld', ' and its multiple connections with a variety of\\nother domains, including statistics, information theory, optimization, game theory,\\nand automata and formal language theory.\\nThe fundamental concepts, a', 'lgorithms, and proof techniques we presented should\\nsupply the reader with the necessary tools for analyzing other learning algorithms,\\nincluding variants of the algorithms analyzed in this book. They', ' are also likely to\\nbe helpful for devising new algorithms or for studying new learning schemes. We\\nstrongly encourage the reader to explore both and more generally to seek enhanced\\nsolutions for all ', 'theoretical, algorithmic, and applied learning problems.\\nThe exercises included at the end of each chapter, as well as the full solutions we\\nprovide separately, should help the reader become more fami', 'liar with the techniques\\nand concepts described. Some of them could also serve as a starting point for\\nresearch work and the investigation of new questions.\\nMany of the algorithms we presented as well', ' as their variants can be directly\\nused in applications to derive e\ufb00ective solutions to real-world learning problems.\\nOur detailed description of the algorithms and discussion should help with their\\ni', 'mplementation or their adaptation to other learning scenarios.\\nMachine learning is a relatively recent \ufb01eld and yet probably one of the most\\nactive ones in computer science. Given the wide accessibili', 'ty of digitized data and\\nits many applications, we can expect it to continue to grow at a very fast pace\\nover the next few decades. Learning problems of di\ufb00erent nature, some arising\\ndue to the substa', 'ntial increase of the scale of the data, which already requires\\nprocessing billions of records in some applications, others related to the introduction\\nof completely new learning frameworks, are likel', 'y to pose new research challenges\\nand require novel algorithmic solutions. In all cases, learning theory, algorithms,\\nand applications form an exciting area of computer science and mathematics, which\\n', 'we hope this book could at least partly communicate.Appendix A Linear Algebra Review\\nIn this appendix, we introduce some basic notions of linear algebra relevant to the\\nmaterial presented in this book', '. This appendix does not represent an exhaustive\\ntutorial, and it is assumed that the reader has some prior knowledge of the subject.\\nA.1 Vectors and norms\\nWe will denote by H a vector space whose dim', 'ension may be in\ufb01nite.\\nA.1.1 Norms\\nDe\ufb01nition A.1\\nA mapping \u03a6: H \u2192 R\\n+ is said to de\ufb01ne a norm on H if it veri\ufb01es the following\\naxioms:\\nde\ufb01niteness: \u2200x \u2208 H, \u03a6(x)=0 \u21d4 x = 0;\\nhomogeneity: \u2200x \u2208 H, \u2200\u03b1 \u2208 R,', ' \u03a6(\u03b1x)= |\u03b1|\u03a6(x);\\ntriangle inequality: \u2200x,y \u2208 H,\u03a6(x + y) \u2264 \u03a6(x)+\u03a6 (y).\\nA norm is typically denoted by \u2225\u00b7\u2225 . Examples of vector norms are the absolute\\nvalue on R and the Euclidean (or L2) norm on RN . M', 'ore generally, for any p \u2265 1\\nthe Lp norm is de\ufb01ned on RN as\\n\u2200x \u2208 RN , \u2225x\u2225p =\\n( N\u2211\\nj=1\\n|xj |p\\n\u23a11/p\\n. (A.1)\\nThe L1, L2,a n dL\u221e norms are the some of the most commonly used norms, where\\n\u2225x\u2225\u221e =m a xj\u2208[1,N', '] xj.T w on o r m s\u2225\u00b7\u2225 and \u2225\u00b7\u2225 \u2032 are said to be equivalent i\ufb00 there\\nexists \u03b1, \u03b2 >0 such that for all x \u2208 H,\\n\u03b1\u2225x\u2225\u2264\u2225 x\u2225\u2032 \u2264 \u03b2\u2225x\u2225. (A.2)342 Linear Algebra Review\\nThe following general inequalities relatin', 'g these norms can be proven straightfor-\\nwardly:\\n\u2225x\u22252 \u2264\u2225 x\u22251 \u2264\\n\u221a\\nN \u2225x\u22252 (A.3)\\n\u2225x\u2225\u221e \u2264\u2225 x\u22252 \u2264\\n\u221a\\nN \u2225x\u2225\u221e (A.4)\\n\u2225x\u2225\u221e \u2264\u2225 x\u22251 \u2264 N \u2225x\u2225\u221e . (A.5)\\nThe second inequality of the \ufb01rst line can be shown using the Ca', 'uchy-Schwarz\\ninequality presented later while the other inequalities are clear. These inequalities\\nshow the equivalence of these three norms. More generally, all norms on a \ufb01nite-\\ndimensional space ar', 'e equivalent. The following additional properties hold for the\\nL\\n\u221e norm: for all x \u2208 H,\\n\u2200p \u2265 1, \u2225x\u2225\u221e \u2264\u2225 x\u2225p \u2264 N1/p\u2225x\u2225\u221e (A.6)\\nlim\\np\u2192 +\u221e\\n\u2225x\u2225p = \u2225x\u2225\u221e . (A.7)\\nThe inequalities of the \ufb01rst line are straigh', 'tforward and imply the limit property of\\nthe second line.\\nWe will often consider a Hilbert space, that is a vector space equipped with an\\ninner product \u27e8\u00b7, \u00b7\u27e9 and that is complete (all Cauchy sequence', 's are convergent). The\\ninner product induces a norm de\ufb01ned as follows:\\n\u2200x \u2208 H, \u2225x\u2225H =\\n\u221a\\n\u27e8x,x\u27e9. (A.8)\\nA.1.2 Dual norms\\nDe\ufb01nition A.2\\nLet \u2225\u00b7\u2225 be a norm on R\\nN. Then, the dual norm \u2225\u00b7\u2225 \u2217 associated to \u2225\u00b7', '\u2225 is the norm\\nde\ufb01ned by\\n\u2200y \u2208 H, \u2225y\u2225\u2217 =s u p\\n\u2225x\u2225=1\\n|\u27e8y,x\u27e9| . (A.9)\\nFor any p, q \u2265 1t h a ta r econjugate that is such that 1\\np + 1\\nq =1 ,t h e Lp and Lq\\nnorms are dual norms of each other. In particula', 'r, the dual norm of L2 is the L2\\nnorm, and the dual norm of the L1 norm is the L\u221e norm.\\nProposition A.1 H\u00a8older\u2019s inequality\\nLet p, q \u2265 1 be conjugate: 1\\np + 1\\nq =1 .T h e n ,f o ra l lx, y \u2208 RN,\\n|\u27e8x,', 'y\u27e9| \u2264 \u2225 x\u2225p\u2225y\u2225q, (A.10)\\nwith equality when |yi| = |xi|p\u22121 for all i \u2208 [1,N ].A.1 Vectors and norms 343\\nProof The statement holds trivially for x = 0 or y = 0; thus, we can assume\\nx \u0338= 0 and y \u0338= 0.L e', ' ta, b >0. By the concavity of log (see de\ufb01nition B.5), we can\\nwrite\\nlog\\n(1\\npap + 1\\nqbq\\n\u23a1\\n\u2265 1\\np log(ap)+ 1\\nq log(bq)=l o g (a)+l o g (b)=l o g (ab).\\nTaking the exponential of the left- and right-hand ', 'sides gives\\n1\\npap + 1\\nqbq \u2265 ab,\\nwhich is known asYoung\u2019s inequality. Using this inequality witha = |xj |/\u2225x\u2225p and\\nb = |yj |/\u2225y\u2225q for j \u2208 [1,N ] and summing up gives\\n\u2211N\\nj=1 |xjyj |\\n\u2225x\u2225p\u2225y\u2225q\\n\u2264 1\\np\\n\u2225x\u2225p\\n', '\u2225x\u2225p + 1\\nq\\n\u2225y\u2225q\\n\u2225y\u2225q = 1\\np + 1\\nq =1 .\\nSince |\u27e8x,y\u27e9| \u2264 \u2211N\\nj=1 |xjyj |, the inequality claim follows. The equality case can be\\nveri\ufb01ed straightforwardly.\\nTaking p = q = 2 immediately yields the followin', 'g result known as the Cauchy-\\nSchwarz inequality .\\nCorollary A.1 Cauchy-Schwarz inequality\\nFor all x,y \u2208 RN,\\n|\u27e8x,y\u27e9| \u2264 \u2225 x\u22252\u2225y\u22252, (A.11)\\nwith equality i\ufb00 x and y are collinear.\\nLet H be the hyperplane', ' in RN whose equation is given by\\nw \u00b7 x + b =0 ,\\nfor some normal vector w \u2208 RN and o\ufb00set b \u2208 R.L e tdp(x, H) denote the distance\\nof x to the hyperplane H,t h a ti s ,\\ndp(x, H)= i n f\\nx\u2032 \u2208H\\n\u2225x\u2032 \u2212 x\u2225p. ', '(A.12)\\nThen, the following identity holds for all p \u2265 1:\\ndp(x, H)= |w \u00b7 x + b|\\n\u2225w\u2225q\\n, (A.13)\\nwhere q is the conjugate of p: 1\\np + 1\\nq = 1. (A.13) can be shown by a straightforward\\napplication of the r', 'esults of appendix B to the constrained optimization problem\\n(A.12).344 Linear Algebra Review\\nA.2 Matrices\\nFor a matrix M \u2208 Rm\u00d7n with m rows and n columns, we denote by Mij its ijth\\nentry, for all i \u2208', ' [1,m]a n d j \u2208 [1,n ]. For any m \u2265 1, we denote by Im the m-\\ndimensional identity matrix, and refer to it as I when the dimension is clear from\\nthe context.\\nThe transpose of M is denoted byM\u22a4 and de\ufb01', 'ned by (M\u22a4)ij = Mji for all (i, j).\\nFor any two matrices M \u2208 Rm\u00d7n and N \u2208 Rn\u00d7p,( MN)\u22a4 = N\u22a4M\u22a4. M is said to\\nbe symmetric i\ufb00 Mij = Mji for all (i, j), that is, i\ufb00 M = M\u22a4.\\nThe trace of a square matrix M ', 'is denoted by Tr[M] and de\ufb01ned as Tr[ M]=\u2211N\\ni=1 Mii. For any two matricesM \u2208 Rm\u00d7n and N \u2208 Rn\u00d7m, the following identity\\nholds: Tr[MN]=T r [NM]. More generally, the following cyclic property holds with\\n', 'the appropriate dimensions for the matrices M, N,a n dP:\\nTr[MNP]=T r [PMN]=T r [NPM]. (A.14)\\nThe inverse of a square matrixM, which exists whenM has full rank, is denoted\\nby M\u22121 and is the unique matr', 'ix satisfying MM\u22121 = M\u22121M = I.\\nA.2.1 Matrix norms\\nA matrix norm is a norm de\ufb01ned over R\\nm\u00d7n where m and n are the dimensions\\nof the matrices considered. Many matrix norms, including those discussed be', 'low,\\nsatisfy the following submultiplicative property:\\n\u2225MN\u2225\u2264\u2225 M\u2225\u2225N\u2225. (A.15)\\nThe matrix norm induced by the vector norm \u2225\u00b7\u2225\\np or the operator norm induced\\nby that norm is also denoted by \u2225\u00b7\u2225 p and de\ufb01n', 'ed by\\n\u2225M\u2225p =s u p\\n\u2225x\u2225p\u22641\\n\u2225Mx\u2225p . (A.16)\\nThe norm induced forp =2i sk n o w na st h espectral norm, which equals the largest\\nsingular value of M (see section A.2.2), or the square-root of the largest e', 'igenvalue\\nof M\u22a4M:\\n\u2225M\u22252 = \u03c31(M)=\\n\u221a\\n\u03bbmax(M\u22a4M). (A.17)A.2 Matrices 345\\nNot all matrix norms are induced by vector norms. The Frobenius norm denoted\\nby \u2225\u00b7\u2225 F is the most notable of such norms and is de\ufb01ne', 'd by:\\n\u2225M\u2225F =\\n( m\u2211\\ni=1\\nn\u2211\\nj=1\\nM2\\nij\\n\u23a11/2\\n.\\nThe Frobenius norm can be interpreted as the L2 norm of a vector when treating\\nM as a vector of size mn. It also coincides with the norm induced by theFrobeni', 'us\\nproduct, which is the inner product de\ufb01ned over for all M,N \u2208 Rm\u00d7n by\\n\u27e8M,N\u27e9F =T r [M\u22a4N]. (A.18)\\nThis relates the Frobenius norm to the singular values of M:\\n\u2225M\u22252\\nF =T r [M\u22a4M]=\\nr\u2211\\ni=1\\n\u03c3i(M)2 ,\\nwhere', ' r =r a n k (M). The second equality follows from properties of SPSD matrices\\n(see section A.2.3).\\nFor anyj \u2208 [1,n ], let Mj denote the jth column of M,t h a ti sM =[ M1 \u00b7\u00b7\u00b7 Mn].\\nThen, for any p, r \u2265 ', '1, the Lp,r group norm of M is de\ufb01ned by\\n\u2225M\u2225p,r =\\n( n\u2211\\nj=1\\n\u2225Mi\u2225r\\np\\n\u23a11/r\\n.\\nO n eo ft h em o s tc o m m o n l yu s e dg r o u pn o r m si st h eL2,1 norm de\ufb01ned by\\n\u2225M\u22252,1 =\\nn\u2211\\ni=1\\n\u2225Mi\u22252 .\\nA.2.2 Singular', ' value decomposition\\nThe compact singular value decomposition (SVD) of M,w i t hr =r a n k (M) \u2264\\nmin(m, n), can be written as follows:\\nM = UM\u03a3MV\u22a4\\nM .\\nThe r \u00d7 r matrix \u03a3M =d i a g (\u03c31,...,\u03c3 r) is diago', 'nal and contains the non-zero\\nsingular values of M sorted in decreasing order, that is \u03c31 \u2265 ... \u2265 \u03c3r > 0.\\nUM \u2208 Rm\u00d7r and VM \u2208 Rn\u00d7r have orthonormal columns that contain the left and\\nright singular vect', 'ors of M corresponding to the sorted singular values.Uk \u2208 Rm\u00d7k\\nare the top k \u2264 r left singular vectors of M.\\nThe orthogonal projection onto the span of Uk c a nb ew r i t t e na sPUk = UkU\u22a4\\nk ,\\nwhere ', 'PUk is SPSD and idempotent, i.e.,P2\\nUk = PUk . Moreover, the orthogonal pro-346 Linear Algebra Review\\njection onto the subspace orthogonal toUk is de\ufb01ned as PUk,\u22a5 . Similar de\ufb01nitions,\\ni.e., Vk,PVk ,P', 'Vk,\u22a5 , hold for the right singular vectors.\\nThe generalized inverse,o r Moore-Penrose pseudo-inverse of a matrix M is\\ndenoted by M\u2020 and de\ufb01ned by\\nM\u2020 = UM\u03a3\u2020\\nMV\u22a4\\nM , (A.19)\\nwhere \u03a3\u2020\\nM =d i a g (\u03c3\u22121\\n1 ,.', '..,\u03c3 \u22121\\nr ). For any square m \u00d7 m matrix M with full rank,\\ni.e., r = m, the pseudo-inverse coincides with the matrix inverse: M\u2020 = M\u22121.\\nA.2.3 Symmetric positive semide\ufb01nite (SPSD) matrices\\nDe\ufb01nition A', '.3\\nA symmetric matrix M \u2208 R\\nm\u00d7m is said to be positive semide\ufb01nite i\ufb00\\nx\u22a4Mx \u2265 0 (A.20)\\nfor all x \u2208 Rm. M is said to be positive de\ufb01nite if the inequality is strict.\\nKernel matrices (see chapter 5) and ', 'orthogonal projection matrices are two examples\\nof SPSD matrices. It is straightforward to show that a matrix M is SPSD i\ufb00 its\\neigenvalues are all non-negative. Furthermore, the following properties h', 'old for any\\nSPSD matrix M:\\nM admits a decomposition M = X\u22a4X for some matrix X and the Cholesky\\ndecomposition p r o v i d e so n es u c hd e c o m p o s i t i o ni nw h i c hX is an upper triangular\\nma', 'trix.\\nThe left and right singular vectors of M are the same and the SVD of M is also\\nits eigenvalue decomposition.\\nThe SVD of an arbitrary matrix X = UX\u03a3XV\u22a4\\nX de\ufb01nes the SVD of two related\\nSPSD matric', 'es: the left singular vectors (UX) are the left singular vectors ofXX\u22a4,\\nthe right singular vectors (VX) are the right singular vectors ofX\u22a4X and the non-\\nzero singular values of X are the square roots', ' of the non-zero singular values of\\nXX\u22a4 and X\u22a4X.\\nThe trace ofM is the sum of its singular values, i.e., Tr[M]= \u2211r\\ni=1 \u03c3i(M), where\\nrank(M)= r.\\nThe top singular vector of M, u1, maximizes the Rayleigh ', 'quotient ,w h i c hi s\\nde\ufb01ned as\\nr(x,M)= x\u22a4Mx\\nx\u22a4x .\\nIn other words, u1 =a r g m a xx r(x,M)a n dr(u,M)= \u03c31(M). Similarly, if M\u2032 =A.2 Matrices 347\\nPUi,\u22a5M, that is, the projection of M onto the subspace', ' orthogonal to Ui,t h e n\\nui+1 =a r g m a xx r(x,M\u2032), where ui+1 is the (i + 1)st singular vector of M.Appendix B Convex Optimization\\nIn this appendix, we introduce the main de\ufb01nitions and results of ', 'convex optimiza-\\ntion needed for the analysis of the learning algorithms presented in this book.\\nB.1 Di\ufb00erentiation and unconstrained optimization\\nWe start with some basic de\ufb01nitions for di\ufb00erentiatio', 'n needed to present Fermat\u2019s\\ntheorem and to describe some properties of convex functions.\\nDe\ufb01nition B.1 Gradient\\nLet f : X\u2286 R\\nN \u2192 R be a di\ufb00erentiable function. Then, thegradient of f at x \u2208X\\nis the v', 'ector in RN denoted by \u2207f(x) a n dd e \ufb01 n e db y\\n\u2207f(x)=\\n\u23a1\\n\u23a2\u23a2\u23a3\\n\u2202f\\n\u2202x1\\n(x)\\n..\\n.\\n\u2202f\\n\u2202xN\\n(x)\\n\u23a4\\n\u23a5\u23a5\u23a6.\\nDe\ufb01nition B.2 Hessian\\nLet f : X\u2286 RN \u2192 R be a twice di\ufb00erentiable function. Then, the Hessian of f at\\nx \u2208', 'X is the matrix in RN \u00d7N denoted by \u22072f(x) a n dd e \ufb01 n e db y\\n\u22072f(x)=\\n[ \u22022f\\n\u2202xi,xj\\n(x)\\n]\\n1\u2264i,j\u2264N\\n.\\nNext, we present a classic result for unconstrained optimization.\\nTheorem B.1 Fermat\u2019s theorem\\nLet f', ' : X\u2286 RN \u2192 R be a di\ufb00erentiable function. If f admits a local extremum at\\nx\u2217 \u2208X ,t h e n\u2207f(x\u2217)=0 , that is, x\u2217 is a stationary point.350 Convex Optimization\\nFigure B.1 Examples of a convex (left) and ', 'a concave (right) functions. Note that\\nany line segment drawn between two points on the convex function lies entirely\\nabove the graph of the function while any line segment drawn between two points\\non', ' the concave function lies entirely below the graph of the function.\\nB.2 Convexity\\nThis section introduces the notions of convex sets and convex functions. Convex\\nfunctions play an important role in t', 'he design and analysis of learning algorithms,\\nin part because a local minimum of a convex function is necessarily also a global\\nminimum. Thus, the properties of a learning hypothesis that is a local ', 'minimum\\nof a convex optimization are often well understood, while for some non-convex\\noptimization problems, there may be a very large number of local minima for which\\nno clear characterization can be', ' given.\\nDe\ufb01nition B.3 Convex set\\nAs e tX\u2286 R\\nN is said to be convex if for any two pointsx,y \u2208X the segment [x,y]\\nlies in X, that is\\n{\u03b1x +( 1 \u2212 \u03b1)y:0 \u2264 \u03b1 \u2264 1}\u2286X .\\nDe\ufb01nition B.4 Convex hull\\nThe convex h', 'ull conv( X) of a set of points X\u2286 RN is the minimal convex set\\ncontaining X and can be equivalently de\ufb01ned as follows:\\nconv(X)=\\n{ m\u2211\\ni=1\\n\u03b1ixi : m \u2265 1, \u2200i \u2208 [1,m],xi \u2208X ,\u03b1i \u2265 0,\\nm\u2211\\ni=1\\n\u03b1i =1\\n}\\n. (B.1)', '\\nLet Epif denote the epigraph of function f : X\u2192 R, that is the set of points lying\\nabove its graph: {(x, y): x \u2208X ,y \u2265 f(x)}.B.2 Convexity 351\\nf(y)\\n(x, f(x))\\nf(x)+ \u2207f(x)\u00b7(y \u2212 x)\\nFigure B.2 Illustrati', 'on of the \ufb01rst-order property satis\ufb01ed by all convex functions.\\nDe\ufb01nition B.5 Convex function\\nLet X be a convex set. A function f : X\u2192 R is said to be convex i\ufb00 Epif is a\\nconvex set, or, equivalently,', ' if for all x,y \u2208X and \u03b1 \u2208 [0, 1],\\nf(\u03b1x +( 1 \u2212 \u03b1)y) \u2264 \u03b1f(x)+( 1 \u2212 \u03b1)f(y) . (B.2)\\nf is said to be strictly convex if inequality (B.2) is strict for all x,y \u2208X where\\nx \u0338= y and \u03b1 \u2208 (0, 1). f is said to ', 'be (strictly) concave when \u2212f is (strictly)\\nconvex. Figure B.1 shows simple examples of a convex and concave functions.\\nConvex functions can also be characterized in terms of their \ufb01rst- or second-ord', 'er\\ndi\ufb00erential.\\nTheorem B.2\\nLet f be a di\ufb00erentiable function, then f is convex if and only if dom(f) is convex\\nand the following inequalities hold:\\n\u2200x,y \u2208 dom(f),f (y) \u2212 f(x) \u2265\u2207 f(x) \u00b7 (y \u2212 x) . (B.3', ')\\nThe property (B.3) is illustrated by \ufb01gure B.2: for a convex function, the hyperplane\\ntangent at x is always below the graph.\\nTheorem B.3\\nLet f be a twice di\ufb00erentiable function, then f is convex i\ufb00', ' dom(f) is convex and\\nits Hessian is positive semide\ufb01nite:\\n\u2200x \u2208 dom(f), \u22072f(x) \u2ab0 0 .\\nRecall that a symmetric matrix is positive semide\ufb01nite if all of its eigenvalues are\\nnon-negative. Further, note th', 'at when f is scalar, this theorem states that f is\\nconvex if and only if its second derivative is always non-negative, that is, for all\\nx \u2208 dom(f),f\\n\u2032\u2032(x) \u2265 0.\\nExample B.1 Linear functions352 Convex O', 'ptimization\\nAny linear function f is both convex and concave, since equation (B.2) holds with\\nequality for both f and \u2212f by the de\ufb01nition of linearity.\\nExample B.2 Quadratic function\\nThe function f : ', 'x \u21a6\u2192 x2 de\ufb01ned over R is convex since it is twice di\ufb00erentiable and\\nfor all x \u2208 R, f \u2032\u2032(x)=2 > 0.\\nExample B.3 Norms\\nAny norm \u2225\u00b7\u2225 de\ufb01ned over a convex set X is convex since by the triangle inequality\\na', 'nd homogeneity property of the norm, for all \u03b1 \u2208 [0, 1],x,y \u2208X , we can write\\n\u2225\u03b1x +( 1 \u2212 \u03b1)y\u2225\u2264\u2225 \u03b1x\u2225 + \u2225(1 \u2212 \u03b1)y\u2225 = \u03b1\u2225x\u2225 +( 1 \u2212 \u03b1)\u2225y\u2225 .\\nExample B.4 Maximum function\\nThe max function de\ufb01ned for all x \u2208 ', 'RN ,b y x \u21a6\u2192 maxj\u2208[1,N] xj is convex. For all\\n\u03b1 \u2208 [0,1],x,y \u2208 RN ,b yt h es u b a d d i t i v i t yo fm a x ,w ec a nw r i t e\\nmax\\nj\\n(\u03b1xj +(1 \u2212\u03b1)yj) \u2264 max\\nj\\n(\u03b1xj)+max\\nj\\n((1\u2212\u03b1)yj)= \u03b1max\\nj\\n(xj)+(1 \u2212\u03b1)m ', 'a x\\nj\\n(yj) .\\nOne useful approach for proving convexity or concavity of functions is to make\\nuse of composition rules. For simplicity of presentation, we will assume twice\\ndi\ufb00erentiability, although th', 'e results can also be proven without this assumption.\\nLemma B.1 Composition of convex/concave functions\\nAssume h : R \u2192 R and g : R\\nN \u2192 R are twice di\ufb00erentiable functions and for all\\nx \u2208 RN, de\ufb01ne f(x', ')= h(g(x)). Then the following implications are valid:\\nh is convex and non-decreasing, andg is convex =\u21d2 f is convex.\\nh is convex and non-increasing, and g is concave =\u21d2 f is convex.\\nh is concave and ', 'non-decreasing, andg is concave =\u21d2 f is concave.\\nh is concave and non-increasing, andg is convex =\u21d2 f is concave.\\nProof We restrict ourselves ton = 1, since it su\ufb03ces to prove convexity (concav-\\nity) ', 'along all arbitrary lines that intersect the domain. Now, consider the second\\nderivative of f:\\nf\\n\u2032\u2032(x)= h\u2032\u2032(g(x))g\u2032(x)2 + h\u2032(g(x))g\u2032\u2032(x) . (B.4)\\nNote that if h is convex and non-decreasing, we have h\u2032', '\u2032 \u2265 0a n d h\u2032 \u2265 0.\\nFurthermore, if g is convex we also have g\u2032\u2032 \u2265 0, and it follows that f \u2032\u2032(x) \u2265 0,\\nwhich proves the \ufb01rst statement. The remainder of the statements are proven in a\\nsimilar manner.\\nE', 'xample B.5 Composition of functionsB.3 Constrained optimization 353\\nT h ep r e v i o u sl e m m ac a nb eu s e dt oi m m e d i a t e l yp r o v et h ec o n v e x i t yo rc o n c a v i t y\\nof the follo', 'wing composed functions:\\nIf f : RN \u2192 R is convex, then exp(f)i sc o n v e x .\\nAny squared norm \u2225\u00b7\u2225 2 is convex.\\nFor all x \u2208 RN the function x \u21a6\u2192 log(\u2211N\\nj=1 xj) is concave.\\nThe following is a useful in', 'equality applied in a variety of contexts. It is in fact a\\nquasi-direct consequence of the de\ufb01nition of convexity.\\nTheorem B.4 Jensen\u2019s inequality\\nLet X be a random variable taking values in a non-emp', 'ty convex setC \u2286 RN with a\\n\ufb01nite expectation E[X],a n df a measurable convex function de\ufb01ned over C.T h e n ,\\nE[X] is in C, E[f(X)] is \ufb01nite, and the following inequality holds:\\nf(E[X]) \u2264 E[f(X)].\\nPro', 'of We give a sketch of the proof, which essentially follows from the de\ufb01nition\\nof convexity. Note that for any \ufb01nite set of elementsx1,...,x n in C and any positive\\nreals \u03b11,...,\u03b1 n such that \u2211n\\ni=1 \u03b1', 'i =1 ,w eh a v e\\nf\\n( n\u2211\\ni=1\\n\u03b1ixi\\n\u23a1\\n\u2264\\nn\u2211\\ni=1\\n\u03b1if(xi) .\\nThis follows straightforwardly by induction from the de\ufb01nition of convexity. Since\\nthe \u03b1is can be interpreted as probabilities, this immediately p', 'roves the inequality\\nfor any distribution with a \ufb01nite support de\ufb01ned by \u03b1 =( \u03b11,...,\u03b1 n):\\nf(E\\n\u03b1\\n[X]) \u2264 E\\n\u03b1\\n[f(X)].\\nExtending this to arbitrary distributions can be shown via the continuity of f on\\nan', 'y open set, which is guaranteed by the convexity of f, and the weak density of\\ndistributions with \ufb01nite support in the family of all probability measures.\\nB.3 Constrained optimization\\nWe now de\ufb01ne a g', 'eneral constrained optimization problem and the speci\ufb01c proper-\\nties associated to convex constrained optimization problems.354 Convex Optimization\\nDe\ufb01nition B.6 Constrained optimization problem\\nLet X', '\u2286 RN and f,g i : X\u2192 R,f o ra l li \u2208 [1,m].T h e n ,aconstrained optimization\\nproblem has the form:\\nmin\\nx\u2208X\\nf(x)\\nsubject to: gi(x) \u2264 0, \u2200i \u2208{ 1,...,m }.\\nThis general formulation does not make any conve', 'xity assumptions and can be\\naugmented with equality constraints. It is referred to as the primal problem in\\ncontrast with a related problem introduced later. We will denote byp\u2217 the optimal\\nvalue of t', 'he objective.\\nFor any x \u2208X ,w ew i l ld e n o t eb yg(x) the vector (g1(x),...,g m(x))\u22a4.T h u s ,t h e\\nconstraints can be written as g(x) \u2264 0. To any constrained optimization problem,\\nwe can associate', ' a Lagrange function that plays an important in the analysis of the\\nproblem and its relationship with another related optimization problem.\\nDe\ufb01nition B.7 Lagrangian\\nThe Lagrange function or the Lagran', 'gian associated to the general constrained\\noptimization problem de\ufb01ned in (B.6) is the function de\ufb01ned over X\u00d7 R+ by:\\n\u2200x \u2208X , \u2200\u03b1 \u2265 0, L(x, \u03b1)= f(x)+\\nm\u2211\\ni=1\\n\u03b1igi(x) ,\\nwhere the variables \u03b1i are known a', 's the Lagrange or dual variables with \u03b1 =\\n(\u03b11,...,\u03b1 m)\u22a4.\\nAny equality constraint of the form g(x)=0f o raf u n c t i o ng can be equivalently\\nexpressed by two inequalities: \u2212g(x) \u2264 0a n d+ g(x) \u2264 0. L', 'et \u03b1\u2212 \u2265 0b et h e\\nLagrange variable associated to the \ufb01rst constraint and \u03b1+ \u2265 0 the one associated\\nto the second constraint. The sum of the terms corresponding to these constraints\\nin the de\ufb01nition o', 'f the Lagrange function can therefore be written as \u03b1g(x)w i t h\\n\u03b1 =( \u03b1+ \u2212\u03b1\u2212 ). Thus, in general, for an equality constraintg(x) = 0 the Lagrangian\\nis augmented with a term\u03b1g(x)b u tw i t h\u03b1 \u2208 R not c', 'onstrained to be non-negative.\\nNote that in the case of a convex optimization problem , equality constraints g(x)\\na r er e q u i r e dt ob ea \ufb03 n es i n c eb o t hg(x)a n d\u2212g(x) are required to be con', 'vex.\\nDe\ufb01nition B.8 Dual function\\nThe (Lagrange) dual function associated to the constrained optimization problem is\\nde\ufb01ned by\\n\u2200\u03b1 \u2265 0,F (\u03b1)= i n f\\nx\u2208X\\nL(x, \u03b1)= i n f\\nx\u2208X\\n(\\nf(x)+\\nm\u2211\\ni=1\\n\u03b1igi(x)\\n\u23a1\\n. (B.5', ')B.3 Constrained optimization 355\\nNote that F is always concave, since the Lagrangian is linear with respect to\u03b1 and\\nsince the in\ufb01mum preserves concavity. We further observe that\\n\u2200\u03b1 \u2265 0,F (\u03b1) \u2264 p\u2217, (B', '.6)\\nsince for any feasible x, f(x)+ \u2211m\\ni=1 \u03b1igi(x) \u2264 f(x). The dual function naturally\\nleads to the following optimization problem.\\nDe\ufb01nition B.9 Dual problem\\nThe dual (optimization) problemassociated', ' to the constrained optimization problem\\nis\\nmax\\n\u03b1\\nF(\u03b1)\\nsubject to: \u03b1 \u2265 0 .\\nThe dual problem is always a convex optimization problem (as a maximization of a\\nconcave problem). Let d\u2217 denote optimal valu', 'e. By (B.6), the following inequality\\nalways holds:\\nd\u2217 \u2264 p\u2217 (weak duality).\\nThe di\ufb00erence (p\u2217 \u2212 d\u2217) is known as the duality gap.T h ee q u a l i t yc a s e\\nd\u2217 = p\u2217 (strong duality)\\ndoes not hold in ge', 'neral. However, strong duality does hold when convex problems\\nsatisfy a constraint quali\ufb01cation. We will denote by int(X) the interior of the set\\nX.\\nDe\ufb01nition B.10 Strong constraint quali\ufb01cation\\nAssum', 'e that int(X) \u0338= \u2205. Then, the strong constraint quali\ufb01cation or Slater\u2019s\\ncondition is de\ufb01ned as\\n\u2203x \u2208 int(X): g(x) < 0. (B.7)\\nAf u n c t i o nh: X\u2192 R is said to be a\ufb03ne if it can be de\ufb01ned for all x \u2208X', ' by\\nh(x)= w \u00b7 x + b,f o rs o m ew \u2208 RN and b \u2208 R.\\nDe\ufb01nition B.11 Weak constraint quali\ufb01cation\\nAssume that int(X) \u0338= \u2205. Then, the weak constraint quali\ufb01cation or weak Slater\u2019s\\ncondition is de\ufb01ned as\\n\u2203x', ' \u2208 int(X): \u2200i \u2208 [1,m],\\n(\\ngi(x) < 0\\n\u23a1\\n\u2228\\n(\\ngi(x)=0 \u2227 gi a\ufb03ne\\n\u23a1\\n. (B.8)356 Convex Optimization\\nWe next present su\ufb03cient and necessary conditions for solutions to constrained\\noptimization problems, based ', 'on the saddle point of the Lagrangian and Slater\u2019s\\ncondition.\\nTheorem B.5 Saddle point \u2014 su\ufb03cient condition\\nLet P be a constrained optimization problem over X = R\\nN.I f (x\u2217,\u03b1\u2217) is a saddle\\npoint of th', 'e associated Lagrangian, that is,\\n\u2200x \u2208 RN , \u2200\u03b1 \u2265 0, L(x\u2217, \u03b1) \u2264L (x\u2217, \u03b1\u2217) \u2264L (x, \u03b1\u2217), (B.9)\\nthen (x\u2217, \u03b1\u2217) is a solution of the problem P.\\nProof By the \ufb01rst inequality, the following holds:\\n\u2200\u03b1 \u2265 0, L(x\u2217', ',\u03b1) \u2264L (x\u2217,\u03b1\u2217) \u21d2\u2200 \u03b1 \u2265 0, \u03b1 \u00b7 g(x\u2217) \u2264 \u03b1\u2217 \u00b7 g(x\u2217)\\n\u21d2 g(x\u2217) \u2264 0 \u2227 \u03b1\u2217 \u00b7 g(x\u2217)=0 , (B.10)\\nwhere g(x\u2217) \u2264 0 in (B.10) follows by letting \u03b1 \u2192 +\u221e and \u03b1\u2217 \u00b7 g(x\u2217) = 0 follows\\nby letting \u03b1 \u2192 0. In view of (B.10), ', 'the second inequality in (B.9) gives,\\n\u2200x, L(x\u2217,\u03b1\u2217) \u2264L (x, \u03b1\u2217) \u21d2\u2200 x,f (x\u2217) \u2264 f(x)+ \u03b1\u2217 \u00b7 g(x).\\nThus, for all x satisfying the constraints, that is g(x) \u2264 0, we have\\nf(x\u2217) \u2264 f(x),\\nwhich completes the pro', 'of.\\nTheorem B.6 Saddle point \u2014 necessary condition\\nAssume that f and gi, i \u2208 [1,m],a r econvex functions and that Slater\u2019s condition\\nholds. Then, if x is a solution of the constrained optimization pro', 'blem, then there\\nexists \u03b1 \u2265 0 such that (x, \u03b1) is a saddle point of the Lagrangian.\\nTheorem B.7 Saddle point \u2014 necessary condition\\nAssume that f and gi, i \u2208 [1,m],a r econvex di\ufb00erentiable functions a', 'nd that the\\nweak Slater\u2019s condition holds. If x is a solution of the constrained optimization\\nproblem, then there exists\u03b1 \u2265 0 such that (x, \u03b1) is a saddle point of the Lagrangian.\\nWe conclude with a t', 'heorem providing necessary and su\ufb03cient optimality con-\\nditions when the problem is convex, the objective function di\ufb00erentiable, and the\\nconstraints quali\ufb01ed.\\nTheorem B.8 Karush-Kuhn-Tucker\u2019s theorem', '\\nAssume that f,g\\ni : X\u2192 R, \u2200i \u2208{ 1,...,m } are convex and di\ufb00erentiable and that\\nthe constraints are quali\ufb01ed. Then x is a solution of the constrained program if andB.4 Chapter notes 357\\nif only there', ' exists \u03b1 \u2265 0 such that,\\n\u2207xL(x,\u03b1)= \u2207xf(x)+ \u03b1 \u00b7\u2207 xg(x) = 0 (B.11)\\n\u2207\u03b1 L(x,\u03b1)= g(x) \u2264 0 (B.12)\\n\u03b1 \u00b7 g(x)=\\nm\u2211\\ni=1\\n\u03b1ig(xi)=0 . (B.13)\\nThe conditions B.11\u2013B.13 are known as theKKT conditions. Note that the l', 'ast two\\nKKT conditions are equivalent to\\ng(x) \u2264 0 \u2227 (\u2200i \u2208{ 1,...,m }, \u00af\u03b1igi(x)=0 ) . (B.14)\\nThese equalities are known as complementarity conditions.\\nProof For the forward direction, since the constra', 'ints are quali\ufb01ed, if x is a\\nsolution, then there exists\u03b1 such that the (x,\u03b1) is a saddle point of the Lagrangian\\nand all three conditions are satis\ufb01ed (the \ufb01rst condition follows by de\ufb01nition of a\\nsa', 'ddle point, and the second two conditions follow from (B.10)).\\nIn the opposite direction, if the conditions are met, then for any x such that\\ng(x) \u2264 0, we can write\\nf(x) \u2212 f(\\nx) \u2265\u2207 xf(x) \u00b7 (x \u2212 x) (co', 'nvexity of f)\\n\u2265\u2212\\nm\u2211\\ni=1\\n\u03b1i\u2207xgi(x) \u00b7 (x \u2212 x) (\ufb01rst condition)\\n\u2265\u2212\\nm\u2211\\ni=1\\n\u03b1i[gi(x) \u2212 gi(x)] (convexity of gis)\\n\u2265\u2212\\nm\u2211\\ni=1\\n\u03b1igi(x) \u2265 0, (third and second condition)\\nwhich shows that f(x) is the minimum of ', 'f over the set of points satisfying the\\nconstraints.\\nB.4 Chapter notes\\nThe results presented in this appendix are based on three main theorems: theo-\\nrem B.1 due to Fermat (1629); theorem B.5 due to L', 'agrange (1797), and theo-\\nrem B.8 due to Karush [1939] and Kuhn and Tucker [1951].\\nFor a more extensive material on convex optimization, we strongly recommend\\nthe book of Boyd and Vandenberghe [2004].', 'Appendix C Probability Review\\nIn this appendix, we give a brief review of some basic notions of probability and\\nwill also de\ufb01ne the notation that is used throughout the textbook.\\nC.1 Probability\\nA pro', 'bability spaceis a model based on three components: asample space,a n events\\nset,a n da probability distribution:\\nsample space \u03a9: \u03a9 is the set of all elementary events or outcomes possible in a\\ntrial,', ' for example, each of the six outcomes in {1,..., 6} when casting a die.\\nevents set F: F is a \u03c3-algebra, that is a set of subsets of \u03a9 containing \u03a9 that\\nis closed under complementation and countable u', 'nion (therefore also countable\\ni n t e r s e c t i o n ) .A ne x a m p l eo fa ne v e n tm a yb e\u201c t h ed i el a n d so na no d dn u m b e r \u201d .\\nprobability distribution: Pr is a mapping from the set ', 'of all eventsF to [0, 1] such\\nthat Pr[\u03a9] = 1 and, for all mutually exclusive eventsA1,...,A n,\\nPr[A1 \u222a ... \u222a An]=\\nn\u2211\\ni=1\\nPr[Ai].\\nThe discrete probability distribution associated with a fair die can be', ' de\ufb01ned by\\nPr[Ai]=1 /6f o ri \u2208{ 1 ... 6},w h e r eAi is the event that the die lands on value i.\\nC.2 Random variables\\nDe\ufb01nition C.1 Random variables\\nA random variable X is a function X :\u03a9 \u2192 R that is ', 'measurable, that is such that\\nfor any interval I, the subset of the sample space {\u03c9 \u2208 \u03a9: X(\u03c9) \u2208 I} is an event.\\nThe probability mass function of a discrete random variable X is de\ufb01ned as the\\nfunction ', 'x \u21a6\u2192 Pr[X = x]. The joint probability mass function of discrete random360 Probability Review\\n0 10 20 300\\n0.05\\n0.1\\n0.15\\nFigure C.1 Approximation of the binomial distribution (in red) by a normal\\ndistri', 'bution (in blue).\\nvariables X and Y is de\ufb01ned as the function ( x, y) \u21a6\u2192 Pr[X = x \u2227 Y = y].\\nA probability distribution is said to be absolutely continuous when it admits a\\nprobability density function', ', that is a functionf associated to a real-valued random\\nvariable X that satis\ufb01es for all a, b \u2208 R\\nPr[a \u2264 X \u2264 b]=\\n\u222b b\\na\\nf(x)dx . (C.1)\\nDe\ufb01nition C.2 Binomial distribution\\nA random variable X is said t', 'o follow a binomial distribution B(n, p) with n \u2208 N\\nand p \u2208 [0, 1] if for any k \u2208{ 0,1,...,n },\\nPr[X = k]=\\n(n\\nk\\n\u23a1\\npk(1 \u2212 p)n\u2212k .\\nDe\ufb01nition C.3 Normal distribution\\nA random variableX is said to follow ', 'anormal (or Gaussian) distribution N(\u03bc, \u03c32)\\nwith \u03bc \u2208 R and \u03c3> 0 if its probability density function is given by,\\nf(x)= 1\u221a\\n2\u03c0\u03c32 exp\\n(\\n\u2212 (x \u2212 \u03bc)2\\n2\u03c32\\n\u23a1\\n.\\nThe standard normal distribution N(0, 1) is the ', 'normal distribution with zero mean\\nand unit variance.\\nThe normal distribution is often used to approximate a binomial distribution.\\nFigure C.1 illustrates that approximation.\\nDe\ufb01nition C.4 Laplace dis', 'tributionC.3 Conditional probability and independence 361\\nA random variableX is said to follow aLaplace distributionwith location parameter\\n\u03bc \u2208 R and scale parameter b> 0 if its probability density fu', 'nction is given by,\\nf(x)= 1\\n2b exp\\n(\\n\u2212 |x \u2212 \u03bc|\\nb\\n\u23a1\\n.\\nDe\ufb01nition C.5 Poisson distribution\\nA random variable X is said to follow a Poisson distribution with \u03bb> 0 if for any\\nk \u2208 N,\\nPr[X = k]= \u03bbke\u2212\u03bb\\nk! .\\nT', 'he de\ufb01nition of the following family of distributions uses the notion of indepen-\\ndence of random variables de\ufb01ned in the next section.\\nDe\ufb01nition C.6 \u03c72-squared distribution\\nThe \u03c72-distribution (or ch', 'i-squared distribution)w i t hk degrees of freedom is the\\ndistribution of the sum of the squares of k independent random variables, each\\nfollowing a standard normal distribution.\\nC.3 Conditional proba', 'bility and independence\\nDe\ufb01nition C.7 Conditional probability\\nThe conditional probability of event A given event B is de\ufb01ned by\\nPr[A | B]= Pr[A \u2229 B]\\nPr[B] , (C.2)\\nwhen Pr[B] \u0338=0 .\\nDe\ufb01nition C.8 Indepe', 'ndence\\nTwo events A and B are said to be independent if\\nPr[A \u2229 B]=P r [A]P r [B]. (C.3)\\nEquivalently, A and B are independent i\ufb00Pr[A | B]=P r [A] when Pr[B] \u0338=0 .\\nA sequence of random variables is sai', 'd to beindependently and identically distributed\\n(i.i.d.) when the random variables are mutually independent and follow the same\\ndistribution.\\nThe following are basic probability formulae related to t', 'he notion of conditional\\nprobability. They hold for any events A, B,a n d A1,...,A n, with the additional362 Probability Review\\nconstraint Pr[B] \u0338= 0 needed for the Bayes formula to be well de\ufb01ned:\\nPr', '[A \u222a B]=P r [A]+P r [B] \u2212 Pr[A \u2229 B]( sum rule)( C . 4 )\\nPr[\\nn\u22c3\\ni=1\\nAi] \u2264\\nn\u2211\\ni=1\\nPr[Ai]( union bound)( C . 5 )\\nPr[A | B]= Pr[B | A]P r [A]\\nPr[B] (Bayes formula)( C . 6 )\\nPr[\\nn\u22c2\\ni=1\\nAi]=P r [A1]P r [A2 ', '| A1] \u00b7\u00b7\u00b7 Pr[An |\\nn\u22121\u22c2\\ni=1\\nAi]( chain rule). (C.7)\\nThe sum rule follows immediately from the decomposition ofA \u222aB as the union of\\nthe disjoint sets A and (B \u2212 A \u2229B). The union bound is a direct conseq', 'uence of the\\nsum rule. The Bayes formula follows immediately from the de\ufb01nition of conditional\\nprobability and the observation that: Pr[A|B]P r [B]=P r [B|A]P r [A]=P r [A \u2229 B].\\nSimilarly, the chain r', 'ule follows the observation that Pr[A1]P r [A2|A1]=P r [A1 \u2229A2];\\nusing the same argument shows recursively that the product of the \ufb01rst k terms of\\nt h er i g h t - h a n ds i d ee q u a l sP r [\u22c2k\\ni=1', ' Ai].\\nFinally, assume that \u03a9 = A1 \u222a A2 \u222a ... \u222a An with Ai \u2229 Aj = \u2205for i \u0338= j, i.e., the\\nAis are mutually disjoint. Then, the following formula is valid for any eventB:\\nPr[B]=\\nn\u2211\\ni=1\\nPr[B | Ai]P r [Ai]', '( theorem of total probability). (C.8)\\nThis follows the observation that Pr[B | Ai]P r [Ai]=P r [B \u2229Ai] by de\ufb01nition of the\\nconditional probability and the fact that the events B \u2229 Ai are mutually dis', 'joint.\\nExample C.1 Application of the Bayes formula\\nLet H be a set of hypotheses. The maximum a posteriori (MAP) principle consists\\nof selecting the hypothesis \u02c6h \u2208 H that is the most probable given t', 'he observation\\nO. Thus, by the Bayes formula, it is given by\\n\u02c6h =a r g m a x\\nh\u2208H\\nPr[h|O] = argmax\\nh\u2208H\\nPr[O|h]P r [h]\\nPr[O] =a r g m a x\\nh\u2208H\\nPr[O|h]P r [h]. (C.9)\\nNow, suppose we need to determine if a', ' patient has a rare disease, given a laboratory\\ntest of that patient. The hypothesis set is reduced to the two outcomes:d (disease)\\nand nd (no disease), thus H = {d, nd}. The laboratory test is either', ' pos (positive)\\nor neg (negative), thus O = {pos, neg}.\\nSuppose that the disease is rare, say Pr[ d]= .005 and that the laboratory is\\nrelatively accurate: Pr[ pos|d]= .98, and Pr[ neg|nd]= .95. Then, ', 'if the test is\\npositive, what should be the diagnosis? We can compute the right-hand side ofC.4 Expectation, Markov\u2019s inequality , and Moment-Generating function 363\\n(C.9) for both hypotheses to deter', 'mine \u02c6h:\\nPr[pos|d]P r [d]= .98 \u00d7 .005 = .0049\\nPr[pos|nd]P r [nd]=( 1 \u2212 .95) \u00d7 .(1 \u2212 .005) = .04975 >. 0049.\\nThus, in this case, the MAP prediction is \u02c6h = nd: with the values indicated, a\\npatient with', ' a positive test result is nonetheless more likely not to have the disease!\\nC.4 Expectation, Markov\u2019s inequality, and oment- enerating\\nfunction\\nDe\ufb01nition C.9 Expectation\\nThe expectation or mean of a r', 'andom variable X is denoted by E[X] and de\ufb01ned\\nby\\nE[X]=\\n\u2211\\nx\\nxPr[X = x]. (C.10)\\nWhen X follows a probability distribution D,w ew i l la l s ow r i t eEx\u223cD[x] instead of\\nE[X] to explicitly indicate the ', 'distribution. A fundamental property of expectation,\\nwhich is straightforward to verify using its de\ufb01nition, is that it is linear, that is, for\\nany two random variables X and Y and any a, b \u2208 R, the f', 'ollowing holds:\\nE[aX + bY ]= aE[X]+ b E[Y ]. (C.11)\\nFurthermore, when X and Y are independent random variables, then the following\\nidentity holds:\\nE[XY ]=E [ X]E [Y ]. (C.12)\\nIndeed, by de\ufb01nition of e', 'xpectation and of independence, we can write\\nE[XY ]=\\n\u2211\\nx,y\\nxy Pr[X = x \u2227 Y = y]=\\n\u2211\\nx,y\\nxy Pr[X = x]P r [Y = y]\\n=\\n(\u2211\\nx\\nxPr[X = x]\\n\u23a1(\u2211\\ny\\ny Pr[Y = y]\\n\u23a1\\n,\\nwhere in the last step we used Fubini\u2019s theorem .', ' The following provides a simple\\nbound for a non-negative random variable in terms of its expectation, known as\\nMarkov\u2019s inequality.\\nmg364 Probability Review\\nTheorem C.1 Markov\u2019s inequality\\nLet X be a', ' non-negative random variable with E[X] < \u221e . Then for all t> 0,\\nPr\\n[\\nX \u2265 t E[X]\\n]\\n\u2264 1\\nt . (C.13)\\nProof The proof steps are as follows:\\nPr[X \u2265 t E[X]] =\\n\u2211\\nx\u2265t E[X]\\nPr[X = x] (by de\ufb01nition)\\n\u2264\\n\u2211\\nx\u2265t E[X', ']\\nPr[X = x] x\\nt E[X]\\n(\\nusing x\\nt E[X] \u2265 1\\n\u23a1\\n\u2264\\n\u2211\\nx\\nPr[X = x] x\\nt E[X] (extending non-negative sum)\\n=E\\n[ X\\nt E[X]\\n]\\n= 1\\nt (linearity of expectation).\\nThis concludes the proof.\\nThe following function bas', 'ed on the notion of expectation is often useful in the\\nanalysis of the properties of a distribution.\\nDe\ufb01nition C.10 Moment-generating function\\nThe moment-generating function of a random variableX is t', 'he functiont \u21a6\u2192 E[etX]\\nde\ufb01ned over the set of t \u2208 R for which the expectation is \ufb01nite.\\nWe will present in the next chapter a general bound on the moment-generating\\nfunction of a zero-mean bounded ran', 'dom variable (Lemma D.1). Here, we illustrate\\nits computation in the case of a \u03c7\\n2-distribution.\\nExample C.2 Moment-generating function of \u03c72-distribution\\nLet X be a random variable following a \u03c72-squ', 'ared distribution with k degrees of\\nfreedom. We can write X = \u2211k\\ni=1 X2\\ni where the Xis are independent and follow a\\nstandard normal distribution.\\nLet t< 1/2. By the i.i.d. assumption about the variab', 'les Xi, we can write\\nE[etX]=E\\n[ k\u220f\\ni=1\\netX2\\ni\\n]\\n=\\nk\u220f\\ni=1\\nE\\n[\\netX2\\ni\\n]\\n=E\\n[\\netX2\\n1\\n]k\\n.\\nBy de\ufb01nition of the standard normal distribution, we have\\nE[etX2\\n1 ]= 1\u221a\\n2\u03c0\\n\u222b +\u221e\\n\u2212\u221e\\netx2\\ne\\n\u2212 x2\\n2 dx = 1\u221a\\n2\u03c0\\n\u222b +\u221e', '\\n\u2212\u221e\\ne(1\u22122t) \u2212 x2\\n2 dx\\n= 1\u221a\\n2\u03c0\\n\u222b +\u221e\\n\u2212\u221e\\ne\\n\u2212 u2\\n2\\n\u221a1 \u2212 2tdu =( 1 \u2212 2t)\\n1\\n2 ,C.5 Variance and Chebyshev\u2019s inequality 365\\nwhere we used the change of variable u = \u221a1 \u2212 2tx. In view of that, the moment-\\ngen', 'erating function of the \u03c72-distribution is given by\\n\u2200t< 1/2, E[etX]=( 1 \u2212 2t)\\nk\\n2 . (C.14)\\nC.5 Variance and Chebyshev\u2019s inequality\\nDe\ufb01nition C.11 Variance \u2014 Standard deviation\\nThe variance of a random', ' variable X is denoted by Var[X] a n dd e \ufb01 n e db y\\nVar[X]=E [ (X \u2212 E[X])2]. (C.15)\\nThe standard deviation of a random variable X is denoted by \u03c3X a n dd e \ufb01 n e db y\\n\u03c3X =\\n\u221a\\nVar[X]. (C.16)\\nFor any ra', 'ndom variable X and any a \u2208 R, the following basic properties hold for\\nthe variance, which can be proven straightforwardly:\\nVar[X]=E [ X2] \u2212 E[X]2 (C.17)\\nVar[aX]= a2 Var[X]. (C.18)\\nFurthermore, when X', ' and Y are independent , then\\nVar[X + Y ]=V a r [X]+V a r [Y ]. (C.19)\\nIndeed, using the linearity of expectation and the identity E[X]E [Y ] \u2212 E[XY ]=0\\nwhich holds by the independence of X and Y , we', ' can write\\nVar[X + Y ]=E [ (X + Y )2] \u2212 E[X + Y ]2\\n=E [X2 + Y 2 +2 XY ] \u2212 (E[X]2 +E [Y ]2 +2E [XY ])\\n=( E [X2] \u2212 E[X]2)+( E [Y 2] \u2212 E[Y ]2)+2 ( E [X]E [Y ] \u2212 E[XY ])\\n= Var[X]+V a r [Y ].\\nThe following', ' inequality known as Chebyshev\u2019s inequality bounds the deviation\\nof a random variable from its expectation in terms of its standard deviation.366 Probability Review\\nTheorem C.2 Chebyshev\u2019s inequality\\n', 'Let X be a random variable with Var[X] < +\u221e .T h e n ,f o ra l lt> 0, the following\\ninequality holds:\\nPr\\n[\\n|X \u2212 E[X]|\u2265 t\u03c3X\\n]\\n\u2264 1\\nt2 . (C.20)\\nProof Observe that:\\nPr\\n[\\n|X \u2212 E[X]|\u2265 t\u03c3X\\n]\\n=P r [ (X \u2212 E[X]', ')2 \u2265 t2\u03c32\\nX].\\nThe result follows by application of Markov\u2019s inequality to (X \u2212 E[X])2.\\nWe will use Chebyshev\u2019s inequality to prove the following theorem.\\nTheorem C.3 Weak law of large numbers\\nLet (Xn)', 'n\u2208N be a sequence of independent random variables with the same mean\u03bc\\nand variance \u03c32 < \u221e .L e tXn = 1\\nn\\n\u2211n\\ni=1 Xi, then, for any \u03f5> 0,\\nlim\\nn\u2192\u221e\\nPr[|Xn \u2212 \u03bc|\u2265 \u03f5]=0 . (C.21)\\nProof Since the variables are', ' independent, we can write\\nVar[Xn]=\\nn\u2211\\ni=1\\nVar\\n[Xi\\nn\\n]\\n= n\u03c32\\nn2 = \u03c32\\nn .\\nThus, by Chebyshev\u2019s inequality (witht = \u03f5/(Var[Xn])1/2), the following holds:\\nPr[|Xn \u2212 \u03bc|\u2265 \u03f5] \u2264 \u03c32\\nn\u03f52 ,\\nwhich implies (C.21).', '\\nExample C.3 Applying Chebyshev\u2019s inequality\\nSuppose we roll a pair of fair dicen times. Can we give a good estimate of the total\\nvalue of the n rolls? If we compute the mean and variance, we \ufb01nd \u03bc =7', ' n and\\n\u03c32 =3 5/6n (we leave it to the reader to verify these expressions). Thus, applying\\nChebyshev\u2019s inequality, we see that the \ufb01nal sum will lie within 7 n \u00b1 10\\n\u221a\\n35\\n6 n in\\nat least 99 percent of a', 'll experiments. Therefore, the odds are better than 99 to 1\\nthat the sum will be between 6.975M and 7.025M after 1M rolls.\\nDe\ufb01nition C.12 Covariance\\nThe covariance of two random variables X and Y is d', 'enoted by Cov(X,Y ) and\\nde\ufb01ned by\\nCov(X,Y )=E\\n[\\n(X \u2212 E[X])(Y \u2212 E[Y ])\\n]\\n. (C.22)C.5 Variance and Chebyshev\u2019s inequality 367\\nIt is straightforward to see that two random variables X and Y are independe', 'nt\\ni\ufb00 Cov(X,Y ) = 0. The covariance de\ufb01nes a positive semide\ufb01nite and symmetric\\nbilinear form:\\nsymmetry: Cov(X,Y )=C o v (Y,X ) for any two random variables X and Y ;\\nbilinearity: Cov( X + X\u2032,Y )=C o ', 'v ( X,Y )+C o v (X\u2032,Y )a n dC o v (aX, Y)=\\naCov(X,Y ) for any random variables X, X\u2032,a n dY and a \u2208 R;\\npositive semide\ufb01niteness: Cov( X,X )=V a r [X] \u2265 0 for any random variable X.\\nThe following Cauch', 'y-Schwarz inequality holds for random variablesX and Y with\\nVar[X] < +\u221e and Var[Y ] < +\u221e :\\n| Cov(X,Y )|\u2264\\n\u221a\\nVar[X] Var[Y ]. (C.23)\\nThe following de\ufb01nition\\nDe\ufb01nition C.13\\nThe covariance matrix of a vect', 'or of random variables X =( X1,...,X N ) is the\\nmatrix in RN \u00d7N denoted by C(X) a n dd e \ufb01 n e db y\\nC(X)=E\\n[\\n(X \u2212 E[X])(X \u2212 E[X])\u22a4]\\n. (C.24)\\nThus, C(X)=( C o v (Xi,X j))ij. It is straightforward to sh', 'ow that\\nC(X)=E [ XX\u22a4] \u2212 E[X]E [X]\u22a4. (C.25)\\nWe close this appendix with the following well-known theorem of probability.\\nTheorem C.4 Central limit theorem\\nLet X1,...,X n be a sequence of i.i.d. random ', 'variables with mean \u03bc and standard\\ndeviation \u03c3.L e tXn = 1\\nn\\n\u2211n\\ni=1 Xi and \u03c32\\nn = \u03c32/n.T h e n ,(Xn \u2212 \u03bc)/\u03c3n converges\\nto the N(0, 1) in distribution, that is for any t \u2208 R,\\nlim\\nn\u2192\u221e\\nPr[(Xn \u2212 \u03bc)/\u03c3n \u2264 t]', '=\\n\u222b t\\n\u2212\u221e\\n1\u221a\\n2\u03c0e\u2212 x2\\n2 dx .Appendix D Concentration inequalities\\nIn this appendix, we present several concentration inequalities used in the proofs\\ngiven in this book. Concentration inequalities give p', 'robability bounds for a random\\nvariable to be concentrated around its mean, or for it to deviate from its mean or\\nsome other value.\\nD.1 Hoe\ufb00ding\u2019s inequality\\nWe \ufb01rst present Hoe\ufb00ding\u2019s inequality , wh', 'ose proof makes use of the general\\nCherno\ufb00 bounding technique. Given a random variableX and \u03f5> 0, this technique\\nconsists of proceeding as follows to bound Pr[X \u2265 \u03f5]. For any t> 0, \ufb01rst Markov\u2019s\\ninequ', 'ality is used to bound Pr[X \u2265 \u03f5]:\\nPr[X \u2265 \u03f5]=P r [e\\ntX \u2265 et\u03f5] \u2264 e\u2212t\u03f5 E[etX] . (D.1)\\nThen, an upper boundg(t) is found for E[etX]a n dt is selected to minimizee\u2212t\u03f5g(t).\\nFor Hoe\ufb00ding\u2019s inequality, the fo', 'llowing lemma provides an upper bound on E[etX].\\nLemma D.1 Hoe\ufb00ding\u2019s lemma\\nLet X be a random variable with E[X]=0 and a \u2264 X \u2264 b with b>a .T h e n ,f o r\\nany t> 0, the following inequality holds:\\nE[et', 'X] \u2264 e\\nt2(b\u2212 a)2\\n8 . (D.2)\\nProof By the convexity of x \u21a6\u2192 ex, for all x \u2208 [a, b], the following holds:\\netx \u2264 b \u2212 x\\nb \u2212 aeta + x \u2212 a\\nb \u2212 a etb .\\nThus, using E[X]=0 ,\\nE[etX] \u2264 E\\n[b \u2212 X\\nb \u2212 a eta + X \u2212 a', '\\nb \u2212 a etb\\n]\\n= b\\nb \u2212 aeta + \u2212a\\nb \u2212 aetb = e\u03c6(t) ,370 Concentration inequalities\\nwhere,\\n\u03c6(t)=l o g\\n( b\\nb \u2212 aeta + \u2212a\\nb \u2212 aetb\\n\u23a1\\n= ta +l o g\\n( b\\nb \u2212 a + \u2212a\\nb \u2212 aet(b\u2212a)\\n\u23a1\\n.\\nFor any t> 0, the \ufb01rst and se', 'cond derivative of \u03c6 are given below:\\n\u03c6\u2032(t)= a \u2212 aet(b\u2212a)\\nb\\nb\u2212a \u2212 a\\nb\u2212a et(b\u2212a) = a \u2212 a\\nb\\nb\u2212a e\u2212t(b\u2212a) \u2212 a\\nb\u2212a\\n,\\n\u03c6\u2032\u2032(t)= \u2212abe\u2212t(b\u2212a)\\n[ b\\nb\u2212a e\u2212t(b\u2212a) \u2212 a\\nb\u2212a ]2\\n= \u03b1(1 \u2212 \u03b1)e\u2212t(b\u2212a)(b \u2212 a)2\\n[(1 \u2212 \u03b1)e\u2212t(', 'b\u2212a) + \u03b1]2\\n= \u03b1\\n[(1 \u2212 \u03b1)e\u2212t(b\u2212a) + \u03b1]\\n(1 \u2212 \u03b1)e\u2212t(b\u2212a)\\n[(1 \u2212 \u03b1)e\u2212t(b\u2212a) + \u03b1](b \u2212 a)2 .\\nwhere \u03b1 denotes \u2212a\\nb\u2212a . Note that \u03c6(0) = \u03c6\u2032(0) = 0 and that\u03c6\u2032\u2032(t)= u(1 \u2212 u)(b \u2212 a)2\\nwhere u = \u03b1\\n[(1\u2212\u03b1)e\u2212 t(b\u2212 a)+\u03b1', '] .S i n c eu is in [0, 1], u(1 \u2212 u) is upper bounded by 1 /4\\nand \u03c6\u2032(t) \u2264 (b\u2212a)2\\n4 . Thus, by the second order expansion of function \u03c6, there exists\\n\u03b8 \u2208 [0,t] such that:\\n\u03c6(t)= \u03c6(0) +t\u03c6\u2032(0) + t2\\n2 \u03c6\u2032\u2032(', '\u03b8) \u2264 t2 (b \u2212 a)2\\n8 , (D.3)\\nwhich completes the proof.\\nThe lemma can be used to prove the following result known asHoe\ufb00ding\u2019s inequality.\\nTheorem D.1 Hoe\ufb00ding\u2019s inequality\\nLet X1,...,X m be independent', ' random variables withXi taking values in [ai,b i] for\\nall i \u2208 [1,m ]. Then for any\u03f5> 0, the following inequalities hold forSm = \u2211m\\ni=1 Xi:\\nPr[Sm \u2212 E[Sm] \u2265 \u03f5] \u2264 e\u22122\u03f52/ Pm\\ni=1(bi\u2212ai)2\\n(D.4)\\nPr[Sm \u2212 E[S', 'm] \u2264\u2212 \u03f5] \u2264 e\u22122\u03f52/ Pm\\ni=1(bi\u2212ai)2\\n. (D.5)\\nProof Using the Cherno\ufb00 bounding technique and lemma D.1, we can write:\\nPr[Sm \u2212 E[Sm] \u2265 \u03f5] \u2264 e\u2212t\u03f5 E[et(Sm\u2212E[Sm])]\\n=\u03a0 m\\ni=1e\u2212t\u03f5 E[et(Xi\u2212E[Xi])] (independence of', ' Xis)\\n\u2264 \u03a0m\\ni=1e\u2212t\u03f5et2(bi\u2212ai)2/8 (lemma D.1)\\n= e\u2212t\u03f5et2 Pm\\ni=1(bi\u2212ai)2/8\\n\u2264 e\u22122\u03f52/ Pm\\ni=1(bi\u2212ai)2\\n,D.2 McDiarmid\u2019s inequality 371\\nw h e r ew ec h o s et =4 \u03f5/ \u2211m\\ni=1(bi \u2212 ai)2 to minimize the upper bound', '. This proves\\nthe \ufb01rst statement of the theorem, and the second statement is shown in a similar\\nway.\\nWhen the variance \u03c32\\nXi of each random variable Xi is known and the \u03c32\\nXi sa r e\\nrelatively small, ', 'better concentration bounds can be derived (see Bennett\u2019s and\\nBernstein\u2019s inequalities proven in exercise D.4).\\nD.2 McDiarmid\u2019s inequality\\nThis section presents a concentration inequality that is more', ' general than Hoe\ufb00d-\\ning\u2019s inequality. Its proof makes use of a Hoe\ufb00ding\u2019s inequality for martingale dif-\\nferences.\\nDe\ufb01nition D.1 Martingale Di\ufb00erence\\nA sequence of random variables V1,V2,... is a mar', 'tingale di\ufb00erence sequence with\\nrespect to X1,X2,... if for all i> 0, Vi is a function of X1,...,X i and\\nE[Vi+1|X1,...,X i]=0 . (D.6)\\nThe following result is similar to Hoe\ufb00ding\u2019s lemma.\\nLemma D.2\\nLet', ' V and Z be random variables satisfying E[V |Z]=0 and, for some function f\\nand constant c \u2265 0, the inequalities:\\nf(Z) \u2264 V \u2264 f(Z)+ c. (D.7)\\nThen, for all t> 0, the following upper bound holds:\\nE[esV |Z', '] \u2264 et2c2/8 . (D.8)\\nProof The proof follows using the same steps as in that of lemma D.1 with\\nconditional expectations used instead of expectations: conditioned on Z, V takes\\nvalues in [a, b]w i t ha ', '= f(Z)a n db = f(Z)+ c and its expectation vanishes.\\nThe lemma is used to prove the following theorem, which is one of the main results\\nof this section.\\nTheorem D.2 Azuma\u2019s inequality\\nLet V1,V2,... be', ' a martingale di\ufb00erence sequence with respect to the random vari-\\nables X1,X2,... , and assume that for all i> 0 there is a constant ci \u2265 0 and372 Concentration inequalities\\nrandom variable Zi, which ', 'is a function of X1,...,X i\u22121, that satisfy\\nZi \u2264 Vi \u2264 Zi + ci . (D.9)\\nThen, for all \u03f5> 0 and m, the following inequalities hold:\\nPr\\n[ m\u2211\\ni=1\\nVi \u2265 \u03f5\\n]\\n\u2264 exp\\n( \u22122\u03f52\\n\u2211m\\ni=1 c2\\ni\\n\u23a1\\n(D.10)\\nPr\\n[ m\u2211\\ni=1\\nVi \u2264', '\u2212 \u03f5\\n]\\n\u2264 exp\\n( \u22122\u03f52\\n\u2211m\\ni=1 c2\\ni\\n\u23a1\\n. (D.11)\\nProof For any k \u2208 [1,m], let Sk = \u2211k\\ni=1 Vk. Then, using Cherno\ufb00\u2019s bounding\\ntechnique, for any t> 0, we can write\\nPr\\n[\\nSm \u2265 \u03f5\\n]\\n\u2264 e\u2212t\u03f5 E[etSm ]\\n= e\u2212t\u03f5 E\\n[\\netS', 'm\u2212 1 E[etVm |X1,...,X m\u22121]\\n]\\n\u2264 e\u2212t\u03f5 E[etSm\u2212 1 ]et2c2\\nm/8 (lemma D.2)\\n\u2264 e\u2212t\u03f5et2 Pm\\ni=1 c2\\ni /8 (iterating previous argument)\\n= e\u22122\u03f52/ Pm\\ni=1 c2\\ni ,\\nw h e r ew ec h o s et =4 \u03f5/ \u2211m\\ni=1 c2\\ni to minimize ', 'the upper bound. This proves the \ufb01rst\\nstatement of the theorem, and the second statement is shown in a similar way.\\nThe following is the second main result of this section. Its proof makes use of\\nAzum', 'a\u2019s inequality.\\nTheorem D.3 McDiarmid\u2019s inequality\\nLet X1,...,X m \u2208X m be a set of m \u2265 1 independent random variables and\\nassume that there exist c1,...,c m > 0 such that f : Xm \u2192 R satis\ufb01es the follo', 'wing\\nconditions:\\n\u23d0\u23d0f(x1,...,x i,...,x m) \u2212 f(x1,...,x \u2032\\ni,...x m)\\n\u23d0\u23d0 \u2264 ci , (D.12)\\nfor all i \u2208 [1,m] and any pointsx1,...,x m,x \u2032\\ni \u2208X .L e tf(S) denote f(X1,...,X m),\\nthen, for all \u03f5> 0, the followin', 'g inequalities hold:\\nPr[f(S) \u2212 E[f(S)] \u2265 \u03f5] \u2264 exp\\n( \u22122\u03f52\\n\u2211m\\ni=1 c2\\ni\\n\u23a1\\n(D.13)\\nPr[f(S) \u2212 E[f(S)] \u2264\u2212 \u03f5] \u2264 exp\\n( \u22122\u03f52\\n\u2211m\\ni=1 c2\\ni\\n\u23a1\\n. (D.14)\\nProof De\ufb01ne a sequence of random variables Vk, k \u2208 [1,m], as f', 'ollows: V =D.3 Other inequalities 373\\nf(S) \u2212 E[f(S)], V1 =E [V |X1] \u2212 E[V ], and for k> 1,\\nVk =E [V |X1,...,X k] \u2212 E[V |X1,...,X k\u22121] .\\nNote that V = \u2211m\\nk=1 Vk. Furthermore, the random variable E[ V |', 'X1,...,X k]i sa\\nfunction of X1,...,X k. Conditioning on X1,...,X k\u22121 and taking its expectation is\\ntherefore:\\nE\\n[\\nE[V |X1,...,X k]|X1,...,X k\u22121\\n]\\n=E [V |X1,...,X k\u22121],\\nwhich implies E[Vk|X1,...,X k\u22121]', ' = 0. Thus, the sequence (Vk)k\u2208[1,m] is a martin-\\ngale di\ufb00erence sequence. Next, observe that, since E[ f(S)] is a scalar, Vk can be\\nexpressed as follows:\\nVk =E [f(S)|X1,...,X k] \u2212 E[f(S)|X1,...,X k\u22121', '] .\\nThus, we can de\ufb01ne an upper bound Wk and lower bound Uk for Vk by:\\nWk =s u p\\nx\\nE[f(S)|X1,...,X k\u22121,x] \u2212 E[f(S)|X1,...,X k\u22121]\\nUk =i n f\\nx\\nE[f(S)|X1,...,X k\u22121,x] \u2212 E[f(S)|X1,...,X k\u22121].\\nNow, by (D.1', '2), for any k \u2208 [1,m], the following holds:\\nWk \u2212 Uk =s u p\\nx,x\u2032\\nE[f(S)|X1,...,X k\u22121,x] \u2212 E[f(S)|X1,...,X k\u22121,x \u2032] \u2264 ck , (D.15)\\nthus, Uk \u2264 Vk \u2264 Uk + ck. In view of these inequalities, we can apply Azu', 'ma\u2019s\\ninequality to V = \u2211m\\nk=1 Vk, which yields exactly (D.13) and (D.14).\\nMcDiarmid\u2019s inequality is used in several of the proofs in this book. It can be\\nunderstood in terms of stability: if changing ', 'any of its argument a\ufb00ectsf only in a\\nlimited way, then, its deviations from its mean can be exponentially bounded. Note\\nalso that Hoe\ufb00ding\u2019s inequality (theorem D.1) is a special instance of McDiarmi', 'd\u2019s\\ninequality where f is de\ufb01ned by f :( x\\n1,...,x m) \u21a6\u2192 1\\nm\\n\u2211m\\ni=1 xi.\\nD.3 Other inequalities\\nThis section presents several other inequalities useful in the proofs of various results\\npresented in thi', 's book.374 Concentration inequalities\\nD.3.1 Binomial distribution: Slud\u2019s inequality\\nLet B(m, p) be a binomial random variable and k an integer such that p \u2264 1\\n4 and\\nk \u2265 mp or p \u2264 1\\n2 and mp \u2264 k \u2264 m(1', ' \u2212 p). Then, the following inequality holds:\\nPr[B \u2265 k] \u2265 Pr\\n[\\nN \u2265 k \u2212 mp\u221a\\nmp(1 \u2212 p)\\n]\\n, (D.16)\\nwhere N is in standard normal form.\\nD.3.2 Normal distribution: tail bound\\nIf N is a random variable follo', 'wing the standard normal distribution, then foru> 0,\\nPr[N \u2265 u] \u2265 1\\n2\\n(\\n1 \u2212\\n\u221a\\n1 \u2212 e\u2212u2\\n\u23a1\\n. (D.17)\\nD.3.3 Khintchine-Kahane inequality\\nThe following inequality is useful in a variety of di\ufb00erent contexts', ', including in the\\nproof of a lower bound for the empirical Rademacher complexity of linear hypotheses\\n(chapter 5).\\nTheorem D.4 Khintchine-Kahane inequality\\nLet (H, \u2225\u00b7\u2225 ) be a normed vector space and ', 'let x\\n1,..., xm be m \u2265 1 elements of\\nH.L e t\u03c3 =( \u03c31,...,\u03c3 m)\u22a4 with \u03c3is independent uniform random variables taking\\nvalues in {\u22121, +1} (Rademacher variables). Then, the following inequalities hold:\\n1\\n2', ' E\\n\u03c3\\n[\\ued79\\ued79\\ued79\\nm\u2211\\ni=1\\n\u03c3ixi\\n\\ued79\\ued79\\ued79\\n2]\\n\u2264\\n(\\nE\\n\u03c3\\n[\\ued79\\ued79\\ued79\\nm\u2211\\ni=1\\n\u03c3ixi\\n\\ued79\\ued79\\ued79\\n]\u23a12\\n\u2264 E\\n\u03c3\\n[\\ued79\\ued79\\ued79\\nm\u2211\\ni=1\\n\u03c3ixi\\n\\ued79\\ued79\\ued79\\n2]\\n. (D.18)\\nProof The second inequality is a direct consequence of the convexity of x \u21a6\u2192 x2\\nand Jensen\u2019s inequa', 'lity (theorem B.4).\\nTo prove the left-hand side inequality, \ufb01rst note that for any \u03b21,...,\u03b2 m \u2208 R,\\nexpanding the product \u220fm\\ni=1(1 + \u03b2i) leads exactly to the sum of all monomi-\\nals \u03b2\u03b41\\n1 \u00b7\u00b7\u00b7 \u03b2\u03b4m\\nm ,w i', ' t he x p o n e n t s\u03b41,...,\u03b4 m in {0,1}. We will use the notation\\n\u03b2\u03b41\\n1 \u00b7\u00b7\u00b7 \u03b2\u03b4m\\nm = \u03b2\u03b4 and |\u03b4| = \u2211m\\ni=1 \u03b4m for any \u03b4 =( \u03b41,...,\u03b4 m) \u2208{ 0, 1}m.I nv i e wo f\\nthat, for any (\u03b11,...,\u03b1 m) \u2208 Rm and t> 0, t', 'he following equality holds:\\nt2\\nm\u220f\\ni=1\\n(1 +\u03b1i/t)= t2 \u2211\\n\u03b4\u2208{0,1}m\\n\u03b1\u03b4/t|\u03b4| =\\n\u2211\\n\u03b4\u2208{0,1}m\\nt2\u2212|\u03b4|\u03b1\u03b4.D.3 Other inequalities 375\\nDi\ufb00erentiating both sides with respect to t and setting t =1y i e l d s\\n2\\nm\u220f\\ni=', '1\\n(1 +\u03b1i) \u2212\\nm\u2211\\nj=1\\n\u03b1j\\n\u220f\\ni\u0338=j\\n(1 +\u03b1i)=\\n\u2211\\n\u03b4\u2208{0,1}m\\n(2 \u2212| \u03b4|)\u03b1\u03b4 . (D.19)\\nFor any \u03c3 \u2208{ \u2212 1, +1}m,l e tS\u03c3 be de\ufb01ned by S\u03c3 = \u2225s\u03c3 \u2225 with s\u03c3 = \u2211m\\ni=1 \u03c3ixi.\\nThen, setting \u03b1i = \u03c3i\u03c3\u2032\\ni, multiplying both sides of', ' (D.19) byS\u03c3 S\u03c3 \u2032 , and taking the\\nsum over all \u03c3,\u03c3\u2032 \u2208{ \u22121, +1}m yields\\n\u2211\\n\u03c3,\u03c3 \u2032\u2208{\u22121,+1}m\\n(\\n2\\nm\u220f\\ni=1\\n(1 +\u03c3i\u03c3\u2032\\ni) \u2212\\nm\u2211\\nj=1\\n\u03c3j\u03c3\u2032\\nj\\n\u220f\\ni\u0338=j\\n(1 +\u03c3i\u03c3\u2032\\ni)\\n\u23a1\\nS\u03c3 S\u03c3 \u2032\\n=\\n\u2211\\n\u03c3,\u03c3 \u2032\u2208{\u22121,+1}m\\n\u2211\\n\u03b4\u2208{0,1}m\\n(2 \u2212| \u03b4|)\u03c3\u03b4\u03c3\u2032', '\u03b4S\u03c3 S\u03c3 \u2032\\n=\\n\u2211\\n\u03b4\u2208{0,1}m\\n(2 \u2212| \u03b4|)\\n\u2211\\n\u03c3,\u03c3 \u2032\u2208{\u22121,+1}m\\n\u03c3\u03b4\u03c3\u2032\u03b4S\u03c3 S\u03c3 \u2032\\n=\\n\u2211\\n\u03b4\u2208{0,1}m\\n(2 \u2212| \u03b4|)\\n[ \u2211\\n\u03c3 \u2208{\u22121,+1}m\\n\u03c3\u03b4S\u03c3\\n]2\\n.\\n(D.20)\\nNote that the terms of the right-hand sum with|\u03b4|\u2265 2 are non-positive. The terms\\nw', 'ith |\u03b4| = 1 are null: sinceS\u03c3 = S\u2212\u03c3 ,w eh a v e\u2211\\n\u03c3 \u2208{\u22121,+1}m \u03c3\u03b4S\u03c3 = 0 in that case.\\nThus, the right-hand side can be upper bounded by the term with \u03b4 =0 ,t h a ti s ,\\n2\\n(\u2211\\n\u03c3 \u2208{\u22121,+1}m S\u03c3\\n\u23a12\\n. The left', '-hand side of (D.20) can be rewritten as follows:\\n\u2211\\n\u03c3 \u2208{\u22121,+1}m\\n(2m+1 \u2212 m2m\u22121)S2\\n\u03c3 +2 m\u22121 \u2211\\n\u03c3 \u2208{\u22121,+1}m\\n\u03c3 \u2032\u2208B(\u03c3,1)\\nS\u03c3 S\u03c3 \u2032\\n=2 m \u2211\\n\u03c3 \u2208{\u22121,+1}m\\nS2\\n\u03c3 +2 m\u22121 \u2211\\n\u03c3 \u2208{\u22121,+1}m\\nS\u03c3\\n( \u2211\\n\u03c3 \u2032 \u2208B(\u03c3,1)\\nS\u03c3 \u2032 \u2212 (m \u2212 2', ')S\u03c3\\n\u23a1\\n,\\n(D.21)\\nwhere B(\u03c3, 1) denotes the set of \u03c3\u2032 that di\ufb00er from \u03c3 in exactly one coordinate\\nj \u2208 [1,m], that is the set of\u03c3\u2032 with Hamming distance one from\u03c3. Note that for any\\nsuch \u03c3\u2032, s\u03c3 \u2212 s\u03c3 \u2032 =2 ', '\u03c3jxj for one coordinate j \u2208 [1,m], thus, \u2211\\n\u03c3 \u2032 \u2208B(\u03c3,1) s\u03c3 \u2212 s\u03c3 \u2032 =\\n2s\u03c3 . In light of that and using the triangle inequality, we can write\\n(m \u2212 2)S\u03c3 = \u2225ms\u03c3 \u2225\u2212\u2225 2s\u03c3 \u2225 =\\n\\ued79\\ued79\\ued79\\n\u2211\\n\u03c3 \u2032 \u2208B(\u03c3,1)\\ns\u03c3\\n\\ued79\\ued79\\ued79 \u2212\\n\\ued79\\ued79\\ued79\\n\u2211\\n', '\u03c3 \u2032\u2208B(\u03c3,1)\\ns\u03c3 \u2212 s\u03c3 \u2032\\n\\ued79\\ued79\\ued79\\n\u2264\\n\\ued79\\ued79\\ued79\\n\u2211\\n\u03c3 \u2032 \u2208B(\u03c3,1)\\ns\u03c3 \u2032\\n\\ued79\\ued79\\ued79 \u2264\\n\u2211\\n\u03c3 \u2032 \u2208B(\u03c3,1)\\nS\u03c3 \u2032 .\\nThus, the second sum of (D.21) is non-negative and the left-hand side of (D.20) can376 Concentration inequalities\\nbe lower ', 'bounded by the \ufb01rst sum 2 m \u2211\\n\u03c3 \u2208{\u22121,+1}m S2\\n\u03c3 . Combining this with the\\nupper bound found for (D.20) gives\\n2m \u2211\\n\u03c3 \u2208{\u22121,+1}m\\nS2\\n\u03c3 \u2264 2\\n[ \u2211\\n\u03c3 \u2208{\u22121,+1}m\\nS\u03c3\\n]2\\n.\\nDividing both sides by 22m and using Pr[\u03c3]', '=1 /2m gives E\u03c3 [S2\\n\u03c3 ] \u2264 2(E\u03c3 [S\u03c3 ])2 and\\ncompletes the proof.\\nThe constant 1/2 appearing in the \ufb01rst inequality of (D.18) is optimal. To see this,\\nconsider the case where m = 2 and x1 = x2 = x for s', 'ome non-zero vector x \u2208 H.\\nThen, the left-hand side of the \ufb01rst inequality is 1\\n2\\n\u2211m\\ni=1 \u2225xi\u22252 = \u2225x\u22252 and the\\nright-hand side\\n(\\nE\u03c3\\n[\\n\u2225(\u03c31 + \u03c32)x\u2225\\n]\u23a12\\n= \u2225x\u22252(E\u03c3 [|\u03c31 + \u03c32|])2 = \u2225x\u22252.\\nNote that when the', ' norm \u2225\u00b7\u2225 corresponds to an inner product, as in the case of\\na Hilbert space H, we can write\\nE\\n\u03c3\\n[\\ued79\\ued79\\ued79\\nm\u2211\\ni=1\\n\u03c3ixi\\n\\ued79\\ued79\\ued79\\n2]\\n=\\nm\u2211\\ni,j=1\\nE\\n\u03c3\\n[\\n\u03c3i\u03c3j(xi \u00b7 xj)\\n]\\n=\\nm\u2211\\ni,j=1\\nE\\n\u03c3\\n[\u03c3i\u03c3j](xi \u00b7 xj)=\\nm\u2211\\ni=1\\n\u2225xi\u22252,\\n', 'since by the independence of the random variables \u03c3i,f o r i \u0338= j,E \u03c3 [\u03c3i\u03c3j]=\\nE\u03c3 [\u03c3i]E\u03c3 [\u03c3j] = 0. Thus, (D.18) can then be rewritten as follows:\\n1\\n2\\nm\u2211\\ni=1\\n\u2225xi\u22252 \u2264\\n(\\nE\\n\u03c3\\n[\\ued79\\ued79\\n\\ued79\\nm\u2211\\ni=1\\n\u03c3ixi\\n\\ued79\\ued79\\n\\ued79\\n]\u23a1\\n2\\n\u2264\\n', 'm\u2211\\ni=1\\n\u2225xi\u22252 . (D.22)\\nD.4 Chapter notes\\nThe improved version of Azuma\u2019s inequality [Hoe\ufb00ding, 1963, Azuma, 1967] pre-\\nsented in this chapter is due to McDiarmid [1989]. The improvement is a reduction\\n', 'of the exponent by a factor of 4. This also appears in McDiarmid\u2019s inequality, which\\nis derived from the inequality for bounded martingale sequences. The inequalities\\npresented in exercise D.4 are due', ' to Bernstein [1927] and Bennett [1962]; the exercise\\nis from Devroye and Lugosi [1995].\\nThe binomial inequality of section D.3.1 is due to Slud [1977]. The tail bound\\nof section D.3.2 is due to Tate ', '[1953] (see also Anthony and Bartlett [1999]). The\\nKhintchine-Kahane inequality was \ufb01rst studied in the case of real-valued variables\\nx\\n1,...,x m by Khintchine [1923], with better constants and simple', 'r proofs later\\nprovided by Szarek [1976], Haagerup [1982], and Tomaszewski [1982]. The inequality\\nwas extended to normed vector spaces by Kahane [1964]. The proof presented here\\nis due to Lata/suppres', 'sla and Oleszkiewicz [1994] and provides the best possible constants.D.5 Exercises 377\\nD.5 Exercises\\nD.1 Twins paradox. Professor Mamoru teaches at a university whose computer\\nscience and math buildin', 'g has F =3 0\ufb02 o o r s .\\n(1) Assume that the \ufb02oors are independent and that they have the same\\nprobability to be selected by someone taking the elevator. How many people\\nshould take the elevator in ord', 'er to make it likely (probability more than half)\\nt h a tt w oo ft h e mg ot ot h es a m e\ufb02 o o r ?(Hint: use the Taylor series expansion of\\ne\\n\u2212x =1 \u2212 x + ... and give an approximate general expressio', 'n of the solution.)\\n(2) Professor Mamoru is popular, and his \ufb02oor is in fact more likely to be\\nselected than others. Assuming that all other \ufb02oors are equiprobable, derive\\nthe general expression of th', 'e probability that two persons go to the same \ufb02oor,\\nusing the same approximation as before. How many people should take the\\nelevator in order to make it likely that two of them go to the same \ufb02oor whe', 'n\\nthe probability of Professor Mamoru\u2019s \ufb02oor is .25, .35, or .5? When q = .5,\\nwould the answer change if the number of \ufb02oors were insteadF =1 ,000?\\n(3) The probability models assumed in (1) and (2) ar', 'e both naive. If you had\\naccess to the data collected by the elevator guard, how would you de\ufb01ne a more\\nfaithful model?\\nD.2 Concentration bounds. Let X be a non-negative random variable satisfying\\nPr[', 'X>t ] \u2264 ce\\n\u22122mt2\\nfor all t> 0a n ds o m ec> 0. Show that E[X2] \u2264 log(ce)\\n2m (Hint:\\nto do that, use the identity E[X2]=\\n\u222b \u221e\\n0 Pr[X2 >t ]dt,w r i t e\\n\u222b \u221e\\n0 =\\n\u222b u\\n0 +\\n\u222b \u221e\\nu ,b o u n d\\nthe \ufb01rst term by u ', 'and \ufb01nd the best u to minimize the upper bound).\\nD.3 Comparison of Hoe\ufb00ding\u2019s and Chebyshev\u2019s inequalities. Let X1,...,X m be\\na sequence of random variables taking values in [0 ,1] with the same mean ', '\u03bc and\\nvariance \u03c32 < \u221e and let X = 1\\nm\\n\u2211m\\ni=1 Xi.\\n(a) For any \u03f5> 0, give a bound on Pr[|X\u2212\u03bc| >\u03f5 ] using Chebyshev\u2019s inequality,\\nthen Hoe\ufb00ding\u2019s inequality. For what values of \u03c3 is Chebyshev\u2019s inequalit', 'y\\ntighter?\\n(b) Assume that the random variables Xi take values in {0, 1}. Show that\\n\u03c32 \u2264 1\\n4 . Use this to simplify Chebyshev\u2019s inequality. Choose \u03f5 = .05 and\\nplot Chebyshev\u2019s inequality thereby modi\ufb01', 'ed and Hoe\ufb00ding\u2019s inequality as a\\nfunction of m (you can use your preferred program for generating the plots).\\nD.4 Bennett\u2019s and Bernstein\u2019s inequalities. The objective of this problem is to prove378 ', 'Concentration inequalities\\nthese two inequalities.\\n(a) Show that for any t> 0, and any random variable X with E[X]=0 ,\\nE[X2]= \u03c32,a n dX \u2264 c,\\nE[etX] \u2264 ef(\u03c32/c2), (D.23)\\nwhere\\nf(x)=l o g\\n( 1\\n1+ xe\u2212ctx +', ' x\\n1+ xect\\n\u23a1\\n.\\n(b) Show that f \u2032\u2032(x) \u2264 0f o rx \u2265 0.\\n(c) Using Cherno\ufb00\u2019s bounding technique, show that\\nPr\\n[ 1\\nm\\nm\u2211\\ni=1\\nXi \u2265 \u03f5\\n]\\n\u2264 e\u2212tm\u03f5+Pm\\ni=1 f(\u03c32\\nXi /c2),\\nwhere (\u03c32\\nXi i st h ev a r i a n c eo fXi.\\n(', 'd) Show that f(x) \u2264 f(0) +xf \u2032(0) = (ect \u2212 1 \u2212 ct)x.\\n(e) Using the bound derived in (4), \ufb01nd the optimal value of t.\\n(f) Bennett\u2019s inequality.L e tX1,...,X m be independent real-valued random\\nvariable', 's with zero mean such that for i =1 ,...,m , Xi \u2264 c.L e t \u03c32 =\\n1\\nm\\n\u2211m\\ni=1 \u03c32\\nXi . Show that\\nPr\\n[ 1\\nm\\nm\u2211\\ni=1\\nXi >\u03f5\\n]\\n\u2264 exp\\n(\\n\u2212 m\u03c32\\nc2 \u03b8\\n( \u03f5c\\n\u03c32\\n\u23a1\u23a1\\n, (D.24)\\nwhere \u03b8(x)=( 1+ x) log(1 +x) \u2212 x.\\n(g) Bernste', 'in\u2019s inequality. Show that under the same conditions as Bennett\u2019s\\ninequality\\nPr\\n[ 1\\nm\\nm\u2211\\ni=1\\nXi >\u03f5\\n]\\n\u2264 exp\\n(\\n\u2212 m\u03f52\\n2\u03c32 +2 c\u03f5/3\\n\u23a1\\n. (D.25)\\n(Hint: show that for all x \u2265 0, \u03b8(x) \u2265 h(x)= 3\\n2\\nx2\\nx+3 .)\\n(h)', ' Write Hoe\ufb00ding\u2019s inequality assuming the same conditions. For what values\\nof \u03c3 is Bernstein\u2019s inequality better than Hoe\ufb00ding\u2019s inequality?Appendix E Notation\\nTa b l e E . 1 Summary of notation.\\nR Se', 't of real numbers\\nR+ Set of non-negative real numbers\\nRn Set of n-dimensional real-valued vectors\\nRn\u00d7m Set of n \u00d7 m real-valued matrices\\n[a, b] Closed interval between a and b\\n(a, b) Open interval bet', 'ween a and b\\n{a, b, c} Set containing elements a, b and c\\nN Set of natural numbers, i.e., {0, 1,... }\\nlog Logarithm with base e\\nloga Logarithm with base a\\nS An arbitrary set\\n|S| Number of elements in ', 'S\\ns \u2208S An element in set S\\nX Input space\\nY Target space\\nH Feature space\\n\u27e8\u00b7, \u00b7\u27e9 Inner product in feature space\\nv An arbitrary vector\\n1 Vector of all ones\\nvi ith component of v\\n\u2225v\u2225 L2 norm of v\\n\u2225v\u2225p Lp ', 'norm of v\\nu \u25e6v Hadamard or entry-wise product of vectors u and v380 Notation\\nf \u25e6g Composition of functions f and g\\nT1 \u25e6T2 C o m p o s i t i o no fw e i g h t e dt r a n s d u c e r sT1 and T2\\nM An arb', 'itrary matrix\\n\u2225M\u22252 Spectral norm of M\\n\u2225M\u2225F Frobenius norm of M\\nM\u22a4 Transpose of M\\nM\u2020 Pseudo-inverse of M\\nTr[M]T r a c e o f M\\nI Identity matrix\\nK : X\u00d7 X\u2192 R Kernel function over X\\nK Kernel matrix\\n1A Ind', 'icator function indicating membership in subset A\\nR(\u00b7) Generalization error or risk\\n\u02c6R(\u00b7) E m p i r i c a le r r o ro rr i s k\\nRm(\u00b7) Rademacher complexity over all samples of size m\\n\u02c6RS(\u00b7) Empirical R', 'ademacher complexity with respect to sample S\\nN(0, 1) Standard normal distribution\\nE\\nx\u223cD\\n[\u00b7] Expectation over x drawn from distribution D\\n\u03a3\u2217 Kleene closure over a set of characters \u03a3References\\nShivani', ' Agarwal, Thore Graepel, Ralf Herbrich, Sariel Har-Peled, and Dan Roth.\\nGeneralization bounds for the area under the ROC curve. Journal of Machine\\nLearning, 6:393\u2013425, 2005.\\nShivani Agarwal and Partha', ' Niyogi. Stability and generalization of bipartite ranking\\nalgorithms. In Conference on Learning Theory, pages 32\u201347, 2005.\\nNir Ailon and Mehryar Mohri. An e\ufb03cient reduction of ranking to classi\ufb01catio', 'n.\\nIn Conference on Learning Theory, pages 87\u201398, 2008.\\nMark A. Aizerman, E. M. Braverman, and Lev I. Rozono`er. Theoretical foundations\\nof the potential function method in pattern recognition learnin', 'g.Automation and\\nRemote Control, 25:821\u2013837, 1964.\\nCyril Allauzen, Corinna Cortes, and Mehryar Mohri. Large-scale training of SVMs\\nwith automata kernels. In International Conference on Implementation ', 'and\\nApplication of Automata, pages 17\u201327, 2010.\\nCyril Allauzen and Mehryar Mohri. N-way composition of weighted \ufb01nite-state\\ntransducers. International Journal of Foundations of Computer Science , 20(4', '):\\n613\u2013627, 2009.\\nErin L. Allwein, Robert E. Schapire, and Yoram Singer. Reducing multiclass to\\nbinary: A unifying approach for margin classi\ufb01ers. Journal of Machine Learning,\\n1:113\u2013141, 2000.\\nNoga Al', 'on, Shai Ben-David, Nicol` o Cesa-Bianchi, and David Haussler. Scale-\\nsensitive dimensions, uniform convergence, and learnability. Journal of ACM ,\\n44:615\u2013631, July 1997.\\nNoga Alon and Joel Spencer. T', 'he Probabilistic Method. John Wiley, 1992.\\nDana Angluin. On the complexity of minimum inference of regular sets.Information\\nand Control, 39(3):337\u2013350, 1978.\\nDana Angluin. Inference of reversible lang', 'uages. Journal of the ACM, 29(3):741\u2013\\n765, 1982.\\nMartin Anthony and Peter L. Bartlett. Neural Network Learning: Theoretical\\nFoundations. Cambridge University Press, 1999.382 REFERENCES\\nNachman Aronsza', 'jn. Theory of reproducing kernels. Transactions of the American\\nMathematical Society, 68(3):337\u2013404, 1950.\\nPatrick Assouad. Densit\u00b4e et dimension.Annales de l\u2019institut Fourier, 33(3):233\u2013282,\\n1983.\\nKa', 'zuoki Azuma. Weighted sums of certain dependent random variables. Tohoku\\nMathematical Journal, 19(3):357\u2013367, 1967.\\nMaria-Florina Balcan, Nikhil Bansal, Alina Beygelzimer, Don Coppersmith, John\\nLangfo', 'rd, and Gregory B. Sorkin. Robust reductions from ranking to classi\ufb01ca-\\ntion. Machine Learning, 72(1-2):139\u2013153, 2008.\\nPeter L. Bartlett, St\u00b4ephane Boucheron, and G\u00b4abor Lugosi. Model selection and\\ner', 'ror estimation. Machine Learning, 48:85\u2013113, September 2002a.\\nPeter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. Localized Rademacher\\ncomplexities. In Conference on Computational Learning Theo', 'ry, volume 2375,\\npages 79\u201397. Springer-Verlag, 2002b.\\nPeter L. Bartlett and Shahar Mendelson. Rademacher and Gaussian complexities:\\nRisk bounds and structural results. Journal of Machine Learning, 3, ', '2002.\\nAmos Beimel, Francesco Bergadano, Nader H. Bshouty, Eyal Kushilevitz, and\\nStefano Varricchio. Learning functions represented as multiplicity automata.\\nJournal of the ACM, 47:2000, 2000.\\nMikhail ', 'Belkin and Partha Niyogi. Laplacian eigenmaps and spectral techniques\\nfor embedding and clustering. In Advances in Neural Information Processing\\nSystems, 2001.\\nGeorge Bennett. Probability inequalities', ' for the sum of independent random\\nvariables. Journal of the American Statistical Association, 57:33\u201345, 1962.\\nChristian Berg, Jens P.R. Christensen, and Paul Ressel. Harmonic Analysis on\\nSemigroups: ', 'Theory of Positive De\ufb01nite and Related Functions , volume 100.\\nSpringer, 1984.\\nFrancesco Bergadano and Stefano Varricchio. Learning behaviors of automata from\\nshortest counterexamples. In Conference o', 'n Computational Learning Theory ,\\npages 380\u2013391, 1995.\\nSergei Natanovich Bernstein. Sur l\u2019extension du th\u00b4 eor`eme limite du calcul des\\nprobabilit\u00b4es aux sommes de quantit\u00b4es d\u00b4ependantes. Mathematisc', 'he Annalen, 97:\\n1\u201359, 1927.\\nDimitri P. Bertsekas.Dynamic Programming: Deterministic and Stochastic Models.\\nPrentice-Hall, 1987.\\nAvrim Blum and Yishay Mansour. From external to internal regret. InConfe', 'rence\\non Learning Theory, pages 621\u2013636, 2005.REFERENCES 383\\nAvrim Blum and Yishay Mansour. Learning, regret minimization, and equilibria.\\nIn Noam Nisan, Tim Roughgarden, \u00b4Eva Tardos, and Vijay Vazira', 'ni, editors,\\nAlgorithmic Game Theory, chapter 4, pages 4\u201330. Cambridge University Press,\\n2007.\\nAnselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth.\\nLearnability and the Vapnik-C', 'hervonenkis dimension. Journal of the ACM,3 6\\n(4):929\u2013965, 1989.\\nBernhard E. Boser, Isabelle M. Guyon, and Vladimir N. Vapnik. A training algo-\\nrithm for optimal margin classi\ufb01ers. In Conference on Co', 'mputational Learning\\nTheory, pages 144\u2013152, 1992.\\nOlivier Bousquet and Andr\u00b4e Elissee\ufb00. Stability and generalization. Journal of\\nMachine Learning, 2:499\u2013526, 2002.\\nStephen P. Boyd and Lieven Vandenber', 'ghe. Convex Optimization. Cambridge\\nUniversity Press, 2004.\\nLeo Breiman. Prediction games and arcing algorithms. Neural Computation, 11:\\n1493\u20131517, October 1999.\\nLeo Breiman, J. H. Friedman, R. A. Ols', 'hen, and C. J. Stone. Classi\ufb01cation and\\nRegression Trees. Wadsworth, 1984.\\nNicol`o Cesa-Bianchi. Analysis of two gradient-based algorithms for on-line regres-\\nsion. Journal of Computer System Sciences', ', 59(3):392\u2013411, 1999.\\nNicol`o Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization abil-\\nity of on-line learning algorithms. In Advances in Neural Information Processing\\nSystems, pa', 'ges 359\u2013366, 2001.\\nNicol`o Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization\\nability of on-line learning algorithms.IEEE Transactions on Information Theory,\\n50(9):2050\u20132057, 2004.', '\\nNicol`o Cesa-Bianchi, Yoav Freund, David Haussler, David P. Helmbold, Robert E.\\nSchapire, and Manfred K. Warmuth. How to use expert advice. Journal of the\\nACM, 44(3):427\u2013485, 1997.\\nNicol`o Cesa-Bianc', 'hi and G\u00b4abor Lugosi. Potential-based algorithms in online pre-\\ndiction and game theory. In Conference on Learning Theory, pages 48\u201364, 2001.\\nNicol`o Cesa-Bianchi and Gabor Lugosi. Prediction, Learnin', 'g, and Games. Cam-\\nbridge University Press, 2006.\\nNicol`o Cesa-Bianchi, Yishay Mansour, and Gilles Stoltz. Improved second-order\\nbounds for prediction with expert advice. In Conference on Learning The', 'ory,\\npages 217\u2013232, 2005.\\nBernard Chazelle. The Discrepancy Method: Randomness and Complexity. Cam-384 REFERENCES\\nbridge University Press, New York, NY, USA, 2000.\\nMichael Collins, Robert E. Schapire,', ' and Yoram Singer. Logistic regression, Ad-\\naboost and Bregman distances. Machine Learning, 48:253\u2013285, September 2002.\\nCorinna Cortes, Patrick Ha\ufb00ner, and Mehryar Mohri. Rational kernels: Theory and\\n', 'algorithms. Journal of Machine Learning, 5:1035\u20131062, 2004.\\nCorinna Cortes, Leonid Kontorovich, and Mehryar Mohri. Learning languages with\\nrational kernels. InConference on Learning Theory, volume 453', '9 ofLecture Notes\\nin Computer Science, pages 349\u2013364. Springer, Heidelberg, Germany, June 2007a.\\nCorinna Cortes, Yishay Mansour, and Mehryar Mohri. Learning bounds for impor-\\ntance weighting. In Advan', 'ces in Neural Information Processing Systems, Van-\\ncouver, Canada, 2010a. MIT Press.\\nCorinna Cortes and Mehryar Mohri. AUC optimization vs. error rate minimization.\\nIn Advances in Neural Information P', 'rocessing Systems, 2003.\\nCorinna Cortes and Mehryar Mohri. Con\ufb01dence intervals for the area under the\\nROC curve. In Advances in Neural Information Processing Systems, volume 17,\\nVancouver, Canada, 200', '5. MIT Press.\\nCorinna Cortes, Mehryar Mohri, Dmitry Pechyony, and Ashish Rastogi. Stability\\nof transductive regression algorithms. In International Conference on Machine\\nLearning, Helsinki, Finland, J', 'uly 2008a.\\nCorinna Cortes, Mehryar Mohri, and Ashish Rastogi. An alternative ranking\\nproblem for search engines. In Workshop on Experimental Algorithms , pages\\n1\u201322, 2007b.\\nCorinna Cortes, Mehryar Moh', 'ri, and Afshin Rostamizadeh. Learning sequence\\nkernels. In Proceedings of IEEE International Workshop on Machine Learning\\nfor Signal Processing,C a n c \u00b4un, Mexico, October 2008b.\\nCorinna Cortes, Mehr', 'yar Mohri, and Ameet Talwalkar. On the impact of kernel\\napproximation on learning accuracy. In Conference on Arti\ufb01cial Intelligence and\\nStatistics, 2010b.\\nCorinna Cortes, Mehryar Mohri, and Jason West', 'on. A general regression framework\\nfor learning string-to-string mappings. In Predicted Structured Data. MIT Press,\\n2007c.\\nCorinna Cortes and Vladimir Vapnik. Support-vector networks.Machine Learning,', '\\n20(3):273\u2013297, 1995.\\nDavid Cossock and Tong Zhang. Statistical analysis of Bayes optimal subset\\nranking. IEEE Transactions on Information Theory, 54(11):5140\u20135154, 2008.\\nT r e v o rF .C o xa n dM i c', ' h a e lA .A .C o x .Multidimensional Scaling. Chapman &\\nHall/CRC, 2nd edition, 2000.REFERENCES 385\\nKoby Crammer and Yoram Singer. Improved output coding for classi\ufb01cation using\\ncontinuous relaxation.', ' In Advances in Neural Information Processing Systems,\\n2001.\\nKoby Crammer and Yoram Singer. On the algorithmic implementation of multiclass\\nkernel-based vector machines. Journal of Machine Learning, 2', ', 2002.\\nRobert Crites and Andrew Barto. Improving elevator performance using reinforce-\\nment learning. In Advances in Neural Information Processing Systems , pages\\n1017\u20131023. MIT Press, 1996.\\nFelipe C', 'ucker and Steve Smale. On the mathematical foundations of learning.\\nBulletin of the American Mathematical Society, 39(1):1\u201349, 2001.\\nSanjoy Dasgupta and Anupam Gupta. An elementary proof of a theorem ', 'of Johnson\\nand Lindenstrauss. Random Structures and Algorithms, 22(1):60\u201365, 2003.\\nLuc Devroye and G\u00b4abor Lugosi. Lower bounds in pattern recognition and learning.\\nPattern Recognition, 28(7):1011\u20131018', ', 1995.\\nLuc Devroye and T. J. Wagner. Distribution-free inequalities for the deleted and\\nholdout error estimates. IEEE Transactions on Information Theory, 25(2):202\u2013\\n207, 1979a.\\nLuc Devroye and T. J. ', 'Wagner. Distribution-free performance bounds for potential\\nfunction rules. IEEE Transactions on Information Theory, 25(5):601\u2013604, 1979b.\\nThomas G. Dietterich. An experimental comparison of three meth', 'ods for construct-\\ning ensembles of decision trees: Bagging, boosting, and randomization. Machine\\nLearning, 40(2):139\u2013157, 2000.\\nThomas G. Dietterich and Ghulum Bakiri. Solving multiclass learning pro', 'blems\\nvia error-correcting output codes. Journal of Arti\ufb01cial Intelligence Research,2 :\\n263\u2013286, 1995.\\nHarris Drucker and Corinna Cortes. Boosting decision trees. InAdvances in Neural\\nInformation Proc', 'essing Systems, pages 479\u2013485, 1995.\\nHarris Drucker, Robert E. Schapire, and Patrice Simard. Boosting performance\\nin neural networks. International Journal of Pattern Recognition and Arti\ufb01cial\\nIntelli', 'gence, 7(4):705\u2013719, 1993.\\nRichard M. Dudley. The sizes of compact subsets of Hilbert space and continuity\\nof Gaussian processes. Journal of Functional Analysis, 1(3):290\u2013330, 1967.\\nRichard M. Dudley.', ' A course on empirical processes.Lecture Notes in Mathematics,\\n1097:2 \u2013 142, 1984.\\nRichard M. Dudley. Universal Donsker classes and metric entropy. Annals of\\nProbability, 14(4):1306\u20131326, 1987.\\nRichar', 'd M. Dudley.Uniform Central Limit Theorems. Cambridge University Press,386 REFERENCES\\n1999.\\nNigel Du\ufb00y and David P. Helmbold. Potential boosters? In Advances in Neural\\nInformation Processing Systems, ', 'pages 258\u2013264, 1999.\\nAryeh Dvoretzky. On stochastic approximation. In Proceedings of the Third\\nBerkeley Symposium on Mathematical Statistics and Probability , pages 39\u201355,\\n1956.\\nCynthia Dwork, Ravi Ku', 'mar, Moni Naor, and D. Sivakumar. Rank aggregation\\nmethods for the web. In International World Wide Web Conference, pages 613\u2013\\n622, 2001.\\nBradley Efron, Trevor Hastie, Iain Johnstone, and Robert Tibsh', 'irani. Least angle\\nregression. Annals of Statistics, 32(2):407\u2013499, 2004.\\nJames P. Egan.Signal Detection Theory and ROC Analysis. Academic Press, 1975.\\nAndrzej Ehrenfeucht, David Haussler, Michael J. ', 'Kearns, and Leslie G. Valiant. A\\ngeneral lower bound on the number of examples needed for learning. InConference\\non Learning Theory, pages 139\u2013154, 1988.\\nEyal Even-Dar and Yishay Mansour. Learning rat', 'es for q-learning. Machine\\nLearning, 5:1\u201325, 2003.\\nDean P. Foster and Rakesh V. Vohra. Calibrated learning and correlated equilib-\\nrium. Games and Economic Behavior, 21:40\u201355, 1997.\\nDean P. Foster and', ' Rakesh V. Vohra. Asymptotic calibration. Biometrika, pages\\n379\u2013390, 1998.\\nDean P. Foster and Rakesh V. Vohra. Regret in the on-line decision problem.Games\\nand Economic Behavior, 29(1-2):7\u201335, 1999.\\nY', 'oav Freund. Boosting a weak learning algorithm by majority. In Conference on\\nComputational Learning Theory, pages 202\u2013216. Morgan Kaufmann Publishers\\nInc., 1990.\\nYoav Freund. Boosting a weak learning ', 'algorithm by majority. Information and\\nComputation, 121:256\u2013285, September 1995.\\nYoav Freund, Raj D. Iyer, Robert E. Schapire, and Yoram Singer. An e\ufb03cient\\nboosting algorithm for combining preferences', '. Journal of Machine Learning,4 ,\\n2003.\\nYoav Freund, Michael J. Kearns, Dana Ron, Ronitt Rubinfeld, Robert E. Schapire,\\nand Linda Sellie. E\ufb03cient learning of typical \ufb01nite automata from random walks.\\n', 'In Proceedings the ACM Symposium on Theory of Computing , pages 315\u2013324,\\n1993.\\nYoav Freund and Robert E. Schapire. Game theory, on-line prediction and boosting.\\nIn Conference on Learning Theory, pages', ' 325\u2013332, 1996.REFERENCES 387\\nYoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line\\nlearning and an application to boosting. Journal of Computer System Sciences,\\n55(1):119', '\u2013139, 1997.\\nYoav Freund and Robert E. Schapire. Large margin classi\ufb01cation using the percep-\\ntron algorithm. Machine Learning, 37:277\u2013296, 1999a.\\nYoav Freund and Robert E. Schapire. Adaptive game play', 'ing using multiplicative\\nweights. Games and Economic Behavior, 29(1-2):79\u2013103, October 1999b.\\nJerome H. Friedman. Greedy function approximation: A gradient boosting machine.\\nAnnals of Statistics, 29:1', '189\u20131232, 2000.\\nJerome H. Friedman, Trevor Hastie, and Robert Tibshirani. Additive logistic\\nregression: A statistical view of boosting. Annals of Statistics, 38(2), 2000.\\nE. Mark Gold. Language identi', '\ufb01cation in the limit. Information and Control,1 0\\n(5):447\u2013474, 1967.\\nE. Mark Gold. Complexity of automaton identi\ufb01cation from given data.Information\\nand Control, 37(3):302\u2013320, 1978.\\nDavid M. Green an', 'd John A Swets. Signal Detection Theory and Psychophysics.\\nWiley, 1966.\\nAdam J. Grove and Dale Schuurmans. Boosting in the limit: Maximizing the margin\\nof learned ensembles. In Proceedings of the Fift', 'eenth National Conference on\\nArti\ufb01cial Intelligence , pages 692\u2013699, 1998.\\nU\ufb00e Haagerup. The best constants in the Khintchine inequality. Studia Math,7 0\\n(3):231\u2013283, 1982.\\nJihun Ham, Daniel D. Lee, S', 'ebastian Mika, and Bernhard Sch\u00a8 olkopf. A kernel\\nview of the dimensionality reduction of manifolds. In International Conference\\non Machine Learning, 2004.\\nJames A. Hanley and Barbara J. McNeil. The m', 'eaning and use of the area under\\na receiver operating characteristic (ROC) curve. Radiology, 143:29\u201336, 1982.\\nJames Hannan. Approximation to Bayes risk in repeated plays. Contributions to\\nthe Theory o', 'f Games, 3:97\u2013139, 1957.\\nSergiu Hart and Andreu M. Mas-Colell. A simple adaptive procedure leading to\\ncorrelated equilibrium. Econometrica, 68(5):1127\u20131150, 2000.\\nDavid Haussler. Decision theoretic ge', 'neralizations of the PAC model for neural net\\nand other learning applications. Information and Computation, 100(1):78\u2013150,\\n1992.\\nDavid Haussler. Sphere packing numbers for subsets of the boolean n-cub', 'e with\\nbounded Vapnik-Chervonenkis dimension. Journal of Combinatorial Theory,\\nSeries A, 69(2):217 \u2013 232, 1995.388 REFERENCES\\nDavid Haussler. Convolution Kernels on Discrete Structures. Technical Repo', 'rt\\nUCSC-CRL-99-10, University of California at Santa Cruz, 1999.\\nDavid Haussler, Nick Littlestone, and Manfred K. Warmuth. Predicting {0,1}-\\nfunctions on randomly drawn points (extended abstract). In ', 'Foundations of\\nComputer Science, pages 100\u2013109, 1988.\\nRalf Herbrich, Thore Graepel, and Klaus Obermayer. Large margin rank boundaries\\nfor ordinal regression. In Advances in Large Margin Classi\ufb01ers, pa', 'ges 115\u2013132.\\nMIT Press, Cambridge, MA, 2000.\\nWassily Hoe\ufb00ding. Probability inequalities for sums of bounded random variables.\\nJournal of the American Statistical Association, 58(301):13\u201330, 1963.\\nArth', 'ur E. Hoerl and Robert W. Kennard. Ridge regression: Biased estimation for\\nnonorthogonal problems. Technometrics, 12(1):55\u201367, 1970.\\nKlaus-Uwe H\u00a8o\ufb00gen, Hans-Ulrich Simon, and Kevin S. Van Horn. Robust', ' trainability\\nof single neurons. Journal of Computer and Systems Sciences , 50(1):114\u2013125,\\n1995.\\nJohn E. Hopcroft and Je\ufb00rey D. Ullman. Introduction to Automata Theory,\\nLanguages and Computation. Addi', 'son-Wesley, 1979.\\nCho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, S. Sathiya Keerthi, and S. Sundarara-\\njan. A dual coordinate descent method for large-scale linear SVM. In Interna-\\ntional Conference on M', 'achine Learning, pages 408\u2013415, 2008.\\nTommi Jaakkola, Michael I. Jordan, and Satinder P. Singh. Convergence of\\nstochastic iterative dynamic programming algorithms. Neural Computation,6 :\\n1185\u20131201, 19', '94.\\nKalervo J\u00a8arvelin and Jaana Kek\u00a8al\u00a8ainen. IR evaluation methods for retrieving highly\\nrelevant documents. In ACM Special Interest Group on Information Retrieval,\\npages 41\u201348, 2000.\\nThorsten Joachi', 'ms. Optimizing search engines using clickthrough data. In Knowl-\\nedge and Discovery and Data Mining, pages 133\u2013142, 2002.\\nWilliam B. Johnson and Joram Lindenstrauss. Extensions of Lipschitz mappings\\ni', 'nto a Hilbert space. Contemporary Mathematics, 26:189\u2013206, 1984.\\nJean-Pierre Kahane. Sur les sommes vectorielles \u2211 \u00b1un. Comptes Rendus Hebdo-\\nmadaires des S\u2019eances de l\u2019Acad\u00b4emie des Sciences, Paris, ', '259:2577\u20132580, 1964.\\nAdam Kalai and Santosh Vempala. E\ufb03cient algorithms for online decision problems.\\nIn Conference on Learning Theory, pages 26\u201340, 2003.\\nWilliam Karush. Minima of Functions of Severa', 'l Variables with Inequalities as Side\\nConstraints. Master\u2019s thesis, Department of Mathematics, University of Chicago,\\n1939.REFERENCES 389\\nMichael J. Kearns and Yishay Mansour. A fast, bottom-up decisi', 'on tree pruning\\nalgorithm with near-optimal generalization. In International Conference on\\nMachine Learning, pages 269\u2013277, 1998.\\nMichael J. Kearns and Yishay Mansour. On the boosting ability of top-d', 'own\\ndecision tree learning algorithms. Journal of Computer and System Sciences ,\\n58(1):109\u2013128, 1999.\\nMichael J. Kearns and Dana Ron. Algorithmic stability and sanity-check bounds\\nfor leave-one-out cr', 'oss-validation. Neural Computation, 11(6):1427\u20131453, 1999.\\nMichael J. Kearns and Robert E. Schapire. E\ufb03cient distribution-free learning of\\nprobabilistic concepts (extended abstract). In Foundations of', ' Computer Science,\\npages 382\u2013391, 1990.\\nMichael J. Kearns and Leslie G. Valiant. Cryptographic limitations on learning\\nboolean formulae and \ufb01nite automata. Technical Report 14, Harvard University,\\n198', '8.\\nMichael J. Kearns and Leslie G. Valiant. Cryptographic limitations on learning\\nboolean formulae and \ufb01nite automata. Journal of ACM, 41(1):67\u201395, 1994.\\nMichael J. Kearns and Umesh V. Vazirani. An In', 'troduction to Computational\\nLearning Theory. MIT Press, 1994.\\nAleksandr Khintchine. \u00a8Uber dyadische br\u00a8uche. Mathematische Zeitschrift, 18(1):\\n109\u2013116, 1923.\\nJack Kiefer and Jacob Wolfowitz. Stochasti', 'c estimation of the maximum of a\\nregression function. Annals of Mathematical Statistics, 23(1):462\u2013466, 1952.\\nGeorge Kimeldorf and Grace Wahba. Some results on tchebyche\ufb03an spline func-\\ntions. Journal', ' of Mathematical Analysis and Applications, 33(1):82\u201395, 1971.\\nJyrki Kivinen and Manfred K. Warmuth. Boosting as entropy projection. In\\nConference on Learning Theory, pages 134\u2013144, 1999.\\nVladimir Kol', 'tchinskii. Rademacher penalties and structural risk minimization.\\nIEEE Transactions on Information Theory, 47(5):1902\u20131914, 2001.\\nVladimir Koltchinskii and Dmitry Panchenko. Rademacher processes and b', 'ounding\\nthe risk of function learning. In High Dimensional Probability II, pages 443\u2013459.\\nBirkh\u00a8auser, 2000.\\nVladmir Koltchinskii and Dmitry Panchenko. Empirical margin distributions and\\nbounding the ', 'generalization error of combined classi\ufb01ers. Annals of Statistics,\\n30, 2002.\\nLeonid Kontorovich, Corinna Cortes, and Mehryar Mohri. Learning linearly sepa-\\nrable languages. In Algorithmic Learning The', 'ory, pages 288\u2013303, 2006.\\nLeonid Kontorovich, Corinna Cortes, and Mehryar Mohri. Kernel methods for390 REFERENCES\\nlearning languages. Theoretical Computer Science, 405:223\u2013236, 2008.\\nHarold W. Kuhn an', 'd Albert W. Tucker. Nonlinear programming. In 2nd Berkeley\\nSymposium, pages 481\u2013492, Berkeley, 1951. University of California Press.\\nHarold J. Kushner and D. S. Clark. Stochastic Approximation Methods', ' for Con-\\nstrained and Unconstrained Systems, volume 26 ofApplied Mathematical Sciences.\\nSpringer-Verlag, 1978.\\nHarold Kushner. Stochastic approximation: a survey. Wiley Interdisciplinary\\nReviews Comp', 'utational Statistics, 2(1):87\u201396, 2010.\\nJohn D. La\ufb00erty, Andrew McCallum, and Fernando C. N. Pereira. Conditional\\nrandom \ufb01elds: Probabilistic models for segmenting and labeling sequence data.\\nIn Inter', 'national Conference on Machine Learning, pages 282\u2013289, 2001.\\nJohn La\ufb00erty. Additive models, boosting, and inference for generalized divergences.\\nIn Conference on Learning Theory, pages 125\u2013133, 1999.', '\\nRafa/suppressl Lata/suppressla and Krzysztof Oleszkiewicz. On the best constant in the khintchine-\\nkahane inequality. Studia Math, 109(1):101\u2013104, 1994.\\nGuy Lebanon and John D. La\ufb00erty. Boosting and ', 'maximum likelihood for expo-\\nnential models. In Advances in Neural Information Processing Systems, pages\\n447\u2013454, 2001.\\nMichel Ledoux and Michel Talagrand. Probability in Banach Spaces: Isoperimetry\\na', 'nd Processes. Springer, New York, 1991.\\nEhud Lehrer. A wide range no-regret theorem. Games and Economic Behavior,4 2\\n(1):101\u2013115, 2003.\\nNick Littlestone. Learning quickly when irrelevant attributes ab', 'ound: A new linear-\\nthreshold algorithm. Machine Learning, 2(4):285\u2013318, 1987.\\nNick Littlestone. From on-line to batch learning. InConference on Learning Theory,\\npages 269\u2013284, 1989.\\nNick Littlestone ', 'and Manfred K. Warmuth. The weighted majority algorithm. In\\nFoundations of Computer Science, pages 256\u2013261, 1989.\\nNick Littlestone and Manfred K. Warmuth. The weighted majority algorithm.\\nInformation ', 'and Computation, 108(2):212\u2013261, 1994.\\nMichael L. Littman.Algorithms for Sequential Decision Making. PhD thesis, Brown\\nUniversity, 1996.\\nPhilip M. Long and Rocco A. Servedio. Random classi\ufb01cation nois', 'e defeats all\\nconvex potential boosters. Machine Learning, 78:287\u2013304, March 2010.\\nM. Lothaire. Combinatorics on Words. Cambridge University Press, 1982.\\nM. Lothaire. Mots.H e r m `es, 1990.REFERENCES', ' 391\\nM. Lothaire. Applied Combinatorics on Words. Cambridge University Press, 2005.\\nYishay Mansour and David A. McAllester. Boosting with multi-way branching\\nin decision trees. In Advances in Neural I', 'nformation Processing Systems, pages\\n300\u2013306, 1999.\\nYishay Mansour and David A. McAllester. Generalization bounds for decision trees.\\nIn Conference on Learning Theory, pages 69\u201374, 2000.\\nLlew Mason, J', 'onathan Baxter, Peter L. Bartlett, and Marcus R. Frean. Boosting\\nalgorithms as gradient descent. In Advances in Neural Information Processing\\nSystems, pages 512\u2013518, 1999.\\nPascal Massart. Some applica', 'tions of concentration inequalities to statistics. An-\\nnales de la Facult\u00b4e des Sciences de Toulouse, IX:245\u2013303, 2000.\\nPeter McCullagh. Regression models for ordinal data. Journal of the Royal\\nStatis', 'tical Society B, 42(2), 1980.\\nPeter McCullagh and John A. Nelder. Generalized Linear Models. Chapman &\\nHall, 1983.\\nColin McDiarmid. On the method of bounded di\ufb00erences. Surveys in Combina-\\ntorics, 141', '(1):148\u2013188, 1989.\\nRon Meir and Gunnar R\u00a8atsch. Advanced lectures on machine learning, machine\\nlearning summer school, canberra, australia. In Machine Learning Summer\\nSchool, pages 118\u2013183, 2002.\\nRon ', 'Meir and Gunnar R\u00a8atsch. An Introduction to Boosting and Leveraging, pages\\n118\u2013183. Springer, 2003.\\nJames Mercer. Functions of positive and negative type, and their connection with\\nthe theory of integ', 'ral equations. Philosophical Transactions of the Royal Society\\nof London. Series A, Containing Papers of a Mathematical or Physical Character,\\n209(441-458):415, 1909.\\nSebastian Mika, Bernhard Scholkop', 'f, Alex J. Smola, Klaus-Robert Muller, Matthias\\nScholz, and Gunnar Ratsch. Kernel PCA and de-noising in feature spaces. In\\nAdvances in Neural Information Processing Systems, pages 536\u2013542, 1999.\\nMarvi', 'n Minsky and Seymour Papert. Perceptrons: An Introduction to Computa-\\ntional Geometry. MIT Press, 1969.\\nMehryar Mohri. Semiring frameworks and algorithms for shortest-distance prob-\\nlems. Journal of A', 'utomata, Languages and Combinatorics, 7(3):321\u2013350, 2002.\\nMehryar Mohri. Weighted automata algorithms. In Manfred Droste, Werner\\nKuich, and Heiko Vogler, editors, Handbook of Weighted Automata, pages ', '213\u2013\\n254. Springer, 2009.\\nMehryar Mohri, Fernando Pereira, and Michael D. Riley. Weighted automata in text392 REFERENCES\\nand speech processing. European Conference on Arti\ufb01cial Intelligence, Workshop\\n', 'on Extended Finite State Models of Language, 2005.\\nMehryar Mohri and Afshin Rostamizadeh. Stability bounds for stationary\u03d5-mixing\\nand \u03b2-mixing processes. Journal of Machine Learning, 11:789\u2013814, 2010.', '\\nJorge Nocedal. Updating quasi-newton matrices with limited storage. Mathematics\\nof Computation, 35(151):773\u2013782, 1980.\\nAlbert B.J. Noviko\ufb00. On convergence proofs on perceptrons. In Proceedings of the', '\\nSymposium on the Mathematical Theory of Automata, volume 12, pages 615\u2013622,\\n1962.\\nJos\u00b4e Oncina, Pedro Garc\u00b4\u0131a, and Enrique Vidal. Learning subsequential transducers\\nfor pattern recognition interpreta', 'tion tasks. IEEE Transactions on Pattern\\nAnalysis and Machine Intelligence, 15(5):448\u2013458, 1993.\\nKarl Pearson. On lines and planes of closest \ufb01t to systems of points in space.\\nPhilosophical Magazine, ', '2(6):559\u2013572, 1901.\\nFernando C. N. Pereira and Michael D. Riley. Speech recognition by composition\\nof weighted \ufb01nite automata. In Finite-State Language Processing, pages 431\u2013453.\\nMIT Press, 1997.\\nDomi', 'nique Perrin. Finite automata. In J. Van Leuwen, editor, Handbook of\\nTheoretical Computer Science, Volume B: Formal Models and Semantics, pages\\n1\u201357. Elsevier, 1990.\\nLeonard Pitt and Manfred K. Warmut', 'h. The minimum consistent DFA problem\\ncannot be approximated within any polynomial. Journal of the ACM , 40(1):\\n95\u2013142, 1993.\\nJohn C. Platt. Fast training of support vector machines using sequential m', 'inimal\\noptimization. In Advances in Kernel Methods, pages 185\u2013208. MIT Press, 1999.\\nDavid Pollard. Convergence of Stochastic Processess. Springer, 1984.\\nDavid Pollard. Asymptotics via empirical proces', 'ses. Statistical Science, 4(4):341 \u2013\\n366, 1989.\\nMartin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic\\nProgramming. John Wiley & Sons, Inc., 1994.\\nJ. Ross Quinlan. Induction of dec', 'ision trees. Machine Learning, 1(1):81\u2013106, 1986.\\nJ. Ross Quinlan. C4.5: Programs for Machine Learning. Morgan Kaufmann, 1993.\\nGunnar R\u00a8atsch, Takashi Onoda, and Klaus-Robert M\u00a8uller. Soft margins for', ' Ad-\\naBoost. Machine Learning, 42:287\u2013320, March 2001.\\nGunnar R\u00a8atsch and Manfred K. Warmuth. Maximizing the margin with boosting.\\nIn Conference on Learning Theory, pages 334\u2013350, 2002.\\nRyan M. Rifkin', '. Everything Old Is New Again: A Fresh Look at Historical Ap-REFERENCES 393\\nproaches in Machine Learning. PhD thesis, Massachusetts Institute of Technol-\\nogy, 2002.\\nRyan Rifkin and Aldebaro Klautau. I', 'n defense of one-vs-all classi\ufb01cation. Journal\\nof Machine Learning, 5:101\u2013141, 2004.\\nH. Robbins and S. Monro. A stochastic approximation method. Annals of Mathe-\\nmatical Statistics, 22(3):400\u2013407, 195', '1.\\nW.H. Rogers and T. J. Wagner. A \ufb01nite sample distribution-free performance bound\\nfor local discrimination rules. Annals of Statistics, 6(3):506\u2013514, 1978.\\nDana Ron, Yoram Singer, and Naftali Tishby', '. On the learnability and usage of\\nacyclic probabilistic \ufb01nite automata. In Conference on Computational Learning\\nTheory, pages 31\u201340, 1995.\\nFrank Rosenblatt. The perceptron: A probabilistic model for ', 'information storage\\nand organization in the brain. Psychological Review, 65(6):386, 1958.\\nSam T. Roweis and Lawrence K. Saul. Nonlinear dimensionality reduction by locally\\nlinear embedding. Science, 2', '90(5500):2323, 2000.\\nCynthia Rudin, Corinna Cortes, Mehryar Mohri, and Robert E. Schapire. Margin-\\nbased ranking meets boosting in the middle. In Conference on Learning Theory,\\n2005.\\nCynthia Rudin, In', 'grid Daubechies, and Robert E. Schapire. The dynamics of\\nAdaBoost: Cyclic behavior and convergence of margins. Journal of Machine\\nLearning, 5:1557\u20131595, 2004.\\nNorbert Sauer. On the density of families', ' of sets.Journal of Combinatorial Theory,\\nSeries A, 13(1):145\u2013147, 1972.\\nCraig Saunders, Alexander Gammerman, and Volodya Vovk. Ridge regression\\nlearning algorithm in dual variables. In International ', 'Conference on Machine\\nLearning, volume 521, 1998.\\nRobert E. Schapire. The strength of weak learnability. Machine Learning, 5:197\u2013\\n227, July 1990.\\nRobert E. Schapire. The boosting approach to machine l', 'earning: An overview. In\\nNonlinear Estimation and Classi\ufb01cation, pages 149\u2013172. Springer, 2003.\\nRobert E. Schapire, Yoav Freund, Peter Bartlett, and Wee Sun Lee. Boosting\\nthe margin: A new explanation', ' for the e\ufb00ectiveness of voting methods. In\\nInternational Conference on Machine Learning, pages 322\u2013330, 1997.\\nRobert E. Schapire and Yoram Singer. Improved boosting algorithms using\\ncon\ufb01dence-rated p', 'redictions. Machine Learning, 37(3):297\u2013336, 1999.\\nRobert E. Schapire and Yoram Singer. Boostexter: A boosting-based system for\\ntext categorization. Machine Learning, 39(2-3):135\u2013168, 2000.394 REFEREN', 'CES\\nLeopold Schmetterer. Stochastic approximation. In Proceedings of the Fourth\\nBerkeley Symposium on Mathematical Statistics and Probability, pages 587\u2013609,\\n1960.\\nIsaac J. Schoenberg. Metric spaces a', 'nd positive de\ufb01nite functions. Transactions of\\nthe American Mathematical Society, 44(3):522\u2013536, 1938.\\nBernhard Sch\u00a8olkopf, Ralf Herbrich, Alex J. Smola, and Robert Williamson. A\\ngeneralized represent', 'er theorem. Technical Report 2000-81, Neuro-COLT, 2000.\\nBernhard Sch\u00a8olkopf and Alex Smola. Learning with Kernels. MIT Press, 2002.\\nShai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridhar', 'an. Learn-\\nability and stability in the general learning setting. In Conference on Learning\\nTheory, 2009.\\nJohn Shawe-Taylor, Peter L. Bartlett, Robert C. Williamson, and Martin Anthony.\\nStructural ris', 'k minimization over data-dependent hierarchies.IEEE Transactions\\non Information Theory, 44(5):1926\u20131940, 1998.\\nJohn Shawe-Taylor and Nello Cristianini. Kernel Methods for Pattern Analysis .\\nCambridge ', 'University Press, 2004.\\nSaharon Shelah. A combinatorial problem; stability and order for models and\\ntheories in in\ufb01nitary languages. Paci\ufb01c Journal of Mathematics , 41(1), 1972.\\nSatinder P. Singh. Lea', 'rning to Solve Markovian Decision Processes.P h D t h e s i s ,\\nUniversity of Massachusetts, 1993.\\nSatinder P. Singh and Dimitri Bertsekas. Reinforcement learning for dynamic chan-\\nnel allocation in c', 'ellular telephone systems. In Advances in Neural Information\\nProcessing Systems, pages 974\u2013980. MIT Press, 1997.\\nMaurice Sion. On general minimax theorems. Paci\ufb01c Journal of Mathematics ,8\\n(1):171\u2013176', ', 1958.\\nEric V. Slud. Distribution inequalities for the binomial law. Annals of Probability,\\n5(3):404\u2013412, 1977.\\nGilles Stoltz and G\u00b4abor Lugosi. Internal regret in on-line portfolio selection. In\\nCon', 'ference on Learning Theory, pages 403\u2013417, 2003.\\nRich Sutton. Temporal Credit Assignment in Reinforcement Learning.P h Dt h e s i s ,\\nUniversity of Massachusetts, 1984.\\nRichard S. Sutton and Andrew G.', ' Barto.Reinforcement Learning : An Introduction.\\nMIT Press, 1998.\\nS.J. Szarek. On the best constants in the Khintchin inequality.Studia Math, 58(2):\\n197\u2013208, 1976.\\nCsaba Szepesv\u00b4ari. Algorithms for Re', 'inforcement Learning. Synthesis Lectures on\\nArti\ufb01cial Intelligence and Machine Learning. Morgan & Claypool, 2010.REFERENCES 395\\nEiji Takimoto and Manfred K. Warmuth. Path kernels and multiplicative up', 'dates.\\nIn Conference on Learning Theory, pages 74\u201389, 2002.\\nBenjamin Taskar, Carlos Guestrin, and Daphne Koller. Max-margin Markov net-\\nworks. In Advances in Neural Information Processing Systems, 200', '3.\\nRobert F. Tate. On a double inequality of the normal distribution. The Annals of\\nMathematical Statistics, 1:132\u2013134, 1953.\\nJoshua Tenenbaum, Vin de Silva, and John C. Langford. A global geometric\\nf', 'ramework for nonlinear dimensionality reduction.Science, 290(5500):2319\u20132323,\\n2000.\\nGerald Tesauro. Temporal di\ufb00erence learning and TD-gammon. Communications\\nof the ACM, 38:58\u201368, March 1995.\\nRobert T', 'ibshirani. Regression shrinkage and selection via the lasso. Journal of the\\nRoyal Statistical Society. Series B, 58(1):267\u2013288, 1996.\\nB. Tomaszewski. Two remarks on the Khintchine-Kahane inequality. I', 'nColloquium\\nMathematicum, volume 46, 1982.\\nBoris Trakhtenbrot and Janis M. Barzdin. Finite Automata: Behavior and Synthe-\\nsis. North-Holland, 1973.\\nJohn N. Tsitsiklis. Asynchronous stochastic approxim', 'ation and q-learning. In\\nMachine Learning, volume 16, pages 185\u2013202, 1994.\\nIoannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin Al-\\ntun. Large margin methods for structured and inter', 'dependent output variables.\\nJournal of Machine Learning, 6:1453\u20131484, 2005.\\nLeslie G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):\\n1134\u20131142, 1984.\\nVladimir N. Vapnik. Statis', 'tical Learning Theory. Wiley-Interscience, 1998.\\nVladimir N. Vapnik. The Nature of Statistical Learning Theory. Springer-Verlag,\\n2000.\\nVladimir N. Vapnik.Estimation of Dependences Based on Empirical D', 'ata.S p r i n g e r -\\nVerlag, 2006.\\nVladimir N. Vapnik and Alexey Chervonenkis. A note on one class of perceptrons.\\nAutomation and Remote Control, 25, 1964.\\nVladimir N. Vapnik and Alexey Chervonenkis.', ' On the uniform convergence of\\nrelative frequencies of events to their probabilities. Theory of Probability and Its\\nApplications, 16:264, 1971.\\nVladimir N. Vapnik and Alexey Chervonenkis. Theory of Pa', 'ttern Recognition .\\nNauka, 1974.\\nSantosh S. Vempala. The random projection method. In DIMACS Series in396 REFERENCES\\nDiscrete Mathematics and Theoretical Computer Science, volume 65. American\\nMathemat', 'ical Society, 2004.\\nMathukumalli Vidyasagar. A Theory of Learning and Generalization: With Appli-\\ncations to Neural Networks and Control Systems. Springer-Verlag, 1997.\\nSethu Vijayakumar and Si Wu. Se', 'quential support vector classi\ufb01ers and regression.\\nInternational Conference on Soft Computing, 1999.\\nJohn von Neumann. Zur Theorie der Gesellschaftsspiele. Mathematische Annalen,\\n100(1):295\u2013320, 1928.', '\\nVladimir G. Vovk. Aggregating strategies. InConference on Learning Theory, pages\\n371\u2013386, 1990.\\nGrace Wahba. Spline Models for Observational Data , volume 59 of CBMS-NSF\\nRegional Conference Series in', ' Applied Mathematics. Society for Industrial and\\nApplied Mathematics, 1990.\\nC h r i s t o p h e rJ .C .H .W a t k i n s .Learning from Delayed Rewards .P h D t h e s i s ,\\nCambridge University, 1989.\\n', 'Christopher J. C. H. Watkins. Dynamic alignment kernels. Technical Report CSD-\\nTR-98-11, Royal Holloway, University of London, 1999.\\nChristopher J. C. H. Watkins and Peter Dayan. Q-learning. Machine L', 'earning,8\\n(3-4):279\u2013292, 1992.\\nKilian Q. Weinberger and Lawrence K. Saul. An introduction to nonlinear dimen-\\nsionality reduction by maximum variance unfolding. In Conference on Arti\ufb01cial\\nIntelligence', ', 2006.\\nJason Weston and Chris Watkins. Support vector machines for multi-class pattern\\nrecognition. European Symposium on Arti\ufb01cial Neural Networks, 4(6), 1999.\\nBernard Widrow and Marcian E. Ho\ufb00. Ada', 'ptive switching circuits. Neurocomput-\\ning: Foundations of Research, 1988.\\nHuan Xu, Shie Mannor, and Constantine Caramanis. Sparse algorithms are not\\nstable: A no-free-lunch theorem. In Conference on ', 'Communication, Control, and\\nComputing, pages 1299\u20131303, 2008.\\nYinyu Ye. The simplex and policy-iteration methods are strongly polynomial for the\\nmarkov decision problem with a \ufb01xed discount rate. Math', 'ematics of Operations\\nResearch, 36(4):593\u2013603, 2011.\\nMartin Zinkevich. Online convex programming and generalized in\ufb01nitesimal gra-\\ndient ascent. In International Conference on Machine Learning, pages ', '928\u2013936,\\n2003.Index\\n\u03b2-stable, see stable\\n\u03f5-transition, 111, 295\\n\u03b3-fat-dimension, see fat-shattering di-\\nmension\\n\u03b3-shattered, see fat-shattered\\n\u03c3-admissible, 271\\n\u03c3-algebra, 359\\nk-CNF formula, 20, 21\\nk-', 'term DNF formula, 20, 21\\naccess string, 298\u2013302\\naccuracy, 13, 19, 29, 52, 124\u2013126, 130,\\n140, 142, 214, 254\\npairwise ranking, 215, 225\\naction, 8, 138, 153, 154, 175, 313\u2013315,\\n317\u2013322, 325, 326, 330, 33', '2\u2013\\n334, 336\\ncolumn, 138, 139\\ngreedy, 333, 334\\npolicy, see policy\\nrandom, 334\\nrow, 138, 139\\nspace, 337\\nAdaBoost, 121\u2013132, 134\u2013146, 169, 192,\\n193, 206, 209, 214, 218, 220,\\n222\u2013224, 233, 234\\nAdaBoost.MH,', ' 192, 193, 206, 207\\nAdaBoost.MR, 192, 206, 208\\nadaptive boosting, see AdaBoost\\nadversarial, 7, 152, 153, 174, 309\\nargument, 150\\nassumption, 148\\nchoice, 229\\nscenario, 147\\nalgebraic transduction, 111, s', 'ee also\\ncontext-free grammar\\nalgorithm, 1, 4\\naggregated, 183, 190\\ndeterministic, 147, 152, 153, 156,\\n209, 227\u2013230, 233\\nlearning, 1\u20135, 21, 28, 29, 48, 49, 51,\\n52, 58, 59\\no\ufb00-policy, 334\\non-policy, 334\\nr', 'andomized, 147, 153, 154, 156, 179,\\n209, 227, 228, 230, 234\\nrobust, 2\\nuncombined, 183, 190, 199, 206\\nalgorithmic stability, see stability\\narea under the curve, see AUC\\narea under the ROC curve, see AU', 'C\\nassumption\\nstochastic, 295, 296\\nAUC, 209, 224\u2013226, 232, 233, 235\\nautomaton\\nk-deterministic, 311\\nk-reversible, 311, 312\\nacyclic, 295, 311\\nprobabilistic, 310\\ndeterministic, 294\u2013296, 304, 305,\\n308, 311', ', see also DFA\\n\ufb01nite, 294\\nlearning with queries, 298, 300, 303,\\nsee also QueryLearnAutomata\\nalgorithm\\nminimization, 295398 INDEX\\nnon-deterministic, 295,see alsoNFA\\npre\ufb01x-tree, 304, 305, 307, 308\\nquoti', 'ent, 303\\nreverse, 304, 305\\nreverse determinism, 304\\nreverse deterministic, 308\\nreversible, 304, 305, 307\u2013309\\nlearning, 306, 312,see also Learn-\\nReversibleAutomata algorithm\\nAzuma\u2019s inequality, 172, 37', '1\u2013373, 376\\nbase\\nclassi\ufb01er, see classi\ufb01er\\nrankers, 214, 216, 218\\nBayes\\nclassi\ufb01er, 25, 52, 118\\nerror, 25, 26, 229\\nformula, 362\\nhypothesis, 25\\nBellman equations, 317\u2013322, 324, 330\\nBennett\u2019s inequality, 3', '71, 377, 378\\nBernstein\u2019s inequality, 378\\nb i a s ,6 ,5 2\\nbigram, 113\\ngappy, 113, 114\\nkernel, 113\\ngappy, 113\\nBipartiteRankBoost, 223, see also Rank-\\nBoost\\nboosting, 8, 121, 122, 132, 136, 138, 140\u2013\\n143', ', 191, 192, 194, 206, 220\\nby \ufb01ltering, 141\\nby majority, 141\\nmulti-class, 8, 183, 191, 192, 207,\\nsee also AdaBoost.MH, see also\\nAdaBoost.MR\\nranking, 209, 214, see also Rank-\\nBoost\\nround, 122\u2013124, 130, ', '131, 134, 140,\\n141, 143\u2013145, 215, 216, 220\\nstump, 129, 130, 144, 193, 207\\ntrees, 263\\nBregman divergence, 142, 271, 272\\ngeneralized, 272, 273\\ncalibration problem, 199, 200, 206\\nCauchy-Schwarz inequalit', 'y, 77, 96, 102,\\n162, 180, 190, 273, 275, 342,\\n343, 367\\ncentral limit theorem, 367\\nchain rule, 362\\nChebyshev\u2019s inequality, 365, 366, 377\\nCherno\ufb00\\nbound, 50\\nbounding technique, 369, 370, 372,\\n378\\nCholesk', 'y decomposition, 99, 346\\npartial, 251, 256\\nclassi\ufb01cation, 2, 124, 229\\nbinary, 11, 38\\ndocument, 1\\nimage, 118\\nlinear, 63\\non-line, 159\\nmulti-class, 8, 183\\non-line, 147\\nstability, 9, 276\\ntext, 2\\ntwo-group', ', 87\\nXOR, 93\\nclassi\ufb01er, see also classi\ufb01cation\\naccuracy, 126\\nbase, 121, 122, 124\u2013126, 128, 129,\\n131, 132, 134, 136, 138, 140,\\n143, 192, 193, 216\\nBayes, 25\\nbinary, 63\\nedge, 125, 136\u2013140, 216\\nerror, 6\\nh', 'yperplane, 42\\nlinear, 63, 159\\nmargin, 131INDEX 399\\nmulti-class, 183\\nclique, 204\\nclustering, 2, 101, 194\\nalgorithm, 2\\nco-accessible, 109\\ncode\\nbinary, 202\\ncontinuous, 202\\ndiscrete, 202\\nerror-correction,', ' 201, 202\\nmatrix, 202\\nternary, 202\\nword, 201, 202\\ncomplementarity conditions, 67, 73, 74,\\n253, 357\\nconcave, 351\\nfunction, 68, 74, 102, 176, 249, 350\u2013\\n353, 355\\nproblem, 355\\nconcavity, 343, 352\\nconcentr', 'ation inequalities, 369\u2013372\\nconcept, 11\\nclass, 1, 3, 11, 13, 14, 16, 18\u201321, 29\u2013\\n31, 33, 57, 59, 121, 149, 295, 297\\nuniversal, 19\\ncondition\\nKKT, 66, 73, 191, 249, 253, 255, 357\\nMercer\u2019s, 91, 120\\nSlater', '\u2019s, 355, 356, see also con-\\nstraint quali\ufb01cation\\nweak, 355\\nConditional Random Fields, see CRFs\\ncon\ufb01dence, 13, 19, 32, 59, 78, 124, 132,\\n185, 211\\ninterval, 233\\nscore, 199, 201, 202\\nconjugate, 132, 171,', ' 342, 343\\nconsistent, 3, 5, 11, 296\\nalgorithm, 17\u201319, 32, 58\\ncase, 17, 23\\nDFA, see DFA learning minimum\\nconsistent\\nexpert, 149\\nhypothesis, 17\u201319, 59, 144\\nlearner, 5\\nNFA, 309\\npairwise, 227\\nconstraint\\nL', '\\n1-, 259\\na\ufb03ne, 66, 68, 72, 73, 191, 253, 260,\\n354\\ndi\ufb00erentiable, 66, 73, 191, 248\\nequality, 354\\nquali\ufb01cation\\nstrong, 355\\nweak, 355\\ncontext-free\\ngrammar, 293, 297\\nlanguage, 111, 293\\nconvex, 72, 83, 126', ', 161, 218, 257, 352,\\n353\\nd-gon, 44, 45\\ncombination, 132\u2013134, 192\\nconstraint, 68, 72, 73\\ndomain, 351\\nfunction, 51, 66, 72, 73, 126, 128,\\n143, 144, 157, 159, 172, 179,\\n191, 192, 196, 205, 207, 208,\\n218', ', 219, 224, 246, 248, 256,\\n271\u2013273, 349\u2013354, 356\\nhull, 42\u201344, 132, 220, 350\\nintersection, 57\\nloss, 128, 147, 153, 156, 157, 159,\\n172, 175, 181, 219, 256, 271\u2013\\n273, 277\\noptimization, 9, 65, 66, 68, 72,', ' 84,\\n94, 191, 248, 257, 349, 350, 353\u2013\\n357\\npolygon, 45\\npotential, 141, 142\\nQP, 66, 74, 253, 255400 INDEX\\nregion, 195\\nset, 350\u2013353\\nstrictly, 66, 351\\nupper bound, 72, 73, 126, 128, 218\\nconvexity, 36, 53', ', 72, 91, 158, 161, 173,\\n180, 181, 207, 218, 248, 352\u2013\\n354, 357, 369, 374\\ncovariance, 366, 367\\nmatrix, 282, 283, 287, 290, 367\\ncovering, 61\\nnumbers, 55, 61, 233\\nCRFs, 205, 207\\ncross-validation, 140, 2', '56\\nn-fold, 5, 6, 28, 72, 87, 198\\nerror, 5, 6, 86\\nleave-one-out, 6\\ndata\\nset, 2\\ntest, 4\\ntraining, 3\\nunseen, 3\\nvalidation, 3\\nDCG, 233, 234\\nnormalized, 233\\ndecision epoch, 315, 330, 332, 334\\ndecision stum', 'p, 130, 140, 141\\ndecision trees, 129, 130, 141, 183, 191,\\n194, 195, 197, 198, 206, 208,\\n263, 299, 300, 302, 310\\nbinary, 150, 194\\nbinary space partition trees, 195\\nclassi\ufb01cation, 299\\nlearning, 195, 197', ', 206, see also\\nGreedyDecisionTrees algorithm\\nnode, 194\\nquestion, 194\u2013196, 299\\ncategorical, 194\\nnumerical, 194\\nsphere trees, 195\\nstump, see also boosting stump, see\\ndecision stump\\nDFA, 295, 296, 298\u20133', '00, 302\u2013304, 309\u2013\\n311\\nacyclic, 295\\nconsistent, 296\\nequivalent, 295\\nlearning, 303, 309\\nminimum consistent, 296\\nlearning with queries, 298, 303\\nminimal, 295, 296, 298, 310\\nminimization, 296\\nreverse, 304', '\\nVC-dimension, 311\\ndichotomy, 41\u201346\\ndi\ufb00erentiable\\nfunction, 349, 351, 352, 356\\nupper bound, 126, 128\\ndimensionality reduction, 2, 7, 101, 281,\\n285, 288, 290\\ndiscounted cumulative gain, see DCG\\ndistrib', 'ution, 359, 360\\n\u03c7\\n2-squared, 288, 289, 361\\nabsolutely continuous, 360\\nbinomial, 360\\nchi-squared, 361\\ndensity function, 360\\nGaussian, 360\\nLaplace, 361\\nnormal, 360\\nPoisson, 361\\nprobability, 359\\ndistribu', 'tion-free model, 13\\nDNF formula, 20, 311\\ndisjoint, 310\\ndoubling trick, 155, 158, 174, 175\\ndual, 251\\nfunction, 354\\nnorm, 342\\noptimization, 66\u201368, 74, 75, 83, 84,\\n100, 191, 207, 249, 255, 264, 355INDEX ', '401\\nproblem, 355\\nSVM, 164\\nSVR, 262\\nvariables, 67, 70, 74, 264, 354\\nduality\\ngap, 355\\nstrong, 68, 355\\nweak, 355\\nDualPerceptron, 167, 168\\nearly stopping, 141\\nedge, see classi\ufb01er edge\\nemphasis function, 2', '31, 232, 235\\nempirical kernel map, see kernel\\nempirical risk minimization, 26, 27, 38\\nensemble\\nalgorithms, 121\\nhypotheses, 133, 220\\nmargin bound, 133\\nmethods, 121, 122, 220\\nranking, 220\\nenvelope, 262\\n', 'environment, 1, 8, 313, 314, 326, 336\\nMDP, 315\\nmodel, 313, 314, 319, 325, 326, 330\\nunknown, 336\\nErd\u00a8os, 48\\nERM, see empirical risk minimization\\nerror, 12, see also risk\\napproximation, 26\\nBayes, 25\\ncro', 'ss-validation, 5\\nempirical, 8, 12, 184, 380\\nestimation, 26\\ngeneralization, 8, 12, 380\\nleave-one-out, 69\\nmean squared, 238\\nreconstruction, 282\\ntest, 5\\ntraining, 5\\ntrue, 12\\nevent, 30, 118, 119, 359, 361', ', 362\\nelementary, 359\\nindependent, 361\\nindicator, 12\\nmutually disjoint, 362\\nmutually exclusive, 359\\nset, 359\\nexamples, 3, 11\\ni.i.d., 12\\nincorrectly labeled, 141\\nlabeled, 4\\nmisclassi\ufb01ed, 144\\nnegative, ', '29\\npositive, 19, 303\\nunlabeled, 7\\nexpectation, 363\\nlinearity, 363\\nexperience, 1, 336\\nexpert, 32, 148\u2013154, 156, 157, 168, 169,\\n171, 174, 175, 179\\nactive, 149\\nadvice, 32, 147, 148\\nalgorithm, 175\\nbest, 1', '48, 151, 152, 175\\nexploitation, 8, 314\\nexploration, 8, 314\\nBoltzmann, 334\\nexploitation dilemma, 8, 314\\nExponential-Weighted-Average algorithm,\\n8, 156, 157, 173, 174\\nfalse negative, 14\\nfalse positive, ', '14\\nerror, 87\\nrate, 225, 226\\nfat-shattered, 244\\nfat-shattering, 262\\ndimension, 244, 245\\nfeature, 3\\nextraction, 281402 INDEX\\nmapping, 96\u201398, 102, 117, 167, 189,\\n190, 214, 247, 252, 254, 255,\\n281, 284\\nmi', 'ssing, 198\\npoor, 96\\nrelevant, 3, 4, 118, 204\\nspace, 76, 82, 83, 90, 91, 96, 117,\\n118, 140, 194, 213, 246, 247,\\n251, 310, 379\\nuncorrelated, 96\\nvector, 4\\nFermat\u2019s theorem, 349\\n\ufb01nal\\nstate, 107\u2013109, 294, ', '295, 299\u2013301,\\n304\u2013308, 312, 330\\nweight, 107, 108, 110, 114\\n\ufb01xed point, 199, 321, 326, 327, 329, 333\\nFrobenius\\nnorm, 283, 345, 380\\nproduct, 345\\nFubini\u2019s theorem, 49, 363\\nfunction\\na\ufb03ne, 66, 246, 355\\ncon', 'cave, see concave function\\ncontinuous, 91, 96, 120\\ncontracting, 320, 321\\nconvex, see convex function\\ndi\ufb00erentiable, 192, 349, 351, 352,\\n356\\n\ufb01nal weight, 107\\nkernel, 120\\nLipschitz, 78, 80, 96, 186, 188', ', 212,\\n240, 254, 255, 271, 274, 276,\\n320, 321\\nmaximum, 352\\nmeasurable, see measurable func-\\ntion\\nmoment-generating, 288, 364, 365,\\n370\\nquasi-concave, 176\\nsemi-continuous, 176\\nstate-action value, 318, ', '326, 331,\\n332\\nsupremum, 36\\nsymmetric, 91\\ngame, 138\\ntheory, 121, 137, 139, 142, 147, 176,\\n339\\nvalue, 139\\nzero-sum, 138, 139, 174\\ngap penalty, 113\\ngeneralization, 5\\nbound, 16, 17, 22, 23, 26, 33, 35, 37', ',\\n38, 40, 48, 54, 55, 59\u201361, 75,\\n77\u201380, 103, 132\u2013134, 183, 185,\\n187, 190, 197, 206, 208, 211,\\n213, 237, 239\u2013242, 244, 247,\\n251, 254, 255, 259, 262, 264,\\n267, 276\u2013278, see also margin\\nbound, see alsost', 'ability bound,\\nsee also VC-dimension bound\\nerror, 8, 12, 13, 18, 21, 22, 24\u201326,\\n29, 48, 61, 63, 69, 70, 82, 118,\\n131, 136, 144, 148, 172, 174,\\n184, 187, 200, 208, 210, 212,\\n213, 221, 238, 268, 270, 27', '6\\ngradient, 66, 73, 224, 349\\ndescent, 337,see also stochastic gra-\\ndient descent\\nGram matrix, 68, 92, 116,see also kernel\\nmatrix\\ngraph, 204, 287\\nacyclic, 111\\nLaplacian, 286, 291\\nneighborhood, 287\\nstru', 'cture, 205\\nGreedyDecisionTrees algorithm, 195\\ngrowth function, 33, 38\u201341, 45, 47, 56\\ngeneralization bound, 40\\nlower bound, 56\\nH\u00a8older\u2019s inequality, 180, 259, 342INDEX 403\\nHalving algorithm, 148\u2013150, 1', '52\\nHamming distance, 184, 201, 202, 204,\\n375\\nHessian, 66, 68, 180, 349, 351\\nHilbert space, 89, 91, 94\u201397, 103, 105,\\n116, 117, 119, 342, 376\\npre-, 96\\nreproducing kernel, 95, 96, 115, 270\\nhinge loss, 72', ', 73, 82, 83, 177, 276\\nquadratic, 72, 73, 278\\nHoe\ufb00ding\u2019s inequality, 21, 39, 61, 158,\\n170, 173, 235, 238, 239, 369\u2013\\n371, 373, 377, 378\\nhorizon, 158, 315\\n\ufb01nite, 315, 316\\nin\ufb01nite, 316, 317\\ndiscounted, 3', '16\\nundiscounted, 316\\nhyperplane, 42, 63\\ncanonical, 65\\nVC-dimension, 76\\nequation, 64\\nmarginal, 65\\nmaximum-margin, 64\\nminimal error, 84\\noptimal, 83\\npseudo-dimension, 242\\nsoft-margin, 84\\ntangent, 271\\nVC-', 'dimension, 42\\nhypothesis, 4\\nBayes, 25\\nbest-in-class, 26\\nconsistent, 17\\nlinear, 63\\nset, 4, 12\\n\ufb01nite, 8, 11\\nin\ufb01nite, 8, 33\\nsingle, 22\\ni.i.d., 361\\nidenti\ufb01cation in the limit, see language\\nidenti\ufb01cation i', 'n the limit\\nimpurity, 196, 197\\nentropy, 196\\nGini index, 196\\nmean squared error, 198\\nmisclassi\ufb01cation, 196\\ninconsistent, 11\\ncase, 21, 239\\nhypothesis, 21\\nindependence, see random variable inde-\\npendence', '\\npairwise on irrelevant alternatives,\\n228\\ninequality\\nAzuma\u2019s, 172, 371\u2013373, 376\\nBennett\u2019s, 371, 377, 378\\nBernstein\u2019s, 371, 377, 378\\nCauchy-Schwarz, 77, 94, 96, 102,\\n162, 180, 190, 273, 275, 342,\\n343, ', '367\\nChebyshev\u2019s, 365, 366, 377\\nconcentration, see concentration in-\\nequalities\\nH\u00a8older\u2019s, 180, 259, 342\\nHoe\ufb00ding\u2019s, 21, 39, 61, 158, 170,\\n173, 235, 238, 239, 369\u2013371,\\n373, 377, 378\\nJensen\u2019s, 36, 39, 5', '3, 76, 77, 102, 158,\\n190, 353, 374\\nKhintchine-Kahane, 103, 156, 374,\\n376\\nMarkov\u2019s, 288, 363, 366, 369\\nMcDiarmid\u2019s, 33, 35, 36, 117, 269,\\n371\u2013373, 376\\nPinsker\u2019s, 279\\nYoung\u2019s, 343\\ninference\\nautomata, 30', '3, 307\\ntransductive, 7\\ninput space, 11404 INDEX\\ninstances, 3, 11\\nsparse, 177\\nweighted, 143\\ninteraction, 1, 313, 314\\nIsomap, 285, 286, 290\\nJensen\u2019s inequality, 36, 39, 53, 76, 77,\\n102, 158, 190, 353, 3', '74\\nJohnson-Lindenstrauss lemma, 288\u2013290\\nKarush-Kuhn-Tucker conditions\\nsee KKT conditions, 356\\nkernel, 89, 90\\nbigram, 113\\ngappy, 113\\ncontinuous, 115\\nconvolution, 115\\ndi\ufb00erence, 116\\nempirical map, 96\u201398', ', 260\\nfunctions, 89, 90\\nGaussian, 94\\nmatrix, 92\\nmethods, 89, 90\\nn-gram, 120\\nnegative de\ufb01nite symmetric, 89, 103\\nnormalized, 97\\npolynomial, 92, 117\\npositive de\ufb01nite symmetric, 8, 89,\\n91, 92\\nclosure pro', 'perties, 99\\npositive semide\ufb01nite, 92\\nrational, 8, 83, 89, 106, 111, 113,\\n115, 119, 310\\nPDS, 112\u2013115\\nridge regression, see KRR\\nsequence, 106, 112, see also kernel\\nrational\\nsigmoid, 94\\nstring, 115\\ntenso', 'r product, 99\\nKernelPerceptron, see Perceptron algo-\\nrithm kernel\\nKhintchine-Kahane inequality, 103, 156,\\n374, 376\\nKKT conditions, 66, 73, 191, 249, 253,\\n255, 356, 357\\nKPCA, see PCA kernel\\nKullback-Le', 'ibler divergence, 279\\nlabels, 3, 8, 11, 25, 31, 42\\ncategories, 3\\nreal-valued, 3\\ntarget, 96\\ntrue, 5\\nvalues, 3\\nLagrange, 357\\nfunction, 354, see also Lagrangian\\nmultipliers, 85, 86\\nvariables, 66, 73, 74,', ' 354\\nLagrangian, 66, 67, 73, 74, 191, 248, 253,\\n255, 354\u2013357\\nlanguage\\nk-reversible, 310\u2013312\\naccepted, 295, 296, 304, 307\\ncomplement, 110\\ncontext-free, see context-free lan-\\nguage\\nformal, 339\\nidenti\ufb01ca', 'tion in the limit, 294, 303,\\n308, 310\\nlearning, 9, 293, 294, 303\\nlinearly separable, 115\\npositive presentation, 308\\nregular, 293, 295, 310\\nreverse, 304\\nreversible, 304, 305, 308\u2013310\\nlearning, 311\\nLapl', 'acian eigenmaps, 285\u2013288, 290, 291\\nLasso, 9, 237, 245, 257\u2013260, 266, 277\\ngroup, 261\\non-line, see OnLineLasso algorithm\\nlaw of large numbers\\nstrong, 326, 327INDEX 405\\nweak, 366\\nlearner, 7\\nactive, 296, ', '313\\nbase, 123, 127, 130, 136, 139, 143,\\n144, 191\\nconsistent, 5\\npassive, 313\\nstrong, 122\\nweak, 121, 129, 130, 136, 141, 143,\\n194, 206, 214\\nlearning, 115, 313\\nactive, 8\\nexact, 294, 295\\non-line, 7\\npolicy', ', 334\\nproblem, 314\\nrandomized, 153\\nreinforcement, 8\\nsemi-supervised, 7\\nsupervised, 7\\ntransductive, 7\\nunsupervised, 7\\nwith queries, 297\\nlearning bound,see generalization bound\\nconsistent case, 17\\n\ufb01nite', ' hypothesis set, 17, 23\\ninconsistent case, 23\\nLearnReversibleAutomata algorithm, 303,\\n304, 306\u2013310\\nlemma\\ncontraction, see Talagrand\u2019s lemma\\nHoe\ufb00ding\u2019s, 369\\nJohnson-Lindenstrauss, 288\u2013290\\nMassart\u2019s, 39', ', 40, 54, 56, 258\\nSauer\u2019s, 45\u201348, 55, 56, 58\\nTalagrand\u2019s, 56, 78, 186, 240, 254\\nlinearly separable, 70, 71, 77, 83, 90,\\n93, 115, 118, 140, 162\u2013164, 166,\\n167, 224,see also realizable set-\\nting\\nLipschit', 'z\\nfunction, see function Lipschitz\\nproperty, 79, 321\\nLLE, 287, 288, 290, 292\\nlocally linear embedding, see LLE\\nlogistic regression, 128, 129, 141, 142\\nloss\\n\u03f5-insensitive, 252\\nquadratic, 255\\n\u03c3-admissib', 'le, 271\\naverage, 172\\nbinary, see loss, zero-one\\nbounded, 171\\nconvex, 128\\nconvex upper bound, 126, 128\\ncumulative, 148\\nexpected, 139\\nexponential, 126\\nfunction, 4, 34, 238\\nHamming, 204\\nhinge, see hinge ', 'loss\\nHuber, 256\\nlogistic, 128\\nmargin, 77, 185\\nempirical, 78\\nmatrix, 138\\nmisclassi\ufb01cation, 4\\nmulti-label, 192\\nnon-convex, 181\\nnon-di\ufb00erentiable, 277\\npairwise ranking, 213\\nexponential, 218\\nranking\\ndisag', 'reement, 227\\ntop k, 232\\nsquared, 4, 148, 238\\nunbounded, 238\\nzero-one, 4, 37, 148, 154\\npairwise misranking, 218\\nM\\n3N, 205, 207406 INDEX\\nmanifold learning, 2, 281, 284, 285, 290,\\nsee also dimensionality', ' reduc-\\ntion\\nmargin, 63, 64, 75, 162, 185\\nL1-, 131, 132\\nbound, 8, 80\\ngeometric, 75\\nhard, 71\\nloss, 77, 78, 185\\nempirical, 78\\nmaximum-, 64, 65, 136, 137, 140,\\n177, 233\\nmulti-class, 185\\npairwise ranking,', ' 211\\nsoft, 71, 84, 141, 142\\ntheory, 8, 64, 75, 83, 121, 137\\nmargin bound\\nbinary classi\ufb01cation, 80\\ncovering numbers, 233\\nensemble\\nRademacher complexity, 133\\nranking, 220\\nVC-Dimension, 133\\nkernel-based ', 'hypotheses, 103\\nmulti-class classi\ufb01cation, 187, 190\\nranking, 212, 234\\nkernel-based hypotheses, 213\\nMarginPerceptron, 177, 178\\nMarkov decision process, see MDP\\nMarkov\u2019s inequality, 288, 363, 366, 369\\nm', 'artingale di\ufb00erences, 371, 373, 376\\nMassart\u2019s lemma, 39, 40, 54, 56, 258\\nmatrix, 344\\nGram, 68\\nidentity, 66\\nkernel, 92\\nloss, 138\\nmultiplication, 108\\nnorm\\ninduced, 344\\npositive semide\ufb01nite, 346\\ntrace, 1', '03, 344, 346\\ntranspose, 344\\nupper triangular, 346\\nmaximum likelihood, 129\\nMaximum-Margin Markov Networks,see\\nM\\n3N\\nMcDiarmid\u2019s inequality, 33, 35, 36, 117,\\n269, 371\u2013373, 376\\nMDP, 313, 314\\nenvironment, ', '315\\n\ufb01nite, 315\\npartially observable, 336\\nmean, 363, 366, 367, 369, 373, 377\\nestimation, 326\\nzero-, 360, 364, 378\\nmeasurable, 12, 34, 359\\nfunction, 25, 118, 243, 353\\nsubset, 237\\nMercer\u2019s\\ncondition\\nsee ', 'condition Mercer\u2019s, 396\\ntheorem, 91\\nmetric space, 320\\ncomplete, 320, 321\\nmirror image, 304\\nmistake, 149\u2013152, 171, 177\\nbound, 8, 149\u2013151, 161, 166, 169,\\n171, 176\\ncumulative, 153\\nmodel, 148, 171\\nrate, 1', '50\\nmodel\\nbased approach, 326\\ncontinuous-time, 315\\ndiscrete-time, 315\\ndistribution-free, 13\\nfree approach, 326\\nselection, 5, 6, 27\\nmoment-generating function, 288, 364,\\n365, 370\\nmono-label case, 183\u201318', '5, 207INDEX 407\\nmulti-label\\ncase, 183, 184, 192, 207\\nerror, 207\\nloss, 192\\nn-way composition, 113, 115\\nNDCG, see DCG normalized\\nNDS kernel, see kernel negative-de\ufb01nite\\nsymmetric\\nNFA, 295, 309\\nconsisten', 't, 309\\nnode impurity, see impurity\\nnoise, 25, 26, 30, 54, 140\u2013142, 144\\nassumption, 26\\naverage, 25, 26\\nlearning in presence of, 30\\nmodel, 31\\nrandom, 34, 141, 142, 328\\nrate, 30, 31\\nsource, 198\\nnon-conve', 'x\\nloss, 181\\nnon-di\ufb00erentiable loss, 271, 277\\nnon-realizable case, 11, 33, 50, 51, 54, 55,\\n150\\nnorm, 341\\nequivalent, 341\\nFrobenius, 345\\ngroup, 189, 261, 345\\nmatrix, see matrix norm\\nspectral, 344\\nvector', ', see vector norm\\nOccam\u2019s razor principle, 24, 29, 48, 63,\\n239, 296\\non-line learning, 147\\non-line to batch conversion, 147, 171,\\n176, 181\\nOn-line-SVM algorithm, 177\\none-versus-all, 8, 198\u2013202, 206\\none', '-versus-one, 8, 199\u2013202, 208\\none-versus-rest, see one-versus-all\\nOnLineDualSVR algorithm, 262\\nOnLineLasso algorithm, 262, 265, 266\\noperator norm, 344\\noptimization\\nconstrained, 354\\ndual, 355\\nprimal, 35', '4\\noutlier, 71, 72, 74, 141\\nOVA, see one-versus-all\\nOVO, see one-versus-one\\nPAC-learning, 8, 11, 13, 14, 16, 18\u201321,\\n26, 28\u201333, 54, 59, 121, 147\\nagnostic, 24, 25, 50\\nalgorithm, 13, 14, 18, 32, 58\\ne\ufb03cien', 'tly, 13\\nmodel, 11, 13, 14, 20, 24, 28, 29\\nweakly, 121\\nwith membership queries, 297\\npacking numbers, 55\\npairwise consistent, 227\\nparadigm\\nstate-partitioning, 303\\nstate-splitting, 303\\nparse tree, 106\\npa', 'rtially observable Markov decision\\nprocess, see POMDP\\npath, 107\u2013111, 114, 115, 161, 175, 294,\\n295\\n\u03f5-, 109, 110, 115\\naccepting, 107, 108, 111, 112, 114,\\n294, 295, 305\\nlabel, 107\\nmatching, 109\\nredundant', ', 109\\nshortest- problem\\non-line, 175\\nsuccessful, see accepting\\nPCA, 9, 281\\nkernel, 9, 281, 283\u2013288, 290, 292408 INDEX\\nPDS kernel, see kernel positive-de\ufb01nite\\nsymmetric\\nPerceptron algorithm, 8, 84, 147', ', 159\u2013\\n163, 166\u2013169, 171, 176\u2013178, 234\\ndual, 167, 168\\nkernel, 168, 176, 181\\nmargin, see MarginPerceptron\\nranking, see RankPerceptron\\nupdate, 177\\nvoted, 163, 168\\nPinsker\u2019s inequality, 279\\npivot, 230\\npl', 'anning, 9\\nalgorithm, 319\\nproblem, 313, 314, 319\\npolicy, 313\u2013315, 322, 326\\n\u03f5-greedy, 333\\niteration, 319, 322\u2013324, 337,see also\\nPolicyIteration algorithm\\nlearning, 334\\nnon-stationary, 316\\nstationary, 31', '5\\nvalue, 313, 316\\nPolicyIteration algorithm, 323\\nPolynomial-Weighted-Average algorithm,\\n179\\nPOMDP, 336\\npositive semide\ufb01nite, 92, 346\\npotential function, 151, 152, 154, 157,\\n170, 179, 180\\nprecision, 23', '2\\naverage, 232\\npreference\\n-based\\nranking, 9\\nsetting, 209, 210, 226, 227, 233\\nfunction, 210, 211, 226\u2013230\\npre\ufb01x, 114, 294, 301, 304, 308\\nprincipal component analysis, see PCA\\nprior knowledge, 4, 96, 98', '\\nprobabilistic method, 48, 55, 288\\nprobability, 359\\nconditional, 361\\ndistribution, 359\\njoint mass function, 359\\nmass function, 359\\ntheorem of total, 362\\nprobably approximately correct,see PAC\\npseudo-d', 'imension, 237, 239, 242\u2013245,\\n262\\npseudo-inverse, 98, 246, 287, 346\\nQ-learning\\nalgorithm, 326, 330\u2013332, 334, 335,\\n337\\nupdate, 332\\nQP, 66, 68, 83, 85, 192, 200, 205, 253,\\n255, 259, 260\\nconvex, 66, 74\\nqu', 'adratic programming, see QP\\nquery\\nequivalence, 297, 298, 300, 303, 311\\nmembership, 297\u2013303, 311\\nsubset, 226, 227\\nQueryLearnAutomata algorithm, 298,\\n300\\nQuickSort algorithm, 230\\nrandomized, 230, 231, 2', '34\\nRademacher complexity, 8, 33\u201340, 54, 56,\\n63, 78, 84, 133, 134, 183, 189,\\n190, 209, 211, 213, 220, 233,\\n237, 239, 241, 245, 267, 380\\nL\\np loss functions, 240\\nbinary classi\ufb01cation bound, 37\\nbound, 48,', ' 240, 254, 259\\nconvex combinations, 132, 133\\nempirical, 34, 37, 38, 55, 77, 102,\\n103, 186, 380\\ngeneralization bounds, 103\\nkernel-based hypotheses, 102, 247\\nlinear hypotheses, 77INDEX 409\\nlinear hypoth', 'eses with bounded L1\\nnorm, 257, 258\\nlocal, 54\\nmargin bound\\nbinary classi\ufb01cation, 80\\nensembles, 133\\nmulti-class classi\ufb01cation, 187\\nranking, 212\\nmulti-class kernel-based hypotheses,\\n189, 206\\nregression ', 'bound, 239, 240, 262\\nRademacher variables, 34\\nradial basis function, 94\\nRadon\u2019s theorem, 43, 44\\nrandom variable, 359\\nindependence, 39, 76, 289, 327, 361,\\n363, 365, 370, 376\\nindependent, 363, 365, 367\\n', 'measurable, 359\\nmoment-generating function, 364\\nRandomized-Weighted-Majority algorithm,\\n147, 153\u2013155, 175, 179\\nrank aggregation, 233\\nRankBoost, 8, 206\u2013209, 214\u2013220, 222\u2013\\n224, 233\u2013235\\nranking, 2, 7, 20', '9, 229\\nbipartite, 221, 234\\nmultipartite, 235\\nRankBoost, 214\\nwith SVMs, 213\\nRankPerceptron, 234\\nrate\\nfalse positive, 225, 226\\ntrue positive, 225, 226, 232\\nrational kernel, 8, 83, 89, 106, 111, 113,\\n115', ', 119, 310\\nPDS, 112\u2013115\\nRayleigh quotient, 283, 346\\nRBF, see radial basis function\\nrealizable case, 11, 49, 55, 59, 149\u2013152,\\n162, 163\\nrecall, 232\\nregression, 2, 237\\nboosting trees, 263\\ndecision trees,', ' 263\\ngroup norm, 260\\nKRR, 245, 247\\nLasso, 245, 257\\nlinear, 237, 245\\nneural networks, 263\\non-line, 261\\nordinal, 234\\nridge, see KRR\\nSVR, 245, 252\\nunbounded, 238, 262\\nregret, 148, 152, 154\u2013157, 159, 172,', ' 173,\\n175, 179\u2013181, 228, 229\\naverage, 155\\nbound, 157\u2013159, 174, 175, 179, 180,\\n209, 229\\nsecond-order, 179\\ncumulative, 179\\nexternal, 148, 175, 176\\ninstantaneous, 179, 180\\ninternal, 175, 176\\nlower bound,', ' 155\\nminimization, 173\u2013175, 179\\nper round, 155\\npreference function, 228, 229\\nranking, 228\\nswap, 175, 176\\nweak, 228\\nregular\\nexpression, 114, 295\\nlanguage, 295\\nregularization, 28, 142, 246\\nL\\n1-, 141, 14', '2\\n-based algorithm, 28\\nparameter, 28, 181, 197\\npath, 259\\nterm, 28, 248, 250, 257, 271\\nregularizer, 28410 INDEX\\nrelative entropy, 142, 170, 171, 279\\nrepresenter theorem, 101, 115\\nreproducing\\nkernel Hil', 'bert space, see Hilbert\\nspace\\nproperty, 95\\nreward, 8, 313\u2013316, 330, 332, 335\\ncumulative, 318\\ndelayed, 314\\ndeterministic, 315, 317, 331\\nexpected, 316, 319, 322\\nfuture, 316, 335\\nimmediate, 8, 314, 316, ', '326, 335\\nlong-term, 8\\nprobability, 315, 319, 325, 326\\nvector, 320\\nrisk, 12, 380, see also error\\nempirical, 12, 380\\nminimization, see ERM\\nempirical minimization, 27\\npenalized empirical, 181\\nstructural\\n', 'minimization, see SRM\\nRKHS, see Hilbert space\\nROC curve, 209, 224\u2013226, 233, see also\\nAUC\\nRWM algorithm, see Randomized-\\nWeighted-Majority algorithm\\nsaddle point, 356, 357\\nnecessary condition, 356\\nsu\ufb03c', 'ient condition, 356\\nsample\\ncomplexity, 1, 11, 14, 16\u201318, 29, 30,\\n33, 52, 58\\ntest, 4\\ntraining, 3\\nvalidation, 3\\nsample space, 359\\nSARSA algorithm, 334, 335\\nSauer\u2019s lemma, 45\u201348, 55, 56, 58\\nscenario\\ndete', 'rministic, 25, 184, 210, 237\\nrandomized, 153\\nstochastic, 24, 25, 147, 184, 210, 227,\\n237\\nscore-based setting, 209, 211, 214, 221,\\n226, 227, 233\\nscores, 4\\nscoring function, 185, 189, 199, 202, 203,\\n210', ', 211, 235\\nsequence, 90, 106, 110, 111\\nkernel, 89, 106, 108, 111, 112\\nbigram, 113\\nmapping, 111\\nprotein, 106\\nsimilarity, 106\\nstochastic, 155\\nsequential minimal optimization algo-\\nrithm, see SMO algorit', 'hm\\nsetting\\ndeterministic, 25\\nstochastic, 24, 25, 171\\nshattering, 41, 241\\ncoe\ufb03cient, 55\\nwitness, 241\\nshortest-distance algorithm, 108, 111,\\n115\\nall-pairs, 286\\nsingular\\nvalue, 283\u2013288, 344\u2013346\\nvalue dec', 'omposition, see SVD\\nvector, 282\u2013288, 291, 346, 347\\nslack variable, 71, 84, 191, 206, 214, 222,\\n248, 252\\nSMO algorithm, 68, 83, 85, 86\\nsort-by-degree algorithm, 229\\nSPSD, see symmetric positive semide\ufb01', '-\\nnite\\nSRM, 27\u201329\\nstability, 233, 251, 256, 267\u2013270, 277,\\n278, 372, 373INDEX 411\\nbound, 268, 277\\nKRR, 275, 278\\nranking, 277\\nregression, 278\\nSVM, 276, 278\\nSVR, 274\\ncoe\ufb03cient, 267, 268, 270\u2013276\\nkernel, ', '263, 278\\nstable, 268, 273\\nstandard deviation, 6, 86, 365, 367\\nstandard normal\\ndistribution, 289, 290, 292, 360, 361,\\n364, 374\\nform, 374\\nrandom variable, 289\\nstate, 107, 313, 315\\ndestination, 294\\n\ufb01nal,', ' 107\\ninitial, 107, 315\\norigin, 294\\nstart, 315\\nstate-action\\npair, 332\\nvalue, 333, 334\\nvalue function, see function\\nstationary point, 349\\nstochastic\\napproximation, 326\\ngradient descent, 161, 177, 261, 2', '63,\\n266\\noptimization, 327, 337\\nstochasticity, 318\\nstrategy, 139\\ngrow-then-prune, 197\\nmixed, 138, 139\\npure, 138, 139\\nstring, 107, 108, 112, 113, 119, 294, 295,\\n298\u2013300, 303\u2013305, 307\u2013312\\naccepted, 295, ', '296, 304, 305\\naccess, 299\\ncounter-example, 300\\ndistinguishing, 299, 301\\nempty, 106, 294, 295\\n\ufb01nality, 299\\nkernel, 106\\nleaf, 300\\nnegative, 296\\npartition, 299, 301\\npositive, 296, 306\\nrejected, 296, 309\\n', 'structural risk minimization, see SRM\\nstructure, 203\\nstructured\\noutput, 203, 204\\nprediction, 2, 183, 184, 203\u2013205, 207\\nsubgradient, 272, 273\\nsubsequence, 119\\nsubsequences, 106\\nsubstring, 106\\nsum rule,', ' 362\\nsupermartingale convergence, 328, 329\\nsupport vector, 67, 74, 162\\nmachine, see SVM\\nnetworks, 83\\nregression, see SVR\\nSVD, 98, 99, 345\\nSVM, 8, 63\u201375, 82\u201387, 89\u201391, 94, 100\u2013\\n102, 106, 115, 118, 119,', ' 131,\\n137, 142, 143, 162\u2013164, 166\u2013\\n168, 176, 177, 191, 192, 200,\\n201, 205, 209, 213, 214, 222,\\n233, 252, 253, 255, 256, 267,\\n271, 276, 278\\nmulti-class, 8, 183, 191, 203, 204,\\n206\\nranking with, 8, 213,', ' 214, 233, 234\\nregression, see SVR\\nSVMStruct, 205\\nSVR, 237, 245, 252, 255\u2013257, 260, 261,\\n263, 267, 271, 274, 275\\ndual, 262, 264412 INDEX\\non-line, see OnLineDualSVR al-\\ngorithm\\nHuber loss, 264\\non-line,', ' 263\\nquadratic, 255, 256, 264\\non-line, 266\\nstability, 274\\ntarget\\nconcept, 12\\nvalues, 11\\nTD(\u03bb) algorithm, 335, 336\\nTD(0) algorithm, 330, 331, 335\\ntheorem\\ncentral limit, 367\\nFermat\u2019s, 349\\nFubini\u2019s, 49, ', '363\\nMercer\u2019s, 91\\nRadon\u2019s, 43, 44\\nrepresenter, 101\\nvon Neumann\u2019s minimax, 139, 174\\ntransducer\\nacyclic, 108\\ncomposition, 108, 109, 115, 380\\ncounting, 113, 114\\ninverse, 112\\nweighted, 106\u2013109, 111\u2013113\\ntra', 'nsition, 107\u2013112, 114, 294, 295, 299\u2013\\n301, 304, 306\u2013308, 310, 315\u2013\\n317, 322, 326\\nlabel, 107\\nprobability, 315, 317\u2013320, 325, 326\\ntrigrams, 90\\ntrue positive rate, 225, 226, 232\\nuniform convergence bound', ', 17, 23\\nuniform stability, see stability\\nuniformly \u03b2-stable, see stable\\nunion bound, 15, 362\\nupdate rule, 85, 169, 334\\nadditive, 169\\nmultiplicative, 169, 176\\nvalue iteration, 319, 324, see also Val-\\n', 'ueIteration algorithm\\nValueIteration algorithm, 320\\nvariance, 6, 54, 70, 166, 282\u2013284, 289,\\n290, 365, 366, 371, 377, 378\\nunit, 287, 288, 360\\nVC-dimension, 8, 33, 41\\nensemble margin bound, 133\\ngenerali', 'zation bound, 48\\nlower bounds, 48, 49, 51\\nvector, 341\\nnorm, 341, 344, 345\\nsingular\\nleft, 345, 346\\nright, 345, 346\\nspace, 341, 342\\nnormed, 374\\nvon Neumann\u2019s minimax theorem, 139,\\n174\\nweight function, 2', '31, 235\\nWeighted-Majority algorithm, 147, 150\u2013\\n152, 154, 156, 169, 175,see also\\nRandomized-Weighted-Majority\\nalgorithm\\nWidrow-Ho\ufb00 algorithm, 261\\non-line, 263\\nWinnow\\nalgorithm, 8, 147, 159, 168\u2013171, 17', '6\\nupdate, 169\\nWM algorithm, see Weighted-Majority\\nalgorithm\\nYoung\u2019s inequality, 343Adaptive Computation and Machine Learning Thomas Dietterich, Editor\\nChristopher Bishop, David Heckerman, Michael Jord', 'an, and Michael Kearns, As-\\nsociate Editors\\nBioinformatics: The Machine Learning Approach, Pierre Baldi and S\u00f8ren Brunak\\nReinforcement Learning: An Introduction, Richard S. Sutton and Andrew G. Barto\\n', 'Graphical Models for Machine Learning and Digital Communication, Brendan J.\\nFrey\\nLearning in Graphical Models, Michael I. Jordan\\nCausation, Prediction, and Search, second edition, Peter Spirtes, Clark', ' Glymour,\\nand Richard Scheines\\nPrinciples of Data Mining, David Hand, Heikki Mannila, and Padhraic Smyth\\nBioinformatics: The Machine Learning Approach, second edition, Pierre Baldi and\\nSoren Brunak\\nLe', 'arning Kernel Classi\ufb01ers: Theory and Algorithms, Ralf Herbrich\\nLearning with Kernels: Support Vector Machines, Regularization, Optimization,\\nand Beyond, Bernhard Sch\u00a8olkopf and Alexander J. Smola\\nIntr', 'oduction to Machine Learning, Ethem Alpaydin\\nGaussian Processes for Machine Learning, Carl Edward Rasmussen and Christo-\\npher K.I. Williams\\nSemi-Supervised Learning, Olivier Chapelle, Bernhard Sch\u00a8 ol', 'kopf, and Alexander\\nZien, Eds.\\nThe Minimum Description Length Principle, Peter D. Gr\u00a8unwald\\nIntroduction to Statistical Relational Learning, Lise Getoor and Ben Taskar, Eds.\\nProbabilistic Graphical Mo', 'dels: Principles and Techniques, Daphne Koller and\\nNir FriedmanIntroduction to Machine Learning, second edition, Ethem Alpaydin\\nBoosting: Foundations and Algorithms, Robert E. Schapire and Yoav Freund', '\\nMachine Learning: A Probabilistic Perspective, Kevin P. Murphy\\nFoundations of Machine Learning, Mehryar Mohri, Afshin Rostamizadeh, and\\nAmeet Talwalkar']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = model.encode(chunks)\n",
    "\n",
    "len(embeddings)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 699,
     "referenced_widgets": [
      "f4dc73cdda5042328b9c1cb805e53fc3",
      "5945cb8b9de44a719f5a56e9aa967a6d",
      "9f5a8be40a234a3981916c5d182edf31",
      "6b2145ff08084bd4ae2242badf95778c",
      "b2a9d5d452fb440dafb072859d13a441",
      "d369b5399c9e411cb54d3ffa804e1cd1",
      "932b9e40600e40f0bef27e741b66a048",
      "9d35279fbcbb46788f17423110222309",
      "274adf58247745be8c0a3a653b67b4c9",
      "effb9666ad6b4e408dc9082d02890340",
      "4fffd0627840477d8295119e2eaeffd0",
      "a0e71d8bab664ed29eb7c620d1c978e9",
      "a08436aead0a455886c16fc8414c0ac4",
      "67d3fbf334754294a14c53a901cf900c",
      "a74fd30ad1994ab0bfa426f769ee3f1e",
      "2d50a2c9a7984d9092d99fc4d94aac55",
      "b31667adac2649468ecb7b9bedfb5937",
      "300707dea0134ec788f7eabaae92971d",
      "62229c4222eb430e9371e69961f2ee24",
      "ff4b4be571034abe973de305e55e74b6",
      "5488e62b356b4af29b4e848b3fa0592e",
      "475a48b0dcf24c9487867240f599bf26",
      "a0e81a79549049c493361ed6e27944e6",
      "554de0085d2b4fe7ba76eb72e39257a0",
      "2827091eb5fe4a92849a3f75b825ed4a",
      "a3548fad419048c29a63cc9b288e89b1",
      "dfb7abb1ac9d427d94d669fdc5da3269",
      "c2cd0a1f12fe4b13a3d46ae37b8b67c9",
      "c319b3380f2f49cbabb9106d3f5402be",
      "ee2235f8a68b41f2ace03afd2ace2170",
      "0c0a59cc263d47e793dad47ef43f1871",
      "d07c59cc331c46e99a65b975678b6ef7",
      "489893ea441a4d819eb747233be2aac6",
      "37fecb728075412eb24b27e2fae86171",
      "07d38d47a83c40a7b197bb57d1f66051",
      "24a608b1e7044a6daaf9fdc52f316c09",
      "a9c61c96c76e439182ceda930c8d2326",
      "b2bb92f551a74997975ae05b32a11ac8",
      "e99a426551b2472bbb98db0db044237e",
      "413ffe15ec3348b6a16f29f42bab87a0",
      "98cd8debcaea4da089bd75e800f240a1",
      "752f470649444356ab5a7324a06ad54e",
      "7e2ba231402c4a20a3642bfce177474b",
      "b8d035a4c23e45749780bd327b400869",
      "325f5f2bf64e45fcbdef7998cf38d3fb",
      "62c5964e511f471ebcc8624aac5b5a43",
      "95a1c4d4f310432daa87aef65069268c",
      "529c4eac06d842788107a9df26bc5570",
      "4f95290669f047d6bd7095b532b4c2f5",
      "830d90d31d934539a6fe711034e4ab55",
      "efc87801d9f64a809a77ed1680dea56c",
      "96b7b11788ca4b95ab90e1fbdfee15e3",
      "21622ebdcb3c4d9580d0eae8e94405b9",
      "8326183fef86499d9079ae96d8d40db9",
      "a6f99d13e9644efa9f96e1ee85aedf56",
      "d858a44f155e44a5914775a6c7810b17",
      "7411f206722949f79175a1b87d36a594",
      "0f78d82e5aa94a238c4ef276277738ef",
      "32293617e1ba402c969637c59c2d8eb6",
      "1bcde5cc389d4e8d9f80a927af605031",
      "d478e46e9cec44b494e7278ecb03ce47",
      "a2a508fecc784b0086714c8b980efb71",
      "23adcb7c40944587b45d904f77c1c70e",
      "9652581594a04940a1e24773d2665e9e",
      "564365b2269441e0979fc9b1aab5166e",
      "ff1eaeafa50c41d786f347982beb45bc",
      "f964e11cc7e345c4baa8ae6d60ac557e",
      "6c05333e0fd9433c9a613f85bc9d54fc",
      "66580ed9369140348a14211a66f973f7",
      "10f605f626a842be901fc1a18f793e7c",
      "0650d6f1b27340b6b2b9cb45d03fdefd",
      "2f13f98a8ab14667973f48ffb7c16197",
      "11b7341cc32840868352990a068cce7c",
      "9269a54df5584cc8a8c9c30838bbe79f",
      "d8e1f145e29d4b73aae18e36015d1e26",
      "7a8b787e6bc64ea8a0d9823d2fdbb30d",
      "23f370f37f59442ebf5d4b74912ac26e",
      "6a036eda18a745ca830dc8bc7421007c",
      "adce58cb217e4f0dab26ee8dfdbf9b0f",
      "a1a5aed199804c92b64d4c767a8f795b",
      "31d1ad87666b487c88264d93f3e2d59e",
      "4b1f677274344c74a7742702d1e4fdab",
      "983fd16a56c24c26aaffcf6b0b6a98d5",
      "c0bbfe2bb48d4c4f8e911253d2717933",
      "280a844c6c6c46c4a3689cc7b523cd56",
      "20ab0ae3c2934212b29a6f3c454f5f27",
      "aaf470808b774fafa122ce3882b72c8b",
      "fac09b8f05664e77856e24e25c4e9d81",
      "8a62e62c29464f2f8c024d6a2d239874",
      "d63cb1fc18614dcba5527c5204d73401",
      "a029e56e21744d1f9f48a3ee9f8cc83a",
      "e4fc46f7b3ac4b6ab76dd2d93c6868c2",
      "a46c874e0a13446c9e5e7a23f0bedd48",
      "bb98a21947dd42fa8b1848ed444cc089",
      "b19c451ef23d4267a01f9a971cbdf741",
      "d1eae5a64d2e4209b9a81f1ac511202b",
      "6999ad4b06854079b885dbf3ba76ba41",
      "2ccb35e7aa7647e4bdaea2680bd165b2",
      "ecbe813298974ff5bf06243135bc1c08",
      "65066ae6e0404ba2a3c81f22474bd216",
      "2e30678f501e4b5ea8609bc36c03bc74",
      "e2793c3a17d3431583af69e4845e05db",
      "5a36fd1d42dd4f00aac380ea891d235c",
      "98c8cf1de6504de9bd4bb21186949451",
      "778672550dc34c1c95c954949101d6cc",
      "677502514c5f4688a51f504866de7193",
      "e1a46dcb55dc499ab883b840ad0d45aa",
      "1284fa80df6a41b7b5393e023946a6b7",
      "749a9dcf4435457488cb3b4d9e4b400e",
      "a7b1db09597a4e9aadfdfe8aee6a65d2",
      "f4d7e623cb784bccbc088ebf161b1956",
      "d45bd6c5087b4d4089f64677547a810f",
      "a391ffc3937f440db2f80b128bf6e48c",
      "2b16782a58d94d78a9fecd5aac159283",
      "b8ec5905fa4e431db74313465667e68a",
      "a6f4b6e8895d4d1ba8d4b6e591786887",
      "2fd33648ab4a45949e18736dfaafb887",
      "9de6f8ec258346528171ed419baba2dd",
      "9c1dc23e09604a95aef823e83168e630",
      "2c820bc573f94884ae943f1a4f68e34c",
      "1bbe3df669554690a9de62cc9e819445",
      "323853043d3c46029c06a0cf81a469b5",
      "db692b49561b4fc2919e8cce6a30cd24",
      "b169499c68e04a44ba3c185d86c71f03",
      "6f3d434bd63b4841bb45b0754a8181f1",
      "26910971e223477ca50f137adfc12870",
      "c0c7c4c21d24439fadfe2af49531118e",
      "43cfe7c4b6f748beaa9f0be6398480b4",
      "489a0c87fafe4ac7884662099c40439e",
      "a52f5313dba44c3981fa3231e2bffb55",
      "bc3845fd18ae42559458df010ddfc8dc",
      "763150679547415cb3256c53b4587d1c"
     ]
    },
    "collapsed": true,
    "id": "UN6MC8W6isAz",
    "outputId": "40dac282-368a-4416-aa32-dd335960e01e"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f4dc73cdda5042328b9c1cb805e53fc3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a0e71d8bab664ed29eb7c620d1c978e9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a0e81a79549049c493361ed6e27944e6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "37fecb728075412eb24b27e2fae86171"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "325f5f2bf64e45fcbdef7998cf38d3fb"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d858a44f155e44a5914775a6c7810b17"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f964e11cc7e345c4baa8ae6d60ac557e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6a036eda18a745ca830dc8bc7421007c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8a62e62c29464f2f8c024d6a2d239874"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "65066ae6e0404ba2a3c81f22474bd216"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f4d7e623cb784bccbc088ebf161b1956"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "323853043d3c46029c06a0cf81a469b5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3977"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "vectors = []\n",
    "\n",
    "for i, doc in enumerate(chunks):\n",
    "    vectors.append({\n",
    "        \"id\": f\"doc_{i}\",\n",
    "        \"values\": embeddings[i].tolist(),\n",
    "        \"metadata\": {\"text\": doc}\n",
    "    })\n",
    "\n",
    "#If you store full PDFs / long text inside metadata, payload size explodes.\n",
    "#If still error \u2192 reduce batch_size to 50 or 20.\n",
    "\n",
    "def batch_upsert(index, vectors, batch_size=200):\n",
    "    for i in range(0, len(vectors), batch_size):\n",
    "        batch = vectors[i:i+batch_size]\n",
    "        index.upsert(vectors=batch)\n",
    "\n",
    "batch_upsert(index, vectors, batch_size=200)"
   ],
   "metadata": {
    "id": "V23BLDjrjSi7"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "query = \"What is a machine learning?\"\n",
    "query_embedding = model.encode([query])[0]\n",
    "\n",
    "results = index.query(\n",
    "    vector=query_embedding.tolist(),\n",
    "    top_k=2,\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "for match in results[\"matches\"]:\n",
    "    print(match[\"metadata\"][\"text\"], \"Score:\", match[\"score\"])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tB7PvLDZjr6L",
    "outputId": "e7360ddf-d208-458a-d87c-ea1cc54018cc"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " to\n",
      "the success of the predictions made by the learner.\n",
      "Machine learning consists of designing e\ufb03cient and accurate prediction algo-\n",
      "rithms. As in other areas of computer science, some critical measur Score: 0.709820807\n",
      "mplementation or their adaptation to other learning scenarios.\n",
      "Machine learning is a relatively recent \ufb01eld and yet probably one of the most\n",
      "active ones in computer science. Given the wide accessibili Score: 0.701968253\n"
     ]
    }
   ]
  }
 ]
}