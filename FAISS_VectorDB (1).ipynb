{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOZkgsu1RyZRrpVuNTXapT7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"b20ca1cf618d4b849c46d13452503996":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_baec4060975045108ef04e38dca25dd7","IPY_MODEL_439b66d25e6548e698e143ed4f8e16c7","IPY_MODEL_7176d3a6db6a43cca948f93598dd480c"],"layout":"IPY_MODEL_ea0645d9c4334441b5ccca882314e791"}},"baec4060975045108ef04e38dca25dd7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bdc44a3b17a349b89c518480c1101153","placeholder":"​","style":"IPY_MODEL_fbb544b5bdf24796b29487600debca03","value":"modules.json: 100%"}},"439b66d25e6548e698e143ed4f8e16c7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d3cc8dec6c84e0396d013196d84fb17","max":349,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4ddf4e006e194a2fad2352ade0eaf1b0","value":349}},"7176d3a6db6a43cca948f93598dd480c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_357c757f430b496fadbbe91f648343c0","placeholder":"​","style":"IPY_MODEL_5030db9b2f0f4b98b4500a934ac0ffc0","value":" 349/349 [00:00&lt;00:00, 19.3kB/s]"}},"ea0645d9c4334441b5ccca882314e791":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bdc44a3b17a349b89c518480c1101153":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fbb544b5bdf24796b29487600debca03":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6d3cc8dec6c84e0396d013196d84fb17":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ddf4e006e194a2fad2352ade0eaf1b0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"357c757f430b496fadbbe91f648343c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5030db9b2f0f4b98b4500a934ac0ffc0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c9e53eb55b7d42aeb0d146e177042126":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d1911460d22843328d34ef230dc10b5e","IPY_MODEL_2ee789fcbf854633a14675c6e84ef843","IPY_MODEL_4f249c79562949498be9c00e68f3aa56"],"layout":"IPY_MODEL_453ad2d9bf9d4ef985c5cd8b9772da12"}},"d1911460d22843328d34ef230dc10b5e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a9ccda1e024541c49691e3a2083fc5fe","placeholder":"​","style":"IPY_MODEL_2f1764ac61f74024a1406eaf70d87a1c","value":"config_sentence_transformers.json: 100%"}},"2ee789fcbf854633a14675c6e84ef843":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a7a6a412aac4fea9bff48f564c4c4f9","max":116,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e0c11e6feaf94707aa04185c86b135af","value":116}},"4f249c79562949498be9c00e68f3aa56":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_06a7041dd8bc4302a86087caeb672438","placeholder":"​","style":"IPY_MODEL_e08299523a544b43b10f9584f9534503","value":" 116/116 [00:00&lt;00:00, 5.88kB/s]"}},"453ad2d9bf9d4ef985c5cd8b9772da12":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9ccda1e024541c49691e3a2083fc5fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f1764ac61f74024a1406eaf70d87a1c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8a7a6a412aac4fea9bff48f564c4c4f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0c11e6feaf94707aa04185c86b135af":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"06a7041dd8bc4302a86087caeb672438":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e08299523a544b43b10f9584f9534503":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ebf981adb1984d32881748dcf2b8df69":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_58fde1ca389b4da0b90170fb489429d8","IPY_MODEL_7a10c2b3a61c430a943cb7329be09cc8","IPY_MODEL_673d63fa0eb847fb8c8d3b886635fdef"],"layout":"IPY_MODEL_d0e0bc9d69b34d02b8b2fd2a149c088e"}},"58fde1ca389b4da0b90170fb489429d8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_65eb81bc0cd54314887d65f8d6c25fb3","placeholder":"​","style":"IPY_MODEL_2c91252501d44994ad22957fd200044a","value":"README.md: "}},"7a10c2b3a61c430a943cb7329be09cc8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_85405b2ade25423893b1868090abe554","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7431982e081d4629a657c1eef50b8f5f","value":1}},"673d63fa0eb847fb8c8d3b886635fdef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2dfb5fc67e3a4b4899cdf71e710f04ff","placeholder":"​","style":"IPY_MODEL_9af6800aa36d48339a95859187d6a6de","value":" 10.5k/? [00:00&lt;00:00, 663kB/s]"}},"d0e0bc9d69b34d02b8b2fd2a149c088e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65eb81bc0cd54314887d65f8d6c25fb3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c91252501d44994ad22957fd200044a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"85405b2ade25423893b1868090abe554":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"7431982e081d4629a657c1eef50b8f5f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2dfb5fc67e3a4b4899cdf71e710f04ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9af6800aa36d48339a95859187d6a6de":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c44b11ecc30b4e37a61db8ba9e38a65c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1feb2ddde91f4dc78f06a68fd5ad5fe3","IPY_MODEL_2559106ae98746a8b056e1c7ba5d266b","IPY_MODEL_a4265518daee48feb9c212b9e9c0a30b"],"layout":"IPY_MODEL_4afd0455163d4219b91a10496f4babf9"}},"1feb2ddde91f4dc78f06a68fd5ad5fe3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f33dda0ce05f4879971ee41c0799f98c","placeholder":"​","style":"IPY_MODEL_ff6db71b1c1048e2bef8e8ea562ca66a","value":"sentence_bert_config.json: 100%"}},"2559106ae98746a8b056e1c7ba5d266b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_143d4c2998094bfc8d60ebf3a5c56e08","max":53,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4c0476e97c54441b893badb536b79591","value":53}},"a4265518daee48feb9c212b9e9c0a30b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e19a9e205bc45c98fa10910dd10c15c","placeholder":"​","style":"IPY_MODEL_be639ec6a0924ecf83193a379a92d288","value":" 53.0/53.0 [00:00&lt;00:00, 4.05kB/s]"}},"4afd0455163d4219b91a10496f4babf9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f33dda0ce05f4879971ee41c0799f98c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff6db71b1c1048e2bef8e8ea562ca66a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"143d4c2998094bfc8d60ebf3a5c56e08":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c0476e97c54441b893badb536b79591":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5e19a9e205bc45c98fa10910dd10c15c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be639ec6a0924ecf83193a379a92d288":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c502f87705e34a26a9bc0910822f273a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5aaa75f74b06478fb887e934cb66b589","IPY_MODEL_f508bcc24d7f4569a6680acf42b16993","IPY_MODEL_4cfcfb4f1ccc4796ac9196d441aad245"],"layout":"IPY_MODEL_9e8553c251ac4b68bb7284782da8c0f7"}},"5aaa75f74b06478fb887e934cb66b589":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd1f60e79bea47d4ac7616b7a2338c48","placeholder":"​","style":"IPY_MODEL_4453e205da2146fd9ab91570d51eb11d","value":"config.json: 100%"}},"f508bcc24d7f4569a6680acf42b16993":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_68db6624565f4d22830c84908d853662","max":612,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e670515a26064b9c986b7c75667cbc51","value":612}},"4cfcfb4f1ccc4796ac9196d441aad245":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c955861da35467885156d30a7a6c052","placeholder":"​","style":"IPY_MODEL_ba72286ea83c4a2393992aaf384e40b7","value":" 612/612 [00:00&lt;00:00, 56.5kB/s]"}},"9e8553c251ac4b68bb7284782da8c0f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd1f60e79bea47d4ac7616b7a2338c48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4453e205da2146fd9ab91570d51eb11d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"68db6624565f4d22830c84908d853662":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e670515a26064b9c986b7c75667cbc51":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8c955861da35467885156d30a7a6c052":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba72286ea83c4a2393992aaf384e40b7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e201a10d5a0b420c9e02181277eb920e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fc3659470e3648b1a76c849048ffa0fa","IPY_MODEL_4f0ed3d077a241b49bcad1d1ae3d38ad","IPY_MODEL_db0dfe33dcdc4403a5ccf6f9a1de27f2"],"layout":"IPY_MODEL_a0ae516d522845fd95171d0bbd544a6f"}},"fc3659470e3648b1a76c849048ffa0fa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f21943c24f14a549e1a04d2c1ecdfbf","placeholder":"​","style":"IPY_MODEL_34695a3522804e239b44a02845a28603","value":"model.safetensors: 100%"}},"4f0ed3d077a241b49bcad1d1ae3d38ad":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_269f0fc174e942fd9ba40fe975d9c7f2","max":90868376,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f692b9a536ae4c0f9da3bd1543f24cb7","value":90868376}},"db0dfe33dcdc4403a5ccf6f9a1de27f2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a611552b86e448dfa3daff4a0c861a51","placeholder":"​","style":"IPY_MODEL_8f634d03f33f4b0fb3f8a6f2a410af6b","value":" 90.9M/90.9M [00:01&lt;00:00, 84.1MB/s]"}},"a0ae516d522845fd95171d0bbd544a6f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f21943c24f14a549e1a04d2c1ecdfbf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34695a3522804e239b44a02845a28603":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"269f0fc174e942fd9ba40fe975d9c7f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f692b9a536ae4c0f9da3bd1543f24cb7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a611552b86e448dfa3daff4a0c861a51":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f634d03f33f4b0fb3f8a6f2a410af6b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7b03617ed98f439fbb0a95e225e24a61":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_86f554a6ed6d4cf9a696e8789a8ea83c","IPY_MODEL_f346c358460b4c13b8088141e90efcd2","IPY_MODEL_c20af6f69c744a79b1b918278860740b"],"layout":"IPY_MODEL_97fe510b7e2043dcbf8ecc65f8e3e0f7"}},"86f554a6ed6d4cf9a696e8789a8ea83c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7e48d30f8864258b255b8d8e94ec307","placeholder":"​","style":"IPY_MODEL_883341fe1f1a49e58585b88550518b59","value":"Loading weights: 100%"}},"f346c358460b4c13b8088141e90efcd2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b55f01da6e1646de8c8be5b00739d3c9","max":103,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c537d3d6877c49d0b1461d78ecfde4ea","value":103}},"c20af6f69c744a79b1b918278860740b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_286bcb22854a4db19f5f06e8f854cd66","placeholder":"​","style":"IPY_MODEL_40eedc37cb6c4ffcbe43a5a910d2ac8d","value":" 103/103 [00:00&lt;00:00, 477.37it/s, Materializing param=pooler.dense.weight]"}},"97fe510b7e2043dcbf8ecc65f8e3e0f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7e48d30f8864258b255b8d8e94ec307":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"883341fe1f1a49e58585b88550518b59":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b55f01da6e1646de8c8be5b00739d3c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c537d3d6877c49d0b1461d78ecfde4ea":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"286bcb22854a4db19f5f06e8f854cd66":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40eedc37cb6c4ffcbe43a5a910d2ac8d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c5c75f0c35c49a2b6a69a6ad2078686":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4cfed9ccc3b74f4fa8dbac2dda7ac7cf","IPY_MODEL_3408b2c4ecc04651bdee6c3f9f02c10d","IPY_MODEL_3407a2df8c4d4341be9a8c5d31261aef"],"layout":"IPY_MODEL_618d5dbbd5e041f9908a58da02c77e07"}},"4cfed9ccc3b74f4fa8dbac2dda7ac7cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_af13f686f06d43fc8608cd55a7b1c141","placeholder":"​","style":"IPY_MODEL_a1d9ac8ad3b3465499b320699a8b99fa","value":"tokenizer_config.json: 100%"}},"3408b2c4ecc04651bdee6c3f9f02c10d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_39e5093dfbc44a1d9787fb1cc711795e","max":350,"min":0,"orientation":"horizontal","style":"IPY_MODEL_292ca77c18e54280b91fe8cc5dd727d8","value":350}},"3407a2df8c4d4341be9a8c5d31261aef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8374aa02bdb946218f9522f2421de271","placeholder":"​","style":"IPY_MODEL_2bcae1d2f0734d9a88c054254e723e15","value":" 350/350 [00:00&lt;00:00, 12.8kB/s]"}},"618d5dbbd5e041f9908a58da02c77e07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af13f686f06d43fc8608cd55a7b1c141":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1d9ac8ad3b3465499b320699a8b99fa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"39e5093dfbc44a1d9787fb1cc711795e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"292ca77c18e54280b91fe8cc5dd727d8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8374aa02bdb946218f9522f2421de271":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2bcae1d2f0734d9a88c054254e723e15":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"67c3e2bca9014ec4b5f5eb2cc1a2d121":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b63f6515b05e4255abf1084789ae18a4","IPY_MODEL_9aa8031cca36474680073061990d505c","IPY_MODEL_614b968caced42cc811c0708a8fc8098"],"layout":"IPY_MODEL_e04ec99b0d234d318eed6987762e759e"}},"b63f6515b05e4255abf1084789ae18a4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c7268b169ec4e77857c9a50cb125a5c","placeholder":"​","style":"IPY_MODEL_b769630311614cada2bc64de1f0b1c7e","value":"vocab.txt: "}},"9aa8031cca36474680073061990d505c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6bf0c66c4dbc4b79880b4835df7191a9","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6910898877404007919e0f462f054386","value":1}},"614b968caced42cc811c0708a8fc8098":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b9c5d4f34da4d04bba44a32fbcee73f","placeholder":"​","style":"IPY_MODEL_93ff7796c11f455e8901f02e8740d70c","value":" 232k/? [00:00&lt;00:00, 3.20MB/s]"}},"e04ec99b0d234d318eed6987762e759e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c7268b169ec4e77857c9a50cb125a5c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b769630311614cada2bc64de1f0b1c7e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6bf0c66c4dbc4b79880b4835df7191a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"6910898877404007919e0f462f054386":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5b9c5d4f34da4d04bba44a32fbcee73f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93ff7796c11f455e8901f02e8740d70c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1108d09ac7d7482780f7b66411dc74f5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8838033bd87d401bbc72d9eec4ba96b2","IPY_MODEL_02e9835911ba4c2ca14d2e5aead2e10a","IPY_MODEL_b62b546a08564c3e8f757aa8ea43572d"],"layout":"IPY_MODEL_006a3398cb424f64b383351264e8b936"}},"8838033bd87d401bbc72d9eec4ba96b2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b502c2ee6a94cad851d89c0c4f2a097","placeholder":"​","style":"IPY_MODEL_755f712d937440cb820b8b4f5e11687b","value":"tokenizer.json: "}},"02e9835911ba4c2ca14d2e5aead2e10a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7da23274f60d44a98608fee0b1452a34","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ae0295f541f94ab7a095e948b05c9de6","value":1}},"b62b546a08564c3e8f757aa8ea43572d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c609ef8e65734d7a85f027ed6cdcc12b","placeholder":"​","style":"IPY_MODEL_73c19384315843538edbad0661cf468b","value":" 466k/? [00:00&lt;00:00, 8.63MB/s]"}},"006a3398cb424f64b383351264e8b936":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b502c2ee6a94cad851d89c0c4f2a097":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"755f712d937440cb820b8b4f5e11687b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7da23274f60d44a98608fee0b1452a34":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"ae0295f541f94ab7a095e948b05c9de6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c609ef8e65734d7a85f027ed6cdcc12b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73c19384315843538edbad0661cf468b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d555e86160dd472aa780393493492aff":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9098a07cfe2e458fa7da27ee1b2fcb4b","IPY_MODEL_be711bc1f0c0414bb35f2bca37438046","IPY_MODEL_fbddb950b5564ea483b02833b64814ab"],"layout":"IPY_MODEL_c3201c8edf924a6ba530bfa5d3815ee7"}},"9098a07cfe2e458fa7da27ee1b2fcb4b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_15a1d4659407431897d9adb4a76a2d07","placeholder":"​","style":"IPY_MODEL_9603b9227be045bc945611312ce120f5","value":"special_tokens_map.json: 100%"}},"be711bc1f0c0414bb35f2bca37438046":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dad76eeb09434780aa84b766e55301e8","max":112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c483475716bf475f8923f1ca70414995","value":112}},"fbddb950b5564ea483b02833b64814ab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_620a8a6960064bf8893c07b383e22e79","placeholder":"​","style":"IPY_MODEL_5baf8653980f4dbf827be500a307a248","value":" 112/112 [00:00&lt;00:00, 5.97kB/s]"}},"c3201c8edf924a6ba530bfa5d3815ee7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15a1d4659407431897d9adb4a76a2d07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9603b9227be045bc945611312ce120f5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dad76eeb09434780aa84b766e55301e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c483475716bf475f8923f1ca70414995":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"620a8a6960064bf8893c07b383e22e79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5baf8653980f4dbf827be500a307a248":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1d0b87eaf7b44d58b39d713b34e89cb4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_407058ee2060424caa16d50eec97d4c3","IPY_MODEL_0eac74743c864bcc9524b20b51317779","IPY_MODEL_6b9d70afe20e48ba8976b5daec15556e"],"layout":"IPY_MODEL_7888acf64f884912a577c482eab85e83"}},"407058ee2060424caa16d50eec97d4c3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_74bcd9e02bcb4cf8bd1b3dad03225acb","placeholder":"​","style":"IPY_MODEL_11f7b481ec6b4432bcbe1a70c1f5e397","value":"config.json: 100%"}},"0eac74743c864bcc9524b20b51317779":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7a639ca6d7ed441cb3b49ecd2043d595","max":190,"min":0,"orientation":"horizontal","style":"IPY_MODEL_41e4d3b79c1b4013a1ff3fd476010e33","value":190}},"6b9d70afe20e48ba8976b5daec15556e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7f6f4c72c6d48dd856593a919b90ea5","placeholder":"​","style":"IPY_MODEL_549a6aa5753e47df8860a70d265876b3","value":" 190/190 [00:00&lt;00:00, 14.0kB/s]"}},"7888acf64f884912a577c482eab85e83":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74bcd9e02bcb4cf8bd1b3dad03225acb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"11f7b481ec6b4432bcbe1a70c1f5e397":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7a639ca6d7ed441cb3b49ecd2043d595":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41e4d3b79c1b4013a1ff3fd476010e33":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b7f6f4c72c6d48dd856593a919b90ea5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"549a6aa5753e47df8860a70d265876b3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4vvqFbVbjjME","executionInfo":{"status":"ok","timestamp":1771672368199,"user_tz":-330,"elapsed":7068,"user":{"displayName":"Avanti Buche","userId":"17362902153167164591"}},"outputId":"a5040b81-8e7f-42ba-9706-bb2b45cc93cd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting faiss-cpu\n","  Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.6 kB)\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.3)\n","Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (26.0)\n","Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (5.0.0)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.4.1)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.10.0+cpu)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n","Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.24.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (0.28.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n","Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.5.4)\n","Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (0.24.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.2)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.3)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (4.12.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (2026.1.4)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (1.0.9)\n","Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (0.16.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n","Requirement already satisfied: typer>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (0.24.0)\n","Requirement already satisfied: click>=8.2.1 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (8.3.1)\n","Requirement already satisfied: rich>=12.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (13.9.4)\n","Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (0.0.4)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (2.19.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (0.1.2)\n","Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: faiss-cpu\n","Successfully installed faiss-cpu-1.13.2\n"]}],"source":["!pip install faiss-cpu sentence-transformers"]},{"cell_type":"code","source":["!pip install pypdf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JPuR4hn8qFfh","executionInfo":{"status":"ok","timestamp":1771672380368,"user_tz":-330,"elapsed":12133,"user":{"displayName":"Avanti Buche","userId":"17362902153167164591"}},"outputId":"909796c1-d314-4612-9909-38298b156a9d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pypdf\n","  Downloading pypdf-6.7.1-py3-none-any.whl.metadata (7.1 kB)\n","Downloading pypdf-6.7.1-py3-none-any.whl (331 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/331.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m327.7/331.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.0/331.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pypdf\n","Successfully installed pypdf-6.7.1\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from pypdf import PdfReader\n","\n","# Define the path to your PDF file\n","path = \"/content/sample_data/Foundations_of_Machine_Learning.pdf\"\n","\n","#extract text from pdf\n","def extract_text_from_pdf(pdf_path):\n","    text = \"\"\n","    with open(pdf_path, 'rb') as file:\n","        reader = PdfReader(file)\n","        for page in reader.pages:\n","            text += page.extract_text() or \"\" # Use or \"\" to handle pages with no extractable text\n","    return text\n","\n","pdf_text = extract_text_from_pdf(path)"],"metadata":{"id":"Iu8-XSuSjzIY","executionInfo":{"status":"ok","timestamp":1771672404262,"user_tz":-330,"elapsed":23860,"user":{"displayName":"Avanti Buche","userId":"17362902153167164591"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":237},"id":"10684261","executionInfo":{"status":"ok","timestamp":1771672410204,"user_tz":-330,"elapsed":1759,"user":{"displayName":"Avanti Buche","userId":"17362902153167164591"}},"outputId":"798fc3e1-f50b-4e0b-804a-cae0cd4793e5"},"source":["pdf_text"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Foundations of Machine LearningAdaptive Computation and Machine Learning\\nThomas Dietterich, Editor\\nChristopher Bishop, David Heckerman, Michael Jordan, and Michael Kearns,\\nAssociate Editors\\nA complete list of books published in The Adaptive Computations and Machine\\nLearning series appears at the back of this book.Foundations of Machine Learning\\nMehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar\\nThe MIT Press\\nCambridge, Massachusetts\\nLondon, Englandc⃝ 2012 Massachusetts Institute of Technology\\nAll rights reserved. No part of this book may be reproduced in any form by any\\nelectronic or mechanical means (including photocopying, recording, or information\\nstorage and retrieval) without permission in writing from the publisher.\\nMIT Press books may be purchased at special quantity discounts for business or\\nsales promotional use. For information, please email special\\nsales@mitpress.mit.edu\\nor write to Special Sales Department, The MIT Press, 55 Hayward Street, Cam-\\nbridge, MA 02142.\\nThis book was set in LATEX by the authors. Printed and bound in the United States\\nof America.\\nLibrary of Congress Cataloging-in-Publication Data\\nMohri, Mehryar.\\nFoundations of machine learning / Mehryar Mohri, Afshin Rostamizadeh, and\\nAmeet Talwalkar.\\np. cm. - (Adaptive computation and machine learning series)\\nIncludes bibliographical references and index.\\nISBN 978-0-262-01825-8 (hardcover : alk. paper) 1. Machine learning. 2. Computer\\nalgorithms. I. Rostamizadeh, Afshin. II. Talwalkar, Ameet. III. Title.\\nQ325.5.M64 2012\\n006.3’1-dc23\\n2012007249\\n1 0987654321Contents\\nPreface xi\\n1 Introduction 1\\n1 . 1 A p p l i c a t i o n s a n d p r o b l e m s ........................ 1\\n1 . 2 D e ﬁ n i t i o n s a n d t e r m i n o l o g y....................... 3\\n1 . 3 C r o s s - v a l i d a t i o n .............................. 5\\n1 . 4 L e a r n i n g s c e n a r i o s ............................ 7\\n1 . 5 O u t l i n e .................................. 8\\n2 The PAC Learning Framework 11\\n2 . 1 T h e P A C l e a r n i n g m o d e l ......................... 1 1\\n2 . 2 G u a r a n t e e s f o r ﬁ n i t e h y p o t h e s i s s e t s — c o n s i s t e n t c a s e........ 1 7\\n2 . 3 G u a r a n t e e s f o r ﬁ n i t e h y p o t h e s i s s e t s — i n c o n s i s t e n t c a s e....... 2 1\\n2 . 4 G e n e r a l i t i e s................................ 2 4\\n2 . 4 . 1 D e t e r m i n i s t i c v e r s u s s t o c h a s t i c s c e n a r i o s............ 2 4\\n2 . 4 . 2 B a y e s e r r o r a n d n o i s e ...................... 2 5\\n2 . 4 . 3 E s t i m a t i o n a n d a p p r o x i m a t i o n e r r o r s .............. 2 6\\n2 . 4 . 4 M o d e l s e l e c t i o n.......................... 2 7\\n2 . 5 C h a p t e r n o t e s............................... 2 8\\n2 . 6 E x e r c i s e s ................................. 2 9\\n3 Rademacher Complexity and VC-Dimension 33\\n3 . 1 R a d e m a c h e r c o m p l e x i t y ......................... 3 4\\n3 . 2 G r o w t h f u n c t i o n ............................. 3 8\\n3 . 3 V C - d i m e n s i o n............................... 4 1\\n3 . 4 L o w e r b o u n d s............................... 4 8\\n3 . 5 C h a p t e r n o t e s............................... 5 4\\n3 . 6 E x e r c i s e s ................................. 5 5\\n4 Support Vector Machines 63\\n4 . 1 L i n e a r c l a s s i ﬁ c a t i o n ............................ 6 3\\n4 . 2 S V M s — s e p a r a b l e c a s e ......................... 6 4vi\\n4 . 2 . 1 P r i m a l o p t i m i z a t i o n p r o b l e m .................. 6 4\\n4 . 2 . 2 S u p p o r t v e c t o r s .......................... 6 6\\n4 . 2 . 3 D u a l o p t i m i z a t i o n p r o b l e m ................... 6 7\\n4 . 2 . 4 L e a v e - o n e - o u t a n a l y s i s...................... 6 9\\n4 . 3 S V M s — n o n - s e p a r a b l e c a s e....................... 7 1\\n4 . 3 . 1 P r i m a l o p t i m i z a t i o n p r o b l e m .................. 7 2\\n4 . 3 . 2 S u p p o r t v e c t o r s .......................... 7 3\\n4 . 3 . 3 D u a l o p t i m i z a t i o n p r o b l e m ................... 7 4\\n4 . 4 M a r g i n t h e o r y ............................... 7 5\\n4 . 5 C h a p t e r n o t e s............................... 8 3\\n4 . 6 E x e r c i s e s ................................. 8 4\\n5 Kernel Methods 89\\n5 . 1 I n t r o d u c t i o n ................................ 8 9\\n5 . 2 P o s i t i v e d e ﬁ n i t e s y m m e t r i c k e r n e l s ................... 9 2\\n5 . 2 . 1 D e ﬁ n i t i o n s ............................ 9 2\\n5 . 2 . 2 R e p r o d u c i n g k e r n e l H i l b e r t s p a c e................ 9 4\\n5 . 2 . 3 P r o p e r t i e s............................. 9 6\\n5 . 3 K e r n e l - b a s e d a l g o r i t h m s......................... 1 0 0\\n5 . 3 . 1 S V M s w i t h P D S k e r n e l s..................... 1 0 0\\n5 . 3 . 2 R e p r e s e n t e r t h e o r e m....................... 1 0 1\\n5 . 3 . 3 L e a r n i n g g u a r a n t e e s ....................... 1 0 2\\n5 . 4 N e g a t i v e d e ﬁ n i t e s y m m e t r i c k e r n e l s ................... 1 0 3\\n5 . 5 S e q u e n c e k e r n e l s ............................. 1 0 6\\n5 . 5 . 1 W e i g h t e d t r a n s d u c e r s ...................... 1 0 6\\n5 . 5 . 2 R a t i o n a l k e r n e l s ......................... 1 1 1\\n5 . 6 C h a p t e r n o t e s............................... 1 1 5\\n5 . 7 E x e r c i s e s ................................. 1 1 6\\n6 Boosting 121\\n6 . 1 I n t r o d u c t i o n ................................ 1 2 1\\n6 . 2 A d a B o o s t ................................. 1 2 2\\n6 . 2 . 1 B o u n d o n t h e e m p i r i c a l e r r o r .................. 1 2 4\\n6 . 2 . 2 R e l a t i o n s h i p w i t h c o o r d i n a t e d e s c e n t.............. 1 2 6\\n6 . 2 . 3 R e l a t i o n s h i p w i t h l o g i s t i c r e g r e s s i o n .............. 1 2 9\\n6 . 2 . 4 S t a n d a r d u s e i n p r a c t i c e..................... 1 2 9\\n6 . 3 T h e o r e t i c a l r e s u l t s ............................ 1 3 0\\n6 . 3 . 1 V C - d i m e n s i o n - b a s e d a n a l y s i s .................. 1 3 1\\n6 . 3 . 2 M a r g i n - b a s e d a n a l y s i s ...................... 1 3 1\\n6 . 3 . 3 M a r g i n m a x i m i z a t i o n ...................... 1 3 6\\n6 . 3 . 4 G a m e - t h e o r e t i c i n t e r p r e t a t i o n.................. 1 3 7vii\\n6 . 4 D i s c u s s i o n................................. 1 4 0\\n6 . 5 C h a p t e r n o t e s............................... 1 4 1\\n6 . 6 E x e r c i s e s ................................. 1 4 2\\n7 On-Line Learning 147\\n7 . 1 I n t r o d u c t i o n ................................ 1 4 7\\n7 . 2 P r e d i c t i o n w i t h e x p e r t a d v i c e...................... 1 4 8\\n7 . 2 . 1 M i s t a k e b o u n d s a n d H a l v i n g a l g o r i t h m ............ 1 4 8\\n7 . 2 . 2 W e i g h t e d m a j o r i t y a l g o r i t h m .................. 1 5 0\\n7 . 2 . 3 R a n d o m i z e d w e i g h t e d m a j o r i t y a l g o r i t h m ........... 1 5 2\\n7 . 2 . 4 E x p o n e n t i a l w e i g h t e d a v e r a g e a l g o r i t h m............ 1 5 6\\n7 . 3 L i n e a r c l a s s i ﬁ c a t i o n ............................ 1 5 9\\n7 . 3 . 1 P e r c e p t r o n a l g o r i t h m ....................... 1 6 0\\n7 . 3 . 2 W i n n o w a l g o r i t h m........................ 1 6 8\\n7 . 4 O n - l i n e t o b a t c h c o n v e r s i o n ....................... 1 7 1\\n7 . 5 G a m e - t h e o r e t i c c o n n e c t i o n ........................ 1 7 4\\n7 . 6 C h a p t e r n o t e s............................... 1 7 5\\n7 . 7 E x e r c i s e s ................................. 1 7 6\\n8 Multi-Class Classiﬁcation 183\\n8 . 1 M u l t i - c l a s s c l a s s i ﬁ c a t i o n p r o b l e m.................... 1 8 3\\n8 . 2 G e n e r a l i z a t i o n b o u n d s.......................... 1 8 5\\n8 . 3 U n c o m b i n e d m u l t i - c l a s s a l g o r i t h m s................... 1 9 1\\n8 . 3 . 1 M u l t i - c l a s s S V M s ......................... 1 9 1\\n8 . 3 . 2 M u l t i - c l a s s b o o s t i n g a l g o r i t h m s................. 1 9 2\\n8 . 3 . 3 D e c i s i o n t r e e s........................... 1 9 4\\n8.4 Aggregated multi-class algorithms ................... 1 9 8\\n8 . 4 . 1 O n e - v e r s u s - a l l ........................... 1 9 8\\n8 . 4 . 2 O n e - v e r s u s - o n e.......................... 1 9 9\\n8 . 4 . 3 E r r o r - c o r r e c t i o n c o d e s...................... 2 0 1\\n8 . 5 S t r u c t u r e d p r e d i c t i o n a l g o r i t h m s .................... 2 0 3\\n8 . 6 C h a p t e r n o t e s............................... 2 0 6\\n8 . 7 E x e r c i s e s ................................. 2 0 7\\n9 Ranking 209\\n9 . 1 T h e p r o b l e m o f r a n k i n g ......................... 2 0 9\\n9 . 2 G e n e r a l i z a t i o n b o u n d .......................... 2 1 1\\n9 . 3 R a n k i n g w i t h S V M s ........................... 2 1 3\\n9 . 4 R a n k B o o s t ................................ 2 1 4\\n9 . 4 . 1 B o u n d o n t h e e m p i r i c a l e r r o r .................. 2 1 6\\n9 . 4 . 2 R e l a t i o n s h i p w i t h c o o r d i n a t e d e s c e n t.............. 2 1 8viii\\n9 . 4 . 3 M a r g i n b o u n d f o r e n s e m b l e m e t h o d s i n r a n k i n g ....... 2 2 0\\n9 . 5 B i p a r t i t e r a n k i n g............................. 2 2 1\\n9 . 5 . 1 B o o s t i n g i n b i p a r t i t e r a n k i n g .................. 2 2 2\\n9 . 5 . 2 A r e a u n d e r t h e R O C c u r v e ................... 2 2 4\\n9 . 6 P r e f e r e n c e - b a s e d s e t t i n g......................... 2 2 6\\n9 . 6 . 1 S e c o n d - s t a g e r a n k i n g p r o b l e m .................. 2 2 7\\n9 . 6 . 2 D e t e r m i n i s t i c a l g o r i t h m ..................... 2 2 9\\n9 . 6 . 3 R a n d o m i z e d a l g o r i t h m...................... 2 3 0\\n9 . 6 . 4 E x t e n s i o n t o o t h e r l o s s f u n c t i o n s................ 2 3 1\\n9 . 7 D i s c u s s i o n................................. 2 3 2\\n9 . 8 C h a p t e r n o t e s............................... 2 3 3\\n9 . 9 E x e r c i s e s ................................. 2 3 4\\n10 Regression 237\\n1 0 . 1T h e p r o b l e m o f r e g r e s s i o n........................ 2 3 7\\n1 0 . 2G e n e r a l i z a t i o n b o u n d s.......................... 2 3 8\\n1 0 . 2 . 1F i n i t e h y p o t h e s i s s e t s ...................... 2 3 8\\n1 0 . 2 . 2R a d e m a c h e r c o m p l e x i t y b o u n d s ................. 2 3 9\\n1 0 . 2 . 3P s e u d o - d i m e n s i o n b o u n d s.................... 2 4 1\\n1 0 . 3R e g r e s s i o n a l g o r i t h m s .......................... 2 4 5\\n1 0 . 3 . 1L i n e a r r e g r e s s i o n......................... 2 4 5\\n1 0 . 3 . 2K e r n e l r i d g e r e g r e s s i o n ...................... 2 4 7\\n1 0 . 3 . 3S u p p o r t v e c t o r r e g r e s s i o n .................... 2 5 2\\n1 0 . 3 . 4L a s s o ............................... 2 5 7\\n1 0 . 3 . 5G r o u p n o r m r e g r e s s i o n a l g o r i t h m s ............... 2 6 0\\n1 0 . 3 . 6O n - l i n e r e g r e s s i o n a l g o r i t h m s .................. 2 6 1\\n1 0 . 4C h a p t e r n o t e s............................... 2 6 2\\n1 0 . 5E x e r c i s e s ................................. 2 6 3\\n11 Algorithmic Stability 267\\n1 1 . 1D e ﬁ n i t i o n s ................................. 2 6 7\\n1 1 . 2S t a b i l i t y - b a s e d g e n e r a l i z a t i o n g u a r a n t e e ................ 2 6 8\\n1 1 . 3S t a b i l i t y o f k e r n e l - b a s e d r e g u l a r i z a t i o n a l g o r i t h m s .......... 2 7 0\\n11.3.1 Application to regression algorithms: SVR and KRR . . . . . 274\\n1 1 . 3 . 2A p p l i c a t i o n t o c l a s s i ﬁ c a t i o n a l g o r i t h m s : S V M s ........ 2 7 6\\n1 1 . 3 . 3D i s c u s s i o n............................. 2 7 6\\n1 1 . 4C h a p t e r n o t e s............................... 2 7 7\\n1 1 . 5E x e r c i s e s ................................. 2 7 7\\n12 Dimensionality Reduction 281\\n1 2 . 1P r i n c i p a l C o m p o n e n t A n a l y s i s ..................... 2 8 2ix\\n1 2 . 2K e r n e l P r i n c i p a l C o m p o n e n t A n a l y s i s ( K P C A ) ............ 2 8 3\\n1 2 . 3 K P C A a n d m a n i f o l d l e a r n i n g...................... 2 8 5\\n1 2 . 3 . 1I s o m a p .............................. 2 8 5\\n1 2 . 3 . 2L a p l a c i a n e i g e n m a p s....................... 2 8 6\\n1 2 . 3 . 3L o c a l l y l i n e a r e m b e d d i n g ( L L E ) ................ 2 8 7\\n1 2 . 4J o h n s o n - L i n d e n s t r a u s s l e m m a...................... 2 8 8\\n1 2 . 5C h a p t e r n o t e s............................... 2 9 0\\n1 2 . 6E x e r c i s e s ................................. 2 9 0\\n13 Learning Automata and Languages 293\\n1 3 . 1I n t r o d u c t i o n ................................ 2 9 3\\n1 3 . 2F i n i t e a u t o m a t a ............................. 2 9 4\\n1 3 . 3E ﬃ c i e n t e x a c t l e a r n i n g.......................... 2 9 5\\n1 3 . 3 . 1P a s s i v e l e a r n i n g ......................... 2 9 6\\n1 3 . 3 . 2L e a r n i n g w i t h q u e r i e s ...................... 2 9 7\\n1 3 . 3 . 3L e a r n i n g a u t o m a t a w i t h q u e r i e s ................ 2 9 8\\n1 3 . 4I d e n t i ﬁ c a t i o n i n t h e l i m i t ........................ 3 0 3\\n1 3 . 4 . 1L e a r n i n g r e v e r s i b l e a u t o m a t a .................. 3 0 4\\n1 3 . 5C h a p t e r n o t e s............................... 3 0 9\\n1 3 . 6E x e r c i s e s ................................. 3 1 0\\n14 Reinforcement Learning 313\\n1 4 . 1L e a r n i n g s c e n a r i o............................. 3 1 3\\n1 4 . 2M a r k o v d e c i s i o n p r o c e s s m o d e l ..................... 3 1 4\\n1 4 . 3P o l i c y ................................... 3 1 5\\n1 4 . 3 . 1D e ﬁ n i t i o n............................. 3 1 5\\n1 4 . 3 . 2P o l i c y v a l u e............................ 3 1 6\\n1 4 . 3 . 3P o l i c y e v a l u a t i o n......................... 3 1 6\\n1 4 . 3 . 4O p t i m a l p o l i c y .......................... 3 1 8\\n1 4 . 4P l a n n i n g a l g o r i t h m s ........................... 3 1 9\\n1 4 . 4 . 1V a l u e i t e r a t i o n .......................... 3 1 9\\n1 4 . 4 . 2P o l i c y i t e r a t i o n.......................... 3 2 2\\n1 4 . 4 . 3L i n e a r p r o g r a m m i n g....................... 3 2 4\\n1 4 . 5L e a r n i n g a l g o r i t h m s ........................... 3 2 5\\n1 4 . 5 . 1S t o c h a s t i c a p p r o x i m a t i o n .................... 3 2 6\\n1 4 . 5 . 2T D ( 0 ) a l g o r i t h m ......................... 3 3 0\\n1 4 . 5 . 3Q - l e a r n i n g a l g o r i t h m....................... 3 3 1\\n1 4 . 5 . 4S A R S A .............................. 3 3 4\\n14.5.5 TD( λ) a l g o r i t h m......................... 3 3 5\\n1 4 . 5 . 6L a r g e s t a t e s p a c e......................... 3 3 6\\n1 4 . 6C h a p t e r n o t e s............................... 3 3 7x\\nConclusion 339\\nA Linear Algebra Review 341\\nA . 1 V e c t o r s a n d n o r m s ............................ 3 4 1\\nA . 1 . 1 N o r m s............................... 3 4 1\\nA . 1 . 2 D u a l n o r m s............................ 3 4 2\\nA . 2 M a t r i c e s.................................. 3 4 4\\nA . 2 . 1 M a t r i x n o r m s........................... 3 4 4\\nA . 2 . 2 S i n g u l a r v a l u e d e c o m p o s i t i o n .................. 3 4 5\\nA . 2 . 3 S y m m e t r i c p o s i t i v e s e m i d e ﬁ n i t e ( S P S D ) m a t r i c e s....... 3 4 6\\nB Convex Optimization 349\\nB . 1 D i ﬀ e r e n t i a t i o n a n d u n c o n s t r a i n e d o p t i m i z a t i o n ............ 3 4 9\\nB . 2 C o n v e x i t y................................. 3 5 0\\nB . 3 C o n s t r a i n e d o p t i m i z a t i o n ........................ 3 5 3\\nB . 4 C h a p t e r n o t e s............................... 3 5 7\\nC Probability Review 359\\nC.1 Probability ................................ 3 5 9\\nC . 2 R a n d o m v a r i a b l e s ............................. 3 5 9\\nC . 3 C o n d i t i o n a l p r o b a b i l i t y a n d i n d e p e n d e n c e............... 3 6 1\\nC.4 Expectation, Markov’s inequality, and moment-generating function\\nC . 5 V a r i a n c e a n d C h e b y s h e v ’ s i n e q u a l i t y.................. 3 6 5\\nD Concentration inequalities 369\\nD . 1 H o e ﬀ d i n g ’ s i n e q u a l i t y .......................... 3 6 9\\nD . 2 M c D i a r m i d ’ s i n e q u a l i t y ......................... 3 7 1\\nD . 3 O t h e r i n e q u a l i t i e s ............................. 3 7 3\\nD . 3 . 1 B i n o m i a l d i s t r i b u t i o n : S l u d ’ s i n e q u a l i t y ............ 3 7 4\\nD . 3 . 2 N o r m a l d i s t r i b u t i o n : t a i l b o u n d................. 3 7 4\\nD . 3 . 3 K h i n t c h i n e - K a h a n e i n e q u a l i t y.................. 3 7 4\\nD . 4 C h a p t e r n o t e s............................... 3 7 6\\nD . 5 E x e r c i s e s ................................. 3 7 7\\nE Notation 379\\nReferences 381\\nIndex 397\\n363.Preface\\nThis book is a general introduction to machine learning that can serve as a textbook\\nfor students and researchers in the ﬁeld. It covers fundamental modern topics in\\nmachine learning while providing the theoretical basis and conceptual tools needed\\nfor the discussion and justiﬁcation of algorithms. It also describes several key aspects\\nof the application of these algorithms.\\nWe have aimed to present the most novel theoretical tools and concepts while\\ngiving concise proofs, even for relatively advanced results. In general, whenever\\npossible, we have chosen to favor succinctness. Nevertheless, we discuss some crucial\\ncomplex topics arising in machine learning and highlight several open research\\nquestions. Certain topics often merged with others or treated with insuﬃcient\\nattention are discussed separately here and with more emphasis: for example, a\\ndiﬀerent chapter is reserved for multi-class classiﬁcation, ranking, and regression.\\nAlthough we cover a very wide variety of important topics in machine learning, we\\nhave chosen to omit a few important ones, including graphical models and neural\\nnetworks, both for the sake of brevity and because of the current lack of solid\\ntheoretical guarantees for some methods.\\nThe book is intended for students and researchers in machine learning, statistics\\nand other related areas. It can be used as a textbook for both graduate and advanced\\nundergraduate classes in machine learning or as a reference text for a research\\nseminar. The ﬁrst three chapters of the book lay the theoretical foundation for the\\nsubsequent material. Other chapters are mostly self-contained, with the exception\\nof chapter 5 which introduces some concepts that are extensively used in later\\nones. Each chapter concludes with a series of exercises, with full solutions presented\\nseparately.\\nThe reader is assumed to be familiar with basic concepts in linear algebra,\\nprobability, and analysis of algorithms. However, to further help him, we present\\nin the appendix a concise linear algebra and a probability review, and a short\\nintroduction to convex optimization. We have also collected in the appendix a\\nnumber of useful tools for concentration bounds used in this book.\\nTo our knowledge, there is no single textbook covering all of the material\\npresented here. The need for a uniﬁed presentation has been pointed out to usxii Preface\\nevery year by our machine learning students. There are several good books for\\nvarious specialized areas, but these books do not include a discussion of other\\nfundamental topics in a general manner. For example, books about kernel methods\\ndo not include a discussion of other fundamental topics such as boosting, ranking,\\nreinforcement learning, learning automata or online learning. There also exist more\\ngeneral machine learning books, but the theoretical foundation of our book and our\\nemphasis on proofs make our presentation quite distinct.\\nM o s to ft h em a t e r i a lp r e s e n t e dh e r et a k e si t so r i g i n si nam a c h i n el e a r n i n g\\ngraduate course ( Foundations of Machine Learning ) taught by the ﬁrst author\\nat the Courant Institute of Mathematical Sciences in New York University over\\nthe last seven years. This book has considerably beneﬁted from the comments\\nand suggestions from students in these classes, along with those of many friends,\\ncolleagues and researchers to whom we are deeply indebted.\\nWe are particularly grateful to Corinna Cortes and Yishay Mansour who have\\nboth made a number of key suggestions for the design and organization of the\\nm a t e r i a lp r e s e n t e dw i t hd e t a i l e dc o m m e n t st h a tw eh a v ef u l l yt a k e ni n t oa c c o u n t\\nand that have greatly improved the presentation. We are also grateful to Yishay\\nMansour for using a preliminary version of the book for teaching and for reporting\\nhis feedback to us.\\nWe also thank for discussions, suggested improvement, and contributions of many\\nkinds the following colleagues and friends from academic and corporate research lab-\\noratories: Cyril Allauzen, Stephen Boyd, Spencer Greenberg, Lisa Hellerstein, Sanjiv\\nKumar, Ryan McDonald, Andres Mu˜noz Medina, Tyler Neylon, Peter Norvig, Fer-\\nnando Pereira, Maria Pershina, Ashish Rastogi, Michael Riley, Umar Syed, Csaba\\nSzepesv´ari, Eugene Weinstein, and Jason Weston.\\nFinally, we thank the MIT Press publication team for their help and support in\\nthe development of this text.1 Introduction\\nMachine learning can be broadly deﬁned as computational methods using experience\\nto improve performance or to make accurate predictions. Here,experience refers to\\nthe past information available to the learner, which typically takes the form of\\nelectronic data collected and made available for analysis. This data could be in the\\nform of digitized human-labeled training sets, or other types of information obtained\\nvia interaction with the environment. In all cases, its quality and size are crucial to\\nthe success of the predictions made by the learner.\\nMachine learning consists of designing eﬃcient and accurate prediction algo-\\nrithms. As in other areas of computer science, some critical measures of the quality\\nof these algorithms are their time and space complexity. But, in machine learning,\\nwe will need additionally a notion of sample complexity to evaluate the sample size\\nrequired for the algorithm to learn a family of concepts. More generally, theoreti-\\ncal learning guarantees for an algorithm depend on the complexity of the concept\\nclasses considered and the size of the training sample.\\nSince the success of a learning algorithm depends on the data used, machine\\nlearning is inherently related to data analysis and statistics. More generally, learning\\ntechniques are data-driven methods combining fundamental concepts in computer\\nscience with ideas from statistics, probability and optimization.\\n1.1 Applications and problems\\nLearning algorithms have been successfully deployed in a variety of applications,\\nincluding\\nText or document classiﬁcation, e.g., spam detection;\\nNatural language processing, e.g., morphological analysis, part-of-speech tagging,\\nstatistical parsing, named-entity recognition;\\nSpeech recognition, speech synthesis, speaker veriﬁcation;\\nOptical character recognition (OCR);\\nComputational biology applications, e.g., protein function or structured predic-2 Introduction\\ntion;\\nComputer vision tasks, e.g., image recognition, face detection;\\nFraud detection (credit card, telephone) and network intrusion;\\nGames, e.g., chess, backgammon;\\nUnassisted vehicle control (robots, navigation);\\nMedical diagnosis;\\nRecommendation systems, search engines, information extraction systems.\\nThis list is by no means comprehensive, and learning algorithms are applied to new\\napplications every day. Moreover, such applications correspond to a wide variety of\\nlearning problems. Some major classes of learning problems are:\\nClassiﬁcation : Assign a category to each item. For example, document classiﬁca-\\ntion may assign items with categories such as politics, business, sports,o r weather\\nwhile image classiﬁcation may assign items with categories such as landscape, por-\\ntrait,o r animal. The number of categories in such tasks is often relatively small,\\nbut can be large in some diﬃcult tasks and even unbounded as in OCR, text clas-\\nsiﬁcation, or speech recognition.\\nRegression: Predict a real value for each item. Examples of regression include\\nprediction of stock values or variations of economic variables. In this problem, the\\npenalty for an incorrect prediction depends on the magnitude of the diﬀerence\\nbetween the true and predicted values, in contrast with the classiﬁcation problem,\\nwhere there is typically no notion of closeness between various categories.\\nRanking: Order items according to some criterion. Web search, e.g., returning\\nweb pages relevant to a search query, is the canonical ranking example. Many other\\nsimilar ranking problems arise in the context of the design of information extraction\\nor natural language processing systems.\\nClustering: Partition items into homogeneous regions. Clustering is often per-\\nformed to analyze very large data sets. For example, in the context of social net-\\nwork analysis, clustering algorithms attempt to identify “communities” within large\\ngroups of people.\\nDimensionality reduction or manifold learning: Transform an initial representa-\\ntion of items into a lower-dimensional representation of these items while preserving\\nsome properties of the initial representation. A common example involves prepro-\\ncessing digital images in computer vision tasks.\\nThe main practical objectives of machine learning consist of generating accurate\\npredictions for unseen items and of designing eﬃcient and robust algorithms to\\nproduce these predictions, even for large-scale problems. To do so, a number of\\nalgorithmic and theoretical questions arise. Some fundamental questions include:1.2 Deﬁnitions and terminology 3\\nFigure 1.1 The zig-zag line on the left panel is consistent over the blue and red\\ntraining sample, but it is a complex separation surface that is not likely to generalize\\nwell to unseen data. In contrast, the decision surface on the right panel is simpler\\nand might generalize better in spite of its misclassiﬁcation of a few points of the\\ntraining sample.\\nWhich concept families can actually be learned, and under what conditions? How\\nwell can these concepts be learned computationally?\\n1.2 Deﬁnitions and terminology\\nWe will use the canonical problem of spam detection as a running example to\\nillustrate some basic deﬁnitions and to describe the use and evaluation of machine\\nlearning algorithms in practice. Spam detection is the problem of learning to\\nautomatically classify email messages as either spam or non-spam.\\nExamples: Items or instances of data used for learning or evaluation. In our spam\\nproblem, these examples correspond to the collection of email messages we will use\\nfor learning and testing.\\nFeatures: The set of attributes, often represented as a vector, associated to an\\nexample. In the case of email messages, some relevant features may include the\\nlength of the message, the name of the sender, various characteristics of the header,\\nthe presence of certain keywords in the body of the message, and so on.\\nLabels: Values or categories assigned to examples. In classiﬁcation problems,\\nexamples are assigned speciﬁc categories, for instance, the spam and non- spam\\ncategories in our binary classiﬁcation problem. In regression, items are assigned\\nreal-valued labels.\\nTraining sample: Examples used to train a learning algorithm. In our spam\\nproblem, the training sample consists of a set of email examples along with their\\nassociated labels. The training sample varies for diﬀerent learning scenarios, as\\ndescribed in section 1.4.\\nValidation sample: Examples used to tune the parameters of a learning algorithm4 Introduction\\nwhen working with labeled data. Learning algorithms typically have one or more\\nfree parameters, and the validation sample is used to select appropriate values for\\nthese model parameters.\\nTest sample: Examples used to evaluate the performance of a learning algorithm.\\nThe test sample is separate from the training and validation data and is not made\\navailable in the learning stage. In the spam problem, the test sample consists of a\\ncollection of email examples for which the learning algorithm must predict labels\\nbased on features. These predictions are then compared with the labels of the test\\nsample to measure the performance of the algorithm.\\nLoss function: A function that measures the diﬀerence, or loss, between a pre-\\ndicted label and a true label. Denoting the set of all labels as Y and the set of\\npossible predictions as Y′, a loss function L is a mapping L: Y×Y ′ → R+. In most\\ncases, Y′ = Y and the loss function is bounded, but these conditions do not always\\nhold. Common examples of loss functions include the zero-one (or misclassiﬁcation)\\nloss deﬁned over {−1, +1}×{ − 1, +1} by L(y,y\\n′)=1 y′ ̸=y and the squared loss\\ndeﬁned over I × I by L(y,y ′)=( y′ − y)2,w h e r eI ⊆ R is typically a bounded\\ninterval.\\nHypothesis set: A set of functions mapping features (feature vectors) to the set of\\nlabels Y. In our example, these may be a set of functions mapping email features\\nto Y = {spam, non-spam}. More generally, hypotheses may be functions mapping\\nfeatures to a diﬀerent setY′. They could be linear functions mapping email feature\\nvectors to real numbers interpreted as scores (Y′ = R), with higher score values\\nmore indicative of spam than lower ones.\\nWe now deﬁne the learning stages of our spam problem. We start with a given\\ncollection of labeled examples. We ﬁrst randomly partition the data into a training\\nsample, a validation sample, and a test sample. The size of each of these samples\\ndepends on a number of diﬀerent considerations. For example, the amount of data\\nreserved for validation depends on the number of free parameters of the algorithm.\\nAlso, when the labeled sample is relatively small, the amount of training data is\\noften chosen to be larger than that of test data since the learning performance\\ndirectly depends on the training sample.\\nNext, we associate relevant features to the examples. This is a critical step in\\nthe design of machine learning solutions. Useful features can eﬀectively guide the\\nlearning algorithm, while poor or uninformative ones can be misleading. Although\\nit is critical, to a large extent, the choice of the features is left to the user. This\\nchoice reﬂects the user’sprior knowledge about the learning task which in practice\\ncan have a dramatic eﬀect on the performance results.\\nNow, we use the features selected to train our learning algorithm by ﬁxing diﬀerent\\nvalues of its free parameters. For each value of these parameters, the algorithm1.3 Cross-validation 5\\nselects a diﬀerent hypothesis out of the hypothesis set. We choose among them\\nthe hypothesis resulting in the best performance on the validation sample. Finally,\\nusing that hypothesis, we predict the labels of the examples in the test sample. The\\nperformance of the algorithm is evaluated by using the loss function associated to\\nthe task, e.g., the zero-one loss in our spam detection task, to compare the predicted\\nand true labels.\\nThus, the performance of an algorithm is of course evaluated based on its test error\\nand not its error on the training sample. A learning algorithm may be consistent,\\nthat is it may commit no error on the examples of the training data, and yet\\nhave a poor performance on the test data. This occurs for consistent learners\\ndeﬁned by very complex decision surfaces, as illustrated in ﬁgure 1.1, which tend\\nto memorize a relatively small training sample instead of seeking to generalize well.\\nThis highlights the key distinction between memorization and generalization, which\\nis the fundamental property sought for an accurate learning algorithm. Theoretical\\nguarantees for consistent learners will be discussed with great detail in chapter 2.\\n1.3 Cross-validation\\nIn practice, the amount of labeled data available is often too small to set aside\\na validation sample since that would leave an insuﬃcient amount of training data.\\nInstead, a widely adopted method known asn-fold cross-validation is used to exploit\\nthe labeled data both for model selection (selection of the free parameters of the\\nalgorithm) and for training.\\nLet θ denote the vector of free parameters of the algorithm. For a ﬁxed value\\nof θ, the method consists of ﬁrst randomly partitioning a given sample S of\\nm labeled examples into n subsamples, or folds. The ith fold is thus a labeled\\nsample ((x\\ni1,y i1),..., (ximi ,y imi )) of size mi. Then, for any i ∈ [1,n ], the learning\\nalgorithm is trained on all but the ith fold to generate a hypothesis hi,a n dt h e\\nperformance of hi is tested on the ith fold, as illustrated in ﬁgure 1.2a. The\\nparameter value θ is evaluated based on the average error of the hypotheses hi,\\nwhich is called the cross-validation error.T h i sq u a n t i t yi sd e n o t e db yˆRCV(θ)a n d\\ndeﬁned by\\nˆRCV(θ)= 1\\nn\\nn∑\\ni=1\\n1\\nmi\\nmi∑\\nj=1\\nL(hi(xij),y ij)\\n\\ued19 \\ued18\\ued17 \\ued1a\\nerror of hi on the ith fold\\n.\\nThe folds are generally chosen to have equal size, that ismi = m/n for all i ∈ [1,n ].\\nHow should n be chosen? The appropriate choice is subject to a trade-oﬀ and the\\ntopic of much learning theory research that we cannot address in this introductory6 Introduction\\ntest train train train train\\ntesttrain train train train\\n.\\n.\\n.\\ntesttrain train traintrain\\nerror\\nm\\n(a) (b)\\nFigure 1.2 n-fold cross validation. (a) Illustration of the partitioning of the\\ntraining data into 5 folds. (b) Typical plot of a classiﬁer’s prediction error as a\\nfunction of the size of the training sample: the error decreases as a function of the\\nnumber of training points.\\nchapter. For a largen, each training sample used in n-fold cross-validation has size\\nm−m/n = m(1−1/n) (illustrated by the right vertical red line in ﬁgure 1.2b), which\\nis close to m, the size of the full sample, but the training samples are quite similar.\\nThus, the method tends to have a small bias but a large variance. In contrast,\\nsmaller values of n lead to more diverse training samples but their size (shown by\\nthe left vertical red line in ﬁgure 1.2b) is signiﬁcantly less than m,t h u st h em e t h o d\\ntends to have a smaller variance but a larger bias.\\nIn machine learning applications, n is typically chosen to be 5 or 10. n-fold cross\\nvalidation is used as follows in model selection. The full labeled data is ﬁrst split\\ninto a training and a test sample. The training sample of size m is then used to\\ncompute the n-fold cross-validation error ˆR\\nCV(θ) for a small number of possible\\nvalues of θ. θ is next set to the value θ0 for which ˆRCV(θ) is smallest and the\\nalgorithm is trained with the parameter setting θ0 over the full training sample of\\nsize m.I t sp e r f o r m a n c ei se v a l u a t e do nt h et e s ts a m p l ea sa l r e a d yd e s c r i b e di nt h e\\nprevious section.\\nThe special case of n-fold cross validation where n = m is called leave-one-out\\ncross-validation, since at each iteration exactly one instance is left out of the training\\nsample. As shown in chapter 4, the average leave-one-out error is an approximately\\nunbiased estimate of the average error of an algorithm and can be used to derive\\nsimple guarantees for some algorithms. In general, the leave-one-out error is very\\ncostly to compute, since it requires training n times on samples of size m − 1, but\\nfor some algorithms it admits a very eﬃcient computation (see exercise 10.9).\\nIn addition to model selection, n-fold cross validation is also commonly used for\\nperformance evaluation. In that case, for a ﬁxed parameter settingθ, the full labeled\\nsample is divided inton random folds with no distinction between training and test\\nsamples. The performance reported is the n-fold cross-validation on the full sample\\nas well as the standard deviation of the errors measured on each fold.1.4 Learning scenarios 7\\n1.4 Learning scenarios\\nWe next brieﬂy describe common machine learning scenarios. These scenarios diﬀer\\nin the types of training data available to the learner, the order and method by which\\ntraining data is received and the test data used to evaluate the learning algorithm.\\nSupervised learning: The learner receives a set of labeled examples as training\\ndata and makes predictions for all unseen points. This is the most common scenario\\nassociated with classiﬁcation, regression, and ranking problems. The spam detection\\nproblem discussed in the previous section is an instance of supervised learning.\\nUnsupervised learning: The learner exclusively receives unlabeled training data,\\nand makes predictions for all unseen points. Since in general no labeled exam-\\nple is available in that setting, it can be diﬃcult to quantitatively evaluate the\\nperformance of a learner. Clustering and dimensionality reduction are example of\\nunsupervised learning problems.\\nSemi-supervised learning: The learner receives a training sample consisting of\\nboth labeled and unlabeled data, and makes predictions for all unseen points. Semi-\\nsupervised learning is common in settings where unlabeled data is easily accessible\\nbut labels are expensive to obtain. Various types of problems arising in applications,\\nincluding classiﬁcation, regression, or ranking tasks, can be framed as instances\\nof semi-supervised learning. The hope is that the distribution of unlabeled data\\naccessible to the learner can help him achieve a better performance than in the\\nsupervised setting. The analysis of the conditions under which this can indeed\\nbe realized is the topic of much modern theoretical and applied machine learning\\nresearch.\\nTransductive inference: As in the semi-supervised scenario, the learner receives\\na labeled training sample along with a set of unlabeled test points. However, the\\nobjective of transductive inference is to predict labels only for these particular test\\npoints. Transductive inference appears to be an easier task and matches the scenario\\nencountered in a variety of modern applications. However, as in the semi-supervised\\nsetting, the assumptions under which a better performance can be achieved in this\\nsetting are research questions that have not been fully resolved.\\nOn-line learning : In contrast with the previous scenarios, the online scenario\\ninvolves multiple rounds and training and testing phases are intermixed. At each\\nround, the learner receives an unlabeled training point, makes a prediction, receives\\nthe true label, and incurs a loss. The objective in the on-line setting is to minimize\\nthe cumulative loss over all rounds. Unlike the previous settings just discussed, no\\ndistributional assumption is made in on-line learning. In fact, instances and their\\nlabels may be chosen adversarially within this scenario.8 Introduction\\nReinforcement learning: The training and testing phases are also intermixed in\\nreinforcement learning. To collect information, the learner actively interacts with the\\nenvironment and in some cases aﬀects the environment, and receives an immediate\\nreward for each action. The object of the learner is to maximize his reward over\\na course of actions and iterations with the environment. However, no long-term\\nreward feedback is provided by the environment, and the learner is faced with the\\nexploration versus exploitation dilemma, since he must choose between exploring\\nunknown actions to gain more information versus exploiting the information already\\ncollected.\\nActive learning: The learner adaptively or interactively collects training examples,\\ntypically by querying an oracle to request labels for new points. The goal in\\nactive learning is to achieve a performance comparable to the standard supervised\\nlearning scenario, but with fewer labeled examples. Active learning is often used\\nin applications where labels are expensive to obtain, for example computational\\nbiology applications.\\nIn practice, many other intermediate and somewhat more complex learning scenarios\\nmay be encountered.\\n1.5 Outline\\nThis book presents several fundamental and mathematically well-studied algo-\\nrithms. It discusses in depth their theoretical foundations as well as their practical\\napplications. The topics covered include:\\nProbably approximately correct (PAC) learning framework; learning guarantees\\nfor ﬁnite hypothesis sets;\\nLearning guarantees for inﬁnite hypothesis sets, Rademacher complexity, VC-\\ndimension;\\nSupport vector machines (SVMs), margin theory;\\nKernel methods, positive deﬁnite symmetric kernels, representer theorem, rational\\nkernels;\\nBoosting, analysis of empirical error, generalization error, margin bounds;\\nOnline learning, mistake bounds, the weighted majority algorithm, the exponen-\\ntial weighted average algorithm, the Perceptron and Winnow algorithms;\\nMulti-class classiﬁcation, multi-class SVMs, multi-class boosting, one-versus-all,\\none-versus-one, error-correction methods;\\nRanking, ranking with SVMs, RankBoost, bipartite ranking, preference-based1.5 Outline 9\\nranking;\\nRegression, linear regression, kernel ridge regression, support vector regression,\\nLasso;\\nStability-based analysis, applications to classiﬁcation and regression;\\nDimensionality reduction, principal component analysis (PCA), kernel PCA,\\nJohnson-Lindenstrauss lemma;\\nLearning automata and languages;\\nReinforcement learning, Markov decision processes, planning and learning prob-\\nlems.\\nThe analyses in this book are self-contained, with relevant mathematical concepts\\nrelated to linear algebra, convex optimization, probability and statistics included in\\nthe appendix.2 The PAC Learning Framework\\nSeveral fundamental questions arise when designing and analyzing algorithms that\\nlearn from examples: What can be learned eﬃciently? What is inherently hard to\\nlearn? How many examples are needed to learn successfully? Is there a general model\\nof learning? In this chapter, we begin to formalize and address these questions by\\nintroducing the Probably Approximately Correct (PAC) learning framework. The\\nPAC framework helps deﬁne the class of learnable concepts in terms of the number\\nof sample points needed to achieve an approximate solution,sample complexity,a n d\\nthe time and space complexity of the learning algorithm, which depends on the cost\\nof the computational representation of the concepts.\\nWe ﬁrst describe the PAC framework and illustrate it, then present some general\\nlearning guarantees within this framework when the hypothesis set used is ﬁnite,\\nboth for the consistent case where the hypothesis set used contains the concept to\\nlearn and for the opposite inconsistent case.\\n2.1 The PAC learning model\\nWe ﬁrst introduce several deﬁnitions and the notation needed to present the PAC\\nmodel, which will also be used throughout much of this book.\\nWe denote byX the set of all possibleexamples or instances. X is also sometimes\\nreferred to as theinput space. The set of all possiblelabels or target values is denoted\\nby Y. For the purpose of this introductory chapter, we will limit ourselves to the\\ncase where Y is reduced to two labels, Y = {0, 1}, so-called binary classiﬁcation .\\nLater chapters will extend these results to more general settings.\\nA concept c: X→ Y is a mapping from X to Y.S i n c eY = {0,1},w ec a ni d e n t i f y\\nc with the subset of X over which it takes the value 1. Thus, in the following, we\\ne q u i v a l e n t l yr e f e rt oac o n c e p tt ol e a r na sam a p p i n gf r o mX to {0, 1},o rt oa\\nsubset of X. As an example, a concept may be the set of points inside a triangle or\\nthe indicator function of these points. In such cases, we will say in short that the\\nconcept to learn is a triangle. A concept class is a set of concepts we may wish to\\nlearn and is denoted byC. This could, for example, be the set of all triangles in the12 The PAC Learning Framework\\nplane.\\nWe assume that examples are independently and identically distributed (i.i.d.)\\naccording to some ﬁxed but unknown distribution D. The learning problem is then\\nformulated as follows. The learner considers a ﬁxed set of possible concepts H,\\ncalled a hypothesis set ,w h i c hm a yn o tc o i n c i d ew i t hC. He receives a sample\\nS =( x1,...,x m) drawn i.i.d. according toD as well as the labels (c(x1),...,c (xm)),\\nw h i c ha r eb a s e do nas p e c i ﬁ ct a r g e tc o n c e p tc ∈ C to learn. His task is to use the\\nlabeled sample S to select a hypothesis hS ∈ H that has a small generalization\\nerror with respect to the conceptc. The generalization error of a hypothesish ∈ H,\\nalso referred to as the true error or just error of h is denoted by R(h) and deﬁned\\nas follows.1\\nDeﬁnition 2.1 Generalization error\\nGiven a hypothesis h ∈ H, a target concept c ∈ C, and an underlying distribution\\nD,t h egeneralization error or risk of h is deﬁned by\\nR(h)= P r\\nx∼D\\n[h(x) ̸= c(x)] = E\\nx∼D\\n[\\n1h(x)̸=c(x)\\n]\\n, (2.1)\\nwhere 1ω is the indicator function of the event ω.2\\nThe generalization error of a hypothesis is not directly accessible to the learner\\nsince both the distribution D and the target concept c are unknown. However, the\\nlearner can measure the empirical error of a hypothesis on the labeled sample S.\\nDeﬁnition 2.2 Empirical error\\nGiven a hypothesis h ∈ H, a target concept c ∈ C, and a sample S =( x1,...,x m),\\nthe empirical error or empirical risk of h is deﬁned by\\nˆR(h)= 1\\nm\\nm∑\\ni=1\\n1h(xi)̸=c(xi). (2.2)\\nThus, the empirical error of h ∈ H is its average error over the sampleS, while the\\ngeneralization error is its expected error based on the distributionD. We will see in\\nthis chapter and the following chapters a number of guarantees relating to these two\\nquantities with high probability, under some general assumptions. We can already\\nnote that for a ﬁxed h ∈ H, the expectation of the empirical error based on an i.i.d.\\n1 .T h ec h o i c eo fR instead of E to denote an error avoids possible confusions with the\\nnotation for expectations and is further justiﬁed by the fact that the termrisk is also used\\nin machine learning and statistics to refer to an error.\\n2. For this and other related deﬁnitions, the family of functionsH and the target concept\\nc must be measurable. The function classes we consider in this book all have this property.2.1 The PAC learning model 13\\nsample S is equal to the generalization error:\\nE[ ˆR(h)] = R(h). (2.3)\\nIndeed, by the linearity of the expectation and the fact that the sample is drawn\\ni.i.d., we can write\\nE\\nS∼Dm\\n[ ˆR(h)] = 1\\nm\\nm∑\\ni=1\\nE\\nS∼Dm\\n[1h(xi)̸=c(xi)]= 1\\nm\\nm∑\\ni=1\\nE\\nS∼Dm\\n[1h(x)̸=c(x)],\\nfor any x in sample S.T h u s ,\\nE\\nS∼Dm\\n[ ˆR(h)] = E\\nS∼Dm\\n[1{h(x)̸=c(x)}]= E\\nx∼D\\n[1{h(x)̸=c(x)}]= R(h).\\nThe following introduces the Probably Approximately Correct (PAC) learning\\nframework. We denote by O(n) an upper bound on the cost of the computational\\nrepresentation of any element x ∈X and by size( c) the maximal cost of the\\ncomputational representation of c ∈ C. For example, x may be a vector in Rn,\\nfor which the cost of an array-based representation would be in O(n).\\nDeﬁnition 2.3 PAC-learning\\nA concept class C is said to be PAC-learnable if there exists an algorithm A and\\na polynomial function poly(·, ·, ·, ·) such that for any ϵ> 0 and δ> 0, for all\\ndistributions D on X and for any target concept c ∈ C, the following holds for any\\nsample size m ≥ poly(1/ϵ,1/δ, n,size(c)):\\nPr\\nS∼Dm\\n[R(hS) ≤ ϵ] ≥ 1 − δ. (2.4)\\nIf A further runs in poly(1/ϵ,1/δ, n,size(c)),t h e nC is said to be eﬃciently PAC-\\nlearnable. When such an algorithm A exists, it is called a PAC-learning algorithm\\nfor C.\\nA concept classC is thus PAC-learnable if the hypothesis returned by the algorithm\\nafter observing a number of points polynomial in 1 /ϵ and 1/δ is approximately\\ncorrect (error at most ϵ) with high probability (at least 1 − δ), which justiﬁes the\\nPAC terminology.δ> 0i su s e dt od e ﬁ n et h econﬁdence 1−δand ϵ> 0t h eaccuracy\\n1 − ϵ. Note that if the running time of the algorithm is polynomial in 1/ϵ and 1/δ,\\nthen the sample size m must also be polynomial if the full sample is received by the\\nalgorithm.\\nSeveral key points of the PAC deﬁnition are worth emphasizing. First, the PAC\\nframework is adistribution-free model : no particular assumption is made about the\\ndistribution D from which examples are drawn. Second, the training sample and the\\ntest examples used to deﬁne the error are drawn according to the same distribution\\nD. This is a necessary assumption for generalization to be possible in most cases.14 The PAC Learning Framework\\nR\\nR’\\nFigure 2.1 Target concept R and possible hypothesis R′. Circles represent training\\ninstances. A blue circle is a point labeled with 1, since it falls within the rectangle\\nR.O t h e r sa r er e da n dl a b e l e dw i t h0.\\nFinally, the PAC framework deals with the question of learnability for a concept\\nclass C and not a particular concept. Note that the concept classC is known to the\\nalgorithm, but of course target concept c ∈ C is unknown.\\nIn many cases, in particular when the computational representation of the con-\\ncepts is not explicitly discussed or is straightforward, we may omit the polynomial\\ndependency on n and size(c) in the PAC deﬁnition and focus only on the sample\\ncomplexity.\\nWe now illustrate PAC-learning with a speciﬁc learning problem.\\nExample 2.1 Learning axis-aligned rectangles\\nC o n s i d e rt h ec a s ew h e r et h es e to fi n s t a n c e sa r ep o i n t si nt h ep l a n e ,X = R\\n2,a n d\\nthe concept class C is the set of all axis-aligned rectangles lying in R2.T h u s ,e a c h\\nconcept c is the set of points inside a particular axis-aligned rectangle. The learning\\nproblem consists of determining with small error a target axis-aligned rectangle\\nusing the labeled training sample. We will show that the concept class of axis-\\naligned rectangles is PAC-learnable.\\nFigure 2.1 illustrates the problem. R represents a target axis-aligned rectangle\\nand R′ a hypothesis. As can be seen from the ﬁgure, the error regions of R′ are\\nformed by the area within the rectangleR but outside the rectangle R′ and the area\\nwithin R′ but outside the rectangle R. The ﬁrst area corresponds to false negatives,\\nthat is, points that are labeled as 0 or negatively by R′, which are in fact positive\\nor labeled with 1. The second area corresponds to false positives,t h a ti s ,p o i n t s\\nlabeled positively by R′ which are in fact negatively labeled.\\nTo show that the concept class is PAC-learnable, we describe a simple PAC-\\nlearning algorithm A. Given a labeled sampleS, the algorithm consists of returning\\nthe tightest axis-aligned rectangle R′ = RS containing the points labeled with 1.\\nFigure 2.2 illustrates the hypothesis returned by the algorithm. By deﬁnition, RS\\ndoes not produce any false positive, since its points must be included in the target\\nconcept R. Thus, the error region of RS is included in R.2.1 The PAC learning model 15\\nR\\nR’\\nFigure 2.2 Illustration of the hypothesis R′ = RS returned by the algorithm.\\nLet R ∈ C be a target concept. Fix ϵ> 0. Let Pr[RS] denote the probability mass\\no ft h er e g i o nd e ﬁ n e db yRS, that is the probability that a point randomly drawn\\naccording to D falls within RS. Since errors made by our algorithm can be due only\\nto points falling inside RS, we can assume that Pr[ RS] >ϵ ; otherwise, the error of\\nRS is less than or equal to ϵ regardless of the training sample S received.\\nNow, since Pr[ RS] >ϵ , we can deﬁne four rectangular regions r1,r2,r3, and r4\\nalong the sides of RS, each with probability at least ϵ/4. These regions can be\\nconstructed by starting with the empty rectangle along a side and increasing its\\nsize until its distribution mass is at leastϵ/4. Figure 2.3 illustrates the deﬁnition of\\nthese regions.\\nObserve that if R\\nS meets all of these four regions, then, because it is a rectangle,\\nit will have one side in each of these four regions (geometric argument). Its error\\narea, which is the part of R that it does not cover, is thus included in these regions\\nand cannot have probability mass more than ϵ. By contraposition, if R(RS) >ϵ ,\\nthen RS must miss at least one of the regionsri, i ∈ [1,4]. As a result, we can write\\nPr\\nS∼Dm\\n[R(RS) >ϵ ] ≤ Pr\\nS∼Dm\\n[∪4\\ni=1{RS ∩ ri = ∅}] (2.5)\\n≤\\n4∑\\ni=1\\nPr\\nS∼Dm\\n[{RS ∩ ri = ∅}] (by the union bound)\\n≤ 4(1 − ϵ/4)m (since Pr[ ri] >ϵ /4)\\n≤ 4e x p (−mϵ/4),\\nwhere for the last step we used the general identity 1− x ≤ e−x valid for all x ∈ R.\\nFor any δ> 0, to ensure that PrS∼Dm [R(RS) >ϵ ] ≤ δ,w ec a ni m p o s e\\n4e x p (−ϵm/4) ≤ δ⇔ m ≥ 4\\nϵ log 4\\nδ. (2.6)\\nThus, for any ϵ> 0a n d δ> 0, if the sample size m is greater than 4\\nϵ log 4\\nδ,\\nthen PrS∼Dm [R(RS) >ϵ ] ≤ 1 − δ. Furthermore, the computational cost of the16 The PAC Learning Framework\\nR\\nR’\\nr1\\nr2\\nr3\\nr4\\nFigure 2.3 Illustration of the regions r1,...,r 4.\\nrepresentation of points in R2 and axis-aligned rectangles, which can be deﬁned by\\ntheir four corners, is constant. This proves that the concept class of axis-aligned\\nrectangles is PAC-learnable and that the sample complexity of PAC-learning axis-\\naligned rectangles is in O(\\n1\\nϵ log 1\\nδ).\\nAn equivalent way to present sample complexity results like (2.6), which we will\\noften see throughout this book, is to give ageneralization bound. It states that with\\nprobability at least 1 − δ, R(RS) is upper bounded by some quantity that depends\\non the sample size m and δ. To obtain this, if suﬃces to set δ to be equal to the\\nupper bound derived in (2.5), that is δ=4e x p (−mϵ/4) and solve for ϵ.T h i sy i e l d s\\nthat with probability at least 1 − δ, the error of the algorithm is bounded as:\\nR(RS) ≤ 4\\nm log 4\\nδ. (2.7)\\nOther PAC-learning algorithms could be considered for this example. One alterna-\\ntive is to return the largest axis-aligned rectangle not containing the negative points,\\nfor example. The proof of PAC-learning just presented for the tightest axis-aligned\\nrectangle can be easily adapted to the analysis of other such algorithms.\\nNote that the hypothesis set H we considered in this example coincided with the\\nconcept class C and that its cardinality was inﬁnite. Nevertheless, the problem\\nadmitted a simple proof of PAC-learning. We may then ask if a similar proof\\ncan readily apply to other similar concept classes. This is not as straightforward\\nbecause the speciﬁc geometric argument used in the proof is key. It is non-trivial\\nto extend the proof to other concept classes such as that of non-concentric circles\\n(see exercise 2.4). Thus, we need a more general proof technique and more general\\nresults. The next two sections provide us with such tools in the case of a ﬁnite\\nhypothesis set.2.2 Guarantees for ﬁnite hypothesis sets — consistent case 17\\n2.2 Guarantees for ﬁnite hypothesis sets — consistent case\\nIn the example of axis-aligned rectangles that we examined, the hypothesis hS\\nreturned by the algorithm was always consistent, that is, it admitted no error on\\nthe training sample S. In this section, we present a general sample complexity\\nbound, or equivalently, a generalization bound, for consistent hypotheses, in the\\ncase where the cardinality |H| of the hypothesis set is ﬁnite. Since we consider\\nconsistent hypotheses, we will assume that the target concept c is in H.\\nTheorem 2.1 Learning bounds — ﬁnite H, consistent case\\nLet H be a ﬁnite set of functions mapping from X to Y.L e tA be an algorithm that\\nfor any target conceptc ∈ H and i.i.d. sample S returns a consistent hypothesish\\nS:\\nˆR(hS)=0 .T h e n ,f o ra n yϵ, δ >0,t h ei n e q u a l i t yPrS∼Dm [R(hS) ≤ ϵ] ≥ 1 − δholds\\nif\\nm ≥ 1\\nϵ\\n(\\nlog |H| +l o g1\\nδ\\n⎡\\n. (2.8)\\nThis sample complexity result admits the following equivalent statement as a gener-\\nalization bound: for any ϵ, δ >0, with probability at least 1 − δ,\\nR(hS) ≤ 1\\nm\\n(\\nlog |H| +l o g1\\nδ\\n⎡\\n. (2.9)\\nProof Fix ϵ> 0. We do not know which consistent hypothesishS ∈ H is selected\\nby the algorithm A. This hypothesis further depends on the training sample S.\\nTherefore, we need to give a uniform convergence bound, that is, a bound that\\nholds for the set of all consistent hypotheses, which a fortiori includes hS.T h u s ,\\nwe will bound the probability that some h ∈ H would be consistent and have error\\nmore than ϵ:\\nPr[∃h ∈ H : ˆR(h)=0 ∧ R(h) >ϵ ]\\n=P r [ (h1 ∈ H, ˆR(h1)=0 ∧ R(h1) >ϵ ) ∨ (h2 ∈ H, ˆR(h2)=0 ∧ R(h2) >ϵ ) ∨··· ]\\n≤\\n∑\\nh∈H\\nPr[ ˆR(h)=0 ∧ R(h) >ϵ ] (union bound)\\n≤\\n∑\\nh∈H\\nPr[ ˆR(h)=0 | R(h) >ϵ ]. (deﬁnition of conditional probability)\\nNow, consider any hypothesis h ∈ H with R(h) >ϵ . Then, the probability that h\\nwould be consistent on a training sample S drawn i.i.d., that is, that it would have\\nno error on any point in S, can be bounded as:\\nPr[ ˆR(h)=0 | R(h) >ϵ ] ≤ (1 − ϵ)m.18 The PAC Learning Framework\\nThe previous inequality implies\\nPr[∃h ∈ H : ˆR(h)=0 ∧ R(h) >ϵ ] ≤| H|(1 − ϵ)m.\\nSetting the right-hand side to be equal toδand solving forϵ concludes the proof.\\nThe theorem shows that when the hypothesis setH is ﬁnite, a consistent algorithm\\nA is a PAC-learning algorithm, since the sample complexity given by (2.8) is\\ndominated by a polynomial in 1 /ϵ and 1/δ. As shown by (2.9), the generalization\\nerror of consistent hypotheses is upper bounded by a term that decreases as\\na function of the sample size m. This is a general fact: as expected, learning\\nalgorithms beneﬁt from larger labeled training samples. The decrease rate ofO(1/m)\\nguaranteed by this theorem, however, is particularly favorable.\\nThe price to pay for coming up with a consistent algorithm is the use of a\\nlarger hypothesis set H containing target concepts. Of course, the upper bound\\n(2.9) increases with |H|. However, that dependency is only logarithmic. Note that\\nthe term log |H|,o rt h er e l a t e dt e r ml o g2 |H| from which it diﬀers by a constant\\nfactor, can be interpreted as the number of bits needed to represent H.T h u s ,t h e\\ngeneralization guarantee of the theorem is controlled by the ratio of this number of\\nbits, log2 |H|,a n dt h es a m p l es i z em.\\nWe now use theorem 2.1 to analyze PAC-learning with various concept classes.\\nExample 2.2 Conjunction of Boolean literals\\nConsider learning the concept classCn of conjunctions of at mostn Boolean literals\\nx1,...,x n. A Boolean literal is either a variablexi, i ∈ [1,n ], or its negation xi.F o r\\nn = 4, an example is the conjunction: x1 ∧ x2 ∧ x4,w h e r ex2 denotes the negation\\nof the Boolean literal x2.( 1, 0, 0, 1) is a positive example for this concept while\\n(1,0,0, 0) is a negative example.\\nObserve that for n = 4, a positive example (1 , 0, 1, 0) implies that the target\\nconcept cannot contain the literalsx1 and x3 and that it cannot contain the literals\\nx2 and x4. In contrast, a negative example is not as informative since it is not\\nknown which of its n bits are incorrect. A simple algorithm for ﬁnding a consistent\\nhypothesis is thus based on positive examples and consists of the following: for each\\npositive example (b1,...,b n)a n di ∈ [1,n ], if bi =1t h e nxi is ruled out as a possible\\nliteral in the concept class and if bi =0t h e nxi is ruled out. The conjunction of all\\nthe literals not ruled out is thus a hypothesis consistent with the target. Figure 2.4\\nshows an example training sample as well as a consistent hypothesis for the case\\nn =6 .\\nWe have |H| = |Cn| =3 n, since each literal can be included positively, with\\nnegation, or not included. Plugging this into the sample complexity bound for\\nconsistent hypotheses yields the following sample complexity bound for any ϵ> 02.2 Guarantees for ﬁnite hypothesis sets — consistent case 19\\n011011 +\\n011111 +\\n001101 -\\n011111 +\\n100110 -\\n010011 +\\n01? ?11\\nFigure 2.4 Each of the ﬁrst six rows of the table represents a training example with\\nits label, + or −, indicated in the last column. The last row contains0 (respectively\\n1) in columni ∈ [1, 6] if theith entry is0 (respectively 1) for all the positive examples.\\nIt contains “?” if both 0 and 1 appear as an ith entry for some positive example.\\nThus, for this training sample, the hypothesis returned by the consistent algorithm\\nd e s c r i b e di nt h et e x ti s\\nx1 ∧ x2 ∧ x5 ∧ x6.\\nand δ> 0:\\nm ≥ 1\\nϵ\\n(\\n(log 3)n +l o g1\\nδ\\n⎡\\n. (2.10)\\nThus, the class of conjunctions of at mostn Boolean literals is PAC-learnable. Note\\nthat the computational complexity is also polynomial, since the training cost per\\ne x a m p l ei si nO(n). Forδ=0 .02, ϵ =0 .1, and n = 10, the bound becomesm ≥ 149.\\nThus, for a labeled sample of at least 149 examples, the bound guarantees 99%\\naccuracy with a conﬁdence of at least 98%.\\nExample 2.3 Universal concept class\\nConsider the set X = {0, 1}\\nn of all Boolean vectors with n components, and let Un\\nbe the concept class formed by all subsets ofX. Is this concept class PAC-learnable?\\nTo guarantee a consistent hypothesis the hypothesis class must include the concept\\nclass, thus |H|≥| Un| =2 (2n). Theorem 2.1 gives the following sample complexity\\nbound:\\nm ≥ 1\\nϵ\\n(\\n(log 2)2n +l o g1\\nδ\\n⎡\\n. (2.11)\\nHere, the number of training samples required is exponential inn, which is the cost\\nof the representation of a point in X. Thus, PAC-learning is not guaranteed by\\nthe theorem. In fact, it is not hard to show that this universal concept class is not\\nPAC-learnable.20 The PAC Learning Framework\\nExample 2.4 k-term DNF formulae\\nA disjunctive normal form (DNF) formula is a formula written as the disjunction of\\nseveral terms, each term being a conjunction of Boolean literals. Ak-term DNF is a\\nDNF formula deﬁned by the disjunction of k terms, each term being a conjunction\\nof at most n Boolean literals. Thus, for k = 2 and n = 3, an example of a k-term\\nDNF is (x\\n1 ∧ x2 ∧ x3) ∨ (x1 ∧ x3).\\nIs the class C of k-term DNF formulae is PAC-learnable? The cardinality of the\\nc l a s si s3nk, since each term is a conjunction of at most n v a r i a b l e sa n dt h e r ea r e\\n3n such conjunctions, as seen previously. The hypothesis set H must contain C for\\nconsistency to be possible, thus |H|≥ 3nk. Theorem 2.1 gives the following sample\\ncomplexity bound:\\nm ≥ 1\\nϵ\\n(\\n(log 3)nk +l o g1\\nδ\\n⎡\\n, (2.12)\\nwhich is polynomial. However, it can be shown that the problem of learning k-\\nterm DNF is in RP, the complexity class of problems that admit a randomized\\npolynomial-time decision solution. The problem is therefore computationally in-\\ntractable unless RP = NP, which is commonly conjectured not to be the case. Thus,\\nwhile the sample size needed for learningk-term DNF formulae is only polynomial,\\neﬃcient PAC-learning of this class is not possible unless RP = NP.\\nExample 2.5 k-CNF formulae\\nA conjunctive normal form (CNF) formula is a conjunction of disjunctions. A k-\\nCNF formula is an expression of the form T\\n1 ∧... ∧Tj with arbitrary length j ∈ N\\nand with each term Ti being a disjunction of at most k Boolean attributes.\\nThe problem of learning k-CNF formulae can be reduced to that of learning\\nconjunctions of Boolean literals, which, as seen previously, is a PAC-learnable\\nconcept class. To do so, it suﬃces to associate to each term T\\ni a new variable.\\nThen, this can be done with the following bijection:\\nai(x1) ∨···∨ ai(xn) → Yai(x1),...,ai(xn), (2.13)\\nwhere ai(xj) denotes the assignment to xj in term Ti. This reduction to PAC-\\nlearning of conjunctions of Boolean literals may aﬀect the original distribution, but\\nthis is not an issue since in the PAC framework no assumption is made about the\\ndistribution. Thus, the PAC-learnability of conjunctions of Boolean literals implies\\nthat of k-CNF formulae.\\nThis is a surprising result, however, since anyk-term DNF formula can be written\\nas a k-CNF formula. Indeed, using associativity, ak-term DNF can be rewritten as2.3 Guarantees for ﬁnite hypothesis sets — inconsistent case 21\\na k-CNF formula via\\nk⋁\\ni=1\\nai(x1) ∧···∧ ai(xn)=\\nn⋀\\ni1,...,ik=1\\na1(xi1 ) ∨···∨ ak(xik ).\\nTo illustrate this rewriting in a speciﬁc case, observe, for example, that\\n(u1 ∧ u2 ∧ u3) ∨ (v1 ∧ v2 ∧ v3)=\\n3⋀\\ni,j=1\\n(ui ∧ vj).\\nBut, as we previously saw,k-term DNF formulae are not eﬃciently PAC-learnable!\\nWhat can explain this apparent inconsistency? Observe that the number of new\\nvariables needed to write ak-term DNF as ak-CNF formula via the transformation\\njust described is exponential ink,i ti si nO(n\\nk). The discrepancy comes from the size\\nof the representation of a concept. A k-term DNF formula can be an exponentially\\nmore compact representation, and eﬃcient PAC-learning is intractable if a time-\\ncomplexity polynomial in that size is required. Thus, this apparent paradox deals\\nwith key aspects of PAC-learning, which include the cost of the representation of a\\nconcept and the choice of the hypothesis set.\\n2.3 Guarantees for ﬁnite hypothesis sets — inconsistent case\\nIn the most general case, there may be no hypothesis in H consistent with the\\nlabeled training sample. This, in fact, is the typical case in practice, where the\\nlearning problems may be somewhat diﬃcult or the concept classes more complex\\nthan the hypothesis set used by the learning algorithm. However, inconsistent\\nhypotheses with a small number of errors on the training sample can be useful and,\\nas we shall see, can beneﬁt from favorable guarantees under some assumptions. This\\nsection presents learning guarantees precisely for this inconsistent case and ﬁnite\\nhypothesis sets.\\nTo derive learning guarantees in this more general setting, we will use Hoeﬀding’s\\ninequality (theorem D.1) or the following corollary, which relates the generalization\\nerror and empirical error of a single hypothesis.22 The PAC Learning Framework\\nCorollary 2.1\\nFix ϵ> 0 and let S denote an i.i.d. sample of size m. Then, for any hypothesis\\nh: X →{ 0, 1}, the following inequalities hold:\\nPr\\nS∼Dm\\n[ ˆR(h) − R(h) ≥ ϵ] ≤ exp(−2mϵ2) (2.14)\\nPr\\nS∼Dm\\n[ ˆR(h) − R(h) ≤− ϵ] ≤ exp(−2mϵ2). (2.15)\\nBy the union bound, this implies the following two-sided inequality:\\nPr\\nS∼Dm\\n[\\n| ˆR(h) − R(h)|≥ ϵ\\n]\\n≤ 2e x p (−2mϵ2). (2.16)\\nProof The result follows immediately theorem D.1.\\nSetting the right-hand side of (2.16) to be equal to δ and solving for ϵ yields\\nimmediately the following bound for a single hypothesis.\\nCorollary 2.2 Generalization bound — single hypothesis\\nFix a hypothesis h: X→ { 0, 1}. Then, for any δ> 0, the following inequality holds\\nwith probability at least 1 − δ:\\nR(h) ≤ ˆR(h)+\\n√\\nlog 2\\nδ\\n2m . (2.17)\\nThe following example illustrates this corollary in a simple case.\\nExample 2.6 Tossing a coin\\nImagine tossing a biased coin that lands heads with probability p,a n dl e to u r\\nhypothesis be the one that always guesses heads. Then the true error rate isR(h)= p\\nand the empirical error rate ˆR(h)= ˆp,w h e r eˆp is the empirical probability of\\nheads based on the training sample drawn i.i.d. Thus, corollary 2.2 guarantees with\\nprobability at least 1 − δthat\\n|p − ˆp|≤\\n√\\nlog 2\\nδ\\n2m . (2.18)\\nTherefore, if we choose δ=0 .02 and use a sample of size 500, with probability at\\nleast 98%, the following approximation quality is guaranteed for ˆp:\\n|p − ˆp|≤\\n√\\nlog(10)\\n1000 ≈ 0.048. (2.19)\\nCan we readily apply corollary 2.2 to bound the generalization error of the\\nhypothesis hS returned by a learning algorithm when training on a sample S? No,\\nsince hS is not a ﬁxed hypothesis, but a random variable depending on the training\\nsample S drawn. Note also that unlike the case of a ﬁxed hypothesis for which2.3 Guarantees for ﬁnite hypothesis sets — inconsistent case 23\\nthe expectation of the empirical error is the generalization error (equation 2.3), the\\ngeneralization error R(hS) is a random variable and in general distinct from the\\nexpectation E[ ˆR(hS)], which is a constant.\\nThus, as in the proof for the consistent case, we need to derive a uniform con-\\nvergence bound, that is a bound that holds with high probability for all hypotheses\\nh ∈ H.\\nTheorem 2.2 Learning bound — ﬁnite H, inconsistent case\\nLet H be a ﬁnite hypothesis set. Then, for any δ> 0, with probability at least1 − δ,\\nthe following inequality holds:\\n∀h ∈ H, R (h) ≤ ˆR(h)+\\n√\\nlog |H| +l o g2\\nδ\\n2m . (2.20)\\nProof Let h1,...,h |H| be the elements ofH. Using the union bound and applying\\ncorollary 2.2 to each hypothesis yield:\\nPr\\n[\\n∃h ∈ H\\n⏐⏐ ˆR(h) − R(h)\\n⏐⏐ >ϵ\\n]\\n=P r\\n[(⏐⏐ ˆR(h1) − R(h1)\\n⏐⏐ >ϵ\\n⎡\\n∨ ... ∨\\n(⏐⏐ ˆR(h|H|) − R(h|H|)\\n⏐⏐ >ϵ\\n⎡]\\n≤\\n∑\\nh∈H\\nPr\\n[⏐⏐ ˆR(h) − R(h)\\n⏐⏐ >ϵ\\n]\\n≤ 2|H| exp(−2mϵ2).\\nSetting the right-hand side to be equal to δcompletes the proof.\\nThus, for a ﬁnite hypothesis set H,\\nR(h) ≤ ˆR(h)+ O\\n(√\\nlog2 |H|\\nm\\n⎡\\n.\\nAs already pointed out, log 2 |H| can be interpreted as the number of bits needed\\nto represent H. Several other remarks similar to those made on the generalization\\nbound in the consistent case can be made here: a larger sample size m guarantees\\nbetter generalization, and the bound increases with |H|, but only logarithmically.\\nBut, here, the bound is a less favorable function of log2 |H|\\nm ;i tv a r i e sa st h es q u a r e\\nroot of this term. This is not a minor price to pay: for a ﬁxed |H|, to attain the\\nsame guarantee as in the consistent case, a quadratically larger labeled sample is\\nneeded.\\nNote that the bound suggests seeking a trade-oﬀ between reducing the empirical\\nerror versus controlling the size of the hypothesis set: a larger hypothesis set is\\npenalized by the second term but could help reduce the empirical error, that is the\\nﬁrst term. But, for a similar empirical error, it suggests using a smaller hypothesis24 The PAC Learning Framework\\ns e t .T h i sc a nb ev i e w e da sa ni n s t a n c eo ft h es o - c a l l e dOccam’s Razor principle\\nnamed after the theologian William of Occam:Plurality should not be posited without\\nnecessity, also rephrased as,the simplest explanation is best. In this context, it could\\nbe expressed as follows: All other things being equal, a simpler (smaller) hypothesis\\nset is better.\\n2.4 Generalities\\nIn this section we will consider several important questions related to the learning\\nscenario, which we left out of the discussion of the earlier sections for simplicity.\\n2.4.1 Deterministic versus stochastic scenarios\\nIn the most general scenario of supervised learning, the distribution D is deﬁned\\nover X× Y , and the training data is a labeled sample S drawn i.i.d. according to\\nD:\\nS =( (x\\n1,y1),..., (xm,y m)).\\nThe learning problem is to ﬁnd a hypothesis h ∈ H with small generalization error\\nR(h)= P r\\n(x,y)∼D\\n[h(x) ̸= y]= E\\n(x,y)∼D\\n[1h(x)̸=y].\\nThis more general scenario is referred to as the stochastic scenario. Within this\\nsetting, the output label is a probabilistic function of the input. The stochastic\\nscenario captures many real-world problems where the label of an input point is not\\nunique. For example, if we seek to predict gender based on input pairs formed by\\nthe height and weight of a person, then the label will typically not be unique. For\\nmost pairs, both male and female are possible genders. For each ﬁxed pair, there\\nwould be a probability distribution of the label being male.\\nThe natural extension of the PAC-learning framework to this setting is known as\\nthe agnostic PAC-learning.\\nDeﬁnition 2.4 Agnostic PAC-learning\\nLet H be a hypothesis set. A is an agnostic PAC-learning algorithm if there\\nexists a polynomial function poly(·, ·, ·, ·) such that for any ϵ> 0 and δ> 0,\\nfor all distributions D over X× Y , the following holds for any sample size m ≥\\npoly(1/ϵ,1/δ, n,size(c)):\\nPr\\nS∼Dm\\n[R(hS) − min\\nh∈H\\nR(h) ≤ ϵ] ≥ 1 − δ. (2.21)2.4 Generalities 25\\nIf A further runs inpoly(1/ϵ,1/δ, n,size(c)),t h e ni ti ss a i dt ob ea neﬃcient agnostic\\nPAC-learning algorithm.\\nWhen the label of a point can be uniquely determined by some measurable func-\\ntion f : X→ Y (with probability one), then the scenario is said to bedeterministic.\\nIn that case, it suﬃces to consider a distribution D over the input space. The\\ntraining sample is obtained by drawing (x1,...,x m)a c c o r d i n gt oD and the labels\\nare obtained via f: yi = f(xi) for all i ∈ [1,m]. Many learning problems can be\\nformulated within this deterministic scenario.\\nIn the previous sections, as well as in most of the material presented in this book,\\nwe have restricted our presentation to the deterministic scenario in the interest of\\nsimplicity. However, for all of this material, the extension to the stochastic scenario\\nshould be straightforward for the reader.\\n2.4.2 Bayes error and noise\\nIn the deterministic case, by deﬁnition, there exists a target function f with no\\ngeneralization error: R(h) = 0. In the stochastic case, there is a minimal non-zero\\nerror for any hypothesis.\\nDeﬁnition 2.5 Bayes error\\nGiven a distribution D over X× Y ,t h eBayes error R\\n∗ is deﬁned as the inﬁmum\\nof the errors achieved by measurable functionsh: X→ Y :\\nR⋆ =i n f\\nh\\nh measurable\\nR(h). (2.22)\\nA hypothesis h with R(h)= R∗ is called a Bayes hypothesis or Bayes classiﬁer.\\nBy deﬁnition, in the deterministic case, we haveR∗ = 0, but, in the stochastic case,\\nR∗ ̸= 0. Clearly, the Bayes classiﬁerhBayes can be deﬁned in terms of the conditional\\nprobabilities as:\\n∀x ∈X ,h Bayes(x) = argmax\\ny∈{0,1}\\nPr[y|x]. (2.23)\\nThe average error made by hBayes on x ∈X is thus min{Pr[0|x], Pr[1|x]},a n dt h i s\\nis the minimum possible error. This leads to the following deﬁnition of noise.\\nDeﬁnition 2.6 Noise\\nGiven a distribution D over X× Y ,t h enoise at point x ∈X is deﬁned by\\nnoise(x)=m i n{Pr[1|x], Pr[0|x]}. (2.24)\\nThe average noise or the noise associated to D is E[noise(x)].26 The PAC Learning Framework\\nThus, the average noise is precisely the Bayes error: noise = E[noise(x)] = R∗.T h e\\nnoise is a characteristic of the learning task indicative of its level of diﬃculty. A\\npoint x ∈X , for which noise(x)i sc l o s et o1/2, is sometimes referred to as noisy\\nand is of course a challenge for accurate prediction.\\n2.4.3 Estimation and approximation errors\\nThe diﬀerence between the error of a hypothesish ∈ H and the Bayes error can be\\ndecomposed as:\\nR(h) − R\\n∗ =( R(h) − R(h∗))\\ued19 \\ued18\\ued17 \\ued1a\\nestimation\\n+(R(h∗) − R∗)\\ued19 \\ued18\\ued17 \\ued1a\\napproximation\\n, (2.25)\\nwhere h∗ is a hypothesis in H w i t hm i n i m a le r r o r ,o rabest-in-class hypothesis.3\\nThe second term is referred to as theapproximation error, since it measures how\\nwell the Bayes error can be approximated usingH. It is a property of the hypothesis\\nset H, a measure of its richness. The approximation error is not accessible, since\\nin general the underlying distribution D is not known. Even with various noise\\nassumptions, estimating the approximation error is diﬃcult.\\nThe ﬁrst term is the estimation error , and it depends on the hypothesis h\\nselected. It measures the quality of the hypothesish with respect to the best-in-class\\nhypothesis. The deﬁnition of agnostic PAC-learning is also based on the estimation\\nerror. The estimation error of an algorithm A, that is, the estimation error of the\\nhypothesis hS returned after training on a sample S,c a ns o m e t i m e sb eb o u n d e di n\\nterms of the generalization error.\\nFor example, let hERM\\nS denote the hypothesis returned by the empirical risk\\nminimization algorithm, that is the algorithm that returns a hypothesishERM\\nS with\\nthe smallest empirical error. Then, the generalization bound given by theorem 2.2,\\nor any other bound on sup\\nh∈H |R(h) − ˆR(h)|, can be used to bound the estimation\\nerror of the empirical risk minimization algorithm. Indeed, rewriting the estimation\\nerror to make ˆR(h\\nERM\\nS ) appear and using ˆR(hERM\\nS ) ≤ ˆR(h∗), which holds by the\\ndeﬁnition of the algorithm, we can write\\nR(hERM\\nS ) − R(h∗)= R(hERM\\nS ) − ˆR(hERM\\nS )+ ˆR(hERM\\nS ) − R(h∗)\\n≤ R(hERM\\nS ) − ˆR(hERM\\nS )+ ˆR(h∗) − R(h∗)\\n≤ 2s u p\\nh∈H\\n|R(h) − ˆR(h)|. (2.26)\\n3. When H is a ﬁnite hypothesis set, h∗ necessarily exists; otherwise, in this discussion\\nR(h∗) can be replaced by infh∈H R(h).2.4 Generalities 27\\nmeasure of capacity\\ntraining error\\ncomplexity term\\nbound on generalization errorerror\\nFigure 2.5 Illustration of structural risk minimization. The plots of three errors\\nare shown as a function of a measure of capacity. Clearly, as the size or capacity of\\nthe hypothesis set increases, the training error decreases, while the complexity term\\nincreases. SRM selects the hypothesis minimizing a bound on the generalization\\nerror, which is a sum of the empirical error, and the complexity term is shown in\\nred.\\nThe right-hand side of (2.26) can be bounded by theorem 2.2 and increases with\\nthe size of the hypothesis set, while R(h\\n∗)d e c r e a s e sw i t h|H|.\\n2.4.4 Model selection\\nHere, we discuss some broad model selection and algorithmic ideas based on the\\ntheoretical results presented in the previous sections. We assume an i.i.d. labeled\\ntraining sample S of size m and denote the error of a hypothesis h on S by ˆR\\nS(h)\\nto explicitly indicate its dependency on S.\\nWhile the guarantee of theorem 2.2 holds only for ﬁnite hypothesis sets, it already\\nprovides us with some useful insights for the design of algorithms and, as we will see\\nin the next chapters, similar guarantees hold in the case of inﬁnite hypothesis sets.\\nSuch results invite us to consider two terms: the empirical error and a complexity\\nterm, which here is a function of |H| and the sample size m.\\nIn view of that, the ERM algorithm , which only seeks to minimize the error on\\nthe training sample\\nh\\nERM\\nS =a r g m i n\\nh∈H\\nˆRS(h), (2.27)\\nmight not be successful, since it disregards the complexity term. In fact, the\\nperformance of the ERM algorithm is typically very poor in practice. Additionally,\\nin many cases, determining the ERM solution is computationally intractable. For\\nexample, ﬁnding a linear hypothesis with the smallest error on the training sample\\ni sN P - h a r d( a saf u n c t i o no ft h ed i m e n s i o no ft h es p a c e ) .\\nAnother method known as structural risk minimization (SRM) consists of con-28 The PAC Learning Framework\\nsidering instead an inﬁnite sequence of hypothesis sets with increasing sizes\\nH0 ⊂ H1 ⊂ ··· ⊂ Hn ··· (2.28)\\nand to ﬁnd the ERM solution hERM\\nn for each Hn. The hypothesis selected is the\\none among the hERM\\nn solutions with the smallest sum of the empirical error and\\na complexity term complexity(Hn,m) that depends on the size (or more generally\\nthe capacity, that is, another measure of the richness of H)o f Hn, and the sample\\nsize m:\\nhSRM\\nS =a r g m i n\\nh∈Hn\\nn∈N\\nˆRS(h)+c o m p l e x i t y (Hn,m). (2.29)\\nFigure 2.5 illustrates the SRM method. While SRM beneﬁts from strong theoretical\\nguarantees, it is typically computationally very expensive, since it requires deter-\\nmining the solution of multiple ERM problems. Note that the number of ERM\\nproblems is not inﬁnite if for some n the minimum empirical error is zero: The\\nobjective function can only be larger for n′ ≥ n.\\nAn alternative family of algorithms is based on a more straightforward optimiza-\\ntion that consists of minimizing the sum of the empirical error and aregularization\\nterm that penalizes more complex hypotheses. The regularization term is typically\\ndeﬁned as ∥h∥2 for some norm ∥·∥ when H is a vector space:\\nhREG\\nS =a r g m i n\\nh∈H\\nˆRS(h)+ λ∥h∥2. (2.30)\\nλ ≥ 0i sa regularization parameter, which can be used to determine the trade-oﬀ\\nbetween empirical error minimization and control of the complexity. In practice, λ\\nis typically selected using n-fold cross-validation. In the next chapters, we will see\\na number of diﬀerent instances of such regularization-based algorithms.\\n2.5 Chapter notes\\nThe PAC learning framework was introduced by Valiant [1984]. The book of Kearns\\nand Vazirani [1994] is an excellent reference dealing with most aspects of PAC-\\nlearning and several other foundational questions in machine learning. Our example\\nof learning axis-aligned rectangles is based on that reference.\\nThe PAC learning framework is a computational framework since it takes into\\naccount the cost of the computational representations and the time complexity of\\nthe learning algorithm. If we omit the computational aspects, it is similar to the\\nlearning framework considered earlier by Vapnik and Chervonenkis [see Vapnik,\\n2000].2.6 Exercises 29\\nOccam’s razor principle is invoked in a variety of contexts, such as in linguistics to\\njustify the superiority of a set of rules or syntax. The Kolmogorov complexity can be\\nviewed as the corresponding framework in information theory. In the context of the\\nlearning guarantees presented in this chapter, the principle suggests selecting the\\nmost parsimonious explanation (the hypothesis set with the smallest cardinality).\\nWe will see in the next sections other applications of this principle with diﬀerent\\nnotions of simplicity or complexity. The idea of structural risk minimization (SRM)\\nis due to Vapnik [1998].\\n2.6 Exercises\\n2.1 Two-oracle variant of the PAC model. Assume that positive and negative\\nexamples are now drawn from two separate distributions D\\n+ and D− . For an\\naccuracy (1 − ϵ), the learning algorithm must ﬁnd a hypothesis h such that:\\nPr\\nx∼D+\\n[h(x)=0 ] ≤ ϵ and Pr\\nx∼D−\\n[h(x)=1 ] ≤ ϵ. (2.31)\\nThus, the hypothesis must have a small error on both distributions. Let C be any\\nconcept class andH be any hypothesis space. Leth0 and h1 represent the identically\\n0 and identically 1 functions, respectively. Prove thatC is eﬃciently PAC-learnable\\nusing H in the standard (one-oracle) PAC model if and only if it is eﬃciently PAC-\\nlearnable using H ∪{ h0,h1} in this two-oracle PAC model.\\n2.2 PAC learning of hyper-rectangles. An axis-aligned hyper-rectangle in Rn is a\\nset of the form [a1,b1] × ... × [an,b n]. Show that axis-aligned hyper-rectangles are\\nPAC-learnable by extending the proof given in Example 2.1 for the case n =2 .\\n2.3 Concentric circles. Let X = R2 and consider the set of concepts of the form\\nc = {(x, y): x2 + y2 ≤ r2} for some real number r. Show that this class can be\\n(ϵ, δ)-PAC-learned from training data of size m ≥ (1/ϵ)l o g( 1/δ).\\n2.4 Non-concentric circles. Let X = R2 and consider the set of concepts of the form\\nc = {x ∈ R2 : ||x−x0|| ≤ r} for some pointx0 ∈ R2 and real numberr. Gertrude, an\\naspiring machine learning researcher, attempts to show that this class of concepts\\nmay be (ϵ, δ)-PAC-learned with sample complexity m ≥ (3/ϵ)l o g( 3/δ), but she is\\nhaving trouble with her proof. Her idea is that the learning algorithm would select\\nthe smallest circle consistent with the training data. She has drawn three regions\\nr\\n1,r2,r3 around the edge of concept c, with each region having probabilityϵ/3( s e e\\nﬁgure 2.6). She wants to argue that if the generalization error is greater than or\\nequal to ϵ, then one of these regions must have been missed by the training data,30 The PAC Learning Framework\\nr1\\nr2\\nr3\\nFigure 2.6 Gertrude’s regions r1,r 2,r 3.\\nand hence this event will occur with probability at most δ.C a ny o ut e l lG e r t r u d e\\nif her approach works?\\n2.5 Triangles. Let X = R2 with orthonormal basis (e1,e2), and consider the set of\\nconcepts deﬁned by the area inside a right triangle ABC with two sides parallel to\\nthe axes, with −−→AB/∥−−→AB∥ = e1 and −→AC/∥−→AC∥ = e2,a n d∥−−→AB∥/∥−→AC∥ = α for some\\npositive real α ∈ R+. Show, using similar methods to those used in the chapter for\\nthe axis-aligned rectangles, that this class can be ( ϵ, δ)-PAC-learned from training\\ndata of size m ≥ (3/ϵ)l o g( 3/δ).\\n2.6 Learning in the presence of noise — rectangles. In example 2.1, we showed that\\nthe concept class of axis-aligned rectangles is PAC-learnable. Consider now the case\\nwhere the training points received by the learner are subject to the following noise:\\npoints negatively labeled are unaﬀected by noise but the label of a positive training\\npoint is randomly ﬂipped to negative with probabilityη ∈ (0,\\n1\\n2 ). The exact value of\\nthe noise rateηis not known to the learner but an upper boundη′ is supplied to him\\nwith η ≤ η′ < 1/2. Show that the algorithm described in class returning the tightest\\nrectangle containing positive points can still PAC-learn axis-aligned rectangles in\\nthe presence of this noise. To do so, you can proceed using the following steps:\\n(a) Using the same notation as in example 2.1, assume that Pr[R] >ϵ .S u p p o s e\\nthat R(R′) >ϵ .G i v ea nu p p e rb o u n do nt h ep r o b a b i l i t yt h a tR′ misses a region\\nrj, j ∈ [1, 4] in terms of ϵ and η′?\\n(b) Use that to give an upper bound on Pr[R(R′) >ϵ ]i nt e r m so fϵ and η′ and\\nconclude by giving a sample complexity bound.\\n2.7 Learning in the presence of noise — general case. In this question, we will seek\\na result that is more general than in the previous question. We consider a ﬁnite\\nhypothesis set H, assume that the target concept is in H, and adopt the following2.6 Exercises 31\\nnoise model: the label of a training point received by the learner is randomly changed\\nwith probability η ∈ (0, 1\\n2 ). The exact value of the noise rate η is not known to the\\nlearner but an upper bound η′ is supplied to him with η ≤ η′ < 1/2.\\n(a) For any h ∈ H,l e td(h) denote the probability that the label of a training\\npoint received by the learner disagrees with the one given by h.L e th∗ be the\\ntarget hypothesis, show that d(h∗)= η.\\n(b) More generally, show that for any h ∈ H, d(h)= η+( 1− 2η) R(h), where\\nR(h) denotes the generalization error of h.\\n(c) Fix ϵ> 0 for this and all the following questions. Use the previous questions\\nto show that if R(h) >ϵ ,t h e nd(h) − d(h∗) ≥ ϵ′,w h e r eϵ′ = ϵ(1 − 2η′).\\n(d) For any hypothesis h ∈ H and sample S of size m,l e t ˆd(h)d e n o t et h e\\nfraction of the points inS whose labels disagree with those given byh.W ew i l l\\nconsider the algorithm L which, after receiving S, returns the hypothesis hS\\nwith the smallest number of disagreements (thus ˆd(hS)i sm i n i m a l ) .T os h o w\\nPAC-learning for L, we will show that for any h,i f R(h) >ϵ , then with high\\nprobability ˆd(h) ≥ ˆd(h∗). First, show that for any δ> 0, with probability at\\nleast 1 − δ/2, for m ≥ 2\\nϵ′2 log 2\\nδ, the following holds:\\nˆd(h∗) − d(h∗) ≤ ϵ′/2\\n(e) Second, show that for any δ> 0, with probability at least 1 − δ/2, for\\nm ≥ 2\\nϵ′2 (log |H| +l o g2\\nδ), the following holds for all h ∈ H:\\nd(h) − ˆd(h) ≤ ϵ′/2\\n(f) Finally, show that for any δ> 0, with probability at least 1 − δ,f o r\\nm ≥ 2\\nϵ2(1−2η′)2 (log |H|+log 2\\nδ), the following holds for allh ∈ H with R(h) >ϵ :\\nˆd(h) − ˆd(h∗) ≥ 0.\\n(Hint:u s e ˆd(h) − ˆd(h∗)=[ ˆd(h) − d(h) ]+[d(h) − d(h∗) ]+[d(h∗) − ˆd(h∗)] and\\nuse previous questions to lower bound each of these three terms).\\n2.8 Learning union of intervals. Let [a, b]a n d[c, d] be two intervals of the real line\\nwith a ≤ b ≤ c ≤ d.L e tϵ> 0, and assume that Pr D((b, c)) >ϵ ,w h e r eD is the\\ndistribution according to which points are drawn.\\n(a) Show that the probability that m points are drawn i.i.d. without any of\\nthem falling in the interval (b, c)i sa tm o s te−mϵ.\\n(b) Show that the concept class formed by the union of two closed intervals32 The PAC Learning Framework\\nin R,e . g . ,[a, b] ∪ [c, d], is PAC-learnable by giving a proof similar to the one\\ngiven in Example 2.1 for axis-aligned rectangles. (Hint: your algorithm might\\nnot return a hypothesis consistent with future negative points in this case.)\\n2.9 Consistent hypotheses. In this chapter, we showed that for a ﬁnite hypothesis\\nset H, a consistent learning algorithm A is a PAC-learning algorithm. Here, we\\nconsider a converse question. LetZ be a ﬁnite set of m labeled points. Suppose that\\nyou are given a PAC-learning algorithm A. Show that you can use A and a ﬁnite\\ntraining sample S to ﬁnd in polynomial time a hypothesis h ∈ H that is consistent\\nwith Z, with high probability. (Hint: you can select an appropriate distribution D\\nover Z and give a condition on R(h)f o rh to be consistent.)\\n2.10 Senate laws. For important questions, President Mouth relies on expert advice.\\nHe selects an appropriate advisor from a collection of H =2 ,800 experts.\\n(a) Assume that laws are proposed in a random fashion independently and\\nidentically according to some distributionD determined by an unknown group\\nof senators. Assume that President Mouth can ﬁnd and select an expert senator\\nout of H who has consistently voted with the majority for the last m = 200\\nlaws. Give a bound on the probability that such a senator incorrectly predicts\\nthe global vote for a future law. What is the value of the bound with 95%\\nconﬁdence?\\n(b) Assume now that President Mouth can ﬁnd and select an expert senator\\nout of H who has consistently voted with the majority for all but m\\n′ =2 0o f\\nthe last m = 200 laws. What is the value of the new bound?3 Rademacher Complexity and VC-\\nDimension\\nThe hypothesis sets typically used in machine learning are inﬁnite. But the sample\\ncomplexity bounds of the previous chapter are uninformative when dealing with\\ninﬁnite hypothesis sets. One could ask whether eﬃcient learning from a ﬁnite sample\\nis even possible when the hypothesis set H is inﬁnite. Our analysis of the family of\\naxis-aligned rectangles (Example 2.1) indicates that this is indeed possible at least\\nin some cases, since we proved that that inﬁnite concept class was PAC-learnable.\\nOur goal in this chapter will be to generalize that result and derive general learning\\nguarantees for inﬁnite hypothesis sets.\\nA general idea for doing so consists of reducing the inﬁnite case to the analysis\\nof ﬁnite sets of hypotheses and then proceed as in the previous chapter. There\\nare diﬀerent techniques for that reduction, each relying on a diﬀerent notion of\\ncomplexity for the family of hypotheses. The ﬁrst complexity notion we will use\\nis that of Rademacher complexity. This will help us derive learning guarantees\\nusing relatively simple proofs based on McDiarmid’s inequality, while obtaining\\nhigh-quality bounds, including data-dependent ones, which we will frequently make\\nuse of in future chapters. However, the computation of the empirical Rademacher\\ncomplexity is NP-hard for some hypothesis sets. Thus, we subsequently introduce\\ntwo other purely combinatorial notions, thegrowth function and the VC-dimension.\\nWe ﬁrst relate the Rademacher complexity to the growth function and then bound\\nthe growth function in terms of the VC-dimension. The VC-dimension is often easier\\nto bound or estimate. We will review a series of examples showing how to compute\\nor bound it, then relate the growth function and the VC-dimensions. This leads to\\ngeneralization bounds based on the VC-dimension. Finally, we present lower bounds\\nbased on the VC-dimension both in the realizable and non-realizable cases, which\\nwill demonstrate the critical role of this notion in learning.34 Rademacher Complexity and VC-Dimension\\n3.1 Rademacher complexity\\nWe will continue to use H to denote a hypothesis set as in the previous chapters,\\nand h an element of H. Many of the results of this section are general and hold for\\nan arbitrary loss function L: Y×Y → R. To each h: X→ Y , we can associate a\\nfunction g that maps (x, y) ∈X×Y to L(h(x),y ) without explicitly describing the\\nspeciﬁc loss L used. In what follows G will generally be interpreted as the family of\\nloss functions associated to H.\\nThe Rademacher complexity captures the richness of a family of functions by\\nmeasuring the degree to which a hypothesis set can ﬁt random noise. The following\\nstates the formal deﬁnitions of the empirical and average Rademacher complexity.\\nDeﬁnition 3.1 Empirical Rademacher complexity\\nLet G be a family of functions mapping fromZ to [a, b] and S =( z1,...,z m) aﬁ x e d\\nsample of size m with elements in Z. Then, the empirical Rademacher complexity\\nof G with respect to the sample S is deﬁned as:\\nˆRS(G)=E\\nσ\\n[\\nsup\\ng∈G\\n1\\nm\\nm∑\\ni=1\\nσig(zi)\\n]\\n, (3.1)\\nwhere σ =( σ1,...,σ m)⊤,w i t hσis independent uniform random variables taking\\nvalues in {−1, +1}.1 The random variables σi are called Rademacher variables.\\nLet gS denote the vector of values taken by function g over the sample S: gS =\\n(g(z1),...,g (zm))⊤. Then, the empirical Rademacher complexity can be rewritten\\nas\\nˆRS(G)=E\\nσ\\n[\\nsup\\ng∈G\\nσ · gS\\nm\\n]\\n.\\nThe inner product σ ·gS measures the correlation of gS with the vector of random\\nnoise σ. The supremum supg∈G\\nσ ·gS\\nm is a measure of how well the function class G\\ncorrelates with σ over the sample S. Thus, the empirical Rademacher complexity\\nmeasures on average how well the function class G correlates with random noise\\non S. This describes the richness of the family G: richer or more complex families\\nG can generate more vectors gS and thus better correlate with random noise, on\\naverage.\\n1. We assume implicitly that the supremum over the family G in this deﬁnition is\\nmeasurable and in general will adopt the same assumption throughout this book for other\\nsuprema over a class of functions. This assumption does not hold for arbitrary function\\nclasses but it is valid for the hypotheses sets typically considered in practice in machine\\nlearning, and the instances discussed in this book.3.1 Rademacher complexity 35\\nDeﬁnition 3.2 Rademacher complexity\\nLet D denote the distribution according to which samples are drawn. For any\\ninteger m ≥ 1,t h eRademacher complexity of G is the expectation of the empirical\\nRademacher complexity over all samples of size m drawn according toD:\\nRm(G)= E\\nS∼Dm\\n[ ˆRS(G)]. (3.2)\\nWe are now ready to present our ﬁrst generalization bounds based on Rademacher\\ncomplexity.\\nTheorem 3.1\\nLet G be a family of functions mapping from Z to [0,1].T h e n ,f o ra n yδ> 0,w i t h\\nprobability at least 1 − δ, each of the following holds for all g ∈ G:\\nE[g(z)] ≤ 1\\nm\\nm∑\\ni=1\\ng(zi)+2 Rm(G)+\\n√\\nlog 1\\nδ\\n2m (3.3)\\nand E[g(z)] ≤ 1\\nm\\nm∑\\ni=1\\ng(zi)+2 ˆRS(G)+3\\n√\\nlog 2\\nδ\\n2m . (3.4)\\nProof For any sample S =( z1,...,z m) and any g ∈ G,w ed e n o t eb yˆES[g]t h e\\nempirical average ofg over S: ˆES[g]= 1\\nm\\n∑m\\ni=1 g(zi). The proof consists of applying\\nMcDiarmid’s inequality to function Φ deﬁned for any sample S by\\nΦ(S)=s u p\\ng∈G\\nE[g] − ˆES[g]. (3.5)\\nLet S and S′ be two samples diﬀering by exactly one point, say zm in S and z′\\nm\\nin S′. Then, since the diﬀerence of suprema does not exceed the supremum of the\\ndiﬀerence, we have\\nΦ(S′) − Φ(S) ≤ sup\\ng∈G\\nˆES[g] − ˆES′ [g]=s u p\\ng∈G\\ng(zm) − g(z′\\nm)\\nm ≤ 1\\nm. (3.6)\\nS i m i l a r l y ,w ec a no b t a i nΦ (S) − Φ(S′) ≤ 1/m,t h u s|Φ(S) − Φ(S′)|≤ 1/m. Then,\\nby McDiarmid’s inequality, for any δ> 0, with probability at least 1 − δ/2, the\\nfollowing holds:\\nΦ(S) ≤ E\\nS\\n[Φ(S)] +\\n√\\nlog 2\\nδ\\n2m . (3.7)36 Rademacher Complexity and VC-Dimension\\nWe next bound the expectation of the right-hand side as follows:\\nE\\nS\\n[Φ(S)] = E\\nS\\n[\\nsup\\ng∈H\\nE[g] − ˆES(g)\\n]\\n=E\\nS\\n[\\nsup\\ng∈H\\nE\\nS′\\n[ˆES′ (g) − ˆES(g)\\n]]\\n(3.8)\\n≤ E\\nS,S′\\n[\\nsup\\ng∈H\\nˆES′ (g) − ˆES(g)\\n]\\n(3.9)\\n=E\\nS,S′\\n[\\nsup\\ng∈H\\n1\\nm\\nm∑\\ni=1\\n(g(z′\\ni) − g(zi))\\n]\\n(3.10)\\n=E\\nσ,S,S′\\n[\\nsup\\ng∈H\\n1\\nm\\nm∑\\ni=1\\nσi(g(z′\\ni) − g(zi))\\n]\\n(3.11)\\n≤ E\\nσ,S′\\n[\\nsup\\ng∈H\\n1\\nm\\nm∑\\ni=1\\nσig(z′\\ni)\\n]\\n+E\\nσ,S\\n[\\nsup\\ng∈H\\n1\\nm\\nm∑\\ni=1\\n−σig(zi)\\n]\\n(3.12)\\n=2 E\\nσ,S\\n[\\nsup\\ng∈H\\n1\\nm\\nm∑\\ni=1\\nσig(zi)\\n]\\n=2 Rm(G). (3.13)\\nEquation 3.8 uses the fact that points inS′ are sampled in an i.i.d. fashion and thus\\nE[g]=E S′ [ˆES′ (g)], as in (2.3). Inequality 3.9 holds by Jensen’s inequality and the\\nconvexity of the supremum function. In equation 3.11, we introduce Rademacher\\nvariables σ\\nis, that is uniformly distributed independent random variables taking\\nvalues in {−1, +1} as in deﬁnition 3.2. This does not change the expectation\\nappearing in (3.10): when σi = 1, the associated summand remains unchanged;\\nwhen σi = −1, the associated summand ﬂips signs, which is equivalent to swapping\\nzi and z′\\ni between S and S′. Since we are taking the expectation over all possibleS\\nand S′, this swap does not aﬀect the overall expectation. We are simply changing the\\norder of the summands within the expectation. (3.12) holds by the sub-additivity of\\nthe supremum function, that is the identity sup(U +V ) ≤ sup(U)+sup( V ). Finally,\\n(3.13) stems from the deﬁnition of Rademacher complexity and the fact that the\\nvariables σ\\ni and −σi are distributed in the same way.\\nThe reduction to Rm(G) in equation 3.13 yields the bound in equation 3.3,\\nusing δ instead of δ/2. To derive a bound in terms of ˆRS(G), we observe that,\\nby deﬁnition 3.2, changing one point in S changes ˆRS(G)b ya tm o s t1/m. Then,\\nusing again McDiarmid’s inequality, with probability 1 − δ/2 the following holds:\\nRm(G) ≤ ˆRS(G)+\\n√\\nlog 2\\nδ\\n2m . (3.14)\\nFinally, we use the union bound to combine inequalities 3.7 and 3.14, which yields3.1 Rademacher complexity 37\\nwith probability at least 1 − δ:\\nΦ(S) ≤ 2 ˆRS(G)+3\\n√\\nlog 2\\nδ\\n2m , (3.15)\\nwhich matches (3.4).\\nThe following result relates the empirical Rademacher complexities of a hypothe-\\nsis set H and to the family of loss functionsG associated to H in the case of binary\\nloss (zero-one loss).\\nLemma 3.1\\nLet H be a family of functions taking values in {−1, +1} and let G be the family of\\nloss functions associated toH for the zero-one loss:G = {(x, y) ↦→ 1h(x)̸=y : h ∈ H\\n}\\n.\\nFor any sample S =( (x1,y1),..., (xm,y m)) of elements in X× { − 1, +1},l e tSX\\ndenote its projection over X: SX =( x1,...,x m). Then, the following relation holds\\nbetween the empirical Rademacher complexities ofG and H:\\nˆRS(G)= 1\\n2\\nˆRSX (H). (3.16)\\nProof For any sample S =( (x1,y1),..., (xm,y m)) of elements in X× { − 1, +1},\\nby deﬁnition, the empirical Rademacher complexity of G c a nb ew r i t t e na s :\\nˆRS(G)=E\\nσ\\n[\\nsup\\nh∈H\\n1\\nm\\nm∑\\ni=1\\nσi1h(xi)̸=yi\\n]\\n=E\\nσ\\n[\\nsup\\nh∈H\\n1\\nm\\nm∑\\ni=1\\nσi\\n1−yih(xi)\\n2\\n]\\n= 1\\n2 E\\nσ\\n[\\nsup\\nh∈H\\n1\\nm\\nm∑\\ni=1\\n−σiyih(xi)\\n]\\n= 1\\n2 E\\nσ\\n[\\nsup\\nh∈H\\n1\\nm\\nm∑\\ni=1\\nσih(xi)\\n]\\n= 1\\n2 RSX (H),\\nw h e r ew eu s e dt h ef a c tt h a t1h(xi)̸=yi =( 1− yih(xi))/2 and the fact that for a ﬁxed\\nyi ∈{ −1, +1}, σi and −yiσi are distributed in the same way.\\nNote that the lemma implies, by taking expectations, that for anym ≥ 1, Rm(G)=\\n1\\n2 Rm(H). These connections between the empirical and average Rademacher com-\\nplexities can be used to derive generalization bounds for binary classiﬁcation in\\nterms of the Rademacher complexity of the hypothesis set H.\\nTheorem 3.2 Rademacher complexity bounds – binary classiﬁcation\\nLet H be a family of functions taking values in{−1, +1} and let D be the distribution\\nover the input space X. Then, for any δ> 0, with probability at least 1 − δ over38 Rademacher Complexity and VC-Dimension\\na sample S of size m drawn according to D, each of the following holds for any\\nh ∈ H:\\nR(h) ≤ ˆR(h)+ Rm(H)+\\n√\\nlog 1\\nδ\\n2m (3.17)\\nand R(h) ≤ ˆR(h)+ ˆRS(H)+3\\n√\\nlog 2\\nδ\\n2m . (3.18)\\nProof The result follows immediately by theorem 3.1 and lemma 3.1.\\nThe theorem provides two generalization bounds for binary classiﬁcation based on\\nthe Rademacher complexity. Note that the second bound, (3.18), is data-dependent:\\nthe empirical Rademacher complexity ˆRS(H) is a function of the speciﬁc sample\\nS drawn. Thus, this bound could be particularly informative if we could compute\\nˆRS(H). But, how can we compute the empirical Rademacher complexity? Using\\nagain the fact that σi and −σi are distributed in the same way, we can write\\nˆRS(H)=E\\nσ\\n[\\nsup\\nh∈H\\n1\\nm\\nm∑\\ni=1\\n−σih(xi)\\n]\\n= − E\\nσ\\n[\\ninf\\nh∈H\\n1\\nm\\nm∑\\ni=1\\nσih(xi)\\n]\\n.\\nNow, for a ﬁxed value of σ, computing inf h∈H 1\\nm\\n∑m\\ni=1 σih(xi) is equivalent to\\nan empirical risk minimization problem, which is known to be computationally\\nhard for some hypothesis sets. Thus, in some cases, computing ˆRS(H)c o u l d\\nbe computationally hard. In the next sections, we will relate the Rademacher\\ncomplexity to combinatorial measures that are easier to compute.\\n3.2 Growth function\\nHere we will show how the Rademacher complexity can be bounded in terms of the\\ngrowth function.\\nDeﬁnition 3.3 Growth function\\nThe growth function Π\\nH : N → N for a hypothesis set H is deﬁned by:\\n∀m ∈ N, ΠH(m)= m a x\\n{x1,...,xm}⊆X\\n⏐⏐\\n⏐\\n{(\\nh(x\\n1),...,h (xm)\\n⎡\\n: h ∈ H\\n}⏐⏐\\n⏐. (3.19)\\nThus, Π\\nH(m) is the maximum number of distinct ways in which m points can be\\nclassiﬁed using hypotheses in H. This provides another measure of the richness of\\nt h eh y p o t h e s i ss e tH. However, unlike the Rademacher complexity, this measure\\ndoes not depend on the distribution, it is purely combinatorial.3.2 Growth function 39\\nTo relate the Rademacher complexity to the growth function, we will use Mas-\\nsart’s lemma.\\nTheorem 3.3 Massart’s lemma\\nLet A ⊆ Rm be a ﬁnite set, with r =m a xx∈A ∥x∥2, then the following holds:\\nE\\nσ\\n[ 1\\nm sup\\nx∈A\\nm∑\\ni=1\\nσixi\\n]\\n≤ r\\n√\\n2l o g|A|\\nm , (3.20)\\nwhere σis are independent uniform random variables taking values in{−1, +1} and\\nx1,...,x m are the components of vectorx.\\nProof For any t> 0, using Jensen’s inequality, rearranging terms, and bounding\\nthe supremum by a sum, we obtain:\\nexp\\n(\\nt E\\nσ\\n[\\nsup\\nx∈A\\nm∑\\ni=1\\nσixi\\n]⎡\\n≤ E\\nσ\\n(\\nexp\\n[\\nt sup\\nx∈A\\nm∑\\ni=1\\nσixi\\n]⎡\\n=E\\nσ\\n(\\nsup\\nx∈A\\nexp\\n[\\nt\\nm∑\\ni=1\\nσixi\\n]⎡\\n≤\\n∑\\nx∈A\\nE\\nσ\\n(\\nexp\\n[\\nt\\nm∑\\ni=1\\nσixi\\n]⎡\\n.\\nWe next use the independence of theσis, then apply Hoeﬀding’s lemma (lemma D.1),\\nand use the deﬁnition of r to write:\\nexp\\n(\\nt E\\nσ\\n[\\nsup\\nx∈A\\nm∑\\ni=1\\nσixi\\n]⎡\\n≤\\n∑\\nx∈A\\nΠm\\ni=1 E\\nσi\\n(exp [tσixi])\\n≤\\n∑\\nx∈A\\nΠm\\ni=1 exp\\n[t2(2xi)2\\n8\\n]\\n=\\n∑\\nx∈A\\nexp\\n[\\nt2\\n2\\nm∑\\ni=1\\nx2\\ni\\n]\\n≤\\n∑\\nx∈A\\nexp\\n[t2r2\\n2\\n]\\n= |A|e\\nt2R2\\n2 .\\nTaking the log of both sides and dividing by t gives us:\\nE\\nσ\\n[\\nsup\\nx∈A\\nm∑\\ni=1\\nσixi\\n]\\n≤ log |A|\\nt + tr2\\n2 . (3.21)\\nIf we choose t =\\n√\\n2 log|A|\\nr , which minimizes this upper bound, we get:\\nE\\nσ\\n[\\nsup\\nx∈A\\nm∑\\ni=1\\nσixi\\n]\\n≤ r\\n√\\n2l o g|A|. (3.22)\\nDividing both sides by m leads to the statement of the lemma.40 Rademacher Complexity and VC-Dimension\\nUsing this result, we can now bound the Rademacher complexity in terms of the\\ngrowth function.\\nCorollary 3.1\\nLet G be a family of functions taking values in {−1, +1}. Then the following holds:\\nRm(G) ≤\\n√\\n2 log ΠG(m)\\nm . (3.23)\\nProof For a ﬁxed sample S =( x1,...,x m), we denote by G|S the set of vectors\\nof function values ( g(x1),...,g (xm))⊤ where g is in G.S i n c eg ∈ G takes values\\nin {−1, +1}, the norm of these vectors is bounded by √m. We can then apply\\nMassart’s lemma as follows:\\nRm(G)=E\\nS\\n[\\nE\\nσ\\n[\\nsup\\nu∈G|S\\n1\\nm\\nm∑\\ni=1\\nσiui\\n]]\\n≤ E\\nS\\n[ √m\\n√\\n2l o g|G|S |\\nm\\n]\\n.\\nBy deﬁnition, |G|S | is bounded by the growth function, thus,\\nRm(G) ≤ E\\nS\\n[ √m\\n√\\n2 log ΠG(m)\\nm\\n]\\n=\\n√\\n2 log ΠG(m)\\nm ,\\nwhich concludes the proof.\\nCombining the generalization bound (3.17) of theorem 3.2 with corollary 3.1 yields\\nimmediately the following generalization bound in terms of the growth function.\\nCorollary 3.2 Growth function generalization bound\\nLet H be a family of functions taking values in{−1, +1}.T h e n ,f o ra n yδ> 0,w i t h\\nprobability at least 1 − δ, for any h ∈ H,\\nR(h) ≤ ˆR(h)+\\n√\\n2 log ΠH(m)\\nm +\\n√\\nlog 1\\nδ\\n2m . (3.24)\\nGrowth function bounds can be also derived directly (without using Rademacher\\ncomplexity bounds ﬁrst). The resulting bound is then the following:\\nPr\\n[⏐⏐⏐R(h) − ˆR(h)\\n⏐⏐\\n⏐ >ϵ\\n]\\n≤ 4Π\\nH(2m)e x p\\n(\\n− mϵ2\\n8\\n⎡\\n, (3.25)\\nwhich only diﬀers from (3.24) by constants.\\nThe computation of the growth function may not be always convenient since, by\\ndeﬁnition, it requires computing Π H(m) for all m ≥ 1. The next section introduces\\nan alternative measure of the complexity of a hypothesis setH that is based instead\\non a single scalar, which will turn out to be in fact deeply related to the behavior\\nof the growth function.3.3 VC-dimension 41\\n- - + -\\n+ +\\n- +\\n+ - +\\n(a) (b)\\nFigure 3.1 VC-dimension of intervals on the real line. (a) Any two points can be\\nshattered. (b) No sample of three points can be shattered as the (+, −, +) labeling\\ncannot be realized.\\n3.3 VC-dimension\\nHere, we introduce the notion of VC-dimension (Vapnik-Chervonenkis dimension).\\nThe VC-dimension is also a purely combinatorial notion but it is often easier to\\ncompute than the growth function (or the Rademacher Complexity). As we shall\\nsee, the VC-dimension is a key quantity in learning and is directly related to the\\ngrowth function.\\nTo deﬁne the VC-dimension of a hypothesis setH, we ﬁrst introduce the concepts\\nof dichotomy and that of shattering. Given a hypothesis set H, a dichotomy of a\\nset S is one of the possible ways of labeling the points of S using a hypothesis in\\nH.As e t S of m ≥ 1p o i n t si ss a i dt ob es h a t t e r e db yah y p o t h e s i ss e tH when H\\nrealizes all possible dichotomies of S,t h a ti sw h e nΠH(m)=2 m.\\nDeﬁnition 3.4 VC-dimension\\nThe VC-dimension of a hypothesis set H is the size of the largest set that can be\\nfully shattered by H:\\nVCdim(H)=m a x{m:Π H(m)=2 m}. (3.26)\\nNote that, by deﬁnition, if VCdim( H)= d, there exists a set of size d that can\\nbe fully shattered. But, this does not imply that all sets of size d or less are fully\\nshattered, in fact, this is typically not the case.\\nTo further illustrate this notion, we will examine a series of examples of hypothesis\\nsets and will determine the VC-dimension in each case. To compute the VC-\\ndimension we will typically show a lower bound for its value and then a matching\\nupper bound. To give a lower boundd for VCdim(H), it suﬃces to show that a set\\nS of cardinality d can be shattered byH. To give an upper bound, we need to prove\\nthat no set S of cardinality d + 1 can be shattered by H, which is typically more\\ndiﬃcult.\\nExample 3.1 Intervals on the real line\\nOur ﬁrst example involves the hypothesis class of intervals on the real line.\\nIt is clear that the VC-dimension is at least two, since all four dichotomies42 Rademacher Complexity and VC-Dimension\\n+\\n+\\n--\\n+\\n+\\n-\\n+\\n(a) (b)\\nFigure 3.2 Unrealizable dichotomies for four points using hyperplanes in R2.( a )\\nAll four points lie on the convex hull. (b) Three points lie on the convex hull while\\nthe remaining point is interior.\\n(+,+), (−, −), (+, −), (−,+) can be realized, as illustrated in ﬁgure 3.1(a). In con-\\ntrast, by the deﬁnition of intervals, no set of three points can be shattered since the\\n(+, −,+) labeling cannot be realized. Hence, VCdim(intervals in R)=2 .\\nExample 3.2 Hyperplanes\\nConsider the set of hyperplanes inR2. We ﬁrst observe that any three non-collinear\\npoints in R2 can be shattered. To obtain the ﬁrst three dichotomies, we choose a\\nhyperplane that has two points on one side and the third point on the opposite\\nside. To obtain the fourth dichotomy we have all three points on the same side of\\nthe hyperplane. The remaining four dichotomies are realized by simply switching\\nsigns. Next, we show that four points cannot be shattered by considering two cases:\\n(i) the four points lie on the convex hull deﬁned by the four points, and (ii) three\\nof the four points lie on the convex hull and the remaining point is internal. In\\nthe ﬁrst case, a positive labeling for one diagonal pair and a negative labeling for\\nthe other diagonal pair cannot be realized, as illustrated in ﬁgure 3.2(a). In the\\nsecond case, a labeling which is positive for the points on the convex hull and\\nnegative for the interior point cannot be realized, as illustrated in ﬁgure 3.2(b).\\nHence, VCdim(hyperplanes in R\\n2)=3 .\\nMore generally in Rd, we derive a lower bound by starting with a set of d +1\\npoints in Rd,s e t t i n gx0 to be the origin and deﬁning xi,f o ri ∈{ 1,...,d },a st h e\\npoint whose ith coordinate is 1 and all others are 0. Lety0,y1,...,y d ∈{ −1, +1} be\\nan arbitrary set of labels forx0,x1,...,x d.L e tw be the vector whoseith coordinate\\nis yi. Then the classiﬁer deﬁned by the hyperplane of equationw·x+ y0\\n2 = 0 shatters\\nx0,x1,...,x d since for any i ∈ [0,d ],\\nsgn\\n(\\nw · xi + y0\\n2\\n⎡\\n=s g n\\n(\\nyi + y0\\n2\\n⎡\\n= yi. (3.27)\\nTo obtain an upper bound, it suﬃces to show that no set of d +2p o i n t sc a nb e\\nshattered by halfspaces. To prove this, we will use the following general theorem.3.3 VC-dimension 43\\n+\\n-\\n--\\n+\\n+\\n--\\n+\\n+\\n-+\\n+\\n-\\n+-\\n(a)\\n+\\n-\\n+\\n+\\n+\\n(b)\\nFigure 3.3 VC-dimension of axis-aligned rectangles. (a) Examples of realizable\\ndichotomies for four points in a diamond pattern. (b) No sample of ﬁve points can\\nbe realized if the interior point and the remaining points have opposite labels.\\nTheorem 3.4 Radon’s theorem\\nAny set X of d+2 points in R\\nd can be partitioned into two subsetsX1 and X2 such\\nthat the convex hulls of X1 and X2 intersect.\\nProof Let X = {x1,..., xd+2}⊂ Rd. The following is a system of d +1l i n e a r\\nequations in α1,...,α d+2:\\nd+2∑\\ni=1\\nαixi = 0 and\\nd+2∑\\ni=1\\nαi =0 , (3.28)\\nsince the ﬁrst equality leads to d equations, one for each component. The number\\nof unknowns, d + 2, is larger than the number of equations, d + 1, therefore\\nthe system admits a non-zero solution β1,...,β d+2.S i n c e∑d+2\\ni=1 βi = 0, both\\nI1 = {i ∈ [1,d +2 ] :βi > 0} and I2 = {i ∈ [1,d +2 ] :βi < 0} are non-empty\\nsets and X1 = {xi : i ∈ I1} and X2 = {xi : i ∈ I2} form a partition of X.B yt h e\\nlast equation of (3.28), ∑\\ni∈I1 βi = − ∑\\ni∈I2 βi.L e tβ = ∑\\ni∈I1 βi. Then, the ﬁrst\\npart of (3.28) implies\\n∑\\ni∈I1\\nβi\\nβ xi =\\n∑\\ni∈I2\\n−βi\\nβ xi,\\nwith ∑\\ni∈I1\\nβi\\nβ = ∑\\ni∈I2\\n−βi\\nβ =1 ,a n dβi\\nβ ≥ 0f o ri ∈ I1 and −βi\\nβ ≥ 0f o ri ∈ I2.B y\\ndeﬁnition of the convex hulls (B.4), this implies that ∑\\ni∈I1\\nβi\\nβ xi belongs both to44 Rademacher Complexity and VC-Dimension\\n+\\n++\\n- -\\n-\\n--\\n--\\n|positive points| < |negative points|\\n+\\n+\\n+ +\\n+\\n+\\n+\\n-\\n-\\n-\\n-\\n|positive points| > |negative points|\\n(a) (b)\\nFigure 3.4 Convex d-gons in the plane can shatter 2d +1 points. (a) d-gon\\nconstruction when there are more negative labels. (b) d-gon construction when\\nthere are more positive labels.\\nt h ec o n v e xh u l lo fX1 and to that of X2.\\nNow, let X be a set of d + 2 points. By Radon’s theorem, it can be partitioned\\ninto two sets X1 and X2 such that their convex hulls intersect. Observe that when\\ntwo sets of points X1 and X2 are separated by a hyperplane, their convex hulls\\nare also separated by that hyperplane. Thus, X1 and X2 cannot be separated by\\nah y p e r p l a n ea n dX is not shattered. Combining our lower and upper bounds, we\\nhave proven that VCdim(hyperplanes in Rd)= d +1 .\\nExample 3.3 Axis-aligned Rectangles\\nWe ﬁrst show that the VC-dimension is at least four, by considering four points\\nin a diamond pattern. Then, it is clear that all 16 dichotomies can be realized,\\nsome of which are illustrated in ﬁgure 3.2(a). In contrast, for any set of ﬁve distinct\\npoints, if we construct the minimal axis-aligned rectangle containing these points,\\none of the ﬁve points is in the interior of this rectangle. Imagine that we assign a\\nnegative label to this interior point and a positive label to each of the remaining\\nfour points, as illustrated in ﬁgure 3.2(b). There is no axis-aligned rectangle that\\ncan realize this labeling. Hence, no set of ﬁve distinct points can be shattered and\\nVCdim(axis-aligned rectangles) = 4.\\nExample 3.4 Convex Polygons\\nWe focus on the class of convex d-gons in the plane. To get a lower bound, we\\nshow that any set of 2 d + 1 points can be fully shattered. To do this, we select\\n2d + 1 points that lie on a circle, and for a particular labeling, if there are more\\nnegative than positive labels, then the points with the positive labels are used as\\nthe polygon’s vertices, as in ﬁgure 3.4(a). Otherwise, the tangents of the negative\\npoints serve as the edges of the polygon, as shown in (3.4)(b). To derive an upper3.3 VC-dimension 45\\nsin(50x)\\nx\\n1\\n-1\\n10\\nFigure 3.5 An example of a sine function (with ω =5 0) used for classiﬁcation.\\nbound, it can be shown that choosing points on the circle maximizes the number\\nof possible dichotomies, and thus VCdim(convex d-gons) = 2d + 1. Note also that\\nVCdim(convex polygons) = +∞ .\\nExample 3.5 Sine Functions\\nThe previous examples could suggest that the VC-dimension of H coincides with\\nthe number of free parameters deﬁning H. For example, the number of parameters\\ndeﬁning hyperplanes matches their VC-dimension. However, this does not hold in\\ngeneral. Several of the exercises in this chapter illustrate this fact. The following\\nprovides a striking example from this point of view. Consider the following family\\nof sine functions: {t ↦→ sin(ωt): ω ∈ R}. One instance of this function class is shown\\nin ﬁgure 3.5. These sine functions can be used to classify the points on the real line:\\na point is labeled positively if it is above the curve, negatively otherwise. Although\\nthis family of sine function is deﬁned via a single parameter, ω, it can be shown\\nthat VCdim(sine functions) = +∞ (exercise 3.12).\\nThe VC-dimension of many other hypothesis sets can be determined or upper-\\nbounded in a similar way (see this chapter’s exercises). In particular, the VC-\\ndimension of any vector space of dimension r< ∞ can be shown to be at most\\nr (exercise 3.11). The next result known as Sauer’s lemma clariﬁes the connection\\nbetween the notions of growth function and VC-dimension.\\nTheorem 3.5 Sauer’s lemma\\nLet H be a hypothesis set with VCdim(H)= d.T h e n ,f o ra l lm ∈ N, the following\\ninequality holds:\\nΠ\\nH(m) ≤\\nd∑\\ni=0\\n(m\\ni\\n⎡\\n. (3.29)46 Rademacher Complexity and VC-Dimension\\nx1 x2 ··· xm−1 xm\\n··· ··· ··· ··· ···\\n11010\\n11011\\n01111\\n10010\\n10001\\nG1 =G|S′ G2 = {g′ ⊆ S′ :( g′ ∈ G) ∧ (g′ ∪{ xm}∈ G)}.\\nFigure 3.6 Illustration of how G1 and G2 are constructed in the proof of Sauer’s\\nlemma.\\nProof The proof is by induction onm+d. The statement clearly holds form =1\\nand d =0o r d = 1. Now, assume that it holds for ( m − 1,d − 1) and (m − 1,d ).\\nFix a set S = {x1,...,x m} with ΠH(m)d i c h o t o m i e sa n dl e tG = H|S be the set of\\nconcepts H induces by restriction to S.\\nNow consider the following families over S′ = {x1,...,x m−1}.W ed e ﬁ n eG1 =\\nG|S′ as the set of conceptsH includes by restriction toS′. Next, by identifying each\\nconcept as the set of points (in S′ or S) for which it is non-zero, we can deﬁne G2\\nas\\nG2 = {g′ ⊆ S′ :( g′ ∈ G) ∧ (g′ ∪{ xm}∈ G)}.\\nSince g′ ⊆ S′, g′ ∈ G means that without adding xm it is a concept of G. Further,\\nthe constraint g′ ∪{ xm}∈ G means that adding xm to g′ also makes it a concept\\nof G. The construction of G1 and G2 is illustrated pictorially in ﬁgure 3.6. Given\\nour deﬁnitions of G1 and G2,o b s e r v et h a t|G1| + |G2| = |G|.\\nSince VCdim(G1) ≤ VCdim(G) ≤ d, then by deﬁnition of the growth function\\nand using the induction hypothesis,\\n|G1|≤ ΠG1 (m − 1) ≤\\nd∑\\ni=0\\n(m − 1\\ni\\n⎡\\n.\\nFurther, by deﬁnition ofG2,i fas e tZ ⊆ S′ is shattered byG2, then the setZ ∪{xm}\\nis shattered by G. Hence,\\nVCdim(G2) ≤ VCdim(G) − 1= d − 1,3.3 VC-dimension 47\\nand by deﬁnition of the growth function and using the induction hypothesis,\\n|G2|≤ ΠG2 (m − 1) ≤\\nd−1∑\\ni=0\\n(m − 1\\ni\\n⎡\\n.\\nThus,\\n|G| = |G1| + |G2|≤\\nd∑\\ni=0\\n(m−1\\ni\\n⎡\\n+\\nd−1∑\\ni=0\\n(m−1\\ni\\n⎡\\n=\\nd∑\\ni=0\\n(m−1\\ni\\n⎡\\n+\\n(m−1\\ni−1\\n⎡\\n=\\nd∑\\ni=0\\n(m\\ni\\n⎡\\n,\\nwhich completes the inductive proof.\\nThe signiﬁcance of Sauer’s lemma can be seen by corollary 3.3, which remarkably\\nshows that growth function only exhibits two types of behavior: either VCdim(H)=\\nd< +∞ ,i nw h i c hc a s eΠH(m)= O(md), or VCdim( H)=+ ∞ ,i nw h i c hc a s e\\nΠH(m)=2 m.\\nCorollary 3.3\\nLet H be a hypothesis set with VCdim(H)= d. Then for all m ≥ d,\\nΠH(m) ≤\\n(em\\nd\\n⎡d\\n= O(md). (3.30)\\nProof The proof begins by using Sauer’s lemma. The ﬁrst inequality multiplies\\neach summand by a factor that is greater than or equal to one since m ≥ d,w h i l e\\nthe second inequality adds non-negative summands to the summation.\\nΠH(m) ≤\\nd∑\\ni=0\\n(m\\ni\\n⎡\\n≤\\nd∑\\ni=0\\n(m\\ni\\n⎡ (m\\nd\\n⎡d−i\\n≤\\nm∑\\ni=0\\n(m\\ni\\n⎡ (m\\nd\\n⎡d−i\\n=\\n(m\\nd\\n⎡d m∑\\ni=0\\n(m\\ni\\n⎡( d\\nm\\n⎡i\\n=\\n(m\\nd\\n⎡d (\\n1+ d\\nm\\n⎡m\\n≤\\n(m\\nd\\n⎡d\\ned.\\nAfter simplifying the expression using the binomial theorem, the ﬁnal inequality\\nfollows using the general identity (1 − x) ≤ e−x.\\nThe explicit relationship just formulated between VC-dimension and the growth\\nfunction combined with corollary 3.2 leads immediately to the following generaliza-48 Rademacher Complexity and VC-Dimension\\ntion bounds based on the VC-dimension.\\nCorollary 3.4 VC-dimension generalization bounds\\nLet H be a family of functions taking values in {−1, +1} with VC-dimension d.\\nThen, for any δ> 0, with probability at least 1 − δ, the following holds for all\\nh ∈ H:\\nR(h) ≤ ˆR(h)+\\n√\\n2d log em\\nd\\nm +\\n√\\nlog 1\\nδ\\n2m . (3.31)\\nThus, the form of this generalization bound is\\nR(h) ≤ ˆR(h)+ O\\n(√\\nlog(m/d)\\n(m/d)\\n⎡\\n, (3.32)\\nwhich emphasizes the importance of the ratio m/d for generalization. The theorem\\nprovides another instance of Occam’s razor principle where simplicity is measured\\nin terms of smaller VC-dimension.\\nVC-dimension bounds can be derived directly without using an intermediate\\nRademacher complexity bound, as for (3.25): combining Sauer’s lemma with (3.25)\\nleads to the following high-probability bound\\nR(h) ≤ ˆR(h)+\\n√\\n8d log 2em\\nd +8l o g4\\nδ\\nm ,\\nwhich has the general form of (3.32). The log factor plays only a minor role in these\\nbounds. A ﬁner analysis can be used in fact to eliminate that factor.\\n3.4 Lower bounds\\nIn the previous section, we presented several upper bounds on the generalization\\nerror. In contrast, this section provides lower bounds on the generalization error of\\nany learning algorithm in terms of the VC-dimension of the hypothesis set used.\\nThese lower bounds are shown by ﬁnding for any algorithm a ‘bad’ distribution.\\nSince the learning algorithm is arbitrary, it will be diﬃcult to specify that particular\\ndistribution. Instead, it suﬃces to prove its existence non-constructively. At a high\\nlevel, the proof technique used to achieve this is the probabilistic method of Paul\\nErd¨os. In the context of the following proofs, ﬁrst a lower bound is given on the\\nexpected error over the parameters deﬁning the distributions. From that, the lower\\nbound is shown to hold for at least one set of parameters, that is one distribution.3.4 Lower bounds 49\\nTheorem 3.6 Lower bound, realizable case\\nLet H be a hypothesis set with VC-dimension d> 1. Then, for any learning\\nalgorithm A, there exist a distribution D over X and a target function f ∈ H\\nsuch that\\nPr\\nS∼Dm\\n[\\nRD(hS,f ) > d − 1\\n32m\\n]\\n≥ 1/100. (3.33)\\nProof Let X = {x0,x1,...,x d−1}⊆X be a set that is fully shattered by H.F o r\\nany ϵ> 0, we choose D such that its support is reduced to X and so that one\\npoint (x0) has very high probability (1 − ϵ), with the rest of the probability mass\\ndistributed uniformly among the other points:\\nPr\\nD\\n[x0]=1 − 8ϵ and ∀i ∈ [1,d − 1],Pr\\nD\\n[xi]= 8ϵ\\nd − 1. (3.34)\\nWith this deﬁnition, most samples would containx0 and, since X is fully shattered,\\nA can essentially do no better than tossing a coin when determining the label of a\\npoint xi not falling in the training set.\\nWe assume without loss of generality that A makes no error on x0. For a sample\\nS,w el e tS d e n o t et h es e to fi t se l e m e n t sf a l l i n gi n{x1,...,x d−1},a n dl e tS be the\\nset of samples S of size m such that |S|≤ (d − 1)/2. Now, ﬁx a sample S ∈S ,a n d\\nconsider the uniform distribution U over all labelings f : X →{ 0, 1},w h i c ha r ea l l\\nin H since the set is shattered. Then, the following lower bound holds:\\nE\\nf ∼U\\n[RD(hS,f )] =\\n∑\\nf\\n∑\\nx∈X\\n1h(x)̸=f(x) Pr[x]P r [f]\\n≥\\n∑\\nf\\n∑\\nx̸∈S\\n1h(x)̸=f(x) Pr[x]P r [f]\\n=\\n∑\\nx̸∈S\\n(∑\\nf\\n1h(x)̸=f(x) Pr[f]\\n⎡\\nPr[x]\\n= 1\\n2\\n∑\\nx̸∈S\\nPr[x] ≥ 1\\n2\\nd − 1\\n2\\n8ϵ\\nd − 1 =2 ϵ. (3.35)\\nThe ﬁrst lower bound holds because we remove non-negative terms from the\\nsummation when we only consider x ̸∈ S i n s t e a do fa l lx in X. After rearranging\\nterms, the subsequent equality holds since we are taking an expectation overf ∈ H\\nwith uniform weight on eachf and H shatters X. The ﬁnal lower bound holds due\\nto the deﬁnitions of D and S, the latter which implies that |X − S|≥ (d − 1)/2.\\nSince (3.35) holds for all S ∈S , it also holds in expectation over all S ∈S :\\nES∈S\\n[\\nEf ∼U [RD(hS,f )]\\n]\\n≥ 2ϵ. By Fubini’s theorem, the expectations can be50 Rademacher Complexity and VC-Dimension\\npermuted, thus,\\nE\\nf ∼U\\n[\\nE\\nS∈S\\n[RD(hS,f )]\\n]\\n≥ 2ϵ. (3.36)\\nThis implies that ES∈S[RD(hS,f0)] ≥ 2ϵ for at least one labeling f0 ∈ H. Decom-\\nposing this expectation into two parts and using RD(hS,f0) ≤ PrD[X −{ x0}], we\\nobtain:\\nE\\nS∈S\\n[RD(hS,f0)] =\\n∑\\nS:RD(hS,f0)≥ϵ\\nRD(hS,f0)P r [RD(hS,f0)] +\\n∑\\nS:RD(hS,f0)<ϵ\\nRD(hS,f0)P r [RD(hS,f0)]\\n≤ Pr\\nD\\n[X −{ x0}]P r\\nS∈S\\n[RD(hS,f0) ≥ ϵ]+ ϵ Pr\\nS∈S\\n[RD(hS,f0) <ϵ ]\\n≤ 8ϵ Pr\\nS∈S\\n[RD(hS,f0) ≥ ϵ]+ ϵ\\n(\\n1 − Pr\\nS∈S\\n[RD(hS,f0) ≥ ϵ]\\n⎡\\n.\\nCollecting terms in PrS∈S[RD(hS,f0) ≥ ϵ] yields\\nPr\\nS∈S\\n[RD(hS,f0) ≥ ϵ] ≥ 1\\n7ϵ(2ϵ − ϵ)= 1\\n7. (3.37)\\nThus, the probability over all samplesS (not necessarily in S) can be lower bounded\\nas\\nPr\\nS\\n[RD(hS,f0) ≥ ϵ] ≥ Pr\\nS∈S\\n[RD(hS,f0) ≥ ϵ]P r [S] ≥ 1\\n7 Pr[S]. (3.38)\\nThis leads us to ﬁnd a lower bound for Pr[ S]. The probability that more than\\n(d − 1)/2 points are drawn in a sample of sizem veriﬁes the Chernoﬀ bound for any\\nγ> 0:\\n1 − Pr[S]=P r [Sm ≥ 8ϵm(1 +γ)] ≤ e−8ϵm γ 2\\n3 . (3.39)\\nTherefore, for ϵ =( d − 1)/(32m)a n dγ =1 ,\\nPr[Sm ≥ d−1\\n2 ] ≤ e−(d−1)/12 ≤ e−1/12 ≤ 1 − 7δ, (3.40)\\nfor δ≤ .01. Thus Pr[S] ≥ 7δand PrS[RD(hS,f0) ≥ ϵ] ≥ δ.\\nThe theorem shows that for any algorithm A, there exists a ‘bad’ distribution over\\nX and a target function f for which the error of the hypothesis returned by A is\\nΩ( d\\nm ) with some constant probability. This further demonstrates the key role played\\nby the VC-dimension in learning. The result implies in particular that PAC-learning\\nin the non-realizable case is not possible when the VC-dimension is inﬁnite.\\nNote that the proof shows a stronger result than the statement of the theorem:\\nthe distribution D is selected independently of the algorithm A. We now present a\\ntheorem giving a lower bound in the non-realizable case. The following two lemmas\\nwill be needed for the proof.3.4 Lower bounds 51\\nLemma 3.2\\nLet α be a uniformly distributed random variable taking values in {α− ,α+},w h e r e\\nα− = 1\\n2 − ϵ\\n2 and α+ = 1\\n2 + ϵ\\n2,a n dl e tS be a sample of m ≥ 1 random variables\\nX1,...,X m taking values in {0, 1} and drawn i.i.d. according to the distributionDα\\ndeﬁned by PrDα [X =1 ]= α.L e th be a function from Xm to {α− ,α+}, then the\\nfollowing holds:\\nE\\nα\\n[\\nPr\\nS∼Dm\\nα\\n[h(S) ̸= α]\\n]\\n≥ Φ(2⌈m/2⌉,ϵ), (3.41)\\nwhere Φ(m, ϵ)= 1\\n4\\n(\\n1 −\\n√\\n1 − exp\\n(\\n− mϵ2\\n1−ϵ2\\n⎡⎡\\nfor all m and ϵ.\\nProof The lemma can be interpreted in terms of an experiment with two coins\\nwith biases α− and α+. It implies that for a discriminant rule h(S) based on a\\nsample S drawn from Dα− or Dα+ , to determine which coin was tossed, the sample\\nsize m must be at least Ω(1/ϵ2). The proof is left as an exercise (exercise 3.19).\\nWe will make use of the fact that for any ﬁxed ϵ the function m ↦→ Φ(m, x)i s\\nconvex, which is not hard to establish.\\nLemma 3.3\\nLet Z be a random variable taking values in [0, 1].T h e n ,f o ra n yγ ∈ [0,1),\\nPr[z>γ ] ≥ E[Z] − γ\\n1 − γ > E[Z] − γ. (3.42)\\nProof Since the values taken by Z are in [0, 1],\\nE[Z]=\\n∑\\nz≤γ\\nPr[Z = z]z +\\n∑\\nz>γ\\nPr[Z = z]z\\n≤\\n∑\\nz≤γ\\nPr[Z = z]γ+\\n∑\\nz>γ\\nPr[Z = z]\\n= γPr[Z ≤ γ]+P r [Z>γ ]\\n= γ(1 − Pr[Z>γ ]) + Pr[Z>γ ]\\n=( 1 − γ)P r [Z>γ ]+ γ,\\nwhich concludes the proof.\\nTheorem 3.7 Lower bound, non-realizable case\\nLet H be a hypothesis set with VC-dimension d> 1. Then, for any learning\\nalgorithm A, there exists a distribution D over X× { 0, 1} such that:\\nPr\\nS∼Dm\\n[\\nRD(hS) − inf\\nh∈H\\nRD(h) >\\n√\\nd\\n320m\\n]\\n≥ 1/64. (3.43)52 Rademacher Complexity and VC-Dimension\\nEquivalently, for any learning algorithm, the sample complexity veriﬁes\\nm ≥ d\\n320ϵ2 . (3.44)\\nProof Let X = {x1,x1,...,x d}⊆X be a set fully shattered by H. For any\\nα ∈ [0, 1] and any vector σ =( σ1,...,σ d)⊤ ∈{ − 1, +1}d, we deﬁne a distribution\\nDσ with support X ×{ 0, 1} as follows:\\n∀i ∈ [1,d ], Pr\\nDσ\\n[(xi, 1)] = 1\\nd\\n(1\\n2 + σiα\\n2\\n⎡\\n. (3.45)\\nT h u s ,t h el a b e lo fe a c hp o i n txi, i ∈ [1,d ], follows the distribution PrDσ [·|xi], that\\nof a biased coin where the bias is determined by the sign of σi and the magnitude\\nof α. To determine the most likely label of each point xi, the learning algorithm\\nwill therefore need to estimate PrDσ [1|xi]w i t ha na c c u r a c yb e t t e rt h a nα. To make\\nthis further diﬃcult, α and σ will be selected based on the algorithm, requiring, as\\nin lemma 3.2, Ω(1/α2) instances of each point xi in the training sample.\\nClearly, the Bayes classiﬁerh∗\\nDσ is deﬁned by h∗\\nDσ (xi) = argmaxy∈{0,1} Pr[y|xi]=\\n1σi>0 for all i ∈ [1,d ]. h∗\\nDσ is in H since X is fully shattered. For all h ∈ H,\\nRDσ (h) − RDσ (h∗\\nDσ )= 1\\nd\\n∑\\nx∈X\\n(α\\n2 + α\\n2\\n⎡\\n1h(x)̸=h∗\\nDσ (x) = α\\nd\\n∑\\nx∈X\\n1h(x)̸=h∗\\nDσ (x). (3.46)\\nLet hS denote the hypothesis returned by the learning algorithm A after receiving\\na labeled sample S drawn according to Dσ . We will denote by |S|x the number of\\noccurrences of a pointx in S.L e tU denote the uniform distribution over{−1, +1}d.3.4 Lower bounds 53\\nThen, in view of (3.46), the following holds:\\nE\\nσ ∼U\\nS∼Dm\\nσ\\n[ 1\\nα\\n[\\nRDσ (hS) − RDσ (h∗\\nDσ )\\n]]\\n= 1\\nd\\n∑\\nx∈X\\nE\\nσ ∼U\\nS∼Dm\\nσ\\n[\\n1hS(x)̸=h∗\\nDσ (x)\\n]\\n= 1\\nd\\n∑\\nx∈X\\nE\\nσ ∼U\\n[\\nPr\\nS∼Dm\\nσ\\n[\\nhS(x) ̸= h∗\\nDσ (x)\\n]]\\n= 1\\nd\\n∑\\nx∈X\\nm∑\\nn=0\\nE\\nσ ∼U\\n[\\nPr\\nS∼Dm\\nσ\\n[\\nhS(x) ̸= h∗\\nDσ (x)\\n⏐⏐ |S|x = n\\n]\\nPr[|S|x = n]\\n]\\n≥ 1\\nd\\n∑\\nx∈X\\nm∑\\nn=0\\nΦ(n +1 ,α)P r [|S|x = n] (lemma 3.2)\\n≥ 1\\nd\\n∑\\nx∈X\\nΦ(m/d +1 ,α) (convexity of Φ( ·,α) and Jensen’s ineq.)\\n=Φ (m/d +1 ,α).\\nSince the expectation over σ is lower-bounded by Φ(m/d +1 ,α), there must exist\\nsome σ ∈{ −1, +1}d for which\\nE\\nS∼Dm\\nσ\\n[ 1\\nα\\n[\\nRDσ (hS) − RDσ (h∗\\nDσ )\\n]]\\n> Φ(m/d +1 ,α). (3.47)\\nThen, by lemma 3.3, for that σ, for any γ ∈ [0, 1],\\nPr\\nS∼Dm\\nσ\\n[ 1\\nα\\n[\\nRDσ (hS) − RDσ (h∗\\nDσ )\\n]\\n>γ u\\n]\\n> (1 − γ)u, (3.48)\\nwhere u =Φ (m/d +1 ,α). Selecting δ and ϵ such that δ ≤ (1 − γ)u and ϵ ≤ γαu\\ngives\\nPr\\nS∼Dmσ\\n[\\nRDσ (hS) − RDσ (h∗\\nDσ ) >ϵ\\n]\\n>δ . (3.49)54 Rademacher Complexity and VC-Dimension\\nTo satisfy the inequalities deﬁning ϵ and δ,l e tγ =1 − 8δ. Then,\\nδ≤ (1 − γ)u ⇐⇒ u ≥ 1\\n8 (3.50)\\n⇐⇒ 1\\n4\\n(\\n1 −\\n√\\n1 − exp\\n(\\n− (m/d +1 )α2\\n1 − α2\\n⎡⎡\\n≥ 1\\n8 (3.51)\\n⇐⇒ (m/d +1 )α2\\n1 − α2 ≤ log 4\\n3 (3.52)\\n⇐⇒ m\\nd ≤ ( 1\\nα2 − 1) log4\\n3 − 1. (3.53)\\nSelecting α =8 ϵ/(1 − 8δ)g i v e sϵ = γα/8 and the condition\\nm\\nd ≤\\n((1 − 8δ)2\\n64ϵ2 − 1\\n⎡\\nlog 4\\n3 − 1. (3.54)\\nLet f(1/ϵ2) denote the right-hand side. We are seeking a suﬃcient condition of the\\nform m/d ≤ ω/ϵ2.S i n c eϵ ≤ 1/64, to ensure that ω/ϵ2 ≤ f(1/ϵ2), it suﬃces to\\nimpose ω/(1/64)2 = f(1/(1/64)2). This condition gives\\nω =( 7/64)2 log(4/3) − (1/64)2(log(4/3) + 1)≈ .003127 ≥ 1/320 = .003125.\\nThus, ϵ2 ≤ 1\\n320(m/d) is suﬃcient to ensure the inequalities.\\nThe theorem shows that for any algorithmA, in the non-realizable case, there exists\\na ‘bad’ distribution overX× { 0, 1} such that the error of the hypothesis returned\\nby A is Ω\\n(√\\nd\\nm\\n⎡\\nwith some constant probability. The VC-dimension appears as a\\ncritical quantity in learning in this general setting as well. In particular, with an\\ninﬁnite VC-dimension, agnostic PAC-learning is not possible.\\n3.5 Chapter notes\\nThe use of Rademacher complexity for deriving generalization bounds in learning\\nwas ﬁrst advocated by Koltchinskii [2001], Koltchinskii and Panchenko [2000], and\\nBartlett, Boucheron, and Lugosi [2002a], see also [Koltchinskii and Panchenko,\\n2002, Bartlett and Mendelson, 2002]. Bartlett, Bousquet, and Mendelson [2002b]\\nintroduced the notion of local Rademacher complexity , that is the Rademacher\\ncomplexity restricted to a subset of the hypothesis set limited by a bound on\\nthe variance. This can be used to derive better guarantees under some regularity\\nassumptions about the noise.\\nTheorem 3.3 is due to Massart [2000]. The notion of VC-dimension was introduced\\nby Vapnik and Chervonenkis [1971] and has been since extensively studied [Vapnik,3.6 Exercises 55\\n2006, Vapnik and Chervonenkis, 1974, Blumer et al., 1989, Assouad, 1983, Dudley,\\n1999]. In addition to the key role it plays in machine learning, the VC-dimension is\\nalso widely used in a variety of other areas of computer science and mathematics\\n(e.g., see Shelah [1972], Chazelle [2000]). Theorem 3.5 is known as Sauer’s lemma\\nin the learning community, however the result was ﬁrst given by Vapnik and\\nChervonenkis [1971] (in a somewhat diﬀerent version) and later independently by\\nSauer [1972] and Shelah [1972].\\nI nt h er e a l i z a b l ec a s e ,l o w e rb o u n d sf o rt h ee x p e c t e de r r o ri nt e r m so ft h eV C -\\ndimension were given by Vapnik and Chervonenkis [1974] and Haussler et al. [1988].\\nLater, a lower bound for the probability of error such as that of theorem 3.6 was\\ngiven by Blumer et al. [1989]. Theorem 3.6 and its proof, which improves upon\\nthis previous result, are due to Ehrenfeucht, Haussler, Kearns, and Valiant [1988].\\nDevroye and Lugosi [1995] gave slightly tighter bounds for the same problem with\\na more complex expression. Theorem 3.7 giving a lower bound in the non-realizable\\ncase and the proof presented are due to Anthony and Bartlett [1999]. For other\\nexamples of application of the probabilistic method demonstrating its full power,\\nconsult the reference book of Alon and Spencer [1992].\\nThere are several other measures of the complexity of a family of functions used\\nin machine learning, including covering numbers, packing numbers, and some other\\ncomplexity measures discussed in chapter 10. A covering number N\\np(G, ϵ)i st h e\\nminimal number ofLp balls of radiusϵ> 0 needed to cover a family of loss functions\\nG.Ap a c k i n gn u m b e rMp(G, ϵ) is the maximum number of non-overlapping Lp\\nballs of radius ϵ centered in G. The two notions are closely related, in particular\\nit can be shown straightfowardly that Mp(G, 2ϵ) ≤N p(G, ϵ) ≤M p(G, ϵ)f o r G\\nand ϵ> 0. Each complexity measure naturally induces a diﬀerent reduction of\\ninﬁnite hypothesis sets to ﬁnite ones, thereby resulting in generalization bounds\\nfor inﬁnite hypothesis sets. Exercise 3.22 illustrates the use of covering numbers\\nfor deriving generalization bounds using a very simple proof. There are also close\\nrelationships between these complexity measures: for example, by Dudley’s theorem,\\nthe empirical Rademacher complexity can be bounded in terms ofN\\n2(G, ϵ)[ D u d l e y ,\\n1967, 1987] and the covering and packing numbers can be bounded in terms of the\\nVC-dimension [Haussler, 1995]. See also [Ledoux and Talagrand, 1991, Alon et al.,\\n1997, Anthony and Bartlett, 1999, Cucker and Smale, 2001, Vidyasagar, 1997] for\\na number of upper bounds on the covering number in terms of other complexity\\nmeasures.\\n3.6 Exercises\\n3 . 1 G r o w t hf u n c t i o no fi n t e r v a l si nR.L e tH be the set of intervals in R.T h eV C -\\ndimension of H is 2. Compute its shattering coeﬃcient Π H(m), m ≥ 0. Compare56 Rademacher Complexity and VC-Dimension\\nyour result with the general bound for growth functions.\\n3.2 Lower bound on growth function. Prove that Sauer’s lemma (theorem 3.5) is\\ntight, i.e., for any set X of m>d elements, show that there exists a hypothesis\\nclass H of VC-dimension d such that ΠH(m)= ∑d\\ni=0\\n(m\\ni\\n⎡\\n.\\n3.3 Singleton hypothesis class. Consider the trivial hypothesis set H = {h0}.\\n(a) Show that Rm(H) = 0 for any m> 0.\\n(b) Use a similar construction to show that Massart’s lemma (theorem 3.3) is\\ntight.\\n3.4 Rademacher identities. Fix m ≥ 1. Prove the following identities for anyα ∈ R\\nand any two hypothesis sets H and H′ of functions mapping from X to R:\\n(a) Rm(αH)= |α|Rm(H).\\n(b) Rm(H + H′)= Rm(H)+ Rm(H′).\\n(c) Rm({max(h, h′): h ∈ H,h ′ ∈ H′}),\\nwhere max(h, h′) denotes the function x ↦→ maxx∈X (h(x),h ′(x)) (Hint:y o u\\ncould use the identity max(a, b)= 1\\n2 [a + b + |a − b|] valid for all a, b ∈ R and\\nTalagrand’s contraction lemma (see lemma 4.2)).\\n3.5 Rademacher complexity. Professor Jesetoo claims to have found a better bound\\non the Rademacher complexity of any hypothesis set H of functions taking values\\nin {−1, +1}, in terms of its VC-dimension VCdim( H). His bound is of the form\\nRm(H) ≤ O\\n(VCdim(H)\\nm\\n⎡\\n. Can you show that Professor Jesetoo’s claim cannot be\\ncorrect? (Hint: consider a hypothesis set H reduced to just two simple functions.)\\n3.6 VC-dimension of union of k intervals. What is the VC-dimension of subsets of\\nthe real line formed by the union of k intervals?\\n3.7 VC-dimension of ﬁnite hypothesis sets. Show that the VC-dimension of a ﬁnite\\nhypothesis set H is at most log2 |H|.\\n3.8 VC-dimension of subsets. What is the VC-dimension of the set of subsets Iα of\\nthe real line parameterized by a single parameter α: Iα =[ α, α+1 ]∪ [α +2 , +∞ )?\\n3.9 VC-dimension of closed balls in Rn. Show that the VC-dimension of the set\\nof all closed balls in Rn, i.e., sets of the form {x ∈ Rn : ∥x − x0∥2 ≤ r} for some\\nx0 ∈ Rn and r ≥ 0, is less than or equal to n +2 .3.6 Exercises 57\\n3.10 VC-dimension of ellipsoids. What is the VC-dimension of the set of all ellipsoids\\nin Rn?\\n3.11 VC-dimension of a vector space of real functions. LetF be a ﬁnite-dimensional\\nvector space of real functions on Rn,d i m (F)= r< ∞ .L e t H be the set of\\nhypotheses:\\nH = {{x: f(x) ≥ 0}: f ∈ F }.\\nShow that d, the VC-dimension of H, is ﬁnite and that d ≤ r.( Hint: select an\\narbitrary set of m = r + 1 points and consider linear mapping u: F → Rm deﬁned\\nby: u(f)=( f(x1),...,f (xm)).)\\n3.12 VC-dimension of sine functions. Consider the hypothesis family of sine func-\\ntions (Example 3.5): {x → sin(ωx): ω ∈ R} .\\n(a) Show that for any x ∈ R the points x,2x,3x and 4x cannot be shattered\\nby this family of sine functions.\\n(b) Show that the VC-dimension of the family of sine functions is inﬁnite.\\n(Hint: show that {2−m : m ∈ N} can be fully shattered for any m> 0.)\\n3.13 VC-dimension of union of halfspaces. Determine the VC-dimension of the\\nsubsets of the real line formed by the union of k intervals.\\n3.14 VC-dimension of intersection of halfspaces. Consider the class Ck of convex\\nintersections ofk halfspaces. Give lower and upper bound estimates for VCdim(Ck).\\n3.15 VC-dimension of intersection concepts.\\n(a) Let C1 and C2 be two concept classes. Show that for any concept class\\nC = {c1 ∩ c2 : c1 ∈ C1,c2 ∈ C2},\\nΠC(m) ≤ ΠC1 (m)Π C2 (m). (3.55)\\n(b) Let C be a concept class with VC-dimension d and let Cs be the concept\\nclass formed by all intersections of s concepts from C, s ≥ 1. Show that the\\nVC-dimension of Cs is bounded by 2ds log2(3s). (Hint: show that log2(3x) <\\n9x/(2e) for any x ≥ 2.)\\n3.16 VC-dimension of union of concepts. Let A and B be two sets of functions\\nmapping from X into {0, 1}, and assume that both A and B have ﬁnite VC-\\ndimension, with VCdim( A)= dA and VCdim(B)= dB.L e t C = A ∪ B be the58 Rademacher Complexity and VC-Dimension\\nunion of A and B.\\n(a) Prove that for all m,Π C(m) ≤ ΠA(m)+Π B(m).\\n(b) Use Sauer’s lemma to show that for m ≥ dA + dB +2 ,Π C(m) < 2m, and\\ngive a bound on the VC-dimension of C.\\n3.17 VC-dimension of symmetric diﬀerence of concepts. For two sets A and B,l e t\\nAΔ B denote the symmetric diﬀerence ofA and B,i . e . ,AΔ B =( A ∪B) − (A ∩B).\\nLet H be a non-empty family of subsets of X with ﬁnite VC-dimension. Let A be\\nan element of H and deﬁne HΔ A = {XΔ A: X ∈ H}. Show that\\nVCdim(HΔ A)=V C d i m (H).\\n3.18 Symmetric functions. A function h: {0,1}n →{ 0, 1} is symmetric if its value\\nis uniquely determined by the number of 1’s in the input. Let C denote the set of\\nall symmetric functions.\\n(a) Determine the VC-dimension of C.\\n(b) Give lower and upper bounds on the sample complexity of any consistent\\nPAC learning algorithm for C.\\n(c) Note that any hypothesish ∈ C can be represented by a vector (y0,y1, ..., yn) ∈\\n{0, 1}n+1,w h e r eyi is the value of h on examples having precisely i 1’s. Devise\\na consistent learning algorithm for C based on this representation.\\n3.19 Biased coins. Professor Moent has two coins in his pocket, coin xA and coin\\nxB. Both coins are slightly biased, i.e., Pr[ xA =0 ]=1 /2 − ϵ/2 and Pr[xB =0 ]=\\n1/2+ ϵ/2, where 0 <ϵ< 1 is a small positive number, 0 denotes heads and 1\\ndenotes tails. He likes to play the following game with his students. He picks a coin\\nx ∈{ xA,xB } from his pocket uniformly at random, tosses it m times, reveals the\\nsequence of 0s and 1s he obtained and asks which coin was tossed. Determine how\\nlarge m needs to be for a student’s coin prediction error to be at most δ> 0.\\n(a) Let S be a sample of size m. Professor Moent’s best student, Oskar, plays\\naccording to the decision rule f\\no : {0, 1}m →{ xA,xB } deﬁned by fo(S)= xA\\niﬀ N(S) <m /2, where N(S) is the number of 0’s in sample S.\\nSuppose m is even, then show that\\nerror(fo) ≥ 1\\n2 Pr\\n[\\nN(S) ≥ m\\n2\\n⏐⏐\\n⏐x = x\\nA\\n]\\n. (3.56)\\n(b) Assuming m even, use the inequalities given in the appendix (section D.3)3.6 Exercises 59\\nto show that\\nerror(fo) > 1\\n4\\n[\\n1 −\\n[\\n1 − e− mϵ2\\n1− ϵ2\\n]1\\n2\\n]\\n. (3.57)\\n(c) Argue that if m is odd, the probability can be lower bounded by using\\nm + 1 in the bound in (a) and conclude that for both odd and even m,\\nerror(fo) > 1\\n4\\n[\\n1 −\\n[\\n1 − e− 2⌈m/2⌉ϵ2\\n1− ϵ2\\n]1\\n2\\n]\\n. (3.58)\\n(d) Using this bound, how large must m be if Oskar’s error is at mostδ,w h e r e\\n0 <δ< 1/4. What is the asymptotic behavior of this lower bound as a function\\nof ϵ?\\n(e) Show that no decision rule f : {0, 1}m →{ xa,xB } can do better than\\nOskar’s rulefo. Conclude that the lower bound of the previous question applies\\nto all rules.\\n3.20 Inﬁnite VC-dimension.\\n(a) Show that if a concept class C has inﬁnite VC-dimension, then it is not\\nPAC-learnable.\\n(b) In the standard PAC-learning scenario, the learning algorithm receives all\\nexamples ﬁrst and then computes its hypothesis. Within that setting, PAC-\\nlearning of concept classes with inﬁnite VC-dimension is not possible as seen\\nin the previous question.\\nImagine now a diﬀerent scenario where the learning algorithm can alternate\\nbetween drawing more examples and computation. The objective of this prob-\\nlem is to prove that PAC-learning can then be possible for some concept classes\\nwith inﬁnite VC-dimension.\\nConsider for example the special case of the concept class C of all subsets of\\nnatural numbers. Professor Vitres has an idea for the ﬁrst stage of a learning\\nalgorithm L PAC-learning C. In the ﬁrst stage, L draws a suﬃcient number of\\npoints m such that the probability of drawing a point beyond the maximum\\nvalue M observed be small with high conﬁdence. Can you complete Professor\\nVitres’ idea by describing the second stage of the algorithm so that it PAC-\\nlearns C? The description should be augmented with the proof that L can\\nPAC-learn C.\\n3.21 VC-dimension generalization bound – realizable case. In this exercise we show\\nthat the bound given in corollary 3.4 can be improved to O(\\nd log(m/d)\\nm )i nt h e\\nrealizable setting. Assume we are in the realizable scenario, i.e. the target concept is\\nincluded in our hypothesis classH. We will show that if a hypothesish is consistent60 Rademacher Complexity and VC-Dimension\\nwith a sample S ∼ Dm then for any ϵ> 0 such that mϵ ≥ 8\\nPr[R(h) >ϵ ] ≤ 2\\n[2em\\nd\\n]d\\n2−mϵ/2 . (3.59)\\n(a) Let HS ⊆ H be the subset of hypotheses consistent with the sample S,\\nlet ˆRS(h) denote the empirical error with respect to the sample S and deﬁne\\nS′ as a another independent sample drawn from Dm. Show that the following\\ninequality holds for any h0 ∈ HS:\\nPr\\n[\\nsup\\nh∈HS\\n| ˆRS(h) − ˆRS′ (h)| > ϵ\\n2\\n]\\n≥ Pr\\n[\\nB[m, ϵ] > mϵ\\n2\\n]\\nPr[R(h0) >ϵ ] ,\\nwhere B[m, ϵ] is a binomial random variable with parameters [ m, ϵ]. (Hint:\\nprove and use the fact that Pr[ ˆR(h) ≥ ϵ\\n2 ] ≥ Pr[ ˆR(h) > ϵ\\n2 ∧ R(h) >ϵ ].)\\n(b) Prove that Pr\\n[\\nB(m, ϵ) > mϵ\\n2\\n]\\n≥ 1\\n2 . Use this inequality along with the\\nresult from (a) to show that for any h0 ∈ HS\\nPr\\n[\\nR(h0) >ϵ\\n]\\n≤ 2P r\\n[\\nsup\\nh∈HS\\n| ˆRS(h) − ˆRS′ (h)| > ϵ\\n2\\n]\\n.\\n(c) Instead of drawing two samples, we can draw one sampleT of size 2m then\\nuniformly at random split it intoS and S′. The right hand side of part (b) can\\nthen be rewritten as:\\nPr\\n[\\nsup\\nh∈HS\\n| ˆRS(h)− ˆRS′ (h)| > ϵ\\n2\\n]\\n=P r\\nT ∼D2m:\\nT → [S,S′]\\n[\\n∃h ∈H : ˆRS(h)=0 ∧ ˆRS′ (h) > ϵ\\n2\\n]\\n.\\nLet h0 be a hypothesis such that ˆRT (h0) > ϵ\\n2 and let l> mϵ\\n2 be the total\\nnumber of errors h0 makes on T. Show that the probability of all l errors\\nfalling into S′ is upper bounded by 2−l.\\n(d) Part (b) implies that for any h ∈ H\\nPr\\nT ∼D2m:\\nT → (S,S′)\\n[\\nˆRS(h)=0 ∧ ˆRS′ (h) > ϵ\\n2\\n⏐⏐⏐ ˆRT (h0) > ϵ\\n2\\n]\\n≤ 2−l .\\nUse this bound to show that for any h ∈ H\\nPr\\nT ∼D2m:\\nT → (S,S′)\\n[\\nˆRS(h)=0 ∧ ˆRS′ (h) > ϵ\\n2\\n]\\n≤ 2− ϵm\\n2 .\\n(e) Complete the proof of inequality (3.59) by using the union bound to upper\\nbound PrT ∼D2m:\\nT → (S,S′)\\n[\\n∃h ∈H : ˆRS(h)=0 ∧ ˆRS′ (h) > ϵ\\n2\\n]\\n. Show that we can achieve\\na high probability generalization bound that is of the order O(d log(m/d)\\nm ).3.6 Exercises 61\\n3.22 Generalization bound based on covering numbers. Let H be a family of\\nfunctions mapping X to a subset of real numbers Y⊆ R. For any ϵ> 0, the\\ncovering number N(H,ϵ )o f H for the L∞ norm is the minimal k ∈ N such that H\\ncan be covered with k balls of radius ϵ, that is, there exists {h1,...,h k}⊆ H such\\nthat, for all h ∈ H, there exists i ≤ k with ∥h − hi∥∞ =m a xx∈X |h(x) − hi(x)|≤ ϵ.\\nIn particular, when H is a compact set, a ﬁnite covering can be extracted from a\\ncovering of H with balls of radius ϵ and thus N(H,ϵ )i sﬁ n i t e .\\nCovering numbers provide a measure of the complexity of a class of functions: the\\nlarger the covering number, the richer is the family of functions. The objective of\\nthis problem is to illustrate this by proving a learning bound in the case of the\\nsquared loss. Let D denote a distribution over X× Y according to which labeled\\nexamples are drawn. Then, the generalization error ofh ∈ H for the squared loss is\\ndeﬁned by R(h)=E (x,y)∼D[(h(x) − y)2] and its empirical error for a labeled sample\\nS =( (x1,y1),..., (xm,y m)) by ˆR(h)= 1\\nm\\n∑m\\ni=1(h(xi)−yi)2.W ew i l la s s u m et h a tH\\nis bounded, that is there existsM> 0 such that|h(x)−y|≤ M for all (x, y) ∈X × Y.\\nThe following is the generalization bound proven in this problem:\\nPr\\nS∼Dm\\n[\\nsup\\nh∈H\\n|R(h) − ˆR(h)|≥ ϵ\\n]\\n≤N\\n(\\nH, ϵ\\n8M\\n⎡\\n2e x p\\n(−mϵ2\\n2M4\\n⎡\\n. (3.60)\\nThe proof is based on the following steps.\\n(a) Let LS = R(h) − ˆR(h), then show that for all h1,h2 ∈ H and any labeled\\nsample S, the following inequality holds:\\n|LS(h1) − LS(h2)|≤ 4M ∥h1 − h2∥∞ .\\n(b) Assume that H can be covered by k subsets B1,...,B k,t h a ti sH =\\nB1 ∪... ∪Bk. Then, show that, for anyϵ> 0, the following upper bound holds:\\nPr\\nS∼Dm\\n[\\nsup\\nh∈H\\n|LS(h)|≥ ϵ\\n]\\n≤\\nk∑\\ni=1\\nPr\\nS∼Dm\\n[\\nsup\\nh∈Bi\\n|LS(h)|≥ ϵ\\n]\\n.\\n(c) Finally, let k = N(H, ϵ\\n8M )a n dl e tB1,...,B k be balls of radius ϵ/(8M)\\ncentered at h1,...,h k covering H. Use part (a) to show that for all i ∈ [1,k ],\\nPr\\nS∼Dm\\n[\\nsup\\nh∈Bi\\n|LS(h)|≥ ϵ\\n]\\n≤ Pr\\nS∼Dm\\n[\\n|LS(hi)|≥ ϵ\\n2\\n]\\n,\\nand apply Hoeﬀding’s inequality (theorem D.1) to prove (3.60).4 Support Vector Machines\\nThis chapter presents one of the most theoretically well motivated and practically\\nmost eﬀective classiﬁcation algorithms in modern machine learning: Support Vector\\nMachines (SVMs). We ﬁrst introduce the algorithm for separable datasets, then\\npresent its general version designed for non-separable datasets, and ﬁnally provide\\na theoretical foundation for SVMs based on the notion of margin. We start with\\nthe description of the problem of linear classiﬁcation.\\n4.1 Linear classiﬁcation\\nConsider an input space X that is a subset of RN with N ≥ 1, and the output\\nor target space Y = {−1,+1},a n dl e tf : X→ Y be the target function. Given\\na hypothesis set H of functions mapping X to Y, the binary classiﬁcation task is\\nformulated as follows. The learner receives a training sampleS of size m drawn i.i.d.\\nfrom X according to some unknown distribution D, S =( (x1,y1),..., (xm,y m)) ∈\\n(X× Y )m,w i t hyi = f(xi) for all i ∈ [1,m]. The problem consists of determining a\\nhypothesis h ∈ H,a binary classiﬁer , with small generalization error:\\nRD(h)= P r\\nx∼D\\n[h(x) ̸= f(x)]. (4.1)\\nDiﬀerent hypothesis sets H can be selected for this task. In view of the results\\npresented in the previous section, which formalized Occam’s razor principle, hy-\\npothesis sets with smaller complexity — e.g., smaller VC-dimension or Rademacher\\ncomplexity — provide better learning guarantees, everything else being equal. A\\nnatural hypothesis set with relatively small complexity is that of linear classiﬁers ,\\nor hyperplanes, which can be deﬁned as follows:\\nH = {x ↦→ sign(w · x + b): w ∈ RN ,b ∈ R}. (4.2)\\nA hypothesis of the form x ↦→ sign(w ·x+b) thus labels positively all points falling\\non one side of the hyperplane w · x + b = 0 and negatively all others. The problem\\nis referred to as a linear classiﬁcation problem.64 Support Vector Machines\\nw·x+b=0\\nw·x+b=0\\nFigure 4.1 Two possible separating hyperplanes. The right-hand side ﬁgure shows\\na hyperplane that maximizes the margin.\\n4.2 SVMs — separable case\\nIn this section, we assume that the training sample S can be linearly separated,\\nthat is, we assume the existence of a hyperplane that perfectly separates the\\ntraining sample into two populations of positively and negatively labeled points,\\nas illustrated by the left panel of ﬁgure 4.1. But there are then inﬁnitely many\\nsuch separating hyperplanes. Which hyperplane should a learning algorithm select?\\nThe solution returned by the SVM algorithm is the hyperplane with the maximum\\nmargin, or distance to the closest points, and is thus known as themaximum-margin\\nhyperplane. The right panel of ﬁgure 4.1 illustrates that choice.\\nWe will present later in this chapter a margin theory that provides a strong\\njustiﬁcation for this solution. We can observe already, however, that the SVM\\nsolution can also be viewed as the “safest” choice in the following sense: a test\\npoint is classiﬁed correctly by a separating hyperplane with margin ρ even when\\nit falls within a distance ρ of the training samples sharing the same label; for the\\nSVM solution, ρ is the maximum margin and thus the “safest” value.\\n4.2.1 Primal optimization problem\\nWe now derive the equations and optimization problem that deﬁne the SVM\\nsolution. The general equation of a hyperplane in R\\nN is\\nw · x + b =0 , (4.3)\\nwhere w ∈ RN is a non-zero vector normal to the hyperplane and b ∈ R a\\nscalar. Note that this deﬁnition of a hyperplane is invariant to non-zero scalar\\nmultiplication. Hence, for a hyperplane that does not pass through any sample\\npoint, we can scale w and b appropriately such that min (x,y)∈S |w · x + b| =1 .4.2 SVMs — separable case 65\\nmargin\\nw·x+b=+1\\nw·x+b= −1\\nw·x+b=0\\nFigure 4.2 Margin and equations of the hyperplanes for a canonical maximum-\\nmargin hyperplane. The marginal hyperplanes are represented by dashed lines on\\nthe ﬁgure.\\nWe deﬁne this representation of the hyperplane, i.e., the corresponding pair (w, b),\\nas the canonical hyperplane. The distance of any point x0 ∈ RN to a hyperplane\\ndeﬁned by (4.3) is given by\\n|w · x0 + b|\\n∥w∥ . (4.4)\\nThus, for a canonical hyperplane, the margin ρ is given by\\nρ=m i n\\n(x,y)∈S\\n|w · x + b|\\n∥w∥ = 1\\n∥w∥ . (4.5)\\nFigure 4.2 illustrates the margin for a maximum-margin hyperplane with a canon-\\nical representation (w, b). It also shows the marginal hyperplanes, which are the\\nhyperplanes parallel to the separating hyperplane and passing through the closest\\npoints on the negative or positive sides. Since they are parallel to the separating\\nhyperplane, they admit the same normal vector w.F u r t h e r m o r e ,b yd e ﬁ n i t i o no fa\\ncanonical representation, for a point x on a marginal hyperplane, |w · x + b| =1 ,\\nand thus the equations of the marginal hyperplanes are w · x + b = ±1.\\nA hyperplane deﬁned by ( w, b) correctly classiﬁes a training point x\\ni, i ∈ [1,m]\\nwhen w · xi + b has the same sign as yi. For a canonical hyperplane, by deﬁnition,\\nwe have |w · xi + b|≥ 1 for all i ∈ [1,m]; thus, xi is correctly classiﬁed when\\nyi(w ·xi +b) ≥ 1. In view of (4.5), maximizing the margin of a canonical hyperplane\\nis equivalent to minimizing ∥w∥ or 1\\n2 ∥w∥2. Thus, in the separable case, the SVM\\nsolution, which is a hyperplane maximizing the margin while correctly classifying all\\ntraining points, can be expressed as the solution to the following convex optimization\\nproblem:66 Support Vector Machines\\nmin\\nw,b\\n1\\n2 ∥w∥2 (4.6)\\nsubject to: yi(w · xi + b) ≥ 1, ∀i ∈ [1,m] .\\nThe objective function F : w ↦→ 1\\n2 ∥w∥2 is inﬁnitely diﬀerentiable. Its gradient is\\n∇w(F)= w and its Hessian the identity matrix∇2F(w)= I, whose eigenvalues are\\nstrictly positive. Therefore, ∇2F(w) ≻ 0 and F is strictly convex. The constraints\\nare all deﬁned by aﬃne functions gi :( w,b ) ↦→ 1−yi(w·xi+b) and are thus qualiﬁed.\\nThus, in view of the results known for convex optimization (see appendix B for\\ndetails), the optimization problem of (4.6) admits a unique solution, an important\\nand favorable property that does not hold for all learning algorithms.\\nMoreover, since the objective function is quadratic and the constraints aﬃne, the\\noptimization problem of (4.6) is in fact a speciﬁc instance of quadratic program-\\nming (QP), a family of problems extensively studied in optimization. A variety of\\ncommercial and open-source solvers are available for solving convex QP problems.\\nAdditionally, motivated by the empirical success of SVMs along with its rich theo-\\nretical underpinnings, specialized methods have been developed to more eﬃciently\\nsolve this particular convex QP problem, notably the block coordinate descent al-\\ngorithms with blocks of just two coordinates.\\n4.2.2 Support vectors\\nThe constraints are aﬃne and thus qualiﬁed. The objective function as well as the\\naﬃne constraints are convex and diﬀerentiable. Thus, the hypotheses of theorem B.8\\nhold and the KKT conditions apply at the optimum. We shall use these conditions\\nto both analyze the algorithm and demonstrate several of its crucial properties,\\nand subsequently derive the dual optimization problem associated to SVMs in\\nsection 4.2.3.\\nWe introduce Lagrange variables α\\ni ≥ 0, i ∈ [1,m ], associated to the m\\nconstraints and denote byα the vector (α1,...,α m)⊤. The Lagrangian can then be\\ndeﬁned for all w ∈ RN , b ∈ R,a n dα ∈ Rm\\n+ ,b y\\nL(w,b ,α)= 1\\n2 ∥w∥2 −\\nm∑\\ni=1\\nαi[yi(w · xi + b) − 1]. (4.7)\\nThe KKT conditions are obtained by setting the gradient of the Lagrangian with\\nrespect to the primal variablesw and b to zero and by writing the complementarity4.2 SVMs — separable case 67\\nconditions:\\n∇wL = w −\\nm∑\\ni=1\\nαiyixi =0 = ⇒ w =\\nm∑\\ni=1\\nαiyixi (4.8)\\n∇bL = −\\nm∑\\ni=1\\nαiyi =0 = ⇒\\nm∑\\ni=1\\nαiyi =0 ( 4 . 9 )\\n∀i, αi[yi(w · xi + b) − 1] = 0 = ⇒ αi =0 ∨ yi(w · xi + b)=1 . (4.10)\\nBy equation 4.8, the weight vectorw solution of the SVM problem is a linear combi-\\nnation of the training set vectorsx1,..., xm.Av e c t o rxi appears in that expansion\\niﬀ αi ̸= 0. Such vectors are called support vectors.B yt h ec o m p l e m e n t a r i t yc o n d i -\\ntions (4.10), if αi ̸=0 ,t h e nyi(w · xi + b) = 1. Thus, support vectors lie on the\\nmarginal hyperplanes w · xi + b = ±1.\\nSupport vectors fully deﬁne the maximum-margin hyperplane or SVM solution,\\nwhich justiﬁes the name of the algorithm. By deﬁnition, vectors not lying on the\\nmarginal hyperplanes do not aﬀect the deﬁnition of these hyperplanes — in their\\nabsence, the solution to the SVM problem remains unchanged. Note that while the\\nsolution w of the SVM problem is unique, the support vectors are not. In dimension\\nN, N + 1 points are suﬃcient to deﬁne a hyperplane. Thus, when more than N +1\\npoints lie on a marginal hyperplane, diﬀerent choices are possible for the N +1\\nsupport vectors.\\n4.2.3 Dual optimization problem\\nTo derive the dual form of the constrained optimization problem (4.6), we plug\\ninto the Lagrangian the deﬁnition of w i nt e r m so ft h ed u a lv a r i a b l e sa se x p r e s s e d\\nin (4.8) and apply the constraint (4.9). This yields\\nL = 1\\n2 ∥\\nm∑\\ni=1\\nαiyixi∥2 −\\nm∑\\ni,j=1\\nαiαjyiyj(xi · xj)\\n\\ued19 \\ued18\\ued17 \\ued1a\\n− 1\\n2\\nPm\\ni,j=1 αiαjyiyj(xi·xj)\\n−\\nm∑\\ni=1\\nαiyib\\n\\ued19 \\ued18\\ued17 \\ued1a\\n0\\n+\\nm∑\\ni=1\\nαi , (4.11)\\nwhich simpliﬁes to\\nL =\\nm∑\\ni=1\\nαi − 1\\n2\\nm∑\\ni,j=1\\nαiαjyiyj(xi · xj) . (4.12)68 Support Vector Machines\\nThis leads to the following dual optimization problem for SVMs in the separable\\ncase:\\nmax\\nα\\nm∑\\ni=1\\nαi − 1\\n2\\nm∑\\ni,j=1\\nαiαjyiyj(xi · xj) (4.13)\\nsubject to: αi ≥ 0 ∧\\nm∑\\ni=1\\nαiyi =0 , ∀i ∈ [1,m] .\\nThe objective function G: α ↦→ ∑m\\ni=1 αi − 1\\n2\\n∑m\\ni,j=1 αiαjyiyj(xi · xj) is inﬁnitely\\ndiﬀerentiable. Its Hessian is given by ∇2G = −A,w i t hA =\\n(\\nyixi · yjxj\\n⎡\\nij. A is\\nthe Gram matrix associated to the vectorsy1x1,...,y mxm and is therefore positive\\nsemideﬁnite, which shows that ∇2G ⪯ 0 and that G is a concave function. Since\\nthe constraints are aﬃne and convex, the maximization problem (4.13) is equivalent\\nto a convex optimization problem. Since G is a quadratic function of α,t h i sd u a l\\noptimization problem is also a QP problem, as in the case of the primal optimization\\nand once again both general-purpose and specialized QP solvers can be used to\\nobtain the solution (see exercise 4.4 for details on the SMO algorithm, which is\\noften used to solve the dual form of the SVM problem in the more general non-\\nseparable setting).\\nMoreover, since the constraints are aﬃne, they are qualiﬁed and strong duality\\nholds (see appendix B). Thus, the primal and dual problems are equivalent, i.e.,\\nthe solution α of the dual problem (4.13) can be used directly to determine the\\nhypothesis returned by SVMs, using equation (4.8):\\nh(x)=s g n (w · x + b)=s g n\\n(\\nm∑\\ni=1\\nαiyi(xi · x)+ b\\n⎡\\n. (4.14)\\nSince support vectors lie on the marginal hyperplanes, for any support vector xi,\\nw · xi + b = yi, and thus b c a nb eo b t a i n e dv i a\\nb = yi −\\nm∑\\nj=1\\nαjyj(xj · xi) . (4.15)\\nThe dual optimization problem (4.13) and the expressions (4.14) and (4.15) reveal\\nan important property of SVMs: the hypothesis solution depends only on inner\\nproducts between vectors and not directly on the vectors themselves.\\nEquation (4.15) can now be used to derive a simple expression of the marginρin\\nterms of α. Since (4.15) holds for all i with αi ̸= 0, multiplying both sides by αiyi\\nand taking the sum leads to\\nm∑\\ni=1\\nαiyib =\\nm∑\\ni=1\\nαiy2\\ni −\\nm∑\\ni,j=1\\nαiαjyiyj(xi · xj) . (4.16)4.2 SVMs — separable case 69\\nUsing the fact that y2\\ni = 1 along with equation 4.8 then yields\\n0=\\nm∑\\ni=1\\nαi −∥ w∥2. (4.17)\\nNoting that αi ≥ 0, we obtain the following expression of the margin ρ in terms of\\nthe L1 norm of α:\\nρ2 = 1\\n∥w∥2\\n2\\n= 1∑m\\ni=1 αi\\n= 1\\n∥α∥1\\n. (4.18)\\n4.2.4 Leave-one-out analysis\\nWe now use the notion of leave-one-out error to derive a ﬁrst learning guarantee\\nfor SVMs based on the fraction of support vectors in the training set.\\nDeﬁnition 4.1 Leave-one-out error\\nLet hS denote the hypothesis returned by a learning algorithm A, when trained on\\naﬁ x e ds a m p l eS. Then, the leave-one-out error of A on a sample S of size m is\\ndeﬁned by\\nˆRLOO(A)= 1\\nm\\nm∑\\ni=1\\n1hS−{ xi}(xi)̸=yi .\\nThus, for each i ∈ [1,m], A is trained on all the points in S except for xi,i . e . ,\\nS −{ xi},a n di t se r r o ri st h e nc o m p u t e du s i n gxi. The leave-one-out error is the\\naverage of these errors. We will use an important property of the leave-one-out error\\nstated in the following lemma.\\nLemma 4.1\\nThe average leave-one-out error for samples of size m ≥ 2 is an unbiased estimate\\nof the average generalization error for samples of size m − 1:\\nE\\nS∼Dm\\n[ ˆRLOO(A)] = E\\nS′ ∼Dm− 1\\n[R(hS′ )], (4.19)\\nwhere D denotes the distribution according to which points are drawn.70 Support Vector Machines\\nProof By the linearity of expectation, we can write\\nE\\nS∼Dm\\n[ ˆRLOO(A)] = 1\\nm\\nm∑\\ni=1\\nE\\nS∼Dm\\n[1hS−{ xi}(xi)̸=yi ]\\n=E\\nS∼Dm\\n[1hS−{ x1}(x1)̸=y1 ]\\n=E\\nS′ ∼Dm− 1,x1∼D\\n[1hS′ (x1)̸=y1 ]\\n=E\\nS′ ∼Dm− 1\\n[E\\nx1∼D\\n[1hS′ (x1)̸=y1 ]]\\n=E\\nS′ ∼Dm− 1\\n[R(hS′ )].\\nFor the second equality, we used the fact that, since the points ofS are drawn in an\\ni.i.d. fashion, the expectation ES∼Dm [1hS−{ xi}(xi)̸=yi ] does not depend on the choice\\nof i ∈ [1,m] and is thus equal to ES∼Dm [1hS−{ x1}(x1)̸=y1 ].\\nIn general, computing the leave-one-out error may be costly since it requires training\\nm times on samples of sizem − 1. In some situations however, it is possible to derive\\nthe expression of ˆRloo(A) much more eﬃciently (see exercise 10.9).\\nTheorem 4.1\\nLet hS be the hypothesis returned by SVMs for a sample S,a n dl e tNSV (S) be the\\nnumber of support vectors that deﬁne hS.T h e n ,\\nE\\nS∼Dm\\n[R(hS)] ≤ E\\nS∼Dm+1\\n[NSV(S)\\nm +1\\n]\\n.\\nProof Let S be a linearly separable sample of m +1 .I f x is not a support vector\\nfor hS, removing it does not change the SVM solution. Thus, hS−{x} = hS and\\nhS−{x} correctly classiﬁes x. By contraposition, if hS−{x} misclassiﬁes x, x must be\\na support vector, which implies\\nˆRloo(SVM) ≤ NSV(S)\\nm +1 . (4.20)\\nTaking the expectation of both sides and using lemma 4.1 yields the result.\\nTheorem 4.1 gives a sparsity argument in favor of SVMs: the average error of\\nthe algorithm is upper bounded by the average fraction of support vectors. One\\nmay hope that for many distributions seen in practice, a relatively small number\\nof the training points will lie on the marginal hyperplanes. The solution will then\\nbe sparse in the sense that a small fraction of the dual variables α\\ni will be non-\\nzero. Note, however, that this bound is relatively weak since it applies only to the\\naverage generalization error of the algorithm over all samples of sizem. It provides\\nno information about the variance of the generalization error. In section 4.4, we\\npresent stronger high-probability bounds using a diﬀerent argument based on the4.3 SVMs — non-separable case 71\\nξi\\nξj\\nw·x+b=+1\\nw·x+b= −1\\nw·x+b=0\\nFigure 4.3 A separating hyperplane with point xi classiﬁed incorrectly and point\\nxj correctly classiﬁed, but with margin less than 1.\\nnotion of margin.\\n4.3 SVMs — non-separable case\\nIn most practical settings, the training data is not linearly separable, i.e., for any\\nhyperplane w · x + b = 0, there exists xi ∈ S such that\\nyi [w · xi + b] ̸≥ 1 . (4.21)\\nThus, the constraints imposed in the linearly separable case discussed in section 4.2\\ncannot all hold simultaneously. However, a relaxed version of these constraints can\\nindeed hold, that is, for each i ∈ [1,m], there exist ξ\\ni ≥ 0 such that\\nyi [w · xi + b] ≥ 1 − ξi . (4.22)\\nThe variablesξi are known asslack variables and are commonly used in optimization\\nto deﬁne relaxed versions of some constraints. Here, a slack variable ξi measures\\nthe distance by which vector xi violates the desired inequality, yi(w · xi + b) ≥ 1.\\nFigure 4.3 illustrates the situation. For a hyperplane w · x + b =0 ,av e c t o rxi\\nwith ξi > 0 can be viewed as an outlier. Each xi must be positioned on the correct\\nside of the appropriate marginal hyperplane to not be considered an outlier. As a\\nconsequence, a vector x\\ni with 0 <y i(w · xi + b) < 1 is correctly classiﬁed by the\\nhyperplane w·x+b = 0 but is nonetheless considered to be an outlier, that is,ξi > 0.\\nIf we omit the outliers, the training data is correctly separated by w · x + b =0\\nwith a margin ρ =1 /∥w∥ that we refer to as the soft margin, as opposed to the\\nhard margin in the separable case.\\nHow should we select the hyperplane in the non-separable case? One idea consists\\nof selecting the hyperplane that minimizes the empirical error. But, that solution72 Support Vector Machines\\n0/1 loss function\\nHinge loss\\nQuadratic hinge loss\\nξ1\\nξ2\\n01\\nloss\\n1\\n0\\nx\\nFigure 4.4 Both the hinge loss and the quadratic hinge loss provide convex upper\\nbounds on the binary zero-one loss.\\nwill not beneﬁt from the large-margin guarantees we will present in section 4.4.\\nFurthermore, the problem of determining a hyperplane with the smallest zero-one\\nloss, that is the smallest number of misclassiﬁcations, is NP-hard as a function of\\nthe dimension N of the space.\\nHere, there are two conﬂicting objectives: on one hand, we wish to limit the\\ntotal amount of slack due to outliers, which can be measured by ∑\\nm\\ni=1 ξi, or, more\\ngenerally by ∑m\\ni=1 ξp\\ni for some p ≥ 1; on the other hand, we seek a hyperplane with\\na large margin, though a larger margin can lead to more outliers and thus larger\\namounts of slack.\\n4.3.1 Primal optimization problem\\nThis leads to the following general optimization problem deﬁning SVMs in the\\nnon-separable case where the parameter C ≥ 0 determines the trade-oﬀ between\\nmargin-maximization (or minimization of ∥w∥\\n2) and the minimization of the slack\\npenalty ∑m\\ni=1 ξp\\ni :\\nmin\\nw,b,ξ\\n1\\n2 ∥w∥2 + C\\nm∑\\ni=1\\nξp\\ni (4.23)\\nsubject to yi(w · xi + b) ≥ 1 − ξi ∧ ξi ≥ 0,i ∈ [1,m ],\\nwhere ξ=( ξ1,...,ξ m)⊤. The parameter C is typically determined via n-fold cross-\\nvalidation (see section 1.3).\\nAs in the separable case, (4.23) is a convex optimization problem since the\\nconstraints are aﬃne and thus convex and since the objective function is convex\\nfor any p ≥ 1. In particular, ξ ↦→ ∑\\nm\\ni=1 ξp\\ni = ∥ξ∥p\\np is convex in view of the convexity\\nof the norm ∥·∥ p.4.3 SVMs — non-separable case 73\\nThere are many possible choices for p leading to more or less aggressive penal-\\nizations of the slack terms (see exercise 4.1). The choices p = 1 and p =2l e a dt o\\nthe most straightforward solutions and analyses. The loss functions associated with\\np = 1 and p = 2 are called the hinge loss and the quadratic hinge loss, respectively.\\nFigure 4.4 shows the plots of these loss functions as well as that of the standard\\nzero-one loss function. Both hinge losses are convex upper bounds on the zero-one\\nloss, thus making them well suited for optimization. In what follows, the analysis is\\npresented in the case of the hinge loss ( p = 1), which is the most widely used loss\\nfunction for SVMs.\\n4.3.2 Support vectors\\nAs in the separable case, the constraints are aﬃne and thus qualiﬁed. The objective\\nfunction as well as the aﬃne constraints are convex and diﬀerentiable. Thus, the\\nhypotheses of theorem B.8 hold and the KKT conditions apply at the optimum.\\nWe use these conditions to both analyze the algorithm and demonstrate several\\nof its crucial properties, and subsequently derive the dual optimization problem\\nassociated to SVMs in section 4.3.3.\\nWe introduce Lagrange variables α\\ni ≥ 0, i ∈ [1,m], associated to the ﬁrst m\\nconstraints and βi ≥ 0, i ∈ [1,m] associated to the non-negativity constraints of\\nthe slack variables. We denote by α the vector (α1,...,α m)⊤ and by β the vector\\n(β1,...,β m)⊤. The Lagrangian can then be deﬁned for all w ∈ RN , b ∈ R,a n d\\nα ∈ Rm\\n+ ,b y\\nL(w,b ,ξ, α, β)= 1\\n2 ∥w∥2 +C\\nm∑\\ni=1\\nξi −\\nm∑\\ni=1\\nαi[yi(w ·xi +b) − 1+ ξi] −\\nm∑\\ni=1\\nβiξi . (4.24)\\nThe KKT conditions are obtained by setting the gradient of the Lagrangian\\nwith respect to the primal variables w, b,a n d ξist oz e r oa n db yw r i t i n gt h e\\ncomplementarity conditions:\\n∇wL = w −\\nm∑\\ni=1\\nαiyixi =0 = ⇒ w =\\nm∑\\ni=1\\nαiyixi (4.25)\\n∇bL = −\\nm∑\\ni=1\\nαiyi =0 = ⇒\\nm∑\\ni=1\\nαiyi = 0 (4.26)\\n∇ξi L = C − αi − βi =0 = ⇒ αi + βi = C (4.27)\\n∀i, αi[yi(w · xi + b) − 1+ ξi]=0 = ⇒ αi =0 ∨ yi(w · xi + b)=1 − ξi (4.28)\\n∀i, βiξi =0 = ⇒ βi =0 ∨ ξi =0 . (4.29)\\nBy equation 4.25, as in the separable case, the weight vector w solution of the\\nSVMproblem is a linear combination of the training set vectorsx1,..., xm.Av e c t o r74 Support Vector Machines\\nxi appears in that expansion iﬀαi ̸= 0. Such vectors are calledsupport vectors. Here,\\nthere are two types of support vectors. By the complementarity condition (4.28), if\\nα\\ni ̸=0 ,t h e nyi(w · xi + b)=1 − ξi.I f ξi =0 ,t h e nyi(w · xi + b)=1a n d xi lies\\non a marginal hyperplane, as in the separable case. Otherwise, ξi ̸= 0 and xi is an\\noutlier. In this case, (4.29) implies βi = 0 and (4.27) then requires αi = C.T h u s ,\\nsupport vectors xi are either outliers, in which case αi = C, or vectors lying on the\\nmarginal hyperplanes. As in the separable case, note that while the weight vector\\nw solution is unique, the support vectors are not.\\n4.3.3 Dual optimization problem\\nTo derive the dual form of the constrained optimization problem (4.23), we plug\\ninto the Lagrangian the deﬁnition of w in terms of the dual variables (4.25) and\\napply the constraint (4.26). This yields\\nL = 1\\n2 ∥\\nm∑\\ni=1\\nαiyixi∥2 −\\nm∑\\ni,j=1\\nαiαjyiyj(xi · xj)\\n\\ued19 \\ued18\\ued17 \\ued1a\\n− 1\\n2\\nPm\\ni,j=1 αiαjyiyj(xi·xj)\\n−\\nm∑\\ni=1\\nαiyib\\n\\ued19 \\ued18\\ued17 \\ued1a\\n0\\n+\\nm∑\\ni=1\\nαi . (4.30)\\nRemarkably, we ﬁnd that the objective function is no diﬀerent than in the separable\\ncase:\\nL =\\nm∑\\ni=1\\nαi − 1\\n2\\nm∑\\ni,j=1\\nαiαjyiyj(xi · xj) . (4.31)\\nHowever, here, in addition toαi ≥ 0, we must impose the constraint on the Lagrange\\nvariables βi ≥ 0. In view of (4.27), this is equivalent to αi ≤ C. This leads to the\\nfollowing dual optimization problem for SVMs in the non-separable case, which only\\ndiﬀers from that of the separable case (4.13) by the constraints αi ≤ C:\\nmax\\nα\\nm∑\\ni=1\\nαi − 1\\n2\\nm∑\\ni,j=1\\nαiαjyiyj(xi · xj) (4.32)\\nsubject to: 0 ≤ αi ≤ C ∧\\nm∑\\ni=1\\nαiyi =0 ,i ∈ [1,m ].\\nThus, our previous comments about the optimization problem (4.13) apply to (4.32)\\nas well. In particular, the objective function is concave and inﬁnitely diﬀerentiable\\nand (4.32) is equivalent to a convex QP. The problem is equivalent to the primal\\nproblem (4.23).\\nThe solution α of the dual problem (4.32) can be used directly to determine the4.4 Margin theory 75\\nhypothesis returned by SVMs, using equation (4.25):\\nh(x)=s g n (w · x + b)=s g n\\n( m∑\\ni=1\\nαiyi(xi · x)+ b\\n⎡\\n. (4.33)\\nMoreover, b can be obtained from any support vector xi lying on a marginal\\nhyperplane, that is any vector xi with 0 <α i <C . For such support vectors,\\nw · xi + b = yi and thus\\nb = yi −\\nm∑\\nj=1\\nαjyj(xj · xi) . (4.34)\\nAs in the separable case, the dual optimization problem (4.32) and the expressions\\n(4.33) and (4.34) show an important property of SVMs: the hypothesis solution\\ndepends only on inner products between vectors and not directly on the vectors\\nthemselves. This fact can be used to extend SVMs to deﬁne non-linear decision\\nboundaries, as we shall see in chapter 5.\\n4.4 Margin theory\\nThis section presents generalization bounds based on the notion of margin, which\\nprovide a strong theoretical justiﬁcation for the SVM algorithm. We ﬁrst give the\\ndeﬁnitions of some basic margin concepts.\\nDeﬁnition 4.2 Margin\\nThe geometric margin ρ(x) of a pointx with labely with respect to a linear classiﬁer\\nh: x ↦→ w · x + b is its distance to the hyperplane w · x + b =0 :\\nρ(x)= y (w · x + b)\\n∥w∥ . (4.35)\\nThe margin of a linear classiﬁer h for a sample S =( x1,..., xm) is the minimum\\nmargin over the points in the sample:\\nρ=m i n\\n1≤i≤m\\nyi (w · xi + b)\\n∥w∥ . (4.36)\\nRecall that the VC-dimension of the family of hyperplanes or linear hypotheses in\\nRN is N+1. Thus, the application of the VC-dimension bound (3.31) of corollary 3.4\\nto this hypothesis set yields the following: for any δ> 0, with probability at least76 Support Vector Machines\\n1 − δ, for any h ∈ H,\\nR(h) ≤ ˆR(h)+\\n√\\n2(N +1 )l o gem\\nN+1\\nm +\\n√\\nlog 1\\nδ\\n2m . (4.37)\\nWhen the dimension of the feature space N is large compared to the sample size,\\nthis bound is uninformative. The following theorem presents instead a bound on the\\nVC-dimension of canonical hyperplanes that does not depend on the dimension of\\nfeature space N, but only on the margin and the radius r of the sphere containing\\nthe data.\\nTheorem 4.2\\nLet S ⊆{ x: ∥x∥≤ r}. Then, the VC-dimensiond of the set of canonical hyperplanes\\n{x ↦→ sgn(w · x): min\\nx∈S |w · x| =1 ∧∥w∥≤ Λ} veriﬁes\\nd ≤ r2Λ2 .\\nProof Assume {x1,..., xd} is a set that can be fully shattered. Then, for all\\ny =( y1,...,y d) ∈{ −1, +1}d,t h e r ee x i s t sw such that,\\n∀i ∈ [1,d ], 1 ≤ yi(w · xi) .\\nSumming up these inequalities yields\\nd ≤ w ·\\nd∑\\ni=1\\nyixi ≤∥ w∥∥\\nd∑\\ni=1\\nyixi∥≤ Λ∥\\nd∑\\ni=1\\nyixi∥ .\\nSince this inequality holds for all y ∈{ −1,+1}d, it also holds on expectation over\\ny1,...,y d drawn i.i.d. according to a uniform distribution over{−1, +1}. In view of\\nthe independence assumption, for i ̸= j we have E[yiyj]=E [ yi]E [yj]. Thus, since\\nthe distribution is uniform, E[yiyj]=0i f i ̸= j,E [yiyj] = 1 otherwise. This gives\\nd ≤ ΛE\\ny\\n[∥\\nd∑\\ni=1\\nyixi∥] (taking expectations)\\n≤ Λ\\n[\\nE\\ny\\n[∥\\nd∑\\ni=1\\nyixi∥2]\\n]1/2\\n(Jensen’s inequality)\\n=Λ\\n[ d∑\\ni,j=1\\nE\\ny\\n[yiyj](xi · xj)\\n]1/2\\n=Λ\\n[ d∑\\ni=1\\n(xi · xi)\\n]1/2\\n≤ Λ\\n[\\ndr2]1/2\\n=Λ r\\n√\\nd.\\nThus,\\n√\\nd ≤ Λr, which completes the proof.4.4 Margin theory 77\\nWhen the training data is linearly separable, by the results of section 4.2, the\\nmaximum-margin canonical hyperplane with ∥w∥ =1 /ρ c a nb ep l u g g e di n t o\\nt h e o r e m4 . 2 .I nt h i sc a s e ,Λc a nb es e tt o1/ρ, and the upper bound can be rewritten\\nas r2/ρ2. Note that the choice of Λ must be made before receiving the sample S.\\nIt is also possible to bound the Rademacher complexity of linear hypotheses with\\nbounded weight vector in a similar way, as shown by the following theorem.\\nTheorem 4.3\\nLet S ⊆{ x: ∥x∥≤ R} be a sample of size m and let H = {x ↦→ w · x: ∥w∥≤ Λ}.\\nThen, the empirical Rademacher complexity of H can be bounded as follows:\\nˆRS(H) ≤\\n√\\nr2Λ2\\nm .\\nProof The proof follows through a series of inequalities similar to those of theo-\\nrem 4.2:\\nˆRS(H)= 1\\nm E\\nσ\\n[ m∑\\ni=1\\nσiw · xi\\n]\\n= 1\\nm E\\nσ\\n[\\nw ·\\nm∑\\ni=1\\nσixi\\n]\\n≤ Λ\\nm E\\nσ\\n[\\ued79\\ued79\\n\\ued79\\nm∑\\ni=1\\nσixi\\n\\ued79\\ued79\\n\\ued79\\n]\\n≤ Λ\\nm\\n[\\nE\\nσ\\n[\\ued79\\ued79\\n\\ued79\\nm∑\\ni=1\\nσixi\\n\\ued79\\ued79\\n\\ued79\\n2]]1/2\\n= Λ\\nm\\n[\\nE\\nσ\\n[ m∑\\ni,j=1\\nσiσj(xi · xj)\\n]]1/2\\n≤ Λ\\nm\\n[ m∑\\ni=1\\n∥xi∥2\\n]1/2\\n≤ Λ\\n√\\nmr2\\nm =\\n√\\nr2Λ2\\nm ,\\nThe ﬁrst inequality makes use of the Cauchy-Schwarz inequality and the bound on\\n∥w∥, the second follows by Jensen’s inequality, the third by E[σiσj]=E [ σi]E [σj]=\\n0f o ri ̸= j, and the last one by ∥xi∥≤ R.\\nTo present the main margin-based generalization bounds of this section, we need\\nto introduce a margin loss function. Here, the training data is not assumed to be\\nseparable. The quantity ρ> 0s h o u l dt h u sb ei n t e r p r e t e da st h em a r g i nw ew i s ht o\\nachieve.\\nDeﬁnition 4.3 Margin loss function\\nFor any ρ> 0,t h eρ-margin loss is the function Lρ: R × R → R+ deﬁned for all\\ny,y ′ ∈ R by Lρ(y,y ′)=Φ ρ(yy′) with,\\nΦρ(x)=\\n⎧\\n⎪⎪\\n⎨\\n⎪⎪\\n⎩\\n0 if ρ ≤ x\\n1 − x/ρ if 0 ≤ x ≤ ρ\\n1 if x ≤ 0 .\\nThis loss function is illustrated in ﬁgure 4.5. The empirical margin loss is then\\ndeﬁned as the margin loss over the training sample.78 Support Vector Machines\\n1\\n0 ρ1\\nFigure 4.5 The margin loss, deﬁned with respect to margin parameter ρ.\\nDeﬁnition 4.4 Empirical margin loss\\nGiven a sample S =( x1,...,x m) and a hypothesis h, the empirical margin loss is\\ndeﬁned by\\nˆRρ(h)= 1\\nm\\nm∑\\ni=1\\nΦρ(yih(xi)). (4.38)\\nNote that for any i ∈ [1,m], Φρ(yih(xi)) ≤ 1yih(xi)≤ρ. Thus, the empirical margin\\nloss can be upper-bounded as follows:\\nˆRρ(h) ≤ 1\\nm\\nm∑\\ni=1\\n1yih(xi)≤ρ . (4.39)\\nIn all the results that follow, the empirical margin loss can be replaced by this upper\\nbound, which admits a simple interpretation: it is the fraction of the points in the\\ntraining sample S that have been misclassiﬁed or classiﬁed with conﬁdence less than\\nρ.W h e nh is a linear function deﬁned by a weight vector w with ∥w∥ =1 , yih(xi)\\nis the margin of point xi. Thus, the upper bound is then the fraction of the points\\nin the training data with margin less than ρ. This corresponds to the loss function\\nindicated by the blue dotted line in ﬁgure 4.5.\\nThe slope of the function Φρ deﬁning the margin loss is at most 1 /ρ,t h u sΦρ is\\n1/ρ-Lipschitz. The following lemma bounds the empirical Rademacher complexity\\nof a hypothesis set H after composition with such a Lipschitz function in terms of\\nthe empirical Rademacher complexity of H. It will be needed for the proof of the\\nmargin-based generalization bound.\\nLemma 4.2 Talagrand’s lemma\\nLet Φ: R → R be an l-Lipschitz. Then, for any hypothesis set H of real-valued\\nfunctions, the following inequality holds:\\nˆRS(Φ ◦H) ≤ l ˆRS(H) .4.4 Margin theory 79\\nProof First we ﬁx a sample S =( x1,...,x m), then, by deﬁnition,\\nˆRS(Φ ◦H)= 1\\nm E\\nσ\\n[\\nsup\\nh∈H\\nm∑\\ni=1\\nσi(Φ ◦h)(xi)\\n]\\n= 1\\nm E\\nσ1,...,σm− 1\\n[\\nE\\nσm\\n[\\nsup\\nh∈H\\num−1(h)+ σm(Φ ◦h)(xm)\\n]]\\n,\\nwhere um−1(h)= ∑m−1\\ni=1 σi(Φ ◦h)(xi). By deﬁnition of the supremum, for anyϵ> 0,\\nthere exist h1,h2 ∈ H such that\\num−1(h1)+( Φ ◦h1)(xm) ≥ (1 − ϵ)\\n[\\nsup\\nh∈H\\num−1(h)+( Φ ◦h)(xm)\\n]\\nand um−1(h2) − (Φ ◦h2)(xm) ≥ (1 − ϵ)\\n[\\nsup\\nh∈H\\num−1(h) − (Φ ◦h)(xm)\\n]\\n.\\nThus, for any ϵ> 0, by deﬁnition of E σm ,\\n(1 − ϵ)E\\nσm\\n[\\nsup\\nh∈H\\num−1(h)+ σm(Φ ◦h)(xm)\\n]\\n=( 1 − ϵ)\\n[1\\n2 sup\\nh∈H\\num−1(h)+( Φ ◦h)(xm)+ 1\\n2 sup\\nh∈H\\num−1(h) − (Φ ◦h)(xm)\\n]\\n≤ 1\\n2[um−1(h1)+( Φ ◦h1)(xm)] + 1\\n2[um−1(h2) − (Φ ◦h2)(xm)].\\nLet s = sgn(h1(xm) − h2(xm)). Then, the previous inequality implies\\n(1 − ϵ)E\\nσm\\n[\\nsup\\nh∈H\\num−1(h)+ σm(Φ ◦h)(xm)\\n]\\n≤ 1\\n2[um−1(h1)+ um−1(h2)+ sl(h1(xm) − h2(xm))] (Lipschitz property)\\n= 1\\n2[um−1(h1)+ slh1(xm)] + 1\\n2[um−1(h2) − slh2(xm)] (rearranging)\\n≤ 1\\n2 sup\\nh∈H\\n[um−1(h)+ slh(xm)] + 1\\n2 sup\\nh∈H\\n[um−1(h) − slh(xm)] (deﬁnition of sup)\\n=E\\nσm\\n[\\nsup\\nh∈H\\num−1(h)+ σmlh(xm)\\n]\\n. (deﬁnition of E\\nσm\\n)\\nSince the inequality holds for all ϵ> 0, we have\\nE\\nσm\\n[\\nsup\\nh∈H\\num−1(h)+ σm(Φ ◦h)(xm)\\n]\\n≤ E\\nσm\\n[\\nsup\\nh∈H\\num−1(h)+ σmlh(xm)\\n]\\n.\\nProceeding in the same way for all other σis( i ̸= m)p r o v e st h el e m m a .\\nThe following is a general margin-based generalization bound that will be used\\nin the analysis of several algorithms.80 Support Vector Machines\\nTheorem 4.4 Margin bound for binary classiﬁcation\\nLet H be a set of real-valued functions. Fix ρ> 0,t h e n ,f o ra n yδ> 0,w i t h\\nprobability at least 1 − δ, each of the following holds for all h ∈ H:\\nR(h) ≤ ˆRρ(h)+ 2\\nρRm(H)+\\n√\\nlog 1\\nδ\\n2m (4.40)\\nR(h) ≤ ˆRρ(h)+ 2\\nρ\\nˆRS(H)+3\\n√\\nlog 2\\nδ\\n2m . (4.41)\\nProof Let ˜H = {z =( x, y) ↦→ yh(x): h ∈ H}. Consider the family of functions\\ntaking values in [0, 1]:\\n˜H = {Φρ ◦f : f ∈ ˜H} .\\nBy theorem 3.1, with probability at least 1 − δ, for all g ∈ ˜H,\\nE[g(z)] ≤ 1\\nm\\nm∑\\ni=1\\ng(zi)+2 Rm( ˜H)+\\n√\\nlog 1\\nδ\\n2m ,\\nand thus, for all h ∈ H,\\nE[Φρ(yh(x))] ≤ ˆRρ(h)+2 Rm\\n(\\nΦρ ◦ ˜H\\n⎡\\n+\\n√\\nlog 1\\nδ\\n2m .\\nSince 1u≤0 ≤ Φρ(u) for all u ∈ R,w eh a v eR(h)=E [ 1yh(x)≤0] ≤ E[Φρ(yh(x))], thus\\nR(h) ≤ ˆRρ(h)+2 Rm\\n(\\nΦρ ◦ ˜H\\n⎡\\n+\\n√\\nlog 1\\nδ\\n2m .\\nRm is invariant to a constant shift, therefore we have\\nRm\\n(\\nΦρ ◦ ˜H\\n⎡\\n= Rm\\n(\\n(Φρ − 1) ◦ ˜H\\n⎡\\n.\\nSince (Φρ − 1)(0) = 0 and since (Φρ − 1) is 1/ρ-Lipschitz as with Φρ,b yl e m m a4 . 2 ,\\nwe have Rm\\n(\\nΦρ ◦ ˜H\\n⎡\\n≤ 1\\nρRm( ˜H)a n dRm( ˜H) can be rewritten as follows:\\nRm( ˜H)= 1\\nm E\\nS,σ\\n[\\nsup\\nh∈H\\nm∑\\ni=1\\nσiyih(xi)\\n]\\n= 1\\nm E\\nS,σ\\n[\\nsup\\nh∈H\\nm∑\\ni=1\\nσih(xi)\\n]\\n= Rm\\n(\\nH\\n⎡\\n.\\nThis proves (4.40). The second inequality, (4.41), can be derived in the same way\\nby using the second inequality of theorem 3.1, (3.4), instead of (3.3).\\nThe generalization bounds of theorem 4.4 shows the conﬂict between two terms:\\nthe larger the desired margin ρ, the smaller the middle term; however, the ﬁrst4.4 Margin theory 81\\nterm, the empirical margin loss ˆRρ, increases as a function of ρ. The bounds of\\nthis theorem can be generalized to hold uniformly for all ρ> 0 at the cost of an\\nadditional term\\n√\\nlog log2\\n2\\nρ\\nm , as shown in the following theorem (a version of this\\ntheorem with better constants can be derived, see exercise 4.2).\\nTheorem 4.5\\nLet H be a set of real-valued functions. Then, for anyδ> 0, with probability at least\\n1 − δ, each of the following holds for all h ∈ H and ρ ∈ (0, 1):\\nR(h) ≤ ˆRρ(h)+ 4\\nρRm(H)+\\n√\\nlog log2\\n2\\nρ\\nm +\\n√\\nlog 2\\nδ\\n2m (4.42)\\nR(h) ≤ ˆRρ(h)+ 4\\nρ\\nˆRS(H)+\\n√\\nlog log2\\n2\\nρ\\nm +3\\n√\\nlog 4\\nδ\\n2m . (4.43)\\nProof Consider two sequences ( ρk)k≥1 and (ϵk)k≥1,w i t hϵk ∈ (0, 1 ) .B yt h e o -\\nrem 4.4, for any ﬁxed k ≥ 1,\\nPr\\n[\\nR(h) − ˆRρk (h) > 2\\nρk\\nRm(H)+ ϵk\\n]\\n≤ exp(−2mϵ2\\nk). (4.44)\\nChoose ϵk = ϵ +\\n√\\nlog k\\nm , then, by the union bound,\\nPr\\n[\\n∃k: R(h) − ˆRρk (h) > 2\\nρk\\nRm(H)+ ϵk\\n]\\n≤\\n∑\\nk≥1\\nexp(−2mϵ2\\nk)\\n=\\n∑\\nk≥1\\nexp\\n[\\n− 2m(ϵ +\\n√\\n(log k)/m)2]\\n≤\\n∑\\nk≥1\\nexp(−2mϵ2)e x p (−2l o gk)\\n=\\n(∑\\nk≥1\\n1/k2⎡\\nexp(−2mϵ2)\\n= π2\\n6 exp(−2mϵ2) ≤ 2e x p (−2mϵ2).\\nWe can choose ρk =1 /2k. For any ρ ∈ (0, 1), there exists k ≥ 1 such that\\nρ ∈ (ρk,ρk−1], with ρ0 = 1. For that k, ρ ≤ ρk−1 =2 ρk,t h u s1/ρk ≤ 2/ρ\\nand log k =\\n√\\nlog log2(1/ρk) ≤\\n√\\nlog log2(2/ρ). Furthermore, for any h ∈ H,\\nˆRρk (h) ≤ ˆRρ(h). Thus,\\nPr\\n[\\n∃k: R(h) − ˆRρ(h) > 4\\nρRm(H)+\\n√\\nlog log2(2/ρ)\\nm + ϵ\\n]\\n≤ 2e x p (−2mϵ2),\\nwhich proves the ﬁrst statement. The second statement can be proven in a similar82 Support Vector Machines\\nway.\\nCombining theorem 4.3 and theorem 4.4 gives directly the following general\\nmargin bound for linear hypotheses with bounded weight vectors, presented in\\ncorollary 4.1.\\nCorollary 4.1\\nLet H = {x ↦→ w · x: ∥w∥≤ Λ} and assume that X ⊆{ x: ∥x∥≤ r}.F i xρ> 0,\\nthen, for any δ> 0, with probability at least 1 − δ, for any h ∈ H,\\nR(h) ≤ ˆR\\nρ(h)+2\\n√\\nr2Λ2/ρ2\\nm +\\n√\\nlog 1\\nδ\\n2m . (4.45)\\nAs with theorem 4.4, the bound of this corollary can be generalized to hold uniformly\\nfor all ρ> 0 at the cost of an additional term\\n√\\nlog log2\\n2\\nρ\\nm by combining theorems 4.3\\nand 4.5. This generalization bound for linear hypotheses is remarkable, since it does\\nnot depend directly on the dimension of the feature space, but only on the margin.\\nIt suggests that a small generalization error can be achieved whenρ/ris large (small\\nsecond term) while the empirical margin loss is relatively small (ﬁrst term). The\\nlatter occurs when few points are either classiﬁed incorrectly or correctly, but with\\nmargin less than ρ.\\nThe fact that the guarantee does not explicitly depend on the dimension of the\\nfeature space may seem surprising and appear to contradict the VC-dimension lower\\nbounds of theorems 3.6 and 3.7. Those lower bounds show that for any learning\\nalgorithm A there exists a bad distribution for which the error of the hypothesis\\nreturned by the algorithm is Ω(\\n√\\nd/m) with a non-zero probability. The bound of\\nthe corollary does not rule out such bad cases, however: for such bad distributions,\\nthe empirical margin loss would be large even for a relatively small margin ρ,a n d\\nthus the bound of the corollary would be loose in that case.\\nThus, in some sense, the learning guarantee of the corollary hinges upon the\\nhope of a good margin value ρ: if there exists a relatively large margin value\\nρ> 0 for which the empirical margin loss is small, then a small generalization\\nerror is guaranteed by the corollary. This favorable margin situation depends on the\\ndistribution: while the learning bound is distribution-independent, the existence of\\na good margin is in fact distribution-dependent. A favorable margin seems to appear\\nrelatively often in applications.\\nThe bound of the corollary gives a strong justiﬁcation for margin-maximization\\nalgorithms such as SVMs. First, note that for ρ= 1, the margin loss can be upper\\nbounded by the hinge loss:\\n∀x ∈ R,Φ\\n1(x) ≤ max(1 − x,0). (4.46)4.5 Chapter notes 83\\nUsing this fact, the bound of the corollary implies that with probability at least\\n1 − δ, for all h ∈ H = {x ↦→ w · x: ∥w∥≤ Λ},\\nR(h) ≤ 1\\nm\\nm∑\\ni=1\\nξi +2\\n√\\nr2Λ2\\nm +\\n√\\nlog 1\\nδ\\n2m , (4.47)\\nwhere ξi =m a x ( 1− yi(w · xi), 0). The objective function minimized by the SVM\\nalgorithm has precisely the form of this upper bound: the ﬁrst term corresponds to\\nthe slack penalty over the training set and the second to the minimization of the\\n∥w∥ which is equivalent to that of∥w∥\\n2. Note that an alternative objective function\\nwould be based on the empirical margin loss instead of the hinge loss. However, the\\nadvantage of the hinge loss is that it is convex, while the margin loss is not.\\nAs already pointed out, the bounds just discussed do not directly depend on the\\ndimension of the feature space and guarantee good generalization with a favorable\\nmargin. Thus, they suggest seeking large-margin separating hyperplanes in a very\\nhigh-dimensional space. In view of the form of the dual optimization problems for\\nSVMs, determining the solution of the optimization and using it for prediction both\\nrequire computing many inner products in that space. For very high-dimensional\\nspaces, the computation of these inner products could become very costly. The\\nnext chapter provides a solution to this problem which further generalizes SVMs to\\nnon-linear separation.\\n4.5 Chapter notes\\nThe maximum-margin or optimal hyperplane solution described in section 4.2\\nwas introduced by Vapnik and Chervonenkis [1964]. The algorithm had limited\\napplications, since in most tasks in practice the data is not linearly separable.\\nIn contrast, the SVM algorithm of section 4.3 for the general non-separable case,\\nintroduced by Cortes and Vapnik [1995] under the name support-vector networks,\\nhas been widely adopted and been shown to be eﬀective in practice. The algorithm\\nand its theory have had a profound impact on theoretical and applied machine\\nlearning and inspired research on a variety of topics. Several specialized algorithms\\nhave been suggested for solving the speciﬁc QP that arises when solving the SVM\\nproblem, for example the SMO algorithm of Platt [1999] (see exercise 4.4) and a\\nvariety of other decomposition methods such as those used in the LibLinear software\\nlibrary [Hsieh et al., 2008], and [Allauzen et al., 2010] for solving the problem when\\nusing rational kernels (see chapter 5).\\nMuch of the theory supporting the SVM algorithm ([Cortes and Vapnik, 1995,\\nVapnik, 1998]), in particular the margin theory presented in section 4.4, has been\\nadopted in the learning theory and statistics communities and applied to a variety84 Support Vector Machines\\nof other problems. The margin bound on the VC-dimension of canonical hyper-\\nplanes (theorem 4.2) is by Vapnik [1998], the proof is very similar to Novikoﬀ’s\\nmargin bound on the number of updates made by the Perceptron algorithm in the\\nseparable case. Our presentation of margin guarantees based on the Rademacher\\ncomplexity follows the elegant analysis of Koltchinskii and Panchenko [2002] (see\\nalso Bartlett and Mendelson [2002], Shawe-Taylor et al. [1998]). Our proof of Ta-\\nlagrand’s lemma 4.2 is a simpler and more concise version of a more general result\\ngiven by Ledoux and Talagrand [1991, pp. 112–114]. See H¨ oﬀgen et al. [1995] for\\nhardness results related to the problem of ﬁnding a hyperplane with the minimal\\nnumber of errors on a training sample.\\n4.6 Exercises\\n4.1 Soft margin hyperplanes. The function of the slack variables used in the opti-\\nmization problem for soft margin hyperplanes has the form: ξ ↦→ ∑\\nm\\ni=1 ξi. Instead,\\nwe could use ξ↦→ ∑m\\ni=1 ξp\\ni ,w i t hp> 1.\\n(a) Give the dual formulation of the problem in this general case.\\n(b) How does this more general formulation ( p> 1) compare to the standard\\nsetting (p = 1)? In the case p = 2 is the optimization still convex?\\nSparse SVM. One can give two types of arguments in favor of the SVM algorithm:\\none based on the sparsity of the support vectors, another based on the notion\\nof margin. Suppose that instead of maximizing the margin, we choose instead to\\nmaximize sparsity by minimizing the L\\np norm of the vector α that deﬁnes the\\nweight vector w,f o rs o m ep ≥ 1. First, consider the case p =2 .T h i sg i v e st h e\\nfollowing optimization problem:\\nmin\\nα ,b\\n1\\n2\\nm∑\\ni=1\\nα2\\ni + C\\nm∑\\ni=1\\nξi (4.48)\\nsubject to yi\\n( m∑\\nj=1\\nαjyjxi · xj + b\\n⎡\\n≥ 1 − ξi,i ∈ [1,m ]\\nξi,αi ≥ 0,i ∈ [1,m].\\n(a) Show that modulo the non-negativity constraint on α, the problem coin-\\ncides with an instance of the primal optimization problem of SVM.\\n(b) Derive the dual optimization of problem of (4.48).\\n(c) Setting p = 1 will induce a more sparseα. Derive the dual optimization in4.6 Exercises 85\\nthis case.\\n4.2 Tighter Rademacher Bound. Derive the following tighter version of the bound\\nof theorem 4.5: for any δ> 0, with probability at least 1 − δ, for all h ∈ H and\\nρ ∈ (0, 1) the following holds:\\nR(h) ≤ ˆRρ(h)+ 2γ\\nρ Rm(H)+\\n√\\nlog logγ\\nγ\\nρ\\nm +\\n√\\nlog 2\\nδ\\n2m (4.49)\\nfor any γ> 1.\\n4.3 Importance weighted SVM. Suppose you wish to use SVMs to solve a learning\\nproblem where some training data points are more important than others. More\\nformally, assume that each training point consists of a triplet ( xi,y i,p i), where\\n0 ≤ pi ≤ 1 is the importance of the ith point. Rewrite the primal SVM constrained\\noptimization problem so that the penalty for mis-labeling a pointxi is scaled by the\\npriority pi. Then carry this modiﬁcation through the derivation of the dual solution.\\n4.4 Sequential minimal optimization (SMO). The SMO algorithm is an optimiza-\\ntion algorithm introduced to speed up the training of SVMs. SMO reduces a (po-\\ntentially) large quadratic programming (QP) optimization problem into a series of\\nsmall optimizations involving only two Lagrange multipliers. SMO reduces memory\\nrequirements, bypasses the need for numerical QP optimization and is easy to im-\\nplement. In this question, we will derive the update rule for the SMO algorithm in\\nthe context of the dual formulation of the SVM problem.\\n(a) Assume that we want to optimize equation 4.32 only overα\\n1 and α2.S h o w\\nthat the optimization problem reduces to\\nmax\\nα1,α2\\nα1 + α2 − 1\\n2K11α2\\n1 − 1\\n2K22α2\\n2 − sK12α1α2 − y1α1v1 − y2α2v2\\n\\ued19 \\ued18\\ued17 \\ued1a\\nΨ 1(α1,α2)\\nsubject to: 0 ≤ α1,α2 ≤ C ∧ α1 + sα2 = γ,\\nwhere γ = y1\\n∑m\\ni=3 yiαi, s = y1y2 ∈{ − 1, +1}, Kij =( xi · xj)a n d vi =∑m\\nj=3 αjyjKij for i =1 , 2.\\n(b) Substitute the linear constraint α1 = γ − sα2 into Ψ 1 to obtain a new\\nobjective function Ψ2 that depends only onα2. Show that theα2 that minimizes\\nΨ2 (without the constraints 0 ≤ α1,α2 ≤ C) can be expressed as\\nα2 = s(K11 − K12)γ+ y2(v1 − v2) − s +1\\nη ,86 Support Vector Machines\\nwhere η= K11 + K22 − 2K12.\\n(c) Show that\\nv1 − v2 = f(x1) − f(x2)+ α∗\\n2y2η− sy2γ(K11 − K12)\\nwhere f(x)= ∑m\\ni=1 α∗\\ni yi(xi · x)+ b∗ and α∗\\ni are values for the Lagrange\\nmultipliers prior to optimization over α1 and α2 (similarly, b∗ is the previous\\nvalue for the oﬀset).\\n(d) Show that\\nα2 = α∗\\n2 + y2\\n(y2 − f(x2)) − (y1 − f(x1))\\nη .\\n(e) For s = +1, deﬁne L =m a x{0,γ − C} and H =m i n{C,γ} as the lower\\nand upper bounds on α2. Similarly, for s = −1, deﬁne L =m a x{0, −γ} and\\nH =m i n{C,C − γ}. The update rule for SMO involves “clipping” the value of\\nα2, i.e.,\\nαclip\\n2 =\\n⎧\\n⎪⎪\\n⎨\\n⎪⎪\\n⎩\\nα\\n2 if L<α 2 <H\\nL if α2 ≤ L\\nH if α2 ≥ H\\n.\\nWe subsequently solve for α1 such that we satisfy the equality constraint,\\nresulting in α1 = α∗\\n1 + s(α∗\\n2 − αclip\\n2 ). Why is “clipping” is required? How are L\\nand H derived for the case s =+ 1 ?\\n4.5 SVMs hands-on.\\n(a) Download and install the libsvm software library from:\\nhttp://www.csie.ntu.edu.tw/~cjlin/libsvm/.\\n(b) Download the satimage data set found at:\\nhttp://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/.\\nMerge the training and validation sets into one. We will refer to the resulting set\\nas the training set from now on. Normalize both the training and test vectors.\\n(c) Consider the binary classiﬁcation that consists of distinguishing class 6\\nfrom the rest of the data points. Use SVMs combined with polynomial kernels\\n(see chapter 5) to solve this classiﬁcation problem. To do so, randomly split the\\ntraining data into ten equal-sized disjoint sets. For each value of the polynomial\\ndegree, d =1 , 2, 3, 4, plot the average cross-validation error plus or minus one\\nstandard deviation as a function of C (let the other parameters of polynomial\\nkernels in libsvm, γ and c, be equal to their default values 1). Report the best4.6 Exercises 87\\nvalue of the trade-oﬀ constant C measured on the validation set.\\n(d) Let ( C∗,d ∗) be the best pair found previously. Fix C to be C∗.P l o tt h e\\nten-fold cross-validation training and test errors for the hypotheses obtained\\nas a function of d. Plot the average number of support vectors obtained as a\\nfunction of d.\\n(e) How many of the support vectors lie on the margin hyperplanes?\\n(f) In the standard two-group classiﬁcation, errors on positive or negative\\npoints are treated in the same manner. Suppose, however, that we wish to\\npenalize an error on a negative point (false positive error)k> 0t i m e sm o r et h a n\\nan error on a positive point. Give the dual optimization problem corresponding\\nto SVMs modiﬁed in this way.\\n(g) Assume that k is an integer. Show how you can uselibsvm without writing\\nany additional code to ﬁnd the solution of the modiﬁed SVMs just described.\\n(h) Apply the modiﬁed SVMs to the classiﬁcation task previously examined\\nand compare with your previous SVMs results for k =2 ,4,8, 16.5 Kernel Methods\\nKernel methods are widely used in machine learning. They are ﬂexible techniques\\nthat can be used to extend algorithms such as SVMs to deﬁne non-linear decision\\nboundaries. Other algorithms that only depend on inner products between sample\\npoints can be extended similarly, many of which will be studied in future chapters.\\nThe main idea behind these methods is based on so-calledkernels or kernel func-\\ntions, which, under some technical conditions of symmetry andpositive-deﬁniteness ,\\nimplicitly deﬁne an inner product in a high-dimensional space. Replacing the orig-\\ninal inner product in the input space with positive deﬁnite kernels immediately\\nextends algorithms such as SVMs to a linear separation in that high-dimensional\\nspace, or, equivalently, to a non-linear separation in the input space.\\nIn this chapter, we present the main deﬁnitions and key properties of positive\\ndeﬁnite symmetric kernels, including the proof of the fact that they deﬁne an inner\\nproduct in a Hilbert space, as well as their closure properties. We then extend the\\nSVM algorithm using these kernels and present several theoretical results including\\ngeneral margin-based learning guarantees for hypothesis sets based on kernels. We\\nalso introduce negative deﬁnite symmetric kernels and point out their relevance to\\nthe construction of positive deﬁnite kernels, in particular from distances or metrics.\\nFinally, we illustrate the design of kernels for non-vectorial discrete structures by\\nintroducing a general family of kernels for sequences, rational kernels. We describe\\nan eﬃcient algorithm for the computation of these kernels and illustrate them with\\nseveral examples.\\n5.1 Introduction\\nIn the previous chapter, we presented an algorithm for linear classiﬁcation, SVMs,\\nwhich is both eﬀective in applications and beneﬁts from a strong theoretical justi-\\nﬁcation. In practice, linear separation is often not possible. Figure 5.1a shows an\\nexample where any hyperplane crosses both populations. However, one can use more\\ncomplex functions to separate the two sets as in ﬁgure 5.1b. One way to deﬁne such\\na non-linear decision boundary is to use a non-linear mapping Φ from the input90 Kernel Methods\\n(a) (b)\\nFigure 5.1 Non-linearly separable case. The classiﬁcation task consists of discrim-\\ninating between solid squares and solid circles. (a) No hyperplane can separate the\\ntwo populations. (b) A non-linear mapping can be used instead.\\nspace X to a higher-dimensional space H, where linear separation is possible.\\nThe dimension of H can truly be very large in practice. For example, in the\\ncase of document classiﬁcation, one may wish to use as features sequences of three\\nconsecutive words, i.e., trigrams. Thus, with a vocabulary of just 100 ,000 words,\\nthe dimension of the feature space H reaches 1015. On the positive side, the margin\\nbounds presented in section 4.4 show that, remarkably, the generalization ability of\\nlarge-margin classiﬁcation algorithms such as SVMs do not depend on the dimension\\nof the feature space, but only on the marginρand the number of training examples\\nm. Thus, with a favorable marginρ, such algorithms could succeed even in very high-\\ndimensional space. However, determining the hyperplane solution requires multiple\\ninner product computations in high-dimensional spaces, which can become be very\\ncostly.\\nAs o l u t i o nt ot h i sp r o b l e mi st ou s ekernel methods, which are based on kernels\\nor kernel functions.\\nDeﬁnition 5.1 Kernels\\nA function K : X× X→ R is called a kernel over X.\\nT h ei d e ai st od e ﬁ n eak e r n e lK such that for any two pointsx, x′ ∈X , K(x, x′)b e5.1 Introduction 91\\nequal to an inner product of vectors Φ(x)a n dΦ (y):1\\n∀x, x′ ∈X ,K (x, x′)= ⟨Φ(x), Φ(x′)⟩ , (5.1)\\nfor some mapping Φ: X→ H to a Hilbert space H called a feature space.S i n c ea n\\ninner product is a measure of the similarity of two vectors, K is often interpreted\\nas a similarity measure between elements of the input space X.\\nAn important advantage of such a kernel K is eﬃciency: K is often signiﬁcantly\\nmore eﬃcient to compute than Φ and an inner product in H.W ew i l ls e es e v e r a l\\ncommon examples where the computation of K(x, x′) can be achieved in O(N)\\nwhile that of ⟨Φ(x), Φ(x′)⟩ typically requires O(dim(H)) work, with dim(H) ≫ N.\\nFurthermore, in some cases, the dimension of H is inﬁnite.\\nPerhaps an even more crucial beneﬁt of such a kernel function K is ﬂexibility:\\nthere is no need to explicitly deﬁne or compute a mapping Φ. The kernel K can\\nbe arbitrarily chosen so long as the existence of Φ is guaranteed, i.e. K satisﬁes\\nMercer’s condition (see theorem 5.1).\\nTheorem 5.1 Mercer’s condition\\nLet X⊂ RN be a compact set and letK : X× X → R be a continuous and symmetric\\nfunction. Then, K admits a uniformly convergent expansion of the form\\nK(x, x′)=\\n∞∑\\nn=0\\nanφn(x)φn(x′),\\nwith an > 0 iﬀ for any square integrable function c (c ∈ L2(X)), the following\\ncondition holds:\\n∫∫\\nX× X\\nc(x)c(x′)K(x, x′)dxdx′ ≥ 0.\\nThis condition is important to guarantee the convexity of the optimization problem\\nfor algorithms such as SVMs and thus convergence guarantees. A condition that\\nis equivalent to Mercer’s condition under the assumptions of the theorem is that\\nthe kernel K be positive deﬁnite symmetric (PDS). This property is in fact more\\ngeneral since in particular it does not require any assumption aboutX. In the next\\nsection, we give the deﬁnition of this property and present several commonly used\\nexamples of PDS kernels, then show that PDS kernels induce an inner product in\\na Hilbert space, and prove several general closure properties for PDS kernels.\\n1. To diﬀerentiate that inner product from the one of the input space, we will typically\\ndenote it by ⟨·, ·⟩.92 Kernel Methods\\n5.2 Positive deﬁnite symmetric kernels\\n5.2.1 Deﬁnitions\\nDeﬁnition 5.2 Positive deﬁnite symmetric kernels\\nA kernel K : X× X → R is said to be positive deﬁnite symmetric (PDS) if for\\nany {x1,...,x m}⊆X ,t h em a t r i xK =[ K(xi,xj)]ij ∈ Rm×m is symmetric positive\\nsemideﬁnite (SPSD).\\nK is SPSD if it is symmetric and one of the following two equivalent conditions\\nholds:\\nthe eigenvalues of K are non-negative;\\nfor any column vector c =( c1,...,c m)⊤ ∈ Rm×1,\\nc⊤Kc =\\nn∑\\ni,j=1\\ncicjK(xi,xj) ≥ 0. (5.2)\\nFor a sample S =( x1,...,x m), K =[ K(xi,xj)]ij ∈ Rm×m is called the kernel\\nmatrix or the Gram matrix associated to K and the sample S.\\nLet us insist on the terminology: the kernel matrix associated to apositive deﬁnite\\nkernel is positive semideﬁnite . This is the correct mathematical terminology.\\nNevertheless, the reader should be aware that in the context of machine learning,\\nsome authors have chosen to use instead the term positive deﬁnite kernel to imply\\na positive deﬁnite kernel matrix or used new terms such as positive semideﬁnite\\nkernel.\\nThe following are some standard examples of PDS kernels commonly used in\\napplications.\\nExample 5.1 Polynomial kernels\\nFor any constantc> 0, a polynomial kernel of degreed ∈ N is the kernel K deﬁned\\nover R\\nN by:\\n∀x,x′ ∈ RN ,K (x,x′)=( x · x′ + c)d. (5.3)\\nPolynomial kernels map the input space to a higher-dimensional space of dimension(N+d\\nd\\n⎡\\n(see exercise 5.9). As an example, for an input space of dimension N =2 ,\\na second-degree polynomial (d = 2) corresponds to the following inner product in5.2 Positive deﬁnite symmetric kernels 93\\n(−1,1) (1, 1)\\n(1, −1)(−1, −1)\\nx2\\nx1\\n(1, 1, −\\n√\\n2, +\\n√\\n2, −\\n√\\n2, 1)\\n√\\n2 x1x2\\n√\\n2 x1\\n(1, 1, −\\n√\\n2, −\\n√\\n2, +\\n√\\n2, 1)\\n(1, 1, +\\n√\\n2, −\\n√\\n2, −\\n√\\n2, 1) (1 , 1, +\\n√\\n2, +\\n√\\n2, +\\n√\\n2, 1)\\n(a) (b)\\nFigure 5.2 Illustration of the XOR classiﬁcation problem and the use of poly-\\nnomial kernels. (a) XOR problem linearly non-separable in the input space. (b)\\nLinearly separable using second-degree polynomial kernel.\\ndimension 6:\\n∀x,x\\n′ ∈ R2,K (x,x′)=( x1x′\\n1 + x2x′\\n2 + c)2 =\\n⎡\\n⎢⎢⎢⎢\\n⎢\\n⎢\\n⎢⎢⎢⎣\\nx\\n2\\n1\\nx2\\n2\\n√\\n2 x1x2\\n√\\n2cx1\\n√\\n2cx2\\nc\\n⎤\\n⎥⎥⎥\\n⎥\\n⎥\\n⎥\\n⎥⎥⎥⎦\\n·\\n⎡\\n⎢⎢⎢\\n⎢\\n⎢\\n⎢\\n⎢⎢⎢⎣\\nx\\n′2\\n1\\nx′2\\n2√\\n2 x′\\n1x′\\n2\\n√\\n2cx ′\\n1√\\n2cx ′\\n2\\nc\\n⎤\\n⎥⎥⎥\\n⎥\\n⎥\\n⎥\\n⎥⎥⎥⎦\\n. (5.4)\\nThus, the features corresponding to a second-degree polynomial are the original\\nfeatures (x\\n1 and x2), as well as products of these features, and the constant feature.\\nMore generally, the features associated to a polynomial kernel of degree d are all\\nthe monomials of degree at most d based on the original features. The explicit\\nexpression of polynomial kernels as inner products, as in (5.4), proves directly that\\nthey are PDS kernels.\\nTo illustrate the application of polynomial kernels, consider the example of ﬁg-\\nure 5.2a which shows a simple data set in dimension two that is not linearly sepa-\\nrable. This is known as the XOR problem due to its interpretation in terms of the\\nexclusive OR (XOR) function: the label of a point is blue iﬀ exactly one of its coor-\\ndinates is 1. However, if we map these points to the six-dimensional space deﬁned\\nby a second-degree polynomial as described in (5.4), then the problem becomes\\nseparable by the hyperplane of equation x\\n1x2 = 0. Figure 5.2b illustrates that by\\nshowing the projection of these points on the two-dimensional space deﬁned by their\\nthird and fourth coordinates.\\nExample 5.2 Gaussian kernels94 Kernel Methods\\nFor any constant σ> 0, a Gaussian kernel or radial basis function (RBF) is the\\nkernel K deﬁned over RN by:\\n∀x,x′ ∈ RN ,K (x,x′)=e x p\\n(\\n− ∥x′ − x∥2\\n2σ2\\n⎡\\n. (5.5)\\nGaussians kernels are among the most frequently used kernels in applications. We\\nwill prove in section 5.2.3 that they are PDS kernels and that they can be derived\\nby normalization from the kernels K\\n′ :( x,x′) ↦→ exp\\n(x·x′\\nσ2\\n⎡\\n. Using the power series\\nexpansion of the function exponential, we can rewrite the expression ofK′ as follows:\\n∀x,x′ ∈ RN ,K ′(x,x′)=\\n+∞∑\\nn=0\\n(x · x′)n\\nσn n! ,\\nwhich shows that the kernels K′, and thus Gaussian kernels, are positive linear\\ncombinations of polynomial kernels of all degrees n ≥ 0.\\nExample 5.3 Sigmoid kernels\\nFor any real constants a, b ≥ 0, a sigmoid kernel is the kernel K deﬁned over RN\\nby:\\n∀x,x′ ∈ RN ,K (x,x′)=t a n h\\n(\\na(x · x′)+ b\\n⎡\\n. (5.6)\\nUsing sigmoid kernels with SVMs leads to an algorithm that is closely related to\\nlearning algorithms based on simple neural networks, which are also often deﬁned\\nvia a sigmoid function. When a< 0o r b< 0, the kernel is not PDS and the\\ncorresponding neural network does not beneﬁt from the convergence guarantees of\\nconvex optimization (see exercise 5.15).\\n5.2.2 Reproducing kernel Hilbert space\\nHere, we prove the crucial property of PDS kernels, which is to induce an inner\\nproduct in a Hilbert space. The proof will make use of the following lemma.\\nLemma 5.1 Cauchy-Schwarz inequality for PDS kernels\\nLet K be a PDS kernel. Then, for any x, x\\n′ ∈X ,\\nK(x, x′)2 ≤ K(x, x)K(x′,x ′). (5.7)\\nProof Consider the matrix K =\\n(\\nK(x,x) K(x,x′)\\nK(x′,x) K(x′,x′)\\n⎡\\n.B yd e ﬁ n i t i o n ,i fK is PDS,\\nthen K is SPSD for all x, x′ ∈X . In particular, the product of the eigenvalues of\\nK,d e t (K), must be non-negative, thus, using K(x′,x)= K(x, x′), we have\\ndet(K)= K(x, x)K(x′,x ′) − K(x, x′)2 ≥ 0,5.2 Positive deﬁnite symmetric kernels 95\\nwhich concludes the proof.\\nT h ef o l l o w i n gi st h em a i nr e s u l to ft h i ss e c t i o n .\\nTheorem 5.2 Reproducing kernel Hilbert space (RKHS)\\nLet K : X× X → R be a PDS kernel. Then, there exists a Hilbert space H and a\\nmapping Φ from X to H such that:\\n∀x, x′ ∈X ,K (x, x′)= ⟨Φ(x),Φ(x′)⟩ . (5.8)\\nFurthermore, H has the following property known as the reproducing property:\\n∀h ∈ H, ∀x ∈X ,h (x)= ⟨h, K(x, ·)⟩ . (5.9)\\nH is called a reproducing kernel Hilbert space (RKHS) associated to K.\\nProof For any x ∈X ,d e ﬁ n eΦ (x): X→ R as follows:\\n∀x′ ∈X , Φ(x)(x′)= K(x, x′).\\nWe deﬁne H0 as the set of ﬁnite linear combinations of such functions Φ( x):\\nH0 =\\n{ ∑\\ni∈I\\naiΦ(xi): ai ∈ R,xi ∈X ,card(I) < ∞\\n}\\n.\\nNow, we introduce an operation ⟨·, ·⟩ on H0 × H0 deﬁned for all f,g ∈ H0 with\\nf = ∑\\ni∈I aiΦ(xi)a n dg = ∑\\nj∈J bjΦ(xj)b y\\n⟨f,g ⟩ =\\n∑\\ni∈I,j ∈J\\naibjK(xi,x ′\\nj)=\\n∑\\nj∈J\\nbjf(x′\\nj)=\\n∑\\ni∈I\\naig(xi).\\nBy deﬁnition, ⟨·, ·⟩ is symmetric. The last two equations show that ⟨f,g ⟩ does not\\ndepend on the particular representations of f and g, and also show that ⟨·, ·⟩ is\\nbilinear. Further, for any f = ∑\\ni∈I aiΦ(xi) ∈ H0,s i n c eK is PDS, we have\\n⟨f,f ⟩ =\\n∑\\ni,j∈I\\naiajK(xi,xj) ≥ 0.\\nThus, ⟨·, ·⟩ is positive semideﬁnite bilinear form. This inequality implies more\\ngenerally using the bilinearity of ⟨·, ·⟩ that for any f1,...,f m and c1,...,c m ∈ R,\\nm∑\\ni,j=1\\ncicj ⟨fi,fj ⟩ =\\n⣨ m∑\\ni=1\\ncifi,\\nm∑\\nj=1\\ncjfj\\n⟩\\n≥ 0.\\nHence, ⟨·, ·⟩ is a PDS kernel on H0. Thus, for any f ∈ H0 and any x ∈X ,b y96 Kernel Methods\\nlemma 5.1, we can write\\n⟨f, Φ(x)⟩2 ≤⟨ f,f ⟩⟨Φ(x), Φ(x)⟩.\\nFurther, we observe the reproducing property of ⟨·, ·⟩: for any f = ∑\\ni∈I aiΦ(xi) ∈\\nH0,b yd e ﬁ n i t i o no f⟨·, ·⟩,\\n∀x ∈X ,f (x)=\\n∑\\ni∈I\\naiK(xi,x)= ⟨f, Φ(x)⟩ . (5.10)\\nThus, [f(x)]2 ≤⟨ f,f ⟩K(x, x) for all x ∈X , which shows the deﬁniteness of ⟨·, ·⟩.\\nThis implies that ⟨·, ·⟩ deﬁnes an inner product on H0, which thereby becomes a\\npre-Hilbert space. H0 can be completed to form a Hilbert space H in which it is\\ndense, following a standard construction. By the Cauchy-Schwarz inequality , for\\nany x ∈X , f ↦→⟨ f, Φ(x)⟩ is Lipschitz, therefore continuous. Thus, sinceH0 is dense\\nin H, the reproducing property (5.10) also holds over H.\\nThe Hilbert space H deﬁned in the proof of the theorem for a PDS kernelK is called\\nthe reproducing kernel Hilbert space (RKHS) associated toK. Any Hilbert space H\\nsuch that there exists Φ: X→ H with K(x, x′)= ⟨Φ(x), Φ(x′)⟩ for all x, x′ ∈X\\nis called a feature space associated to K and Φ is called a feature mapping.W e\\nwill denote by ∥·∥ H the norm induced by the inner product in feature space H:\\n∥w∥H =\\n√\\n⟨w,w⟩ for all w ∈ H. Note that the feature spaces associated toK are in\\ngeneral not unique and may have diﬀerent dimensions. In practice, when referring to\\nthe dimension of the feature spaceassociated to K, we either refer to the dimension\\nof the feature space based on a feature mapping described explicitly, or to that of\\nthe RKHS associated to K.\\nTheorem 5.2 implies that PDS kernels can be used to implicitly deﬁne a feature\\nspace or feature vectors. As already underlined in previous chapters, the role played\\nby the features in the success of learning algorithms is crucial: with poor features,\\nuncorrelated with the target labels, learning could become very challenging or\\neven impossible; in contrast, good features could provide invaluable clues to the\\nalgorithm. Therefore, in the context of learning with PDS kernels and for a ﬁxed\\ninput space, the problem of seeking useful features is replaced by that of ﬁnding\\nuseful PDS kernels. While features represented the user’s prior knowledge about the\\ntask in the standard learning problems, here PDS kernels will play this role. Thus,\\nin practice, an appropriate choice of PDS kernel for a task will be crucial.\\n5.2.3 Properties\\nThis section highlights several important properties of PDS kernels. We ﬁrst show\\nthat PDS kernels can be normalized and that the resulting normalized kernels are\\nalso PDS. We also introduce the deﬁnition of empirical kernel maps and describe5.2 Positive deﬁnite symmetric kernels 97\\ntheir properties and extension. We then prove several important closure properties\\nof PDS kernels, which can be used to construct complex PDS kernels from simpler\\nones.\\nTo any kernel K, we can associate a normalized kernel K\\n′ deﬁned by\\n∀x, x′ ∈X ,K ′(x, x′)=\\n⎧\\n⎨\\n⎩\\n0i f ( K(x, x)=0 ) ∧ (K(x′,x ′)=0 )\\nK(x,x′)√\\nK(x,x)K(x′,x′)\\notherwise.\\n(5.11)\\nBy deﬁnition, for a normalized kernel K′, K′(x, x) = 1 for all x ∈X such that\\nK(x, x) ̸= 0. An example of normalized kernel is the Gaussian kernel with parameter\\nσ> 0, which is the normalized kernel associated to K′ :( x,x′) ↦→ exp\\n(x·x′\\nσ2\\n⎡\\n:\\n∀x,x′ ∈ RN , K′(x,x′)√\\nK′(x,x)K′(x′,x′)\\n= e\\nx·x′\\nσ 2\\ne\\n∥x∥2\\n2σ 2 e\\n∥x′ ∥2\\n2σ 2\\n=e x p\\n(\\n− ∥x′ − x′∥2\\n2σ2\\n⎡\\n. (5.12)\\nLemma 5.2 Normalized PDS kernels\\nLet K be a PDS kernel. Then, the normalized kernel K′ associated to K is PDS.\\nProof Let {x1,...,x m}⊆X and let c be an arbitrary vector inRm. We will show\\nthat the sum ∑m\\ni,j=1 cicjK′(xi,xj) is non-negative. By lemma 5.1, if K(xi,xi)=0\\nthen K(xi,xj)=0a n dt h u sK′(xi,xj) = 0 for all j ∈ [1,m]. Thus, we can assume\\nthat K(xi,xi) > 0 for all i ∈ [1,m]. Then, the sum can be rewritten as follows:\\nm∑\\ni,j=1\\ncicjK(xi,xj)√\\nK(xi,xi)K(xj,xj)\\n=\\nm∑\\ni,j=1\\ncicj ⟨Φ(xi), Φ(xj)⟩\\n∥Φ(xi)∥H ∥Φ(xj)∥H\\n=\\n\\ued79\\ued79\\n\\ued79\\ued79\\ued79\\nm∑\\ni=1\\nciΦ(xi)\\n∥Φ(xi)∥H\\n\\ued79\\ued79\\n\\ued79\\ued79\\ued79\\n2\\nH\\n≥ 0,\\nwhere Φ is a feature mapping associated to K, which exists by theorem 5.2.\\nAs indicated earlier, PDS kernels can be interpreted as a similarity measure since\\nthey induce an inner product in some Hilbert space H.T h i si sm o r ee v i d e n tf o ra\\nnormalized kernel K since K(x, x′) is then exactly the cosine of the angle between\\nthe feature vectors Φ(x)a n dΦ (x′), provided that none of them is zero: Φ( x)a n d\\nΦ(x′) are then unit vectors since ∥Φ(x)∥H = ∥Φ(x′)∥H =\\n√\\nK(x, x)=1 .\\nWhile one of the advantages of PDS kernels is an implicit deﬁnition of a feature\\nmapping, in some instances, it may be desirable to deﬁne an explicit feature\\nmapping based on a PDS kernel. This may be to work in the primal for various\\noptimization and computational reasons, to derive an approximation based on an\\nexplicit mapping, or as part of a theoretical analysis where an explicit mapping\\nis more convenient. The empirical kernel map Φ associated to a PDS kernel K is\\na feature mapping that can be used precisely in such contexts. Given a training98 Kernel Methods\\nsample containing points x1,...,x m ∈X ,Φ : X→ Rm is deﬁned for all x ∈X by\\nΦ(x)=\\n⎡\\n⎢⎢⎣\\nK(x, x1)\\n..\\n.\\nK(x, x\\nm)\\n⎤\\n⎥⎥⎦.\\nThus, Φ(x) is the vector of theK-similarity measures of x with each of the training\\npoints. Let K be the kernel matrix associated to K and ei the ith unit vector.\\nNote that for any i ∈ [1,m], Φ(xi)i st h eith column of K,t h a ti sΦ (xi)= Kei.I n\\nparticular, for all i, j ∈ [1,m],\\n⟨Φ(xi),Φ(xj)⟩ =( Kei)⊤(Kej)= e⊤\\ni K2ej.\\nThus, the kernel matrix K′ associated to Φ is K2. It may desirable in some cases\\nto deﬁne a feature mapping whose kernel matrix coincides with K.L e tK†\\n1\\n2 denote\\nthe SPSD matrix whose square isK† , the pseudo-inverse ofK. K†\\n1\\n2 can be derived\\nfrom K† via singular value decomposition and if the matrix K is invertible, K†\\n1\\n2\\ncoincides with K−1/2 (see appendix A for properties of the pseudo-inverse). Then,\\nΨ can be deﬁned as follows using the empirical kernel map Φ:\\n∀x ∈X , Ψ(x)= K†\\n1\\n2\\nΦ(x).\\nUsing the identityKK† K = K valid for any symmetric matrixK, for alli, j ∈ [1,m],\\nthe following holds:\\n⟨Ψ(xi), Ψ(xj)⟩ =( K†\\n1\\n2\\nKei)⊤(K†\\n1\\n2\\nKej)= e⊤\\ni KK† Kej = e⊤\\ni Kej.\\nThus, the kernel matrix associated to Ψ is K. Finally, note that for the feature\\nmapping Ω: X→ Rm deﬁned by\\n∀x ∈X , Ω(x)= K† Φ(x),\\nfor all i, j ∈ [1,m], we have⟨Ω(xi), Ω(xj)⟩ = e⊤\\ni KK† K† Kej = e⊤\\ni KK† ej,u s i n gt h e\\nidentity K† K† K = K† valid for any symmetric matrix K.T h u s ,t h ek e r n e lm a t r i x\\nassociated to Ω is KK† , which reduces to the identity matrixI ∈ Rm×m when K is\\ninvertible, since K† = K−1 in that case.\\nAs pointed out in the previous section, kernels represent the user’s prior knowl-\\nedge about a task. In some cases, a user may come up with appropriate similarity\\nmeasures or PDS kernels for some subtasks — for example, for diﬀerent subcat-\\negories of proteins or text documents to classify. But how can he combine these\\nPDS kernels to form a PDS kernel for the entire class? Is the resulting combined\\nkernel guaranteed to be PDS? In the following, we will show that PDS kernels are\\nclosed under several useful operations which can be used to design complex PDS5.2 Positive deﬁnite symmetric kernels 99\\nk e r n e l s .T h e s eo p e r a t i o n sa r et h es u ma n dt h ep r o d u c to fk e r n e l s ,a sw e l la st h e\\ntensor product of two kernels K and K′, denoted by K ⊗ K′ and deﬁned by\\n∀x1,x2,x ′\\n1,x ′\\n2\\n∈X , (K ⊗ K′)(x1,x ′\\n1,x2,x ′\\n2\\n)= K(x1,x2)K′(x′\\n1,x ′\\n2\\n).\\nThey also include the pointwise limit: given a sequence of kernels (Kn)n∈N such that\\nfor all x, x′ ∈X (Kn(x, x′))n∈N admits a limit, the pointwise limit of ( Kn)n∈N is\\nthe kernel K deﬁned for all x, x′ ∈X by K(x, x′) = limn→ +∞ (Kn)(x, x′). Similarly,\\nif ∑∞\\nn=0 anxn is a power series with radius of convergence ρ> 0a n dK ak e r n e l\\ntaking values in (−ρ,+ρ), then ∑∞\\nn=0 anKn is the kernel obtained by composition\\nof K with that power series. The following theorem provides closure guarantees for\\nall of these operations.\\nTheorem 5.3 PDS kernels — closure properties\\nPDS kernels are closed under sum, product, tensor product, pointwise limit, and\\ncomposition with a power series ∑\\n∞\\nn=0 anxn with an ≥ 0 for all n ∈ N.\\nProof We start with two kernel matrices,K and K′, generated from PDS kernels\\nK and K′ for an arbitrary set of m points. By assumption, these kernel matrices\\nare SPSD. Observe that for any c ∈ Rm×1,\\n(c⊤Kc ≥ 0) ∧ (c⊤K′c ≥ 0) ⇒ c⊤(K + K′)c ≥ 0.\\nBy (5.2), this shows that K + K′ is SPSD and thus that K + K′ is PDS. To show\\nclosure under product, we will use the fact that for any SPSD matrixK there exists\\nM such that K = MM⊤. The existence of M is guaranteed as it can be generated\\nvia, for instance, singular value decomposition ofK, or by Cholesky decomposition.\\nT h ek e r n e lm a t r i xa s s o c i a t e dt oKK ′ is (KijK′\\nij)ij. For any c ∈ Rm×1,e x p r e s s i n g\\nKij in terms of the entries of M, we can write\\nm∑\\ni,j=1\\ncicj(KijK′\\nij)=\\nm∑\\ni,j=1\\ncicj\\n([ m∑\\nk=1\\nMikMjk\\n]\\nK′\\nij\\n⎡\\n=\\nm∑\\nk=1\\n[ m∑\\ni,j=1\\ncicjMikMjkK′\\nij\\n]\\n=\\nm∑\\nk=1\\nz⊤\\nk K′zk ≥ 0,\\nwith zk =\\n[ c1M1k..\\n.\\ncmMmk\\n]\\n. This shows that PDS kernels are closed under product.\\nThe tensor product of K and K′ is PDS as the product of the two PDS kernels\\n(x1,x ′\\n1,x2,x ′\\n2\\n) ↦→ K(x1,x2)a n d( x1,x ′\\n1,x2,x ′\\n2\\n) ↦→ K′(y1,y2). Next, let ( Kn)n∈N\\nbe a sequence of PDS kernels with pointwise limit K.L e tK be the kernel matrix100 Kernel Methods\\nassociated to K and Kn the one associated to Kn for any n ∈ N.O b s e r v et h a t\\n(∀n,c⊤Knc ≥ 0) ⇒ lim\\nn→∞\\nc⊤Knc = c⊤Kc ≥ 0.\\nThis shows the closure under pointwise limit. Finally, assume that K is a PDS\\nkernel with |K(x, x′)| <ρ for all x, x′ ∈X and let f : x ↦→ ∑∞\\nn=0 anxn,an ≥ 0b ea\\npower series with radius of convergenceρ. Then, for any n ∈ N, Kn and thus anKn\\nare PDS by closure under product. For any N ∈ N, ∑N\\nn=0 anKn is PDS by closure\\nunder sum of anKnsa n df ◦K is PDS by closure under the limit of ∑N\\nn=0 anKn\\nas N tends to inﬁnity.\\nThe theorem implies in particular that for any PDS kernel matrix K,e x p (K)i s\\nPDS, since the radius of convergence of exp is inﬁnite. In particular, the kernel\\nK\\n′ :( x,x′) ↦→ exp\\n(x·x′\\nσ2\\n⎡\\nis PDS since (x,x′) ↦→ x·x′\\nσ2 is PDS. Thus, by lemma 5.2,\\nthis shows that a Gaussian kernel, which is the normalized kernel associated toK′,\\nis PDS.\\n5.3 Kernel-based algorithms\\nIn this section we discuss how SVMs can be used with kernels and analyze the\\nimpact that kernels have on generalization.\\n5.3.1 SVMs with PDS kernels\\nIn chapter 4, we noted that the dual optimization problem for SVMs as well as the\\nform of the solution did not directly depend on the input vectors but only on inner\\nproducts. Since a PDS kernel implicitly deﬁnes an inner product (theorem 5.2), we\\ncan extend SVMs and combine it with an arbitrary PDS kernelK by replacing each\\ninstance of an inner product x ·x\\n′ with K(x, x′). This leads to the following general\\nform of the SVM optimization problem and solution with PDS kernels extending\\n(4.32):\\nmax\\nα\\nm∑\\ni=1\\nαi − 1\\n2\\nm∑\\ni,j=1\\nαiαjyiyjK(xi,xj) (5.13)\\nsubject to: 0 ≤ αi ≤ C ∧\\nm∑\\ni=1\\nαiyi =0 ,i ∈ [1,m ].\\nIn view of (4.33), the hypothesis h solution can be written as:\\nh(x)=s g n\\n( m∑\\ni=1\\nαiyiK(xi,x)+ b\\n⎡\\n, (5.14)5.3 Kernel-based algorithms 101\\nwith b = yi − ∑m\\nj=1 αjyjK(xj,xi) for any xi with 0 <α i <C . We can rewrite\\nthe optimization problem (5.13) in a vector form, by using the kernel matrix K\\nassociated to K for the training sample (x1,...,x m) as follows:\\nmax\\nα\\n2 1⊤α − (α ◦y)⊤K(α ◦y) (5.15)\\nsubject to: 0 ≤ α ≤ C ∧ α⊤y =0 .\\nIn this formulation, α ◦y is the Hadamard product or entry-wise product of the\\nvectors α and y. Thus, it is the column vector in Rm×1 whose ith component\\nequals αiyi. The solution in vector form is the same as in (5.14), but with b =\\nyi − (α ◦y)⊤Kei for any xi with 0 <α i <C .\\nThis version of SVMs used with PDS kernels is the general form of SVMs we\\nwill consider in all that follows. The extension is important, since it enables an\\nimplicit non-linear mapping of the input points to a high-dimensional space where\\nlarge-margin separation is sought.\\nMany other algorithms in areas including regression, ranking, dimensionality\\nreduction or clustering can be extended using PDS kernels following the same\\nscheme (see in particular chapters 8, 9, 10, 12).\\n5.3.2 Representer theorem\\nObserve that modulo the oﬀset b, the hypothesis solution of SVMs can be written\\nas a linear combination of the functions K(x\\ni, ·), where xi is a sample point. The\\nfollowing theorem known as the representer theorem s h o w st h a tt h i si si nf a c ta\\ngeneral property that holds for a broad class of optimization problems, including\\nthat of SVMs with no oﬀset.\\nTheorem 5.4 Representer theorem\\nLet K : X× X → R be a PDS kernel and H its corresponding RKHS. Then, for any\\nnon-decreasing function G: R → R and any loss function L: R\\nm → R ∪{+∞} ,t h e\\noptimization problem\\nargmin\\nh∈H\\nF(h) = argmin\\nh∈H\\nG(∥h∥H)+ L\\n(\\nh(x1),...,h (xm)\\n⎡\\nadmits a solution of the form h∗ = ∑m\\ni=1 αiK(xi, ·).I f G is further assumed to be\\nincreasing, then any solution has this form.\\nProof Let H1 =s p a n ({K(xi, ·): i ∈ [1,m]}). Anyh ∈ H admits the decomposition\\nh = h1 + h⊥ according to H = H1 ⊕ H⊥\\n1 ,w h e r e⊕ is the direct sum. Since G is\\nnon-decreasing, G(∥h1∥H) ≤ G(\\n√\\n∥h1∥2\\nH + ∥h⊥ ∥2\\nH)= G(∥h∥H). By the reproducing\\nproperty, for all i ∈ [1,m], h(xi)= ⟨h, K(xi, ·)⟩ = ⟨h1,K (xi, ·)⟩ = h1(xi). Thus,\\nL\\n(\\nh(x1),...,h (xm)\\n⎡\\n= L\\n(\\nh1(x1),...,h 1(xm)\\n⎡\\nand F(h1) ≤ F(h). This proves the102 Kernel Methods\\nﬁrst part of the theorem. If G is further increasing, then F(h1) <F (h)w h e n\\n∥h⊥ ∥H > 0 and any solution of the optimization problem must be in H1.\\n5.3.3 Learning guarantees\\nHere, we present general learning guarantees for hypothesis sets based on PDS\\nkernels, which hold in particular for SVMs combined with PDS kernels.\\nThe following theorem gives a general bound on the empirical Rademacher\\ncomplexity of kernel-based hypotheses with bounded norm, that is a hypothesis\\nset of the form H = {h ∈ H: ∥h∥\\nH ≤ Λ}, for some Λ ≥ 0, where H is the\\nRKHS associated to a kernel K.B yt h er e p r o d u c i n gp r o p e r t y ,a n yh ∈ H is of\\nthe form x ↦→⟨ h, K(x, ·)⟩ = ⟨h, Φ(x)⟩ with ∥h∥H ≤ Λ, where Φ is a feature mapping\\nassociated to K, that is of the form x ↦→⟨ w, Φ(x)⟩ with ∥w∥H ≤ Λ.\\nTheorem 5.5 Rademacher complexity of kernel-based hypotheses\\nLet K : X× X → R be a PDS kernel and let Φ: X→ H be a feature mapping\\nassociated to K.L e t S ⊆{ x: K(x, x) ≤ r2} be a sample of size m,a n dl e t\\nH = {x ↦→ w · Φ(x): ∥w∥H ≤ Λ} for some Λ ≥ 0.T h e n\\nˆRS(H) ≤ Λ\\n√\\nTr[K]\\nm ≤\\n√\\nr2Λ2\\nm . (5.16)\\nProof The proof steps are as follows:\\nˆRS(H)= 1\\nm E\\nσ\\n[\\nsup\\n∥w∥≤Λ\\n⣨\\nw,\\nm∑\\ni=1\\nσiΦ(xi)\\n⟩]\\n= Λ\\nm E\\nσ\\n[\\ued79\\ued79\\ued79\\nm∑\\ni=1\\nσiΦ(xi)\\n\\ued79\\ued79\\ued79\\nH\\n]\\n(Cauchy-Schwarz , eq. case)\\n≤ Λ\\nm\\n[\\nE\\nσ\\n[\\ued79\\ued79\\n\\ued79\\nm∑\\ni=1\\nσiΦ(xi)\\n\\ued79\\ued79\\n\\ued79\\n2\\nH\\n]]1/2\\n(Jensen’s ineq.)\\n= Λ\\nm\\n[\\nE\\nσ\\n[ m∑\\ni=1\\n∥Φ(xi)∥2\\nH\\n]]1/2\\n(i ̸= j ⇒ E\\nσ\\n[σiσj]=0 )\\n= Λ\\nm\\n[\\nE\\nσ\\n[ m∑\\ni=1\\nK(xi,xi)\\n]]1/2\\n= Λ\\n√\\nTr[K]\\nm ≤\\n√\\nr2Λ2\\nm .\\nThe initial equality holds by deﬁnition of the empirical Rademacher complexity\\n(deﬁnition 3.2). The ﬁrst inequality is due to the Cauchy-Schwarz inequality and\\n∥w∥\\nH ≤ Λ. The following inequality results from Jensen’s inequality (theorem B.4)\\napplied to the concave function √ ·. The subsequent equality is a consequence of5.4 Negative deﬁnite symmetric kernels 103\\nEσ [σiσj]=E σ [σi]Eσ [σj]=0f o r i ̸= j, since the Rademacher variables σi and\\nσj are independent. The statement of the theorem then follows by noting that\\nTr[K] ≤ mr2.\\nThe theorem indicates that the trace of the kernel matrix is an important quantity\\nfor controlling the complexity of hypothesis sets based on kernels. Observe that\\nby the Khintchine-Kahane inequality (D.22), the empirical Rademacher complexity\\nˆR\\nS(H)= Λ\\nm Eσ [∥ ∑m\\ni=1 σiΦ(xi)∥H] can also be lower bounded by 1√\\n2\\nΛ\\n√\\nTr[K]\\nm ,w h i c h\\nonly diﬀers from the upper bound found by the constant 1√\\n2 . Also, note that if\\nK(x, x) ≤ r2 for all x ∈X , then the inequalities 5.16 hold for all samples S.\\nThe bound of theorem 5.5 or the inequalities 5.16 can be plugged into any of the\\nRademacher complexity generalization bounds presented in the previous chapters.\\nIn particular, in combination with theorem 4.4, they lead directly to the following\\nmargin bound similar to that of corollary 4.1.\\nCorollary 5.1 Margin bounds for kernel-based hypotheses\\nLet K : X× X→ R be a PDS kernel with r =s u px∈X K(x, x).L e tΦ: X→ H be a\\nfeature mapping associated toK and let H = {x ↦→ w · Φ(x): ∥w∥H ≤ Λ} for some\\nΛ ≥ 0.F i xρ> 0.T h e n ,f o ra n yδ> 0, each of the following statements holds with\\nprobability at least 1 − δfor any h ∈ H:\\nR(h) ≤ ˆRρ(h)+2\\n√\\nr2Λ2/ρ2\\nm +\\n√\\nlog 1\\nδ\\n2m (5.17)\\nR(h) ≤ ˆRρ(h)+2\\n√\\nTr[K]Λ2/ρ2\\nm +3\\n√\\nlog 2\\nδ\\n2m . (5.18)\\n5.4 Negative deﬁnite symmetric kernels\\nOften in practice, a natural distance or metric is available for the learning task\\nconsidered. This metric could be used to deﬁne a similarity measure. As an example,\\nGaussian kernels have the form exp(−d\\n2), where d i sam e t r i cf o rt h ei n p u tv e c t o r\\nspace. Several natural questions arise such as: what other PDS kernels can we\\nconstruct from a metric in a Hilbert space? What technical condition should d\\nsatisfy to guarantee that exp(−d\\n2) is PDS? A natural mathematical deﬁnition that\\nhelps address these questions is that of negative deﬁnite symmetric (NDS) kernels .\\nDeﬁnition 5.3 Negative deﬁnite symmetric (NDS) kernels\\nA kernel K : X× X → R is said to be negative-deﬁnite symmetric (NDS) if it\\nis symmetric and if for all {x1,...,x m}⊆X and c ∈ Rm×1 with 1⊤c =0 ,t h e104 Kernel Methods\\nfollowing holds:\\nc⊤Kc ≤ 0.\\nClearly, if K is PDS, then −K is NDS, but the converse does not hold in general.\\nThe following gives a standard example of an NDS kernel.\\nExample 5.4 Squared distance — NDS kernel\\nThe squared distance (x, x′) ↦→∥ x′ − x∥2 in RN deﬁnes an NDS kernel. Indeed, let\\nc ∈ Rm×1 with ∑m\\ni=1 ci = 0. Then, for any {x1,...,x m}⊆X , we can write\\nm∑\\ni,j=1\\ncicj ||xi − xj ||2 =\\nm∑\\ni,j=1\\ncicj(∥xi∥2 + ∥xj ∥2 − 2xi · xj)\\n=\\nm∑\\ni,j=1\\ncicj(∥xi∥2 + ∥xj ∥2) − 2\\nm∑\\ni=1\\ncixi ·\\nm∑\\nj=1\\ncjxj\\n=\\nm∑\\ni,j=1\\ncicj(∥xi∥2 + ∥xj ∥2) − 2\\n\\ued79\\ued79\\nm∑\\ni=1\\ncixi\\n\\ued79\\ued792\\n≤\\nm∑\\ni,j=1\\ncicj(∥xi∥2 + ∥xj ∥2)\\n=\\n( m∑\\nj=1\\ncj\\n⎡( m∑\\ni=1\\nci(∥xi∥2\\n⎡\\n+\\n( m∑\\ni=1\\nci\\n⎡( m∑\\nj=1\\ncj ∥xj ∥2\\n⎡\\n=0 .\\nThe next theorems show connections between NDS and PDS kernels. These\\nresults provide another series of tools for designing PDS kernels.\\nTheorem 5.6\\nLet K′ be deﬁned for any x0 by\\nK′(x, x′)= K(x, x0)+ K(x′,x0) − K(x, x′) − K(x0,x0)\\nfor all x, x′ ∈X .T h e nK is NDS iﬀ K′ is PDS.\\nProof Assume that K′ is PDS and deﬁne K such that for any x0 we have\\nK(x, x′)= K(x, x0)+ K(x0,x ′) − K(x0,x0) − K′(x, x′). Then for any c ∈ Rm\\nsuch that c⊤1 =0a n da n ys e to fp o i n t s(x1,...,x m) ∈X m we have\\nm∑\\ni,j=1\\ncicjK(xi,xj)=\\n( m∑\\ni=1\\nciK(xi,x0)\\n⎡( m∑\\nj=1\\ncj\\n⎡\\n+\\n( m∑\\ni=1\\nci\\n⎡( m∑\\nj=1\\ncjK(x0,xj)\\n⎡\\n−\\n( m∑\\ni=1\\nci\\n⎡2\\nK(x0,x0) −\\nm∑\\ni,j=1\\ncicjK′(xi,xj)= −\\nm∑\\ni,j=1\\ncicjK′(xi,xj) ≤ 0 .5.4 Negative deﬁnite symmetric kernels 105\\nwhich proves K is NDS.\\nNow, assume K is NDS and deﬁne K′ for any x0 as above. Then, for anyc ∈ Rm,\\nwe can deﬁne c0 = −c⊤1 and the following holds by the NDS property for any points\\n(x1,...,x m) ∈X m as well as x0 deﬁned previously: ∑m\\ni,j=0 cicjK(xi,xj) ≤ 0. This\\nimplies that\\n( m∑\\ni=0\\nciK(xi,x0)\\n⎡( m∑\\nj=0\\ncj\\n⎡\\n+\\n( m∑\\ni=0\\nci\\n⎡( m∑\\nj=0\\ncjK(x0,xj)\\n⎡\\n−\\n( m∑\\ni=0\\nci\\n⎡2\\nK(x0,x0) −\\nm∑\\ni,j=0\\ncicjK′(xi,xj)= −\\nm∑\\ni,j=0\\ncicjK′(xi,xj) ≤ 0 ,\\nwhich implies 2 ∑m\\ni,j=1 cicjK′(xi,xj) ≥− 2c0\\n∑m\\ni=0 ciK′(xi,x0)+ c2\\n0K′(x0,x0)=0 .\\nThe equality holds since ∀x ∈X ,K ′(x, x0)=0 .\\nThis theorem is useful in showing other connections, such the following theorems,\\nwhich are left as exercises (see exercises 5.14 and 5.15).\\nTheorem 5.7\\nLet K : X× X → R be a symmetric kernel. Then,K is NDS iﬀexp(−tK) is a PDS\\nkernel for all t> 0.\\nThe theorem provides another proof that Gaussian kernels are PDS: as seen earlier\\n(Example 5.4), the squared distance ( x, x′) ↦→∥ x − x′∥2 in RN is NDS, thus\\n(x, x′) ↦→ exp(−t||x − x′||2)i sP D Sf o ra l lt> 0.\\nTheorem 5.8\\nLet K : X× X→ R be an NDS kernel such that for all x, x′ ∈X ,K (x, x′)=0 iﬀ\\nx = x′. Then, there exists a Hilbert space H and a mapping Φ: X→ H such that\\nfor all x, x′ ∈X ,\\nK(x, x′)= ∥Φ(x) − Φ(x′)∥2.\\nThus, under the hypothesis of the theorem,\\n√\\nK deﬁnes a metric.\\nThis theorem can be used to show that the kernel (x, x′) ↦→ exp(−|x − x′|p)i n R\\nis not PDS for p> 2. Otherwise, for any t> 0, {x1,...,x m}⊆X and c ∈ Rm×1,\\nwe would have:\\nm∑\\ni,j=1\\ncicje−t|xi−xj |p\\n=\\nm∑\\ni,j=1\\ncicje−|t1/pxi−t1/pxj |p\\n≥ 0.\\nThis would imply that (x, x′) ↦→| x − x′|p is NDS for p> 2, which can be proven\\n(via theorem 5.8) not to be valid.106 Kernel Methods\\n5.5 Sequence kernels\\nThe examples given in the previous sections, including the commonly used poly-\\nnomial or Gaussian kernels, were all for PDS kernels over vector spaces. In many\\nlearning tasks found in practice, the input space X is not a vector space. The ex-\\namples to classify in practice could be protein sequences, images, graphs, parse\\ntrees, ﬁnite automata, or other discrete structures which may not be directly given\\nas vectors. PDS kernels provide a method for extending algorithms such as SVMs\\noriginally designed for a vectorial space to the classiﬁcation of such objects. But,\\nh o wc a nw ed e ﬁ n eP D Sk e r n e l sf o rt h e s es t r u c t u r e s ?\\nThis section will focus on the speciﬁc case of sequence kernels,t h a t i s , k e r n e l s\\nfor sequences or strings. PDS kernels can be deﬁned for other discrete structures\\nin somewhat similar ways. Sequence kernels are particularly relevant to learning\\nalgorithms applied to computational biology or natural language processing, which\\nare both important applications.\\nHow can we deﬁne PDS kernels for sequences, which are similarity measures for\\nsequences? One idea consists of declaring two sequences, e.g., two documents or\\ntwo biosequences, as similar when they share common substrings or subsequences.\\nOne example could be the kernel between two sequences deﬁned by the sum\\nof the product of the counts of their common substrings. But which substrings\\nshould be used in that deﬁnition? Most likely, we would need some ﬂexibility in\\nthe deﬁnition of the matching substrings. For computational biology applications,\\nfor example, the match could be imperfect. Thus, we may need to consider some\\nnumber of mismatches, possibly gaps, or wildcards. More generally, we might need\\nto allow various substitutions and might wish to assign diﬀerent weights to common\\nsubstrings to emphasize some matching substrings and deemphasize others.\\nAs can be seen from this discussion, there are many diﬀerent possibilities and\\nwe need a general framework for deﬁning such kernels. In the following, we will\\nintroduce a general framework for sequence kernels, rational kernels, which will\\ninclude all the kernels considered in this discussion. We will also describe a general\\nand eﬃcient algorithm for their computation and will illustrate them with some\\nexamples.\\nThe deﬁnition of these kernels relies on that of weighted transducers.T h u s ,w e\\nstart with the deﬁnition of these devices as well as some relevant algorithms.\\n5.5.1 Weighted transducers\\nSequence kernels can be eﬀectively represented and computed usingweighted trans-\\nducers. In the following deﬁnition, let Σ denote a ﬁnite input alphabet, Δ a ﬁnite\\noutput alphabet, and ϵ the empty string or null label, whose concatenation with5.5 Sequence kernels 107\\n2/8 b:b/2\\n0\\nb:b/2\\n3/2\\nb:a/3\\n1\\na:b/3\\na:a/2\\nb:a/4\\na:a/1\\nFigure 5.3 Example of weighted transducer.\\nany string leaves it unchanged.\\nDeﬁnition 5.4\\nA weighted transducer T is a 7-tuple T =( Σ, Δ ,Q ,I,F,E,ρ ) where Σ is a ﬁnite\\ninput alphabet, Δ a ﬁnite output alphabet, Q is a ﬁnite set of states, I ⊆ Q the\\nset of initial states, F ⊆ Q the set of ﬁnal states, E a ﬁnite multiset of transitions\\nelements of Q ×(Σ ∪{ϵ}) ×(Δ ∪{ϵ}) × R ×Q,a n dρ: F → R a ﬁnal weight function\\nmapping F to R.T h esize of transducer T is the sum of its number of states and\\ntransitions and is denoted by |T |.2\\nThus, weighted transducers are ﬁnite automata in which each transition is labeled\\nwith both an input and an output label and carries some real-valued weight.\\nFigure 5.3 shows an example of a weighted ﬁnite-state transducer. In this ﬁgure,\\nthe input and output labels of a transition are separated by a colon delimiter, and\\nthe weight is indicated after the slash separator. The initial states are represented\\nby a bold circle and ﬁnal states by double circles. The ﬁnal weight ρ[q]a taﬁ n a l\\nstate q is displayed after the slash.\\nThe input label of a path π is a string element of Σ\\n∗ obtained by concatenating\\ninput labels along π. Similarly, the output label of a path π is obtained by\\nconcatenating output labels along π. A path from an initial state to a ﬁnal state is\\nan accepting path. The weight of an accepting path is obtained by multiplying the\\nweights of its constituent transitions and the weight of the ﬁnal state of the path.\\nA weighted transducer deﬁnes a mapping from Σ ∗ × Δ ∗ to R.T h ew e i g h t\\nassociated by a weighted transducer T to a pair of strings ( x, y) ∈ Σ∗ × Δ ∗ is\\ndenoted by T(x, y) and is obtained by summing the weights of all accepting paths\\n2. A multiset in the deﬁnition of the transitions is used to allow for the presence of several\\ntransitions from a state p to a state q with the same input and output label, and even the\\nsame weight, which may occur as a result of various operations.108 Kernel Methods\\nwith input label x and output label y. For example, the transducer of ﬁgure 5.3\\nassociates to the pair (aab, baa)t h ew e i g h t3× 1 × 4 × 2+3 × 2 × 3 × 2, since there\\nis a path with input label aab and output label baa and weight 3 × 1 × 4 × 2, and\\nanother one with weight 3 × 2 × 3 × 2.\\nThe sum of the weights of all accepting paths of an acyclic transducer, that\\nis a transducer T with no cycle, can be computed in linear time, that is O(|T |),\\nusing a general shortest-distance or forward-backward algorithm. These are simple\\nalgorithms, but a detailed description would require too much of a digression from\\nthe main topic of this chapter.\\nComposition An important operation for weighted transducers is composition,\\nwhich can be used to combine two or more weighted transducers to form more\\ncomplex weighted transducers. As we shall see, this operation is useful for the\\ncreation and computation of sequence kernels. Its deﬁnition follows that of com-\\nposition of relations. Given two weighted transducersT\\n1 =( Σ, Δ ,Q1,I1,F1,E1,ρ1)\\nand T2 =( Δ , Ω,Q2,I2,F2,E2,ρ2), the result of the composition of T1 and T2 is a\\nweighted transducer denoted by T1 ◦T2 and deﬁned for all x ∈ Σ∗ and y ∈ Ω∗ by\\n(T1 ◦T2)(x, y)=\\n∑\\nz∈Δ ∗\\nT1(x, z) · T2(z,y ), (5.19)\\nwhere the sum runs over all strings z over the alphabet Δ. Thus, composition is\\nsimilar to matrix multiplication with inﬁnite matrices.\\nThere exists a general and eﬃcient algorithm to compute the composition of two\\nweighted transducers. In the absence of ϵs on the input side of T1 or the output\\nside of T2, the states of T1 ◦T2 =( Σ, Δ ,Q ,I,F,E,ρ ) can be identiﬁed with pairs\\nmade of a state of T1 and a state of T2, Q ⊆ Q1 × Q2. Initial states are those\\nobtained by pairing initial states of the original transducers, I = I1 × I2,a n d\\nsimilarly ﬁnal states are deﬁned by F = Q ∩ (F1 × F2). The ﬁnal weight at a state\\n(q1,q2) ∈ F1 × F2 is ρ(q)= ρ1(q1)ρ2(q2), that is the product of the ﬁnal weights at\\nq1 and q2. Transitions are obtained by matching a transition of T1 with one of T2\\nfrom appropriate transitions of T1 and T2:\\nE =\\n⨄\\n(q1,a,b,w1,q2)∈E1\\n(q′\\n1,b,c,w2,q′\\n2)∈E2\\n{(\\n(q1,q ′\\n1),a ,c ,w1 ⊗ w2,(q2,q ′\\n2)\\n⎡}\\n.\\nHere, ⊎ denotes the standard join operation of multisets as in {1, 2}⊎{ 1, 3} =\\n{1,1,2, 3}, to preserve the multiplicity of the transitions.\\nIn the worst case, all transitions of T1 leaving a state q1 match all those of T2\\nleaving state q′\\n1, thus the space and time complexity of composition is quadratic:\\nO(|T1||T2|). In practice, such cases are rare and composition is very eﬃcient.\\nFigure 5.4 illustrates the algorithm in a particular case.5.5 Sequence kernels 109\\n0\\n1\\na:b/0.1\\na:b/0.2\\n2\\nb:b/0.3 3/0.7\\nb:b/0.4\\n          a:b/0.5\\na:a/0.6\\n0\\n1\\nb:b/0.1\\nb:a/0.2\\n2\\na:b/0.3 3/0.6\\na:b/0.4\\nb:a/0.5\\n(a) (b)\\n(0, 0) (1, 1)a:b/.01\\n(0, 1)\\na:a/.04\\n(2, 1)\\nb:a/.06 (3, 1)\\nb:a/.08\\na:a/.02\\na:a/0.1\\n(3, 2)\\na:b/.18\\n(3, 3)a:b/.24\\n(c)\\nFigure 5.4 (a) Weighted transducer T1. (b) Weighted transducer T2. (c) Result\\nof composition of T1 and T2, T1 ◦T2. Some states might be constructed during the\\nexecution of the algorithm that are not co-accessible, that is, they do not admit a\\npath to a ﬁnal state, e.g., (3, 2). Such states and the related transitions (in red) can\\nbe removed by a trimming (or connection) algorithm in linear time.\\nAs illustrated by ﬁgure 5.5, when T1 admits output ϵ labels or T2 input ϵ labels,\\nthe algorithm just described may create redundant ϵ-paths, which would lead to\\nan incorrect result. The weight of the matching paths of the original transducers\\nwould be counted p times, where p is the number of redundant paths in the result\\nof composition. To avoid with this problem, all but oneϵ-path must be ﬁltered out\\nof the composite transducer. Figure 5.5 indicates in boldface one possible choice for\\nthat path, which in this case is the shortest. Remarkably, that ﬁltering mechanism\\nitself can be encoded as a ﬁnite-state transducer F (ﬁgure 5.5b).\\nT oa p p l yt h a tﬁ l t e r ,w en e e dt oﬁ r s ta u g m e n tT\\n1 and T2 with auxiliary symbols\\nthat make the semantics ofϵ explicit: let ˜T1 ( ˜T2) be the weighted transducer obtained\\nfrom T1 (respectively T2) by replacing the output (respectively input)ϵ labels with\\nϵ2 (respectively ϵ1) as illustrated by ﬁgure 5.5. Thus, matching with the symbol ϵ1\\ncorresponds to remaining at the same state ofT1 and taking a transition ofT2 with\\ninput ϵ. ϵ2 can be described in a symmetric way. The ﬁlter transducerF disallows a\\nmatching (ϵ2,ϵ2) immediately after (ϵ1,ϵ1) since this can be done instead via (ϵ2,ϵ1).110 Kernel Methods\\n/g1 /g2/g7/g6/g7 /g3/g8/g6/g1 /g4/g9/g6/g1 /g5/g10/g6/g10 /g1 /g2/g6/g5/g7 /g3/g1/g1/g8 /g4/g7/g5/g6\\nT1 T2\\n/g1\\n/g1/g3/g1/g1/g1/g3/g1/g1\\n/g2/g7/g6/g7\\n/g1/g3/g1/g1/g1/g3/g1/g1\\n/g3/g8/g6/g1/g2\\n/g1/g3/g1/g1/g1/g3/g1/g1\\n/g4/g9/g6/g1/g2\\n/g1/g3/g1/g1/g1/g3/g1/g1\\n/g5/g10/g6/g10\\n/g1/g3/g1/g1/g1/g3/g1/g1\\n/g2\\n/g1/g3/g4/g1\\n/g3/g7/g6/g8\\n/g1/g3/g4/g1\\n/g4/g1/g2/g4/g1/g1/g1/g1/g1/g1/g1/g9\\n/g1/g3/g4/g1\\n/g5/g8/g6/g7\\n/g1/g3/g4/g1\\n˜T1 ˜T2\\n/g4/g3/g4 /g5/g3/g5 /g5/g3/g6\\n/g6/g3/g5 /g6/g3/g6\\n/g7/g3/g5 /g7/g3/g6\\n/g8/g3/g7\\n/g10/g9/g13 /g1/g9/g14\\n/g11/g9/g1\\n/g12/g9/g1\\n/g11/g9/g1\\n/g12/g9/g1\\n/g1/g9/g14\\n/g1/g9/g14\\n/g13/g9/g10\\n/g11/g9/g14\\n/g1/g15/g9/g15/g2 /g1/g1/g2/g4/g1/g2/g2\\n/g1/g1/g2/g4/g1/g2/g2\\n/g1/g1/g2/g4/g1/g2/g2\\n/g1/g1/g3/g4/g1/g3/g2/g1/g1/g3/g4/g1/g3/g2\\n/g1/g1/g3/g4/g1/g3/g2 /g1/g1/g3/g4/g1/g3/g2\\n/g1/g15/g9/g15/g2\\n/g1/g1/g3/g4/g1/g2/g1\\n/g1\\n/g5/g4/g5\\n/g1/g2/g3/g1/g1 /g2/g1/g1/g3/g1/g1\\n/g3\\n/g1/g2/g3/g1/g2\\n/g5/g4/g5\\n/g1/g1/g3/g1/g1\\n/g5/g4/g5\\n/g1/g2/g3/g1/g2\\n(a) (b)\\nFigure 5.5 Redundant ϵ-paths in composition. All transition and ﬁnal weights are\\nequal to one. (a) A straightforward generalization of theϵ-free case would generate\\nall the paths from(1, 1) to (3, 2) when composing T1 and T2 and produce an incorrect\\nresults in non-idempotent semirings. (b) Filter transducer F. The shorthand x is\\nused to represent an element of Σ.\\nBy symmetry, it also disallows a matching (ϵ1,ϵ1) immediately after (ϵ2,ϵ2). In the\\ns a m ew a y ,am a t c h i n g(ϵ1,ϵ1) immediately followed by ( ϵ2,ϵ1) is not permitted\\nby the ﬁlter F since a path via the matchings ( ϵ2,ϵ1)(ϵ1,ϵ1) is possible. Similarly,\\n(ϵ2,ϵ2)(ϵ2,ϵ1) is ruled out. It is not hard to verify that the ﬁlter transducer F is\\nprecisely a ﬁnite automaton over pairs accepting the complement of the language\\nL = σ∗((ϵ1,ϵ1)(ϵ2,ϵ2)+( ϵ2,ϵ2)(ϵ1,ϵ1)+( ϵ1,ϵ1)(ϵ2,ϵ1)+( ϵ2,ϵ2)(ϵ2,ϵ1))σ∗,\\nwhere σ = {(ϵ1,ϵ1), (ϵ2,ϵ2), (ϵ2,ϵ1),x }.T h u s ,t h eﬁ l t e rF guarantees that exactly\\none ϵ-path is allowed in the composition of each ϵ sequences. To obtain the correct\\nresult of composition, it suﬃces then to use theϵ-free composition algorithm already\\ndescribed and compute\\n˜T1 ◦F ◦ ˜T2. (5.20)\\nIndeed, the two compositions in ˜T1 ◦F ◦ ˜T2 no longer involve ϵs. Since the size of\\nthe ﬁlter transducer F is constant, the complexity of general composition is the5.5 Sequence kernels 111\\nsame as that of ϵ-free composition, that is O(|T1||T2|). In practice, the augmented\\ntransducers ˜T1 and ˜T2 are not explicitly constructed, instead the presence of the\\nauxiliary symbols is simulated. Further ﬁlter optimizations help limit the number of\\nnon-coaccessible states created, for example, by examining more carefully the case\\nof states with only outgoing non-ϵ-transitions or only outgoing ϵ-transitions.\\n5.5.2 Rational kernels\\nThe following establishes a general framework for the deﬁnition of sequence kernels.\\nDeﬁnition 5.5 Rational kernels\\nA kernel K :Σ\\n∗ × Σ∗ → R is said to be rational if it coincides with the mapping\\ndeﬁned by some weighted transducer U: ∀x, y ∈ Σ∗,K (x, y)= U(x, y).\\nNote that we could have instead adopted a more general deﬁnition: instead of using\\nweighted transducers, we could have used more powerful sequence mappings such\\nas algebraic transductions, which are the functional counterparts of context-free\\nlanguages, or even more powerful ones. However, an essential need for kernels is\\nan eﬃcient computation, and more complex deﬁnitions would lead to substantially\\nmore costly computational complexities for kernel computation. For rational kernels,\\nthere exists a general and eﬃcient computation algorithm.\\nComputation We will assume that the transducer U deﬁning a rational kernel\\nK does not admit any ϵ-cycle with non-zero weight, otherwise the kernel value is\\ninﬁnite for all pairs. For any sequence x,l e tT\\nx denote a weighted transducer with\\njust one accepting path whose input and output labels are both x and its weight\\nequal to one. Tx can be straightforwardly constructed from x in linear time O(|x|).\\nThen, for any x, y ∈ Σ∗, U(x, y) can be computed by the following two steps:\\n1. Compute V = Tx ◦U ◦Ty using the composition algorithm in timeO(|U ||Tx||Ty |).\\n2. Compute the sum of the weights of all accepting paths of V using a general\\nshortest-distance algorithm in time O(|V |).\\nBy deﬁnition of composition, V is a weighted transducer whose accepting paths are\\nprecisely those accepting paths of U that have input label x and output label y.\\nT h es e c o n ds t e pc o m p u t e st h es u mo ft h ew e i g h t so ft h e s ep a t h s ,t h a ti s ,e x a c t l y\\nU(x, y). Since U admits no ϵ-cycle, V is acyclic, and this step can be performed in\\nlinear time. The overall complexity of the algorithm for computing U(x, y)i st h e n\\nin O(|U ||Tx||Ty |). Since U is ﬁxed for a rational kernel K and |Tx| = O(|x|) for any\\nx, this shows that the kernel values can be obtained in quadratic time O(|x||y|).\\nFor some speciﬁc weighted transducers U, the computation can be more eﬃcient,\\nfor example in O(|x| + |y|)( s e ee x e r c i s e5 . 1 7 ) .112 Kernel Methods\\nPDS rational kernels For any transducer T,l e tT −1 denote the inverse of T,\\nthat is the transducer obtained from T by swapping the input and output labels of\\nevery transition. For all x, y,w eh a v eT −1(x, y)= T(y,x ). The following theorem\\ngives a general method for constructing a PDS rational kernel from an arbitrary\\nweighted transducer.\\nTheorem 5.9\\nFor any weighted transducerT =( Σ, Δ ,Q ,I,F,E,ρ ), the function K = T ◦T\\n−1 is\\na PDS rational kernel.\\nProof By deﬁnition of composition and the inverse operation, for all x, y ∈ Σ∗,\\nK(x, y)=\\n∑\\nz∈Δ ∗\\nT(x, z) T(y,z ).\\nK is the pointwise limit of the kernel sequence (Kn)n≥0 deﬁned by:\\n∀n ∈ N, ∀x, y ∈ Σ∗,K n(x, y)=\\n∑\\n|z|≤n\\nT(x, z) T(y,z ),\\nwhere the sum runs over all sequences in Δ ∗ of length at most n. Kn is PDS\\nsince its corresponding kernel matrix Kn for any sample ( x1,...,x m)i sS P S D .\\nThis can be see form the fact that Kn c a nb ew r i t t e na sKn = AA⊤ with\\nA =( Kn(xi,z j))i∈[1,m],j∈[1,N],w h e r ez1,...,z N is some arbitrary enumeration of\\nthe set of strings in Σ ∗ with length at most n.T h u s ,K is PDS as the pointwise\\nlimit of the sequence of PDS kernels (Kn)n∈N.\\nThe sequence kernels commonly used in computational biology, natural language\\nprocessing, computer vision, and other applications are all special instances of\\nrational kernels of the formT ◦T −1. All of these kernels can be computed eﬃciently\\nusing the same general algorithm for the computational of rational kernels presented\\nin the previous paragraph. Since the transducer U = T ◦T\\n−1 deﬁning such PDS\\nrational kernels has a speciﬁc form, there are diﬀerent options for the computation\\nof the composition T\\nx ◦U ◦Ty:\\ncompute U = T ◦T −1 ﬁrst, then V = Tx ◦U ◦Ty;\\ncompute V1 = Tx ◦T and V2 = Ty ◦T ﬁrst, then V = V1 ◦V −1\\n2 ;\\ncompute ﬁrst V1 = Tx ◦T,t h e nV2 = V1 ◦T −1,t h e nV = V2 ◦Ty,o rt h es i m i l a r\\nseries of operations with x and y permuted.\\nAll of these methods lead to the same result after computation of the sum of the\\nweights of all accepting paths, and they all have the same worst-case complexity.\\nHowever, in practice, due to the sparsity of intermediate compositions, there may\\nbe substantial diﬀerences between their time and space computational costs. An5.5 Sequence kernels 113\\n0\\na:ε/1\\nb:ε/1\\n1a:a/1\\nb:b/1 2/1a:a/1\\nb:b/1\\na:ε/1\\nb:ε/1\\n0\\na:ε/1\\nb:ε/1\\n1a:a/1\\nb:b/1\\na:ε/λ\\nb:ε/λ\\n2/1a:a/1\\nb:b/1\\na:ε/1\\nb:ε/1\\n(a) (b)\\nFigure 5.6 (a) TransducerTbigram deﬁning the bigram kernelTbigram ◦T − 1\\nbigram for Σ=\\n{a, b}. (b) Transducer Tgappy bigram deﬁning the gappy bigram kernel Tgappy bigram ◦\\nT − 1\\ngappy bigram with gap penalty λ ∈ (0, 1).\\nalternative method based on an n-way composition can further lead to signiﬁcantly\\nmore eﬃcient computations.\\nExample 5.5 Bigram and gappy bigram sequence kernels\\nFigure 5.6a shows a weighted transducer Tbigram deﬁning a common sequence\\nkernel, the bigram sequence kernel , for the speciﬁc case of an alphabet reduced\\nto Σ = {a, b}. The bigram kernel associates to any two sequences x and y the sum\\nof the product of the counts of all bigrams inx and y. For any sequencex ∈ Σ∗ and\\nany bigram z ∈{ aa, ab, ba, bb}, Tbigram(x, z) is exactly the number of occurrences\\nof the bigram z in x. Thus, by deﬁnition of composition and the inverse operation,\\nTbigram ◦T −1\\nbigram computes exactly the bigram kernel.\\nFigure 5.6b shows a weighted transducerTgappy bigram deﬁning the so-called gappy\\nbigram kernel. The gappy bigram kernel associates to any two sequences x and y\\nthe sum of the product of the counts of all gappy bigrams in x and y penalized\\nb yt h el e n g t ho ft h e i rgaps. Gappy bigrams are sequences of the form aua, aub,\\nbua,o r bub,w h e r eu ∈ Σ∗ is called the gap. The count of a gappy bigram is\\nmultiplied by |u|λ for some ﬁxed λ ∈ (0, 1) so that gappy bigrams with longer\\ngaps contribute less to the deﬁnition of the similarity measure. While this deﬁnition\\ncould appear to be somewhat complex, ﬁgure 5.6 shows that Tgappy bigram can be\\nstraightforwardly derived from Tbigram. The graphical representation of rational\\nkernels helps understanding or modifying their deﬁnition.\\nCounting transducers The deﬁnition of most sequence kernels is based on the\\ncounts of some common patterns appearing in the sequences. In the examples\\njust examined, these were bigrams or gappy bigrams. There exists a simple and\\ngeneral method for constructing a weighted transducer counting the number of\\noccurrences of patterns and using them to deﬁne PDS rational kernels. Let X be\\na ﬁnite automaton representing the set of patterns to count. In the case of bigram\\nkernels with Σ = {a, b}, X would be an automaton accepting exactly the set of\\nstrings {aa, ab, ba, bb}. Then, the weighted transducer of ﬁgure 5.7 can be used to\\ncompute exactly the number of occurrences of each pattern accepted by X.114 Kernel Methods\\n0\\na:ε/1\\nb:ε/1\\n1/1X:X/1\\na:ε/1\\nb:ε/1\\nFigure 5.7 Counting transducer Tcount for Σ= {a, b}. The “transition” X : X/1\\nstands for the weighted transducer created from the automaton X by adding to\\neach transition an output label identical to the existing label, and by making all\\ntransition and ﬁnal weights equal to one.\\nTheorem 5.10\\nFor any x ∈ Σ\\n∗ and any sequence z accepted by X, Tcount(x, z) is the number of\\noccurrences ofz in x.\\nProof Let x ∈ Σ∗ be an arbitrary sequence and let z be a sequence accepted by\\nX. Since all accepting paths of Tcount have weight one, Tcount(x, z) is equal to the\\nnumber of accepting paths in Tcount with input label x and output z.\\nNow, an accepting pathπ in Tcount with input x and output z can be decomposed\\nas π = π0 π01 π1,w h e r eπ0 is a path through the loops of state 0 with input label\\nsome preﬁx x0 of x and output label ϵ, π01 an accepting path from 0 to 1 with input\\nand output labels equal to z,a n dπ1 a path through the self-loops of state 1 with\\ninput label a suﬃx x1 of x and output ϵ. Thus, the number of such paths is exactly\\nthe number of distinct ways in which we can write sequencex as x = x0zx1,w h i c h\\nis exactly the number of occurrences of z in x.\\nThe theorem provides a very general method for constructing PDS rational kernels\\nTcount ◦T −1\\ncount that are based on counts of some patterns that can be deﬁned\\nvia a ﬁnite automaton, or equivalently a regular expression. Figure 5.7 shows the\\ntransducer for the case of an input alphabet reduced to Σ = {a, b}. The general\\ncase can be obtained straightforwardly by augmenting states 0 and 1 with other\\nself-loops using other symbols than a and b. In practice, a lazy evaluation can be\\nused to avoid the explicit creation of these transitions for all alphabet symbols and\\ninstead creating them on-demand based on the symbols found in the input sequence\\nx. Finally, one can assign diﬀerent weights to the patterns counted to emphasize\\nor deemphasize some, as in the case of gappy bigrams. This can be done simply by\\nchanging the transitions weight or ﬁnal weights of the automaton X used in the\\ndeﬁnition of T\\ncount.5.6 Chapter notes 115\\n5.6 Chapter notes\\nThe mathematical theory of PDS kernels in a general setting originated with the\\nfundamental work of Mercer [1909] who also proved the equivalence of a condition\\nsimilar to that of theorem 5.1 for continuous kernels with the PDS property. The\\nconnection between PDS and NDS kernels, in particular theorems 5.8 and 5.7,\\nare due to Schoenberg [1938]. A systematic treatment of the theory of reproducing\\nkernel Hilbert spaces was presented in a long and elegant paper by Aronszajn [1950].\\nFor an excellent mathematical presentation of PDS kernels and positive deﬁnite\\nfunctions we refer the reader to Berg, Christensen, and Ressel [1984], which is also\\nthe source of several of the exercises given in this chapter.\\nThe fact that SVMs could be extended by using PDS kernels was pointed out\\nby Boser, Guyon, and Vapnik [1992]. The idea of kernel methods has been since\\nthen widely adopted in machine learning and applied in a variety of diﬀerent tasks\\nand settings. The following two books are in fact speciﬁcally devoted to the study\\nof kernel methods: Sch¨olkopf and Smola [2002] and Shawe-Taylor and Cristianini\\n[2004]. The classical representer theorem is due to Kimeldorf and Wahba [1971].\\nA generalization to non-quadratic cost functions was stated by Wahba [1990]. The\\ngeneral form presented in this chapter was given by Sch¨ olkopf, Herbrich, Smola,\\nand Williamson [2000].\\nRational kernels were introduced by Cortes, Haﬀner, and Mohri [2004]. A general\\nclass of kernels, convolution kernels, was earlier introduced by Haussler [1999]. The\\nconvolution kernels for sequences described by Haussler [1999], as well as the pair-\\nHMM string kernels described by Watkins [1999], are special instances of rational\\nkernels. Rational kernels can be straightforwardly extended to deﬁne kernels for\\nﬁnite automata and even weighted automata [Cortes et al., 2004]. Cortes, Mohri,\\nand Rostamizadeh [2008b] study the problem of learning rational kernels such as\\nthose based on counting transducers.\\nThe composition of weighted transducers and the ﬁlter transducers in the presence\\nof ϵ-paths are described in Pereira and Riley [1997], Mohri, Pereira, and Riley [2005],\\nand Mohri [2009]. Composition can be further generalized to theN-way composition\\nof weighted transducers [Allauzen and Mohri, 2009]. N-way composition of three\\nor more transducers can substantially speed up computation, in particular for PDS\\nrational kernels of the formT ◦T\\n−1. A genericshortest-distance algorithm which can\\nbe used with a large class of semirings and arbitrary queue disciplines is described by\\nMohri [2002]. A speciﬁc instance of that algorithm can be used to compute the sum\\nof the weights of all paths as needed for the computation of rational kernels after\\ncomposition. For a study of the class of languages linearly separable with rational\\nkernels , see Cortes, Kontorovich, and Mohri [2007a].116 Kernel Methods\\n5.7 Exercises\\n5.1 Let K : X× X → R be a PDS kernel, and let α: X→ R be a positive function.\\nShow that the kernel K′ deﬁned for all x, y ∈X by K′(x, y)= K(x,y)\\nα(x)α(y) is a PDS\\nkernel.\\n5.2 Show that the following kernels K are PDS:\\n(a) K(x, y)=c o s (x − y)o v e rR × R.\\n(b) K(x, y)=c o s (x2 − y2)o v e rR × R.\\n(c) K(x, y)=( x + y)−1 over (0, +∞ ) × (0, +∞ ).\\n(d) K(x,x′)=c o s∠(x,x′)o v e rRn × Rn,w h e r e∠(x,x′) is the angle between\\nx and x′.\\n(e) ∀λ> 0,K (x, x′)=e x p\\n(\\n− λ[sin(x′ − x)]2⎡\\nover R × R.( Hint:r e w r i t e\\n[sin(x′ − x)]2 as the square of the norm of the diﬀerence of two vectors.)\\n5.3 Show that the following kernels K are NDS:\\n(a) K(x, y) = [sin(x − y)]2 over R × R.\\n(b) K(x, y)=l o g (x + y)o v e r( 0,+∞ ) × (0, +∞ ).\\n5.4 Deﬁne a diﬀerence kernel as K(x, x′)= |x − x′| for x, x′ ∈ R. Show that this\\nkernel is not positive deﬁnite symmetric (PDS).\\n5.5 Is the kernel K deﬁned over Rn × Rn by K(x,y)= ∥x − y∥3/2 PDS? Is it NDS?\\n5.6 Let H be a Hilbert space with the corresponding dot product ⟨·, ·⟩. Show that\\nthe kernel K deﬁned over H × H by K(x, y)=1 −⟨ x, y⟩ is negative deﬁnite.\\n5.7 For any p> 0, let Kp be the kernel deﬁned over R+ × R+ by\\nKp(x, y)= e−(x+y)p\\n. (5.21)\\nShow that Kp is positive deﬁnite symmetric (PDS) iﬀ p ≤ 1. (Hint: you can use the\\nfact that if K is NDS, then for any 0 <α ≤ 1, Kα is also NDS.)\\n5.8 Explicit mappings.\\n(a) Denote a data set x1,...,x m and a kernel K(xi,xj) with a Gram matrix\\nK.A s s u m i n gK is positive semideﬁnite, then give a map Φ( ·) such that5.7 Exercises 117\\nK(xi,xj)= ⟨Φ(xi), Φ(xj)⟩.\\n(b) Show the converse of the previous statement, i.e., if there exists a mapping\\nΦ(x) from input space to some Hilbert space, then the corresponding matrix\\nK is positive semideﬁnite.\\n5.9 Explicit polynomial kernel mapping. Let K be a polynomial kernel of degree d,\\ni.e., K : RN ×RN → R, K(x,x′)=( x·x′ +c)d,w i t hc> 0, Show that the dimension\\nof the feature space associated to K is\\n(N + d\\nd\\n⎡\\n. (5.22)\\nWrite K in terms of kernels ki :( x,x′) ↦→ (x · x′)i, i ∈ [0,d ]. What is the weight\\nassigned to each ki in that expression? How does it vary as a function of c?\\n5.10 High-dimensional mapping. Let Φ: X→ H b eaf e a t u r em a p p i n gs u c ht h a t\\nthe dimension N of H is very large and letK : X× X → R be a PDS kernel deﬁned\\nby\\nK(x, x′)= E\\ni∼D\\n[\\n[Φ(x)]i[Φ(x′)]i\\n]\\n, (5.23)\\nwhere [Φ(x)]i is the ith component of Φ( x) (and similarly for Φ ′(x)) and where\\nD is a distribution over the indices i. We shall assume that |[Φ(x)]i|≤ R for all\\nx ∈X and i ∈ [1,N ]. Suppose that the only method available to compute K(x, x′)\\ninvolved direct computation of the inner product (5.23), which would requireO(N)\\ntime. Alternatively, an approximation can be computed based on random selection\\nof a subset I of the N components of Φ(x)a n dΦ (x\\n′)a c c o r d i n gt oD,t h a ti s :\\nK′(x, x′)= 1\\nn\\n∑\\ni∈I\\nD(i)[Φ(x)]i[Φ(x′)]i, (5.24)\\nwhere |I| = n.\\n(a) Fix x and x′ in X.P r o v et h a t\\nPr\\nI∼Dn\\n[|K(x, x′) − K′(x, x′)| >ϵ ] ≤ 2e\\n− nϵ2\\n2r2 . (5.25)\\n(Hint: use McDiarmid’s inequality).\\n(b) Let K and K′ b et h ek e r n e lm a t r i c e sa s s o c i a t e dt oK and K′.S h o w\\nthat for any ϵ, δ >0, for n> r2\\nϵ2 log m(m+1)\\nδ , with probability at least 1 − δ,\\n|K′\\nij − Kij |≤ ϵ for all i, j ∈ [1,m].\\n5.11 Classiﬁer based kernel. Let S b eat r a i n i n gs a m p l eo fs i z em. Assume that118 Kernel Methods\\nS has been generated according to some probability distribution D(x, y), where\\n(x, y) ∈ X ×{ −1, +1}.\\n(a) Deﬁne the Bayes classiﬁer h∗: X →{ − 1, +1}. Show that the kernel K∗\\ndeﬁned by K∗(x, x′)= h∗(x)h∗(x′) for any x, x′ ∈ X is positive deﬁnite\\nsymmetric. What is the dimension of the natural feature space associated to\\nK∗?\\n(b) Give the expression of the solution obtained using SVMs with this kernel.\\nWhat is the number of support vectors? What is the value of the margin? What\\nis the generalization error of the solution obtained? Under what condition are\\nthe data linearly separable?\\n(c) Let h : X → R be an arbitrary real-valued function. Under what condition\\non h is the kernel K deﬁned by K(x, x′)= h(x)h(x′), x, x′ ∈ X, positive\\ndeﬁnite symmetric?\\n5.12 Image classiﬁcation kernel. For α ≥ 0, the kernel\\nKα :( x,x′) ↦→\\nN∑\\nk=1\\nmin(|xk|α, |x′\\nk|α) (5.26)\\nover RN × RN is used in image classiﬁcation. Show that Kα is PDS for all α ≥ 0.\\nTo do so, proceed as follows.\\n( a ) U s et h ef a c tt h a t(f,g ) ↦→\\n∫ +∞\\nt=0 f(t)g(t)dt is an inner product over the set\\nof measurable functions over [0 , +∞ ) to show that ( x, x′) ↦→ min(x, x′)i sa\\nPDS kernel. (Hint: associate an indicator function tox and another one to x′.)\\n(b) Use the result from (a) to ﬁrst show that K1 is PDS and similarly thatKα\\nwith other values of α is also PDS.\\n5.13 Fraud detection. To prevent fraud, a credit-card company decides to contact\\nProfessor Villebanque and provides him with a random list of several thousand\\nfraudulent and non-fraudulent events. There are many diﬀerent types of events,\\ne.g., transactions of various amounts, changes of address or card-holder information,\\nor requests for a new card. Professor Villebanque decides to use SVMs with an\\nappropriate kernel to help predict fraudulent events accurately. It is diﬃcult for\\nProfessor Villebanque to deﬁne relevant features for such a diverse set of events.\\nHowever, the risk department of his company has created a complicated method to\\nestimate a probability Pr[U] for any event U. Thus, Professor Villebanque decides\\nto make use of that information and comes up with the following kernel deﬁned5.7 Exercises 119\\nover all pairs of events (U, V):\\nK(U, V)=P r [U ∧ V ] − Pr[U]P r [V ]. (5.27)\\nHelp Professor Villebanque show that his kernel is positive deﬁnite symmetric.\\n5.14 Relationship between NDS and PDS kernels. Prove the statement of theo-\\nrem 5.7. (Hint: Use the fact that if K is PDS then exp(K) is also PDS, along with\\ntheorem 5.6.)\\n5.15 Metrics and Kernels. Let X be a non-empty set and K : X× X → R be a\\nnegative deﬁnite symmetric kernel such that K(x, x) = 0 for all x ∈X .\\n(a) Show that there exists a Hilbert space H and a mapping Φ(x)f r o mX to\\nH such that:\\nK(x, y)= ||Φ(x) − Φ(x′)||2 .\\nAssume that K(x, x′)=0 ⇒ x = x′. Use theorem 5.6 to show that\\n√\\nK deﬁnes\\nam e t r i co nX.\\n(b) Use this result to prove that the kernelK(x, y)=e x p (−|x−x′|p), x, x′ ∈ R,\\nis not positive deﬁnite for p> 2.\\n(c) The kernel K(x, x′)=t a n h (a(x·x′)+b) was shown to be equivalent to a two-\\nlayer neural network when combined with SVMs. Show that K is not positive\\ndeﬁnite if a< 0o r b< 0. What can you conclude about the corresponding\\nneural network when a< 0o r b< 0?\\n5.16 Sequence kernels. Let X = {a, c, g, t}. To classify DNA sequences using SVMs,\\nwe wish to deﬁne a kernel between sequences deﬁned over X. We are given a ﬁnite\\nset I ⊂ X∗ of non-coding regions (introns). For x ∈ X∗, denote by |x| the length\\nof x and by F(x)t h es e to ff a c t o r so fx, i.e., the set of subsequences of x with\\ncontiguous symbols. For any two stringsx, y ∈ X∗ deﬁne K(x, y)b y\\nK(x, y)=\\n∑\\nz ∈(F(x)∩F(y))−I\\nρ|z|, (5.28)\\nwhere ρ ≥ 1 is a real number.\\n(a) Show that K is a rational kernel and that it is positive deﬁnite symmetric.\\n(b) Give the time and space complexity of the computation of K(x, y)w i t h\\nrespect to the size s of a minimal automaton representing X∗ − I.\\n(c) Long common factors between x and y of length greater than or equal to120 Kernel Methods\\nn are likely to be important coding regions (exons). Modify the kernel K to\\nassign weight ρ|z|\\n2 to z when |z|≥ n, ρ|z|\\n1 otherwise, where 1 ≤ ρ1 ≪ ρ2.S h o w\\nthat the resulting kernel is still positive deﬁnite symmetric.\\n5.17 n-gram kernel. Show that for all n ≥ 1, and any n-gram kernel Kn, Kn(x, y)\\nc a nb ec o m p u t e di nl i n e a rt i m eO(|x| + |y|), for all x, y ∈ Σ∗ assuming n and the\\nalphabet size are constants.\\n5.18 Mercer’s condition. Let X⊂ RN be a compact set and K : X× X → R a\\ncontinuous kernel function. Prove that ifK veriﬁes Mercer’s condition (theorem 5.1),\\nthen it is PDS. (Hint: assume that K is not PDS and consider a set{x1,...,x m}⊆\\nX and a column-vector c ∈ Rm×1 such that ∑m\\ni,j=1 cicjK(xi,xj) < 0.)6B o o s t i n g\\nEnsemble methods are general techniques in machine learning for combining several\\npredictors to create a more accurate one. This chapter studies an important family of\\nensemble methods known asboosting, and more speciﬁcally theAdaBoost algorithm.\\nThis algorithm has been shown to be very eﬀective in practice in some scenarios and\\nis based on a rich theoretical analysis. We ﬁrst introduce AdaBoost, show how it can\\nrapidly reduce the empirical error as a function of the number of rounds of boosting,\\nand point out its relationship with some known algorithms. Then we present a\\ntheoretical analysis of its generalization properties based on the VC-dimension of\\nits hypothesis set and based on a notion of margin that we will introduce. Much of\\nthat margin theory can be applied to other similar ensemble algorithms. A game-\\ntheoretic interpretation of AdaBoost further helps analyzing its properties. We end\\nwith a discussion of AdaBoost’s beneﬁts and drawbacks.\\n6.1 Introduction\\nIt is often diﬃcult, for a non-trivial learning task, to directly devise an accurate\\nalgorithm satisfying the strong PAC-learning requirements of chapter 2. But, there\\ncan be more hope for ﬁnding simple predictors guaranteed only to perform slightly\\nbetter than random. The following gives a formal deﬁnition of such weak learners.\\nDeﬁnition 6.1 Weak learning\\nA concept class C is said to be weakly PAC-learnable if there exists an algorithm\\nA, γ> 0, and a polynomial functionpoly(·, ·, ·, ·) such that for any ϵ> 0 and δ> 0,\\nfor all distributions D on X and for any target concept c ∈ C, the following holds\\nfor any sample size m ≥ poly(1/ϵ,1/δ, n,size(c)):\\nPr\\nS∼Dm\\n[\\nR(hS) ≤ 1\\n2 − γ\\n]\\n≥ 1 − δ. (6.1)\\nWhen such an algorithm A exists, it is called a weak learning algorithm for C or a\\nweak learner. The hypotheses returned by a weak learning algorithm are calledbase\\nclassiﬁers .122 Boosting\\nAdaBoost(S =( (x1,y1),..., (xm,y m)))\\n1 for i ← 1 to m do\\n2 D1(i) ← 1\\nm\\n3 for t ← 1 to T do\\n4 ht ← base classiﬁer in H with small error ϵt =P ri∼Dt [ht(xi) ̸= yi]\\n5 αt ← 1\\n2 log 1−ϵt\\nϵt\\n6 Zt ← 2[ϵt(1 − ϵt)]\\n1\\n2 ⊿ normalization factor\\n7 for i ← 1 to m do\\n8 Dt+1(i) ← Dt(i)e x p (−αtyiht(xi))\\nZt\\n9 g ← ∑T\\nt=1 αtht\\n10 return h = sgn(g)\\nFigure 6.1 AdaBoost algorithm for H ⊆{ − 1, +1}X .\\nThe key idea behind boosting techniques is to use a weak learning algorithm\\nto build a strong learner, that is, an accurate PAC-learning algorithm. To do so,\\nboosting techniques use an ensemble method: they combine diﬀerent base classiﬁers\\nreturned by a weak learner to create a more accurate predictor. But which base\\nclassiﬁers should be used and how should they be combined? The next section\\naddresses these questions by describing in detail one of the most prevalent and\\nsuccessful boosting algorithms, AdaBoost.\\n6.2 AdaBoost\\nWe denote by H the hypothesis set out of which the base classiﬁers are selected.\\nFigure 6.1 gives the pseudocode of AdaBoost in the case where the base classiﬁers\\nare functions mapping from X to {−1, +1},t h u sH ⊆{ − 1, +1}X .\\nThe algorithm takes as input a labeled sample S =( (x1,y1),..., (xm,y m)), with\\n(xi,y i) ∈X× { − 1, +1} for all i ∈ [1,m], and maintains a distribution over the\\nindices {1,...,m }. Initially (lines 1-2), the distribution is uniform ( D1). At each\\nround of boosting, that is each iterationt ∈ [1,T ] of the loop 3–8, a new base classiﬁer\\nht ∈ H is selected that minimizes the error on the training sample weighted by the6.2 AdaBoost 123\\nt = 1 t = 2 t = 3\\ndecision \\nboundary\\nupdated\\nweights\\n(a)\\n=α1 +α3+α2\\n(b)\\nFigure 6.2 Example of AdaBoost with axis-aligned hyperplanes as base learners.\\n(a) The top row shows decision boundaries at each boosting round. The bottom row\\nshows how weights are updated at each round, with incorrectly (resp., correctly)\\npoints given increased (resp., decreased) weights. (b) Visualization of ﬁnal classiﬁer,\\nconstructed as a linear combination of base learners.\\ndistribution D\\nt:\\nht ∈ argmin\\nh∈H\\nPr\\ni∼Dt\\n[ht(xi) ̸= yi] = argmin\\nh∈H\\nm∑\\ni=1\\nDt(i)1h(xi)̸=yi .\\nZt is simply a normalization factor to ensure that the weights Dt+1(i)s u mt oo n e .\\nThe precise reason for the deﬁnition of the coeﬃcient αt will become clear later. For\\nnow, observe that ifϵt, the error of the base classiﬁer, is less than 1/2, then 1−ϵt\\nϵt\\n> 1\\nand αt > 0. Thus, the new distribution Dt+1 is deﬁned from Dt by substantially\\nincreasing the weight on i if point xi is incorrectly classiﬁed (yiht(xi) < 0), and, on\\nthe contrary, decreasing it ifxi is correctly classiﬁed. This has the eﬀect of focusing\\nmore on the points incorrectly classiﬁed at the next round of boosting, less on those\\ncorrectly classiﬁed by h\\nt.124 Boosting\\nAfter T rounds of boosting, the classiﬁer returned by AdaBoost is based on the\\nsign of functiong, which is a linear combination of the base classiﬁersht.T h ew e i g h t\\nαt assigned to ht in that sum is a logarithmic function of the ratio of the accuracy\\n1 − ϵt and error ϵt of ht. Thus, more accurate base classiﬁers are assigned a larger\\nweight in that sum. Figure 6.2 illustrates the AdaBoost algorithm. The size of the\\npoints represents the distribution weight assigned to them at each boosting round.\\nFor anyt ∈ [1,T ], we will denote byg\\nt the linear combination of the base classiﬁers\\nafter t rounds of boosting: ft = ∑t\\ns=1 αtht.I np a r t i c u l a r ,w eh a v egT = g.T h e\\ndistribution Dt+1 can be expressed in terms ofgt and the normalization factors Zs,\\ns ∈ [1,t ], as follows:\\n∀i ∈ [1,m],D t+1(i)= e−yigt(xi)\\nm ∏t\\ns=1 Zs\\n. (6.2)\\nWe will make use of this identity several times in the proofs of the following sections.\\nIt can be shown straightforwardly by repeatedly expanding the deﬁnition of the\\ndistribution over the point xi:\\nDt+1(i)= Dt(i)e−αtyiht(xi)\\nZt\\n= Dt−1(i)e−αt− 1yiht− 1(xi)e−αtyiht(xi)\\nZt−1Zt\\n= e−yi\\nPt\\ns=1 αshs(xi)\\nm ∏t\\ns=1 Zs\\n.\\nThe AdaBoost algorithm can be generalized in several ways:\\ninstead of a hypothesis with minimal weighted error, ht can be more generally\\nthe base classiﬁer returned by a weak learning algorithm trained on Dt;\\nthe range of the base classiﬁers could be [ −1,+1], or more generally R.T h e\\ncoeﬃcients αt can then be diﬀerent and may not even admit a closed form. In\\ngeneral, they are chosen to minimize an upper bound on the empirical error, as\\ndiscussed in the next section. Of course, in that general case, the hypothesis ht are\\nnot binary classiﬁers , but the sign of their values could indicate the label, and their\\nmagnitude could be interpreted as a measure of conﬁdence.\\nIn the remainder of this section, we will further analyze the properties of Ad-\\naBoost and discuss its typical use in practice.\\n6.2.1 Bound on the empirical error\\nWe ﬁrst show that the empirical error of AdaBoost decreases exponentially fast as\\naf u n c t i o no ft h en u m b e ro fr o u n d so fb o o s t i n g .6.2 AdaBoost 125\\nTheorem 6.1\\nThe empirical error of the classiﬁer returned by AdaBoost veriﬁes:\\nˆR(h) ≤ exp\\n[\\n− 2\\nT∑\\nt=1\\n(1\\n2 − ϵt\\n⎡2]\\n. (6.3)\\nFurthermore, if for all t ∈ [1,T ], γ ≤ (1\\n2 − ϵt),t h e n\\nˆR(h) ≤ exp(−2γ2T) . (6.4)\\nProof Using the general inequality 1 u≤0 ≤ exp(−u) valid for all u ∈ R and\\nidentity 6.2, we can write:\\nˆR(h)= 1\\nm\\nm∑\\ni=1\\n1yig(xi)≤0 ≤ 1\\nm\\nm∑\\ni=1\\ne−yig(xi) = 1\\nm\\nm∑\\ni=1\\n[\\nm\\nT∏\\nt=1\\nZt\\n]\\nDT+1(i)=\\nT∏\\nt=1\\nZt.\\nSince, for all t ∈ [1,T ], Zt is a normalization factor, it can be expressed in terms of\\nϵt by:\\nZt =\\nm∑\\ni=1\\nDt(i)e−αtyiht(xi) =\\n∑\\ni:yiht(xi)=+1\\nDt(i)e−αt +\\n∑\\ni:yiht(xi)=−1\\nDt(i)eαt\\n=( 1 − ϵt)e−αt + ϵteαt\\n=( 1 − ϵt)\\n√ ϵt\\n1 − ϵt\\n+ ϵt\\n√\\n1 − ϵt\\nϵt\\n=2\\n√\\nϵt(1 − ϵt) .\\nThus, the product of the normalization factors can be expressed and upper bounded\\nas follows:\\nT∏\\nt=1\\nZt =\\nT∏\\nt=1\\n2\\n√\\nϵt(1 − ϵt)=\\nT∏\\nt=1\\n√\\n1 − 4\\n(1\\n2 − ϵt\\n⎡2\\n≤\\nT∏\\nt=1\\nexp\\n[\\n− 2\\n(1\\n2 − ϵt\\n⎡2]\\n=e x p\\n[\\n− 2\\nT∑\\nt=1\\n(1\\n2 − ϵt\\n⎡2]\\n,\\nwhere the inequality follows from the identity 1 − x ≤ e−x valid for all x ∈ R.\\nNote that the value of γ,w h i c hi sk n o w na st h eedge, and the accuracy of the base\\nclassiﬁers do not need to be known to the algorithm. The algorithm adapts to their\\naccuracy and deﬁnes a solution based on these values. This is the source of the\\nextended name of AdaBoost: adaptive boosting.\\nThe proof of theorem 6.1 reveals several other important properties. First, observe\\nthat α\\nt is the minimizer of the function g: α ↦→ (1 − ϵt)e−α + ϵteα. Indeed, g is126 Boosting\\n-4 -2 0 2 4\\n0\\n1\\n2\\n3\\n4\\n5\\ne−x\\n0–1 loss\\nx\\nloss function\\nFigure 6.3 Visualization of the zero-one loss (blue) and the convex and diﬀeren-\\ntiable upper bound on the zero-one loss (red) that is optimized by AdaBoost.\\nconvex and diﬀerentiable, and setting its derivative to zero yields:\\ng′(α)= −(1 − ϵt)e−α + ϵteα =0 ⇔ (1 − ϵt)e−α = ϵteα ⇔ α = 1\\n2 log 1 − ϵt\\nϵt\\n. (6.5)\\nThus, αt is chosen to minimizeZt = g(αt), and in light of the boundˆR(h) ≤ ∏T\\nt=1 Zt\\nshown in the proof, these coeﬃcients are selected to minimize an upper bound on\\nthe empirical error. In fact, for base classiﬁers whose range is [ −1, +1] or R, αt\\ncan be chosen in a similar fashion to minimize Zt, and this is the way AdaBoost is\\nextended to these more general cases.\\nObserve also that the equality (1 − ϵt)e−αt = ϵteαt just shown in (6.5) implies\\nthat at each iteration, AdaBoost assigns equal distribution mass to correctly and\\nincorrectly classiﬁed instances, since (1−ϵ\\nt)e−αt is the total distribution assigned to\\ncorrectly classiﬁed points and ϵteαt that of incorrectly classiﬁed ones. This may seem\\nto contradict the fact that AdaBoost increases the weights of incorrectly classiﬁed\\npoints and decreases that of others, but there is in fact no inconsistency: the reason\\nis that there are always fewer incorrectly classiﬁed points, since the base classiﬁer’s\\naccuracy is better than random.\\n6.2.2 Relationship with coordinate descent\\nAdaBoost was designed to address a novel theoretical question, that of designing a\\nstrong learning algorithm using a weak learning algorithm. We will show, however,\\nthat it coincides in fact with a very simple and classical algorithm, which consists\\nof applying a coordinate descent technique to a convex and diﬀerentiable objective\\nfunction. The objective function F for AdaBoost is deﬁned for all samples S =6.2 AdaBoost 127\\n((x1,y1),..., (xm,y m)) and α =( α1,...,α n) ∈ Rn, n ≥ 1, by\\nF(α)=\\nm∑\\ni=1\\ne−yign(xi) =\\nm∑\\ni=1\\ne−yi\\nPn\\nt=1 αtht(xi), (6.6)\\nwhere gn = ∑n\\nt=1 αtht. This function is an upper bound on the zero-one loss\\nfunction we wish to minimize, as shown in ﬁgure 6.3. Let et denote the unit vector\\ncorresponding to the tth coordinate in Rn and let αt−1 denote the vector based\\non the ( t − 1) ﬁrst coeﬃcients, i.e. αt−1 =( α1,...,α t−1, 0,..., 0)⊤ if t − 1 > 0,\\nαt−1 = 0 otherwise. At each iterationt ≥ 1, the direction et selected by coordinate\\ndescent is the one minimizing the directional derivative:\\net =a r g m i n\\nt\\ndF(αt−1 + ηet)\\ndη\\n⏐⏐\\n⏐\\n⏐\\nη=0\\n.\\nSince F(αt−1 + ηet)= ∑m\\ni=1 e−yi\\nPt− 1\\ns=1 αshs(xi)−yiηht(xi), the directional derivative\\nalong et can be expressed as follows:\\ndF(αt−1 + ηet)\\ndη\\n⏐⏐\\n⏐\\n⏐\\nη=0\\n= −\\nm∑\\ni=1\\nyiht(xi)e x p\\n[\\n− yi\\nt−1∑\\ns=1\\nαshs(xi)\\n]\\n= −\\nm∑\\ni=1\\nyiht(xi)Dt(i)\\n[\\nm\\nt−1∏\\ns=1\\nZs\\n]\\n= −\\n[ ∑\\ni:yiht(xi)=+1\\nDt(i) −\\n∑\\ni:yiht(xi)=−1\\nDt(i)\\n][\\nm\\nt−1∏\\ns=1\\nZs\\n]\\n= −[(1 − ϵt) − ϵt]\\n[\\nm\\nt−1∏\\ns=1\\nZs\\n]\\n=[ 2ϵt − 1]\\n[\\nm\\nt−1∏\\ns=1\\nZs\\n]\\n.\\nThe ﬁrst equality holds by diﬀerentiation and evaluation at η = 0, and the second\\none follows from (6.2). The third equality divides the sample set into points correctly\\nand incorrectly classiﬁed by ht, and the fourth equality uses the deﬁnition of ϵt.I n\\nview of the ﬁnal equality, since m ∏t−1\\ns=1 Zs is ﬁxed, the direction et selected by\\ncoordinate descent is the one minimizing ϵt, which corresponds exactly to the base\\nlearner ht selected by AdaBoost.\\nThe step size η is identiﬁed by setting the derivative to zero in order to minimize\\nthe function in the chosen direction et. Thus, using identity 6.2 and the deﬁnition128 Boosting\\nsquare lossboosting loss\\nlogistic loss\\nhinge loss\\nzero-one loss\\n-4 -2 0 2 4\\n0\\n2\\n4\\n6\\n8\\n10\\nx\\nloss function\\nx ↦→ (1 − x)2 1x≤1x ↦→ e−x\\nx ↦→ max(1 − x,0)\\nx ↦→ 1x<0\\nx ↦→ log2(1 +e−x)\\nFigure 6.4 Examples of several convex upper bounds on the zero-one loss.\\nof ϵt,w ec a nw r i t e :\\ndF(αt−1 + ηet)\\ndη =0 ⇔−\\nm∑\\ni=1\\nyiht(xi)e x p\\n[\\n− yi\\nt−1∑\\ns=1\\nαshs(xi)\\n]\\ne−ηyiht(xi) =0\\n⇔−\\nm∑\\ni=1\\nyiht(xi)Dt(i)\\n[\\nm\\nt−1∏\\ns=1\\nZs\\n]\\ne−yiht(xi)η =0\\n⇔−\\nm∑\\ni=1\\nyiht(xi)Dt(i)e−yiht(xi)η =0\\n⇔− [(1 − ϵt)e−η − ϵteη]=0\\n⇔ η= 1\\n2 log 1 − ϵt\\nϵt\\n.\\nThis proves that the step size chosen by coordinate descent matches the base\\nclassiﬁer weight αt of AdaBoost. Thus, coordinate descent applied to F precisely\\ncoincides with the AdaBoost algorithm.\\nIn light of this relationship, one may wish to consider similar applications of\\ncoordinate descent to other convex and diﬀerentiable functions ofα upper-bounding\\nthe zero-one loss. In particular, the logistic loss x ↦→ log2(1 + e−x)i sc o n v e xa n d\\ndiﬀerentiable and upper bounds the zero-one loss. Figure 6.4 shows other examples\\nof alternative convex loss functions upper-bounding the zero-one loss. Using the\\nlogistic loss, instead of the exponential loss used by AdaBoost, leads to an algorithm\\nthat coincides with logistic regression.6.2 AdaBoost 129\\n6.2.3 Relationship with logistic regression\\nLogistic regression is a widely used binary classiﬁcation algorithm, a speciﬁc instance\\nof conditional maximum entropy models. For the purpose of this chapter, we\\nﬁrst give a very brief description of the algorithm. Logistic regression returns a\\nconditional probability distribution of the form\\np\\nα [y|x]= 1\\nZ(x) exp(y α · Φ(x)), (6.7)\\nwhere y ∈{ −1, +1} is the label predicted for x ∈X , Φ(x) ∈ RN is a feature vector\\nassociated to x, α ∈ RN a parameter weight vector, andZ(x)= e+α ·Φ(x) +e−α ·Φ(x)\\na normalization factor. Dividing both the numerator and denominator by e+α ·Φ(x)\\nand taking the log leads to:\\nlog(pα [y|x]) = − log(1 +e−2yα ·Φ(x)). (6.8)\\nThe parameter α is learned via maximum likelihood by logistic regression, that is,\\nby maximizing the probability of the sampleS =( (x1,y s),..., (xm,y m)). Since the\\npoints are sampled i.i.d., this can be written as follows: argmax α\\n∏m\\ni=1 pα [yi|xi].\\nTaking the negative log of the probabilities shows that the objective function\\nminimized by logistic regression is\\nG(α)=\\nm∑\\ni=1\\nlog(1 +e−2yiα ·Φ(xi)) . (6.9)\\nThus, modulo constants, which do not aﬀect the solution sought, the objective\\nfunction coincides with the one based on the logistic loss. AdaBoost and Logistic\\nregression have in fact many other relationships that we will not discuss in detail\\nhere. In particular, it can be shown that both algorithms solve exactly the same\\noptimization problem, except for a normalization constraint required for logistic\\nregression not imposed in the case of AdaBoost.\\n6.2.4 Standard use in practice\\nHere we brieﬂy describe the practical use of AdaBoost. An important requirement\\nfor the algorithm is the choice of the base classiﬁers or that of the weak learner. The\\nfamily of base classiﬁers typically used with AdaBoost in practice is that ofdecision\\ntrees, which are equivalent to hierarchical partitions of the space (see chapter 8,\\nsection 8.3.3). In fact, more precisely, decision trees of depth one, also known as\\nstumps or boosting stumps are by far the most frequently used base classiﬁers.\\nBoosting stumps are threshold functions associated to a single feature. Thus,\\na stump corresponds to a single axis-aligned partition of the space, as illustrated130 Boosting\\ntraining error\\ntest error\\nerror\\n0\\n10 100 1000\\nnumber of rounds -        log(T)\\nFigure 6.5 An empirical result using AdaBoost with C4.5 decision trees as base\\nlearners. In this example, the training error goes to zero after about 5 rounds of\\nboosting (T ≈ 5), yet the test error continues to decrease for larger values of T.\\nin ﬁgure 6.2. If the data is in RN , we can associate a stump to each of the N\\ncomponents. Thus, to determine the stump with the minimal weighted error at\\neach of round of boosting, the best component and the best threshold for each\\ncomponent must be computed.\\nTo do so, we can ﬁrst presort each component in O(m log m)t i m ew i t hat o t a l\\ncomputational cost of O(mN log m). For a given component, there are only m +1\\npossible distinct thresholds, since two thresholds between the same consecutive\\ncomponent values are equivalent. To ﬁnd the best threshold at each round of\\nboosting, all of these possible m + 1 values can be compared, which can be done in\\nO(m) time. Thus, the total computational complexity of the algorithm forT rounds\\nof boosting is O(mN log m + mNT ).\\nObserve, however, that while boosting stumps are widely used in combination\\nwith AdaBoost and can perform well in practice, the algorithm that returns the\\nstump with the minimal empirical error is not a weak learner (see deﬁnition 6.1)!\\nConsider, for example, the simple XOR example with four data points lying in\\nR\\n2 (see ﬁgure 5.2a), where points in the second and fourth quadrants are labeled\\npositively and those in the ﬁrst and third quadrants negatively. Then, no decision\\nstump can achieve an accuracy better than 1/2.\\n6.3 Theoretical results\\nIn this section we present a theoretical analysis of the generalization properties of\\nAdaBoost.6.3 Theoretical results 131\\n6.3.1 VC-dimension-based analysis\\nW es t a r tw i t ha na n a l y s i so fA d a B o o s tb a s e do nt h eV C - d i m e n s i o no fi t sh y p o t h e s i s\\nset. For T rounds of boosting, its hypothesis set is\\nFT =\\n{\\nsgn\\n( T∑\\nt=1\\nαtht\\n⎡\\n: αt ∈ R,h t ∈ H,t ∈ [1,T ]\\n}\\n. (6.10)\\nThe VC-dimension of FT can be bounded as follows in terms of the VC-dimension\\nd of the family of base hypothesis H (exercise 6.1):\\nVCdim(FT ) ≤ 2(d +1 ) (T +1 )l o g2((T +1 )e) . (6.11)\\nThe upper bound grows as O(dT log T), thus the bound suggests that AdaBoost\\ncould overﬁt for large values of T, and indeed this can occur. However, in many\\ncases, it has been observed empirically that the generalization error of AdaBoost\\ndecreases as a function of the number of rounds of boosting T, as illustrated in\\nﬁgure 6.5! How can these empirical results be explained? The following sections\\np r e s e n ta na n a l y s i sb a s e do nac o n c e p to fm a r g i n ,s i m i l a rt ot h eo n ep r e s e n t e df o r\\nSVMs.\\n6.3.2 Margin-based analysis\\nIn chapter 4 we gave a deﬁnition of margin for linear classiﬁers such as SVMs\\n(deﬁnition 4.2). Here we will need a somewhat diﬀerent but related deﬁnition of\\nmargin for linear combinations of base classiﬁers, as in the case of AdaBoost.\\nFirst note that a linear combination of base classiﬁers g = ∑\\nT\\nt=1 αtht can be\\ndeﬁned equivalently via g(x)= α · h(x) for all x ∈X ,w i t hα =( α1,...,α T )⊤ and\\nh(x)=[ h1(x),...,h T (x)]⊤. This makes their similarity with the linear hypotheses\\nconsidered in chapter 4 and chapter 5 evident:h(x) is the feature vector associated\\nto x, which was previously denoted by Φ( x), and α is the weight vector that was\\ndenoted by w. The base classiﬁers values ht(x) are the components of the feature\\nvector associated tox. For AdaBoost, additionally, the weight vector is non-negative:\\nα ≥ 0.\\nWe will use the same notation to introduce the following deﬁnition.\\nDeﬁnition 6.2 L1-margin\\nThe L1-margin ρ(x) of a point x ∈X with label y ∈{ − 1,+1} for a linear\\ncombination of base classiﬁers g = ∑T\\nt=1 αtht with α ̸=0 and ht ∈ H for all\\nt ∈ [1,T ] is deﬁned as\\nρ(x)= yg(x)∑m\\nt=T |αt| = y ∑T\\nt=1 αtht(x)\\n∥α∥1\\n= yα · h(x)\\n∥α∥1\\n. (6.12)132 Boosting\\nThe L1-margin of a linear combination classiﬁer g w i t hr e s p e c tt oas a m p l eS =\\n(x1,...,x m) is the minimum margin of the points within the sample:\\nρ=m i n\\ni∈[1,m]\\nyi\\nα · h(xi)\\n∥α∥1\\n. (6.13)\\nWhen the coeﬃcients αt are non-negative, as in the case of AdaBoost, ρ(x)i sa\\nconvex combination of the base classiﬁer values ht(x). In particular, if the base\\nclassiﬁers ht take values in [ −1, +1], then ρ(x)i si n[ −1,+1]. The absolute value\\n|ρ(x)| can be interpreted as the conﬁdence of the classiﬁer g in that label.\\nThis deﬁnition of margin diﬀers from deﬁnition 4.2 given for linear classiﬁers only\\nby the norm used for the weight vector: L1 norm here, L2 norm in deﬁnition 4.2.\\nIndeed, in the case of a linear hypothesisx ↦→ w ·Φ(x), the margin for point x with\\nlabel y was deﬁned as follows:\\nρ(x)= yw · φ(x)\\n∥w∥2\\nand was based on the L2 norm of w. When the prediction is correct, that is\\ny(α · h(x)) ≥ 0, the L1-margin and L2 margin of deﬁnition 4.2 can be rewritten as\\nρ1(x)= |α · h(x)|\\n∥α∥1\\nand ρ2(x)= |α · h(x)|\\n∥α∥2\\n.\\nIt is known that forp, q ≥ 1, p and q conjugate, i.e. 1/p+1/q =1 ,t h a t|α ·x|/∥α∥p\\nis the Lq distance of x to the hyperplane of equationα ·x =0 .T h u s ,b o t hρ1(x)a n d\\nρ2(x) measure the distance of the feature vectorh(x)t ot h eh y p e r p l a n eα ·x =0i n\\nRT , ρ1(x)i t s∥·∥ ∞ distance, ρ2(x)i t s∥·∥ 2 or Euclidean distance (see ﬁgure 6.6).\\nTo examine the generalization properties of AdaBoost, we ﬁrst analyze the\\nRademacher complexity of convex combinations of hypotheses such as those deﬁned\\nby AdaBoost. Next, we use the margin-based analysis from chapter 4 to derive a\\nmargin-based generalization bound for boosting with the deﬁnition of margin just\\nintroduced.\\nFor any hypothesis set H of real-valued functions, we denote by conv( H)i t s\\nconvex hull deﬁned by\\nconv(H)=\\n{\\np∑\\nk=1\\nμkhk : p ≥ 1, ∀k ∈ [1,p ],μk ≥ 0,h k ∈ H,\\np∑\\nk=1\\nμk ≤ 1\\n}\\n. (6.14)\\nThe following theorem shows that, remarkably, the empirical Rademacher complex-\\nity of conv(H), which in general is a strictly larger set including H,c o i n c i d e sw i t h\\nthat of H.6.3 Theoretical results 133\\nTheorem 6.2\\nLet H be a set of functions mapping fromX to R.T h e n ,f o ra n ys a m p l eS, we have\\nˆRS(conv(H)) = ˆRS(H) .\\nProof The proof follows from a straightforward series of equalities:\\nˆRS(conv(H)) = 1\\nm E\\nσ\\n[\\nsup\\nh1,...,hp∈H,μ ≥0,∥μ ∥1≤1\\nm∑\\ni=1\\nσi\\np∑\\nk=1\\nμkhk(xi)\\n]\\n= 1\\nm E\\nσ\\n[\\nsup\\nh1,...,hp∈H\\nsup\\nμ ≥0,∥μ ∥1≤1\\np∑\\nk=1\\nμk\\n( m∑\\ni=1\\nσihk(xi)\\n⎡]\\n= 1\\nm E\\nσ\\n[\\nsup\\nh1,...,hp∈H\\nmax\\nk∈[1,p]\\n( m∑\\ni=1\\nσihk(xi)\\n⎡]\\n= 1\\nm E\\nσ\\n[\\nsup\\nh∈H\\nm∑\\ni=1\\nσih(xi)\\n]\\n= ˆRS(H).\\nThe main equality to recognize is the third one, which is based on the observation\\nthat the maximizing vector μ for the convex combination of p t e r m si st h eo n e\\nplacing all the weight on the largest term of the sum.\\nThis theorem can be used directly in combination with theorem 4.4 to derive\\nthe following Rademacher complexity generalization bound for convex combination\\nensembles of hypotheses.\\nCorollary 6.1 Ensemble Rademacher margin bound\\nLet H denote a set of real-valued functions. Fix ρ> 0.T h e n ,f o ra n yδ> 0,w i t h\\nprobability at least 1 − δ, each of the following holds for all h ∈ conv(H):\\nR(h) ≤ ˆRρ(h)+ 2\\nρRm\\n(\\nH\\n⎡\\n+\\n√\\nlog 1\\nδ\\n2m (6.15)\\nR(h) ≤ ˆRρ(h)+ 2\\nρ\\nˆRS\\n(\\nH\\n⎡\\n+3\\n√\\nlog 2\\nδ\\n2m . (6.16)\\nUsing corollary 3.1 and corollary 3.3 to bound the Rademacher complexity in\\nterms of the VC-dimension yields immediately the following VC-dimension-based\\ngeneralization bounds for convex combination ensembles of hypotheses.\\nCorollary 6.2 Ensemble VC-Dimension margin bound\\nLet H be a family of functions taking values in{+1, −1} with VC-dimension d.F i x\\nρ> 0.T h e n ,f o ra n yδ> 0, with probability at least 1 − δ, the following holds for134 Boosting\\nall h ∈ conv(H):\\nR(h) ≤ ˆRρ(h)+ 2\\nρ\\n√\\n2d log em\\nd\\nm +\\n√\\nlog 1\\nδ\\n2m . (6.17)\\nThese bounds can be generalized to hold uniformly for all ρ> 0, instead of a ﬁxed\\nρ, at the price of an additional term of the form\\n√\\n(log log2\\n2\\nδ)/m as in theorem 4.5.\\nThey cannot be directly applied to the linear combinationg generated by AdaBoost,\\nsince it is not a convex combination of base hypotheses, but they can be applied to\\nthe following normalized version of g:\\nx ↦→ g(x)\\n∥α∥1\\n=\\n∑T\\nt=1 αtht(x)\\n∥α∥1\\n∈ conv(H) . (6.18)\\nNote that from the point of view of binary classiﬁcation,g and g/∥α∥1 are equivalent\\nsince sgn(g)=s g n (g/∥α∥1), thus R(g)= R(g/∥α∥1), but their empirical margin\\nloss are distinct. Let g = ∑T\\nt=1 αtht denote the function deﬁning the classiﬁer\\nreturned by AdaBoost after T rounds of boosting when trained on sampleS. Then,\\nin view of (6.15), for any δ> 0, the following holds with probability at least 1 − δ:\\nR(g) ≤ ˆRρ(g/∥α∥1)+ 2\\nρRm\\n(\\nH\\n⎡\\n+\\n√\\nlog 1\\nδ\\n2m . (6.19)\\nSimilar bounds can be derived from (6.16) and (6.17). Remarkably, the number\\nof rounds of boosting T does not appear in the generalization bound (6.19). The\\nbound depends only on the margin ρ, the sample size m, and the Rademacher\\ncomplexity of the family of base classiﬁers H. Thus, the bound guarantees an\\neﬀective generalization if the margin loss ˆRρ(g/∥α∥1) is small for a relatively large\\nρ. Recall that the margin loss can be upper bounded by the fraction of the points\\nx in the training sample with g(x)/∥α∥1 ≥ ρ(see (4.39)). Thus, with our deﬁnition\\nof L1-margin, it can be bounded by the fraction of the points in S with L1-margin\\nmore than ρ:\\nˆRρ(g/∥α∥1) ≤ |{i ∈ [1,m]: ρ(xi) ≥ ρ}|\\nm . (6.20)\\nAdditionally, the following theorem provides a bound on the empirical margin loss,\\nwhich decreases with T under conditions discussed later.\\nTheorem 6.3\\nLet g = ∑T\\nt=1 αtht denote the function deﬁning the classiﬁer returned by AdaBoost\\nafter T rounds of boosting and assume for all t ∈ [1,T ] that ϵt < 1\\n2, which implies6.3 Theoretical results 135\\nat > 0.T h e n ,f o ra n yρ> 0, the following holds:\\nˆRρ\\n( g\\n∥α∥1\\n⎡\\n≤ 2T\\nT∏\\nt=1\\n√\\nϵ1−ρ\\nt (1 − ϵt)1+ρ .\\nProof Using the general inequality 1 u≤0 ≤ exp(−u) valid for all u ∈ R, iden-\\ntity 6.2, that is Dt+1(i)= e− yig(xi)\\nm QT\\nt=1 Zt\\n, the equality Zt =2\\n√\\nϵt(1 − ϵt) from the proof\\nof theorem 6.1, and the deﬁnition of α in AdaBoost, we can write:\\n1\\nm\\nm∑\\ni=1\\n1yig(xi)−ρ∥α ∥1≤0 ≤ 1\\nm\\nm∑\\ni=1\\nexp(−yig(xi)+ ρ∥α∥1)\\n= 1\\nm\\nm∑\\ni=1\\neρ∥α ∥1\\n[\\nm\\nT∏\\nt=1\\nZt\\n]\\nDT+1(i)\\n= eρ∥α ∥1\\nT∏\\nt=1\\nZt = eρ P\\ni αi\\nT∏\\nt=1\\nZt\\n=2 T\\nT∏\\nt=1\\n[√\\n1−ϵt\\nϵt\\n]ρ√\\nϵt(1 − ϵt) ,\\nwhich concludes the proof.\\nMoreover, if for all t ∈ [1,T ]w eh a v eγ ≤ (1\\n2 − ϵt)a n dρ ≤ 2γ, then the expression\\n4ϵ1−ρ\\nt (1−ϵt)1+ρ is maximized atϵt = 1\\n2 −γ.1 Thus, the upper bound on the empirical\\nm a r g i nl o s sc a nt h e nb eb o u n d e db y\\nˆRρ\\n( g\\n∥α∥1\\n⎡\\n≤\\n[\\n(1 − 2γ)1−ρ(1 + 2γ)1+ρ\\n]T/2\\n. (6.21)\\nObserve that (1 − 2γ)1−ρ(1 + 2γ)1+ρ =( 1 − 4γ2)\\n(1+2γ\\n1−2γ\\n⎡ρ\\n.T h i si sa ni n c r e a s i n g\\nfunction of ρ s i n c ew eh a v e\\n(1+2γ\\n1−2γ\\n⎡\\n> 1 as a consequence of γ> 0. Thus, if ρ<γ ,\\nit can be strictly upper bounded as follows\\n(1 − 2γ)1−ρ(1 + 2γ)1+ρ < (1 − 2γ)1−γ(1 + 2γ)1+γ.\\nThe function γ ↦→ (1 − 2γ)1−γ(1 + 2γ)1+γ is strictly upper bounded by 1 over the\\ninterval (0, 1/2), thus, if ρ<γ ,t h e n( 1− 2γ)1−ρ(1+2 γ)1+ρ < 1 and the right-hand\\nside of (6.21) decreases exponentially with T. Since the condition ρ ≫ O(1/√m)i s\\nnecessary in order for the given margin bounds to converge, this places a condition\\n1. The diﬀerential of f : ϵ ↦→ log[ϵ1−ρ(1 − ϵ)1+ρ]=( 1 − ρ)l o gϵ +( 1+ ρ)l o g( 1− ϵ)o v e rt h e\\ninterval (0, 1) is given byf ′(ϵ)= 1−ρ\\nϵ − 1+ρ\\n1−ϵ =2\\n( 1\\n2 − ρ\\n2 )−ϵ\\nϵ(1−e) .T h u s ,f is an increasing function\\nover (0, 1\\n2 − ρ\\n2 ), which implies that it is increasing over (0, 1\\n2 − γ) when γ ≥ ρ\\n2 .136 Boosting\\nNorm | |·| |2. Norm | |·| |∞.\\nFigure 6.6 Maximum margins with respect to both the L2 and L∞ norm.\\nof γ ≫ O(1/√m) on the edge value. In practice, the errorϵt of the base classiﬁer at\\nround t may increase as a function of t. Informally, this is because boosting presses\\nthe weak learner to concentrate on instances that are harder and harder to classify,\\nfor which even the best base classiﬁer could not achieve an error signiﬁcantly better\\nthan random. If ϵt becomes close to 1/2 relatively fast as a function of t, then the\\nbound of theorem 6.3 becomes uninformative.\\nThe margin bounds of corollary 6.1 and corollary 6.2, combined with the bound\\non the empirical margin loss of theorem 6.3, suggest that under some conditions,\\nAdaBoost can achieve a large margin on the training sample. They could also serve\\nas a theoretical explanation of the empirical observation that in some tasks the\\ngeneralization error increases as a function ofT even after the error on the training\\nsample is zero: the margin would continue to increase. But does AdaBoost maximize\\nthe L\\n1-margin?\\nNo. It has been shown that AdaBoost may converge to a margin that is signiﬁ-\\ncantly smaller than the maximum margin (e.g., 1/3 instead of 3/8 ) .H o w e v e r ,u n d e r\\nsome general assumptions, when the data is separable and the base learners satisfy\\nparticular conditions, it has been proven that AdaBoost can asymptotically achieve\\na margin that is at least half the maximum margin, ρmax/2.\\n6.3.3 Margin maximization\\nIn view of these results, several algorithms have been devised with the explicit goal\\nof maximizing theL\\n1-margin. These algorithms correspond to diﬀerent methods for\\nsolving a linear program (LP).\\nBy deﬁnition of the L1-margin, the maximum margin for a sample S =\\n((x1,y1),..., (xm,y m)) is given by\\nρ=m a x\\nα\\nmin\\ni∈[1,m]\\nyi\\nα · h(xi)\\n∥α∥1\\n. (6.22)6.3 Theoretical results 137\\nBy deﬁnition of the maximization, the optimization problem can be written as:\\nmax\\nα\\nρ\\nsubject to : yi\\nα · h(xi)\\n∥α∥1\\n≥ ρ, ∀i ∈ [1,m].\\nSince α ·h(xi)\\n∥α ∥1\\nis invariant to the scaling ofα, we can restrict ourselves to ∥α∥1 =1 .\\nFurther seeking a non-negative α as in the case of AdaBoost leads to the following\\noptimization:\\nmax\\nα\\nρ\\nsubject to : yi(α · h(xi)) ≥ ρ, ∀i ∈ [1,m]\\n( T∑\\nt=1\\nαt =1\\n⎡\\n∧ (αt ≥ 0, ∀t ∈ [1,T ]).\\nThis is a linear program (LP), that is, an optimization problem with a linear\\nobjective function and linear constraints. There are several diﬀerent methods for\\nsolving relative large LPs in practice, using the simplex method, interior-point\\nmethods, or a variety of special-purpose solutions.\\nNote that the solution of this algorithm diﬀers from the margin-maximization\\ndeﬁning SVMs in the separable case only by the deﬁnition of the margin used ( L\\n1\\nversus L2) and the non-negativity constraint on the weight vector. Figure 6.6 illus-\\ntrates the margin-maximizing hyperplanes found using these two distinct margin\\ndeﬁnitions in a simple case. The left ﬁgure shows the SVM solution, where the dis-\\ntance to the closest points to the hyperplane is measured with respect to the norm\\n∥·∥\\n2. The right ﬁgure shows the solution for the L1-margin, where the distance to\\nthe closest points to the hyperplane is measured with respect to the norm ∥·∥ ∞ .\\nBy deﬁnition, the solution of the LP just described admits an L1-margin that\\nis larger or equal to that of the AdaBoost solution. However, empirical results do\\nnot show a systematic beneﬁt for the solution of the LP. In fact, it appears that in\\nmany cases, AdaBoost outperforms that algorithm. The margin theory described\\ndoes not seem suﬃcient to explain that performance.\\n6.3.4 Game-theoretic interpretation\\nIn this section, we ﬁrst show that AdaBoost admits a natural game-theoretic\\ninterpretation. The application of von Neumann’s theorem then helps us relate the\\nmaximum margin and the optimal edge and clarify the connection of AdaBoost’s\\nweak-learning assumption with the notion of L\\n1-margin. We ﬁrst introduce the\\ndeﬁnition of the edge for a speciﬁc classiﬁer and a particular distribution.138 Boosting\\nrock paper scissors\\nrock 0 +1 -1\\npaper -1 0 +1\\nscissors +1 -1 0\\nT able 6.1 The loss matrix for the standard rock-paper-scissors game.\\nDeﬁnition 6.3\\nThe edge of a base classiﬁer ht for a distribution D o v e rt h et r a i n i n gs a m p l ei s\\ndeﬁned by\\nγt(D)= 1\\n2 − ϵt = 1\\n2\\nm∑\\ni=1\\nyiht(xi)D(i). (6.23)\\nAdaBoost’s weak learning condition can now be formulated as: there exists γ> 0\\nsuch that for any distributionD over the training sample and any base classiﬁerht,\\nthe following holds:\\nγt(D) ≥ γ. (6.24)\\nThis condition is required for the analysis of theorem 6.1 and the non-negativity of\\nthe coeﬃcients αt. We will frame boosting as a two-person zero-sum game.\\nDeﬁnition 6.4 Zero-sum game\\nA two-person zero-sum game consists of a loss matrix M ∈ Rm×n,w h e r em is the\\nnumber of possible actions (or pure strategies) for the row player andn the number\\nof possible actions for the column player. The entry Mij i st h el o s sf o rt h er o w\\nplayer (or equivalently the payoﬀ for the column payer) when the row player takes\\naction i and the column player takes action j.\\n2\\nAn example of a loss matrix for the familiar “rock-paper-scissors” game is shown\\nin table 6.1.\\nDeﬁnition 6.5 Mixed strategy\\nA mixed strategy for the row player is a distribution p over the m possible row\\nactions, a distribution q over the n possible column actions for the column player.\\nThe expected loss for the row player (expected payoﬀ for the column player) with\\n2. To be consistent with the results discussed in other chapters, we consider the loss matrix\\nas opposed to the payoﬀ matrix (its opposite).6.3 Theoretical results 139\\nrespect to the mixed strategiesp and q is\\nE[loss]= p⊤Mq =\\nm∑\\ni=1\\nn∑\\nj=1\\npiMijqj .\\nThe following is a fundamental result in game theory proven in chapter 7.\\nTheorem 6.4 Von Neumann’s minimax theorem\\nFor any two-person zero-sum game deﬁned by matrix M,\\nmin\\np\\nmax\\nq\\np⊤Mq =m a x\\nq\\nmin\\np\\np⊤Mq. (6.25)\\nThe common value in (6.25) is called the value of the game . The theorem states\\nthat for any two-person zero-sum game, there exists a mixed strategy for each player\\nsuch that the expected loss for one is the same as the expected payoﬀ for the other,\\nboth of which are equal to the value of the game. Note that, given the row player’s\\nstrategy, the column player can choose an optimal pure strategy, that is, the column\\nplayer can choose the single strategy corresponding the smallest coordinate of the\\nvector p\\n⊤M. A similar comment applies to the reverse. Thus, an alternative and\\nequivalent form of the minimax theorem is\\nmax\\np\\nmin\\nj∈[1,n]\\np⊤Mej =m i n\\nq\\nmax\\ni∈[1,m]\\ne⊤\\ni Mq, (6.26)\\nwhere ei denotes the ith unit vector. We can now view AdaBoost as a zero-sum\\ngame, where an action of the row player is the selection of a training instance\\nx\\ni, i ∈ [1,m ], and an action of the column player the selection of a base learner\\nht, t ∈ [1,T ]. A mixed strategy for the row player is thus a distribution D over\\nthe training points’ indices [1 ,m]. A mixed strategy for the column player is a\\ndistribution over the based classiﬁers’ indices [1 ,T ]. This can be deﬁned from a\\nnon-negative vector α ≥ 0: the weight assigned to t ∈ [1,T ]i s αt/∥α∥1.T h e\\nloss matrix M ∈{ − 1, +1}m×T for AdaBoost is deﬁned by Mit = yiht(xi)f o r\\nall (i, t) ∈ [1,m] × [1,T ]. By von Neumann’s theorem (6.26), the following holds:\\nmin\\nD∈D\\nmax\\nt∈[1,T]\\nm∑\\ni=1\\nD(i)yiht(xi)=m a x\\nα ≥0\\nmin\\ni∈[1,m]\\nT∑\\nt=1\\nαt\\n∥α∥1\\nyiht(xi), (6.27)\\nwhere D denotes the set of all distributions over the training sample. Let ρα (x)\\ndenote the margin of point x for the classiﬁer deﬁned by g = ∑T\\nt=1 αtht.T h er e s u l t\\nc a nb er e w r i t t e na sf o l l o w si nt e r m so ft h em a r g i n sa n de d g e s :\\n2γ∗ =2m i n\\nD\\nmax\\nt∈[1,T]\\nγt(D)=m a x\\nα\\nmin\\ni∈[1,m]\\nρα (xi)= ρ∗, (6.28)140 Boosting\\nwhere ρ∗ is the maximum margin of a classiﬁer and γ∗ the best possible edge. This\\nresult has several implications. First, it shows that the weak learning condition\\n(γ\\n∗ > 0) implies ρ∗ > 0 and thus the existence of a classiﬁer with positive margin,\\nwhich motivates the search for a non-zero margin. AdaBoost can be viewed as an\\nalgorithm seeking to achieve such a non-zero margin, though, as discussed earlier,\\nAdaBoost does not always achieve an optimal margin and is thus suboptimal in that\\nrespect. Furthermore, we see that the “weak learning” assumption, which originally\\nappeared to be the weakest condition one could require for an algorithm (that of\\nperforming better than random), is in fact a strong condition: it implies that the\\ntraining sample is linearly separable with margin 2γ\\n∗ > 0. Linear separability often\\ndoes not hold for the data sets found in practice.\\n6.4 Discussion\\nAdaBoost oﬀers several advantages: it is simple, its implementation is straightfor-\\nward, and the time complexity of each round of boosting as a function of the sample\\nsize is rather favorable. As already discussed, when using decision stumps, the time\\ncomplexity of each round of boosting is in O(mN). Of course, if the dimension of\\nthe feature space N is very large, then the algorithm could become in fact quite\\nslow.\\nAdaBoost additionally beneﬁts from a rich theoretical analysis. Nevertheless,\\nthere are still many theoretical questions. For example, as we saw, the algorithm in\\nfact does not maximize the margin, and yet algorithms that do maximize the margin\\ndo not always outperform it. This suggests that perhaps a ﬁner analysis based on a\\nnotion diﬀerent from that of margin could shed more light on the properties of the\\nalgorithm.\\nThe main drawbacks of the algorithm are the need to select the parameterT and\\nthe base classiﬁers, and its poor performance in the presence of noise. The choice of\\nthe number of rounds of boostingT (stopping criterion) is crucial to the performance\\nof the algorithm. As suggested by the VC-dimension analysis, larger values ofT can\\nlead to overﬁtting. In practice, T is typically determined via cross-validation.\\nThe choice of the base classiﬁers is also crucial. The complexity of the family\\nof base classiﬁers H appeared in all the bounds presented and it is important to\\ncontrol it in order to guarantee generalization. On the other hand, insuﬃciently\\ncomplex hypothesis sets could lead to low margins.\\nProbably the most serious disadvantage of AdaBoost is its performance in the\\npresence of noise; it has been shown empirically that noise severely damages its\\naccuracy. The distribution weight assigned to examples that are harder to classify\\nsubstantially increases with the number of rounds of boosting, by the nature of the6.5 Chapter notes 141\\nalgorithm. These examples end up dominating the selection of the base classiﬁers,\\nwhich, with a large enough number of rounds, will play a detrimental role in the\\ndeﬁnition of the linear combination deﬁned by AdaBoost.\\nSeveral solutions have been proposed to address these issues. One consists of using\\na “less aggressive” objective function than the exponential function of AdaBoost,\\nsuch as the logistic loss, to penalize less incorrectly classiﬁed points. Another\\nsolution is based on a regularization, e.g., an L\\n1-regularization, which consists of\\nadding a term to the objective function to penalize larger weights. This could be\\nviewed as a soft margin approach for boosting. However, recent theoretical results\\nshow that boosting algorithms based on convex potentials do not tolerate even low\\nlevels of random noise, even with L1-regularization or early stopping.\\nThe behavior of AdaBoost in the presence of noise can be used, however, as a\\nuseful feature for detecting outliers, that is, examples that are incorrectly labeled\\nor that are hard to classify. Examples with large weights after a certain number of\\nrounds of boosting can be identiﬁed as outliers.\\n6.5 Chapter notes\\nThe question of whether a weak learning algorithm could be boosted to derive a\\nstrong learning algorithm was ﬁrst posed by Kearns and Valiant [1988, 1994], who\\nalso gave a negative proof of this result for a distribution-dependent setting. The\\nﬁrst positive proof of this result in a distribution-independent setting was given by\\nSchapire [1990], and later by Freund [1990].\\nThese early boosting algorithms, boosting by ﬁltering [Schapire, 1990] or boosting\\nby majority [Freund, 1990, 1995] were not practical. The AdaBoost algorithm\\nintroduced by Freund and Schapire [1997] solved several of these practical issues.\\nFreund and Schapire [1997] further gave a detailed presentation and analysis of the\\nalgorithm including the bound on its empirical error, a VC-dimension analysis, and\\nits applications to multi-class classiﬁcation and regression.\\nEarly experiments with AdaBoost were carried out by Drucker, Schapire, and\\nSimard [1993], who gave the ﬁrst implementation in OCR with weak learners based\\non neural networks and Drucker and Cortes [1995], who reported the empirical\\nperformance of AdaBoost combined with decision trees, in particular decision\\nstumps.\\nThe fact that AdaBoost coincides with coordinate descent applied to an expo-\\nnential objective function was later shown by Duﬀy and Helmbold [1999], Mason\\net al. [1999], and Friedman [2000]. Friedman, Hastie, and Tibshirani [2000] also\\ngave an interpretation of boosting in terms of additive models. They also pointed\\nout the close connections between AdaBoost and logistic regression, in particular142 Boosting\\nthe fact that their objective functions have a similar behavior near zero or the\\nfact that their expectation admit the same minimizer, and derived an alternative\\nboosting algorithm, LogitBoost, based on the logistic loss. Laﬀerty [1999] showed\\nhow an incremental family of algorithms, including LogitBoost, can be derived from\\nBregman divergences and designed to closely approximate AdaBoost when varying\\na parameter. Kivinen and Warmuth [1999] observed that boosting can be viewed\\nas a type of entropy projection. Collins, Schapire, and Singer [2002] later showed\\nthat boosting and logistic regression were special instances of a common framework\\nbased on Bregman divergences and used that to give the ﬁrst convergence proof\\nof AdaBoost. Probably the most direct relationship between AdaBoost and logis-\\ntic regression is the proof by Lebanon and Laﬀerty [2001] that the two algorithms\\nminimize the same extended relative entropy objective function subject to the same\\nfeature constraints, except from an additional normalization constraint for logistic\\nregression.\\nA margin-based analysis of AdaBoost was ﬁrst presented by Schapire, Freund,\\nBartlett, and Lee [1997], including theorem 6.3 which gives a bound on the empirical\\nmargin loss. Our presentation is based on the elegant derivation of margin bounds\\nby Koltchinskii and Panchenko [2002] using the notion of Rademacher complexity.\\nRudin et al. [2004] gave an example showing that, in general, AdaBoost does not\\nmaximize the L\\n1-margin. R¨atsch and Warmuth [2002] provided asymptotic lower\\nbounds for the margin achieved by AdaBoost under some conditions. TheL1-margin\\nmaximization based on a LP is due to Grove and Schuurmans [1998]. The game-\\ntheoretic interpretation of boosting and the application of von Neumann’s minimax\\ntheorem [von Neumann, 1928] in that context were pointed out by Freund and\\nSchapire [1996, 1999b]; see also Grove and Schuurmans [1998], Breiman [1999].\\nDietterich [2000] provided extensive empirical evidence for the fact that noise can\\nseverely damage the accuracy of AdaBoost. This has been reported by a number of\\nother authors since then. R¨atsch, Onoda, and M¨uller [2001] suggested the use of a\\nsoft margin for AdaBoost based on a regularization of the objective function and\\npointed out its connections with SVMs. Long and Servedio [2010] recently showed\\nthe failure of boosting algorithms based on convex potentials to tolerate random\\nnoise, even with L\\n1-regularization or early stopping.\\nThere are several excellent surveys and tutorials related to boosting [Schapire,\\n2003, Meir and R¨atsch, 2002, Meir and R¨atsch, 2003].\\n6.6 Exercises\\n6.1 VC-dimension of the hypothesis set of AdaBoost.\\nProve the upper bound on the VC-dimension of the hypothesis setFT of AdaBoost6.6 Exercises 143\\nafter T rounds of boosting, as stated in equation 6.11.\\n6.2 Alternative objective functions.\\nThis problem studies boosting-type algorithms deﬁned with objective functions\\ndiﬀerent from that of AdaBoost. We assume that the training data are given as\\nm labeled examples ( x\\n1,y1),..., (xm,y m) ∈ X ×{ − 1, +1}.W ef u r t h e ra s s u m e\\nthat Φ is a strictly increasing convex and diﬀerentiable function over R such that:\\n∀x ≥ 0, Φ(x) ≥ 1a n d∀x< 0, Φ(x) > 0.\\n(a) Consider the loss function L(α)= ∑m\\ni=1 Φ(−yig(xi)) where g is a linear\\ncombination of base classiﬁers, i.e., g = ∑T\\nt=1 αtht (as in AdaBoost). Derive a\\nnew boosting algorithm using the objective function L. In particular, charac-\\nterize the best base classiﬁer hu to select at each round of boosting if we use\\ncoordinate descent.\\n(b) Consider the following functions: (1) zero-one loss Φ1(−u)=1 u≤0; (2) least\\nsquared loss Φ2(−u)=( 1 − u)2; (3) SVM loss Φ3(−u)=m a x{0, 1 − u};a n d( 4 )\\nlogistic loss Φ4(−u)=l o g ( 1+e−u). Which functions satisfy the assumptions\\non Φ stated earlier in this problem?\\n(c) For each loss function satisfying these assumptions, derive the correspond-\\ning boosting algorithm. How do the algorithm(s) diﬀer from AdaBoost?\\n6.3 Update guarantee. Assume that the main weak learner assumption of AdaBoost\\nholds. Let ht be the base learner selected at round t. Show that the base learner\\nht+1 selected at round t + 1 must be diﬀerent fromht.\\n6.4 Weighted instances. Let the training sample be S =( (x1,y1),..., (xm,y m)).\\nS u p p o s ew ew i s ht op e n a l i z ed i ﬀ e r e n t l ye r r o r sm a d eo nxi versus xj.T od ot h a t ,w e\\nassociate some non-negative importance weight wi to each point xi and deﬁne the\\nobjective function F(α)= ∑m\\ni=1 wie−yig(xi),w h e r eg = ∑T\\nt=1 αtht. Show that this\\nfunction is convex and diﬀerentiable and use it to derive a boosting-type algorithm.\\n6.5 Deﬁne the unnormalized correlation of two vectorsx and x′ as the inner product\\nbetween these vectors. Prove that the distribution vector ( Dt+1(1),...,D t+1(m))\\ndeﬁned by AdaBoost and the vector of components yiht(xi) are uncorrelated.\\n6.6 Fix ϵ ∈ (0, 1/2 ) .L e tt h et r a i n i n gs a m p l eb ed e ﬁ n e db ym points in the plane\\nwith m\\n4 negative points all at coordinate (1 , 1), another m\\n4 negative points all at\\ncoordinate ( −1, −1), m(1−ϵ)\\n4 positive points all at coordinate (1 , −1), and m(1+ϵ)\\n4\\npositive points all at coordinate (−1, +1). Describe the behavior of AdaBoost when144 Boosting\\nrun on this sample using boosting stumps. What solution does the algorithm return\\nafter T rounds?\\n6.7 Noise-tolerant AdaBoost. AdaBoost may signiﬁcantly overﬁtting in the presence\\nof noise, in part due to the high penalization of misclassiﬁed examples. To reduce\\nthis eﬀect, one could use instead the following objective function:\\nF =\\nm∑\\ni=1\\nG(−yig(xi)), (6.29)\\nwhere G is the function deﬁned on R by\\nG(x)=\\n{\\nex if x ≤ 0\\nx + 1 otherwise .\\n(6.30)\\n(a) Show that the function G is convex and diﬀerentiable.\\n(b) Use F and greedy coordinate descent to derive an algorithm similar to\\nAdaBoost.\\n(c) Compare the reduction of the empirical error rate of this algorithm with\\nthat of AdaBoost.\\n6.8 Simpliﬁed AdaBoost. Suppose we simplify AdaBoost by setting the parameter\\nαt to a ﬁxed value αt = α> 0, independent of the boosting round t.\\n(a) Let γ be such that (1\\n2 − ϵt) ≥ γ> 0. Find the best value ofα as a function\\nof γ by analyzing the empirical error.\\n(b) For this value of α, does the algorithm assign the same probability mass\\nto correctly classiﬁed and misclassiﬁed examples at each round? If not, which\\nset is assigned a higher probability mass?\\n(c) Using the previous value of α, give a bound on the empirical error of the\\nalgorithm that depends only on γ and the number of rounds of boosting T.\\n(d) Using the previous bound, show that forT>\\nlog m\\n2γ2 , the resulting hypothesis\\nis consistent with the sample of size m.\\n(e) Let s be the VC-dimension of the base learners used. Give a bound on the\\ngeneralization error of the consistent hypothesis obtained afterT =\\n⌊\\nlog m\\n2γ2\\n⌋\\n+1\\nrounds of boosting. ( Hint: Use the fact that the VC-dimension of the family\\nof functions {sgn(∑T\\nt=1 αtht): αt ∈ R} is bounded by 2( s +1 )T log2(eT)).\\nSuppose now that γ varies with m. Based on the bound derived, what can be\\nsaid if γ(m)= O(\\n√\\nlog m\\nm )?)6.6 Exercises 145\\nMatrix-based AdaBoost(M,tmax)\\n1 λ1,j ← 0f o ri =1 ,...,m\\n2 for t ← 1 to tmax do\\n3 dt,i ← exp(−(Mλt)i)Pm\\nk=1 exp(−(Mλt)k) for i =1 ,...,m\\n4 jt ← argmaxj(d⊤\\nt M)j\\n5 rt ← (d⊤\\nt M)jt\\n6 αt ← 1\\n2 log\\n(1+rt\\n1−rt\\n⎡\\n7 λt+1 ← λt + αtejt, where ejt is 1 in position jt and 0 elsewhere.\\n8 return λtmax\\n∥λtmax ∥1\\nFigure 6.7 Matrix-based AdaBoost.\\n6.9 Matrix-based AdaBoost.\\n(a) Deﬁne an m×n matrix M where Mij = yihj(xi), i.e.,Mij = +1 if training\\nexample i is classiﬁed correctly by weak classiﬁer hj,a n d −1 otherwise. Let\\ndt, λt ∈ Rn, ∥dt∥1 = 1 anddt,i (respectively λt,i) equal theith component ofdt\\n(respectively λt). Now, consider the matrix-based form of AdaBoost described\\nin ﬁgure 6.7 and deﬁne M as below with eight training points and eight weak\\nclassiﬁers.\\nM =\\n⎛\\n⎜⎜\\n⎜\\n⎜\\n⎜⎜⎜\\n⎜\\n⎜\\n⎜\\n⎜⎜⎜\\n⎜⎝\\n−11 1 1 1 −1 −11\\n−11 1 −1 −11 1 1\\n1 −11 1 1 −11 1\\n1 −11 1 −11 1 1\\n1 −11 −11 1 1 −1\\n11 −11 1 1 1 −1\\n11 −11 1 1 −11\\n1111 −1 −11 −1\\n⎞\\n⎟⎟\\n⎟\\n⎟\\n⎟⎟⎟\\n⎟\\n⎟\\n⎟\\n⎟⎟⎟\\n⎟⎠\\nAssume that we start with the following initial distribution over the datapoints:\\nd\\n1 =\\n(3 −\\n√\\n5\\n8 , 3 −\\n√\\n5\\n8 , 1\\n6, 1\\n6, 1\\n6,\\n√\\n5 − 1\\n8 ,\\n√\\n5 − 1\\n8 , 0\\n⎡⊤\\nCompute the ﬁrst few steps of the matrix-based AdaBoost algorithm usingM,\\nd1,a n dtmax = 7. What weak classiﬁer is picked at each round of boosting?146 Boosting\\nDo you notice any pattern?\\n(b) What is the L1 norm margin produced by AdaBoost for this example?\\n(c) Instead of using AdaBoost, imagine we combined our classiﬁers using the\\nfollowing coeﬃcients: [2,3, 4, 1, 2, 2, 1,1] × 1\\n16 . What is the margin in this case?\\nDoes AdaBoost maximize the margin?7 On-Line Learning\\nThis chapter presents an introduction to on-line learning, an important area with a\\nrich literature and multiple connections with game theory and optimization that\\nis increasingly inﬂuencing the theoretical and algorithmic advances in machine\\nlearning. In addition to the intriguing novel learning theory questions that they\\nraise, on-line learning algorithms are particularly attractive in modern applications\\nsince they form an attractive solution for large-scale problems.\\nT h e s ea l g o r i t h m sp r o c e s so n es a m p l ea tat i m ea n dc a nt h u sb es i g n i ﬁ c a n t l y\\nmore eﬃcient both in time and space and more practical than batch algorithms,\\nwhen processing modern data sets of several million or billion points. They are\\nalso typically easy to implement. Moreover, on-line algorithms do not require any\\ndistributional assumption; their analysis assumes an adversarial scenario. This\\nm a k e st h e ma p p l i c a b l ei nav a r i e t yo fs c e n a r i o sw h e r et h es a m p l ep o i n t sa r en o t\\ndrawn i.i.d. or according to a ﬁxed distribution.\\nWe ﬁrst introduce the general scenario of on-line learning, then present and\\nanalyze several key algorithms for on-line learning with expert advice, including\\nthe deterministic and randomized weighted majority algorithms for the zero-one\\nloss and an extension of these algorithms for convex losses. We also describe and\\nanalyze two standard on-line algorithms for linear classiﬁcations, the Perceptron and\\nWinnow algorithms, as well as some extensions. While on-line learning algorithms\\nare designed for an adversarial scenario, they can be used, under some assumptions,\\nto derive accurate predictors for a distributional scenario. We derive learning\\nguarantees for this on-line to batch conversion. Finally, we brieﬂy point out the\\nconnection of on-line learning with game theory by describing their use to derive a\\nsimple proof of von Neumann’s minimax theorem.\\n7.1 Introduction\\nThe learning framework for on-line algorithms is in stark contrast to the PAC\\nlearning or stochastic models discussed up to this point. First, instead of learning\\nfrom a training set and then testing on a test set, the on-line learning scenario mixes148 On-Line Learning\\nthe training and test phases. Second, PAC learning follows the key assumption\\nthat the distribution over data points is ﬁxed over time, both for training and test\\npoints, and that points are sampled in an i.i.d. fashion. Under this assumption, the\\nnatural goal is to learn a hypothesis with a small expected loss or generalization\\nerror. In contrast, with on-line learning, no distributional assumption is made,\\nand thus there is no notion of generalization. Instead, the performance of on-line\\nlearning algorithms is measured using a mistake model and the notion of regret.T o\\nderive guarantees in this model, theoretical analyses are based on a worst-case or\\nadversarial assumption.\\nThe general on-line setting involves T rounds. At the tth round, the algorithm\\nreceives an instancex\\nt ∈X and makes a prediction ˆyt ∈Y . It then receives the true\\nlabel yt ∈ Y and incurs a loss L(ˆyt,y t), where L: Y×Y → R+ is a loss function.\\nMore generally, the prediction domain for the algorithm may beY′ ̸= Y and the loss\\nfunction deﬁned over Y′ ×Y . For classiﬁcation problems, we often have Y = {0, 1}\\nand L(y,y ′)= |y′ − y|, while for regression Y⊆ R and typically L(y,y ′)=( y′ − y)2.\\nThe objective in the on-line setting is to minimize the cumulative loss:∑T\\nt=1 L(ˆyt,y t)\\nover T rounds.\\n7.2 Prediction with expert advice\\nWe ﬁrst discuss the setting of online learning with expert advice, and the associated\\nnotion of regret.I nt h i ss e t t i n g ,a tt h etth round, in addition to receiving xt ∈X ,\\nthe algorithm also receives advice yt,i ∈ Y , i ∈ [1,N ], from N experts. Following\\nthe general framework of on-line algorithms, it then makes a prediction, receives\\nthe true label, and incurs a loss. After T rounds, the algorithm has incurred a\\ncumulative loss. The objective in this setting is to minimize the regret RT ,a l s o\\ncalled external regret, which compares the cumulative loss of the algorithm to that\\nof the best expert in hindsight after T rounds:\\nRT =\\nT∑\\nt=1\\nL(ˆyt,y t) −\\nN\\nmin\\ni=1\\nT∑\\nt=1\\nL(ˆyt,i,y t). (7.1)\\nThis problem arises in a variety of diﬀerent domains and applications. Figure 7.1\\nillustrates the problem of predicting the weather using several forecasting sources\\nas experts.\\n7.2.1 Mistake bounds and Halving algorithm\\nHere, we assume that the loss function is the standard zero-one loss used in\\nclassiﬁcation. To analyze the expert advice setting, we ﬁrst consider the realizable7.2 Prediction with expert advice 149\\nwunderground.com bbc.com weather.com cnn.com\\n?\\nalgorithm\\nbb\\nFigure 7.1 Weather forecast: an example of a prediction problem based on expert\\nadvice.\\ncase. As such, we discuss the mistake bound model,w h i c ha s k st h es i m p l eq u e s t i o n\\n“How many mistakes before we learn a particular concept?” Since we are in the\\nrealizable case, after some number of rounds T, we will learn the concept and no\\nlonger make errors in subsequent rounds. For any ﬁxed concept c,w ed e ﬁ n et h e\\nmaximum number of mistakes a learning algorithm A makes as\\nMA(c)= m a x\\nx1,...,xT\\n|mistakes(A,c)|. (7.2)\\nFurther, for any concept in a concept class C, the maximum number of mistakes a\\nlearning algorithm makes is\\nMA(C)=m a x\\nc∈C\\nMA(c). (7.3)\\nOur goal in this setting is to derivemistake bounds,t h a ti s ,ab o u n dM on MA(C).\\nWe will ﬁrst do this for the Halving algorithm, an elegant and simple algorithm for\\nwhich we can generate surprisingly favorable mistake bounds. At each round, the\\nHalving algorithm makes its prediction by taking the majority vote over all active\\nexperts. After any incorrect prediction, it deactivates all experts that gave faulty\\nadvice. Initially, all experts are active, and by the time the algorithm has converged\\nto the correct concept, the active set contains only those experts that are consistent\\nwith the target concept. The pseudocode for this algorithm is shown in ﬁgure 7.2.\\nWe also present straightforward mistake bounds in theorems 7.1 and 7.2, where\\nthe former deals with ﬁnite hypothesis sets and the latter relates mistake bounds to\\nVC-dimension. Note that the hypothesis complexity term in theorem 7.1 is identical\\nto the corresponding complexity term in the PAC model bound of theorem 2.1.\\nTheorem 7.1\\nLet H be a ﬁnite hypothesis set. Then\\nMHalving (H) ≤ log2 |H|. (7.4)\\nProof Since the algorithm makes predictions using majority vote from the active\\nset, at each mistake, the active set is reduced by at least half. Hence, after log2 |H|\\nmistakes, there can only remain one active hypothesis, and since we are in the150 On-Line Learning\\nHalving(H)\\n1 H1 ← H\\n2 for t ← 1 to T do\\n3 Receive(xt)\\n4 ˆyt ← MajorityVote(Ht,xt)\\n5 Receive(yt)\\n6 if (ˆyt ̸= yt) then\\n7 Ht+1 ←{ c ∈ Ht : c(xt)= yt}\\n8 return HT+1\\nFigure 7.2 Halving algorithm.\\nrealizable case, this hypothesis must coincide with the target concept.\\nTheorem 7.2\\nLet opt(H) be the optimal mistake boundfor H.T h e n ,\\nVCdim(H) ≤ opt(H) ≤ MHalving (H) ≤ log2 |H|. (7.5)\\nProof The second inequality is true by deﬁnition and the third inequality holds\\nbased on theorem 7.1. To prove the ﬁrst inequality, we let d =V C d i m (H). Then\\nthere exists a shattered set ofd points, for which we can form a complete binary tree\\nof the mistakes with heightd, and we can choose labels at each round of learning to\\nensure that d mistakes are made. Note that this adversarial argument is valid since\\nthe on-line setting makes no statistical assumptions about the data.\\n7.2.2 Weighted majority algorithm\\nIn the previous section, we focused on the realizable setting in which the Halving\\nalgorithm simply discarded experts after a single mistake. We now move to the\\nnon-realizable setting and use a more general and less extreme algorithm, the\\nWeighted Majority (WM) algorithm, that weights the importance of experts as\\na function of their mistake rate. The WM algorithm begins with uniform weights\\nover allN experts. At each round, it generates predictions using a weighted majority\\nvote. After receiving the true label, the algorithm then reduces the weight of each\\nincorrect expert by a factor of β ∈ [0, 1). Note that this algorithm reduces to the\\nHalving algorithm when β = 0. The pseudocode for the WM algorithm is shown in7.2 Prediction with expert advice 151\\nWeighted-Majority(N)\\n1 for i ← 1 to N do\\n2 w1,i ← 1\\n3 for t ← 1 to T do\\n4 Receive(xt)\\n5 if ∑\\ni: yt,i=1 wt,i ≥ ∑\\ni: yt,i=0 wt,i then\\n6 ˆyt ← 1\\n7 else ˆyt ← 0\\n8 Receive(yt)\\n9 if (ˆyt ̸= yt) then\\n10 for i ← 1 to N do\\n11 if (yt,i ̸= yt) then\\n12 wt+1,i ← βwt,i\\n13 else wt+1,i ← wt,i\\n14 return wT+1\\nFigure 7.3 Weighted majority algorithm, yt,y t,i ∈{ 0, 1}.\\nﬁgure 7.3.\\nSince we are not in the realizable setting, the mistake bounds of theorem 7.1\\ncannot apply. However, the following theorem presents a bound on the number of\\nmistakes mT made by the WM algorithm after T ≥ 1 rounds of on-line learning as\\na function of the number of mistakes made by the best expert, that is the expert\\nwho achieves the smallest number of mistakes for the sequence y\\n1,...,y T .L e tu s\\nemphasize that this is the best expert in hindsight.\\nTheorem 7.3\\nFix β ∈ (0, 1).L e tmT be the number of mistakes made by algorithm WM afterT ≥ 1\\nrounds, and m∗\\nT be the number of mistakes made by the best of theN experts. Then,\\nthe following inequality holds:\\nmT ≤\\nlog N + m∗\\nT log 1\\nβ\\nlog 2\\n1+β\\n. (7.6)\\nProof To prove this theorem, we ﬁrst introduce a potential function. We then\\nderive upper and lower bounds for this function, and combine them to obtain our152 On-Line Learning\\nresult. This potential function method is a general proof technique that we will use\\nthroughout this chapter.\\nFor any t ≥ 1, we deﬁne our potential function as Wt = ∑N\\ni=1 wt,i.S i n c e\\npredictions are generated using weighted majority vote, if the algorithm makes\\nan error at round t,t h i si m p l i e st h a t\\nW\\nt+1 ≤\\n[\\n1/2+( 1/2)β\\n]\\nWt =\\n[1+ β\\n2\\n]\\nWt. (7.7)\\nSince W1 = N and mT mistakes are made afterT rounds, we thus have the following\\nupper bound:\\nWT ≤\\n[1+ β\\n2\\n]mT\\nN. (7.8)\\nNext, since the weights are all non-negative, it is clear that for any expert i,\\nWT ≥ wT,i = βmT,i ,w h e r emT,i is the number of mistakes made by the ith expert\\nafter T rounds. Applying this lower bound to the best expert and combining it with\\nthe upper bound in (7.8) gives us:\\nβm∗\\nT ≤ WT ≤\\n[1+ β\\n2\\n]mT\\nN\\n⇒ m∗\\nT log β ≤ log N + mT log\\n[1+ β\\n2\\n]\\n⇒ mT log\\n[ 2\\n1+ β\\n]\\n≤ log N + m∗\\nT log 1\\nβ,\\nwhich concludes the proof.\\nThus, the theorem guarantees a bound of the following form for algorithm WM:\\nmT ≤ O(log N)+c o n s t a n t×| mistakes of best expert|.\\nSince the ﬁrst term varies only logarithmically as a function of N,t h et h e o r e m\\nguarantees that the number of mistakes is roughly a constant times that of the best\\nexpert in hindsight. This is a remarkable result, especially because it requires no\\nassumption about the sequence of points and labels generated. In particular, the\\nsequence could be chosen adversarially. In the realizable case where m\\n∗\\nT =0 ,t h e\\nbound reduces to mT ≤ O(log N) as for the Halving algorithm.\\n7.2.3 Randomized weighted majority algorithm\\nIn spite of the guarantees just discussed, the WM algorithm admits a drawback that\\naﬀects all deterministic algorithms in the case of the zero-one loss: no deterministic\\nalgorithm can achieve a regret R\\nT = o(T) over all sequences. Clearly, for any7.2 Prediction with expert advice 153\\nRandomized-Weighted-Majority (N)\\n1 for i ← 1 to N do\\n2 w1,i ← 1\\n3 p1,i ← 1/N\\n4 for t ← 1 to T do\\n5 for i ← 1 to N do\\n6 if (lt,i =1 ) then\\n7 wt+1,i ← βwt,i\\n8 else wt+1,i ← wt,i\\n9 Wt+1 ← ∑N\\ni=1 wt+1,i\\n10 for i ← 1 to N do\\n11 pt+1,i ← wt+1,i/Wt+1\\n12 return wT+1\\nFigure 7.4 Randomized weighted majority algorithm.\\ndeterministic algorithm A and any t ∈ [1,T ], we can adversarially select yt to\\nbe 1 if the algorithm predicts 0, and choose it to be 0 otherwise. Thus, A errs at\\nevery point of such a sequence and its cumulative mistake is mT = T.A s s u m ef o r\\nexample that N = 2 and that one expert always predicts 0, the other one always 1.\\nThe error of the best expert over that sequence (and in fact any sequence of that\\nl e n g t h )i st h e na tm o s tm\\n∗\\nT ≤ T/2. Thus, for that sequence, we have\\nRT = mT − m∗\\nT ≥ T/2,\\nwhich shows that RT = o(T) cannot be achieved in general. Note that this does\\nnot contradict the bound proven in the previous section, since for any β ∈ (0, 1),\\nlog 1\\nβ\\nlog 2\\n1+β\\n≥ 2. As we shall see in the next section, this negative result does not hold\\nfor any loss that is convex with respect to one of its arguments. But for the zero-one\\nloss, this leads us to consider randomized algorithms instead.\\nIn the randomized scenario of on-line learning, we assume that a set A =\\n{1,...,N } of N actions is available. At each round t ∈ [1,T ], an on-line algorithm\\nA selects a distributionpt over the set of actions, receives a loss vectorlt,w h o s eith\\ncomponent lt,i ∈ [0,1] is the loss associated with action i, and incurs the expected\\nloss Lt = ∑N\\ni=1 pt,i lt,i. The total loss incurred by the algorithm over T rounds\\nis LT = ∑T\\nt=1 Lt.T h et o t a ll o s sa s s o c i a t e dt oa c t i o ni is LT,i = ∑T\\nt=1 lt,i.T h e154 On-Line Learning\\nminimal loss of a single action is denoted by Lmin\\nT =m i ni∈A LT,i.T h er e g r e tRT of\\nthe algorithm after T rounds is then typically deﬁned by the diﬀerence of the loss\\nof the algorithm and that of the best single action:1\\nRT = LT −L min\\nT .\\nHere, we consider speciﬁcally the case of zero-one losses and assume thatlt,i ∈{ 0,1}\\nfor all t ∈ [1,T ]a n di ∈ A.\\nThe WM algorithm admits a straightforward randomized version, the randomized\\nweighted majority (RWM) algorithm. The pseudocode of this algorithm is given in\\nﬁgure 7.4. The algorithm updates the weightw\\nt,i of experti as in the case of the WM\\nalgorithm by multiplying it by β. The following theorem gives a strong guarantee\\non the regret RT of the RWM algorithm, showing that it is in O(√T log N).\\nTheorem 7.4\\nFix β ∈ [1/2,1).T h e n ,f o ra n yT ≥ 1, the loss of algorithm RWM on any sequence\\ncan be bounded as follows:\\nLT ≤ log N\\n1 − β +( 2 − β)Lmin\\nT . (7.9)\\nIn particular, for β =m a x{1/2, 1 −\\n√\\n(log N)/T }, the loss can be bounded as:\\nLT ≤L min\\nT +2\\n√\\nT log N. (7.10)\\nProof As in the proof of theorem 7.3, we derive upper and lower bounds for the\\npotential function Wt = ∑N\\ni=1 wt,i, t ∈ [1,T ], and combine these bounds to obtain\\nthe result. By deﬁnition of the algorithm, for any t ∈ [1,T ], Wt+1 can be expressed\\nas follows in terms of Wt:\\nWt+1 =\\n∑\\ni: lt,i=0\\nwt,i + β\\n∑\\ni: lt.i=1\\nwt,i = Wt +( β − 1)\\n∑\\ni: lt,i=1\\nwt,i\\n= Wt +( β − 1)Wt\\n∑\\ni: lt,i=1\\npt,i\\n= Wt +( β − 1)WtLt\\n= Wt(1 − (1 − β)Lt).\\nThus, since W1 = N, it follows that WT+1 = N ∏T\\nt=1(1 − (1 − β)Lt). On the other\\nhand, the following lower bound clearly holds: WT+1 ≥ maxi∈[1,N] wT+1,i = βLmin\\nT .\\nThis leads to the following inequality and series of derivations after taking the log\\n1. Alternative deﬁnitions of the regret with comparison classes diﬀerent from the set of\\nsingle actions can be considered.7.2 Prediction with expert advice 155\\nand using the inequalities log(1 − x) ≤− x valid for all x< 1, and − log(1 − x) ≤\\nx + x2 valid for all x ∈ [0, 1/2]:\\nβLmin\\nT ≤ N\\nT∏\\nt=1\\n(1 − (1 − β)Lt)= ⇒L min\\nT log β ≤ log N +\\nT∑\\nt=1\\nlog(1 − (1 − β)Lt)\\n=⇒L min\\nT log β ≤ log N − (1 − β)\\nT∑\\nt=1\\nLt\\n=⇒L min\\nT log β ≤ log N − (1 − β)LT\\n=⇒L T ≤ log N\\n1 − β − log β\\n1 − βLmin\\nT\\n=⇒L T ≤ log N\\n1 − β − log(1 − (1 − β))\\n1 − β Lmin\\nT\\n=⇒L T ≤ log N\\n1 − β +( 2 − β)Lmin\\nT .\\nThis shows the ﬁrst statement. Since Lmin\\nT ≤ T, this also implies\\nLT ≤ log N\\n1 − β +( 1 − β)T + Lmin\\nT . (7.11)\\nDiﬀerentiating the upper bound with respect to β and setting it to zero gives\\nlog N\\n(1−β)2 − T =0 ,t h a ti s β = β0 =1 −\\n√\\n(log N)/T,s i n c eβ< 1. Thus, if\\n1 −\\n√\\n(log N)/T ≥ 1/2, β0 is the minimizing value ofβ, otherwise 1/2 is the optimal\\nvalue. The second statement follows by replacing β with β0 in (7.11).\\nThe bound (7.10) assumes that the algorithm additionally receives as a parameter\\nthe number of rounds T.A sw es h a l ls e ei nt h en e x ts e c t i o n ,h o w e v e r ,t h e r ee x i s t s\\na general doubling trick that can be used to relax this requirement at the price of\\na small constant factor increase. Inequality 7.10 can be written directly in terms of\\nthe regret R\\nT of the RWM algorithm:\\nRT ≤ 2\\n√\\nT log N. (7.12)\\nThus, for N constant, the regret veriﬁes RT = O(\\n√\\nT)a n dt h eaverage regret or\\nregret per roundRT /T decreases as O(1/\\n√\\nT) .T h e s er e s u l t sa r eo p t i m a l ,a ss h o w n\\nby the following theorem.\\nTheorem 7.5\\nLet N =2 . There exists a stochastic sequence of losses for which the regret of any\\non-line learning algorithm veriﬁes E[RT ] ≥\\n√\\nT/8.\\nProof For any t ∈ [1,T ], let the vector of losses lt take the values l01 =( 0,1)⊤\\nand l10 =( 1, 0)⊤ with equal probability. Then, the expected loss of any randomized156 On-Line Learning\\nalgorithm A is\\nE[LT ]=E\\n[ T∑\\nt=1\\npt · lt\\n]\\n=\\nT∑\\nt=1\\npt · E[lt]=\\nT∑\\nt=1\\n1\\n2pt,1 + 1\\n2(1 − pt,1)= T/2,\\nw h e r ew ed e n o t e db ypt the distribution selected by A at round t. By deﬁnition,\\nLmin\\nT c a nb ew r i t t e na sf o l l o w s :\\nLmin\\nT =m i n{LT,1, LT,2} = 1\\n2(LT,1 + LT,2 −| LT,1 −L T,2|)= T/2 −| LT,1 − T/2|,\\nusing the fact that LT,1 + LT,2 = T. Thus, the expected regret of A is\\nE[RT ]=E [ LT ] − E[Lmin\\nT ]=E [ |LT,1 − T/2|].\\nLet σt, t ∈ [1,T ], denote Rademacher variables taking values in {−1,+1},t h e n\\nLT,1 can be rewritten as LT,1 = ∑T\\nt=1\\n1+σt\\n2 = T/2+ 1\\n2\\n∑T\\nt=1 σt. Thus, introducing\\nscalars xt =1 /2, t ∈ [1,T ], by the Khintchine-Kahane inequality (D.22), we have:\\nE[RT ]=E\\n[\\n|\\nT∑\\nt=1\\nσtxt|\\n]\\n≥\\n\\ued6a\\ued6b\\ued6b\\n√\\n1\\n2\\nT∑\\nt=1\\nx2\\nt =\\n√\\nT/8,\\nwhich concludes the proof.\\nMore generally, for T ≥ N, a lower bound of RT =Ω (√T log N) can be proven for\\nthe regret of any algorithm.\\n7.2.4 Exponential weighted average algorithm\\nThe WM algorithm can be extended to other loss functions L taking values in\\n[0,1]. The Exponential Weighted Average algorithm presented here can be viewed\\nas that extension for the case whereL is convex in its ﬁrst argument. Note that this\\nalgorithm is deterministic and yet, as we shall see, admits a very favorable regret\\nguarantee. Figure 7.5 gives its pseudocode. At round t ∈ [1,T ], the algorithm’s\\nprediction is\\nˆy\\nt =\\n∑N\\ni=1 wt,iyt,i\\n∑N\\ni=1 wt,i\\n, (7.13)\\nwhere yt,i is the prediction by experti and wt,i the weight assigned by the algorithm\\nto that expert. Initially, all weights are set to one. The algorithm then updates the\\nweights at the end of round t according to the following rule:\\nwt+1,i ← wt,i e−ηL(byt,i,yt) = e−ηLt,i , (7.14)7.2 Prediction with expert advice 157\\nExponential-Weighted-A verage(N)\\n1 for i ← 1 to N do\\n2 w1,i ← 1\\n3 for t ← 1 to T do\\n4 Receive(xt)\\n5 ˆyt ←\\nPN\\ni=1 wt,iyt,iPN\\ni=1 wt,i\\n6 Receive(yt)\\n7 for i ← 1 to N do\\n8 wt+1,i ← wt,i e−ηL(byt,i,yt)\\n9 return wT+1\\nFigure 7.5 Exponential weighted average, L(byt,i,y t) ∈ [0, 1].\\nwhere Lt,i is the total loss incurred by expert i after t rounds. Note that this\\nalgorithm, as well as the others presented in this chapter, are simple, since they\\ndo not require keeping track of the losses incurred by each expert at all previous\\nrounds but only of their cumulative performance. Furthermore, this property is also\\ncomputationally advantageous. The following theorem presents a regret bound for\\nthis algorithm.\\nTheorem 7.6\\nAssume that the loss function L is convex in its ﬁrst argument and takes values\\nin [0,1].T h e n ,f o ra n yη> 0 and any sequence y\\n1,...,y T ∈ Y , the regret of the\\nExponential Weighted Average algorithm after T rounds satisﬁes\\nRT ≤ log N\\nη + ηT\\n8 . (7.15)\\nIn particular, for η=\\n√\\n8l o gN/T, the regret is bounded as\\nRT ≤\\n√\\n(T/2) logN. (7.16)\\nProof We apply the same potential function analysis as in previous proofs but\\nusing as potential Φt =l o g∑N\\ni=1 wt,i, t ∈ [1,T ]. Let pt denote the distribution over\\n{1,...,N } with pt,i = wt,iPN\\ni=1 wt,i\\n. To derive an upper bound on Φt,w eﬁ r s te x a m i n e158 On-Line Learning\\nthe diﬀerence of two consecutive potential values:\\nΦt+1 − Φt =l o g\\n∑N\\ni=1 wt,i e−ηL(byt,i,yt)\\n∑N\\ni=1 wt,i\\n=l o g\\n(\\nE\\npt\\n[eηX]\\n⎡\\n,\\nwith X = −L(ˆyt,i,y t) ∈ [−1, 0]. To upper bound the expression appearing in the\\nright-hand side, we apply Hoeﬀding’s lemma (lemma D.1) to the centered random\\nvariable X − Ept [X], then Jensen’s inequality (theorem B.4) using the convexity of\\nL with respect to its ﬁrst argument:\\nΦt+1 − Φt =l o g\\n(\\nE\\npt\\n[\\neη(X−E[X])+ηE[X]]⎡\\n≤ η2\\n8 + ηE\\npt\\n[X]= η2\\n8 − ηE\\npt\\n[L(ˆyt,i,y t)] (Hoeﬀding’s lemma)\\n≤− ηL\\n(\\nE\\npt\\n[ˆyt,i],y t\\n⎡\\n+ η2\\n8 (convexity of ﬁrst arg. of L)\\n= −ηL(ˆyt,y t)+ η2\\n8 .\\nSumming up these inequalities yields the following upper bound:\\nΦT+1 − Φ1 ≤− η\\nT∑\\nt=1\\nL(ˆyt,y t)+ η2T\\n8 . (7.17)\\nWe obtain a lower bound for the same quantity as follows:\\nΦT+1 −Φ1 =l o g\\nN∑\\ni=1\\ne−ηLT,i −log N ≥ log\\nN\\nmax\\ni=1\\ne−ηLT,i −log N = −η\\nN\\nmin\\ni=1\\nLT,i −log N.\\nCombining the upper and lower bounds yields:\\n− η\\nN\\nmin\\ni=1\\nLT,i − log N ≤− η\\nT∑\\nt=1\\nL(ˆyt,y t)+ η2T\\n8\\n=⇒\\nT∑\\nt=1\\nL(ˆyt,y t) −\\nN\\nmin\\ni=1\\nLT,i ≤ log N\\nη + ηT\\n8 ,\\nand concludes the proof.\\nThe optimal choice ofηin theorem 7.6 requires knowledge of the horizonT,w h i c hi s\\nan apparent disadvantage of this analysis. However, we can use a standarddoubling\\ntrick to eliminate this requirement, at the price of a small constant factor. This\\nconsists of dividing time into periods [2 k,2k+1 − 1] of length 2k with k =0 ,...,n\\nand T ≥ 2n−1, and then chooseηk =\\n√\\n8 logN\\n2k in each period. The following theorem\\npresents a regret bound when using the doubling trick to select η. A more general7.3 Linear classiﬁcation 159\\nmethod consists of interpreting η as a function of time, i.e., ηt =\\n√\\n(8 logN)/t,\\nwhich can lead to a further constant factor improvement over the regret bound of\\nthe following theorem.\\nTheorem 7.7\\nAssume that the loss function L is convex in its ﬁrst argument and takes values\\nin [0,1].T h e n ,f o ra n yT ≥ 1 and any sequence y\\n1,...,y T ∈ Y , the regret of the\\nExponential Weighted Average algorithm after T rounds is bounded as follows:\\nRT ≤\\n√\\n2√\\n2 − 1\\n√\\n(T/2) logN +\\n√\\nlog N/2. (7.18)\\nProof Let T ≥ 1a n dl e tIk =[ 2k, 2k+1 − 1], for k ∈ [0,n ], with n = ⌊log(T +1 )⌋.\\nLet LIk denote the loss incurred in the interval Ik. By theorem 7.6 (7.16), for any\\nk ∈ [0,n ], we have\\nLIk −\\nN\\nmin\\ni=1\\nLIk,i ≤\\n√\\n2k/2l o gN. (7.19)\\nThus, we can bound the total loss incurred by the algorithm after T rounds as:\\nLT =\\nn∑\\nk=0\\nLIk ≤\\nn∑\\nk=0\\nN\\nmin\\ni=1\\nLIk,i +\\nn∑\\nk=0\\n√\\n2k (log N)/2\\n≤\\nN\\nmin\\ni=1\\nLT,i +\\n√\\n(log N)/2 ·\\nn∑\\nk=0\\n2\\nk\\n2 , (7.20)\\nwhere the second inequality follows from the super-additivity of min, that is\\nmini Xi +m i ni Yi ≤ mini(Xi + Yi) for any sequences (Xi)i and (Yi)i, which implies∑n\\nk=0 minN\\ni=1 LIk,i ≤ minN\\ni=1\\n∑n\\nk=0 LIk,i. The geometric sum appearing in the right-\\nhand side of (7.20) can be expressed as follows:\\nn∑\\nk=0\\n2\\nk\\n2 = 2(n+1)/2 − 1√\\n2 − 1 ≤\\n√\\n2\\n√\\nT +1 − 1√\\n2 − 1 ≤\\n√\\n2(\\n√\\nT +1 ) − 1√\\n2 − 1 =\\n√\\n2\\n√\\nT√\\n2 − 1 +1 .\\nPlugging back into (7.20) and rearranging terms yields (7.18).\\nThe O(\\n√\\nT) dependency on T presented in this bound cannot be improved for\\ngeneral loss functions.\\n7.3 Linear classiﬁcation\\nThis section presents two well-known on-line learning algorithms for linear classiﬁ-\\ncation: the Perceptron and Winnow algorithms.160 On-Line Learning\\nPerceptron(w0)\\n1 w1 ← w0 ⊿ typically w0 = 0\\n2 for t ← 1 to T do\\n3 Receive(xt)\\n4 ˆyt ← sgn(wt · xt)\\n5 Receive(yt)\\n6 if (ˆyt ̸= yt) then\\n7 wt+1 ← wt + ytxt ⊿ more generally ηytxt,η> 0.\\n8 else wt+1 ← wt\\n9 return wT+1\\nFigure 7.6 Perceptron algorithm.\\n7.3.1 Perceptron algorithm\\nThe Perceptron algorithm is one of the earliest machine learning algorithms. It is\\nan on-line linear classiﬁcation algorithm. Thus, it learns a decision function based\\non a hyperplane by processing training points one at a time. Figure 7.6 gives its\\npseudocode.\\nThe algorithm maintains a weight vector w\\nt ∈ RN deﬁning the hyperplane\\nlearned, starting with an arbitrary vector w0. At each round t ∈ [1,T ], it predicts\\nthe label of the point xt ∈ RN received, using the current vectorwt (line 4). When\\nthe prediction made does not match the correct label (lines 6-7), it updates wt by\\nadding ytxt. More generally, when a learning rate η> 0 is used, the vector added\\nis ηytxt. This update can be partially motivated by examining the inner product of\\nthe current weight vector with ytxt, whose sign determines the classiﬁcation of xt.\\nJust before an update, xt is misclassiﬁed and thus ytwt · xt is negative; afterward,\\nytwt+1 ·xt = ytwt ·ytxt +η∥xt∥2, thus, the update corrects the weight vector in the\\ndirection of making the inner product positive by augmenting it with this quantity\\nwith η∥x\\nt∥2 > 0.\\nThe Perceptron algorithm can be shown in fact to seek a weight vector w\\nminimizing an objective function F precisely based on the quantities ( −ytw · xt),\\nt ∈ [1,T ]. Since (−ytw · xt) is positive when xt is misclassiﬁed by w, F is deﬁned7.3 Linear classiﬁcation 161\\nw1\\nw2\\nw3\\nw4\\nw5\\nFigure 7.7 An example path followed by the iterative stochastic gradient descent\\ntechnique. Each inner contour indicates a region of lower elevation.\\nfor all w ∈ RN by\\nF(w)= 1\\nT\\nT∑\\nt=1\\nmax\\n(\\n0, −yt(w · xt)\\n⎡\\n=E\\nx∼ bD\\n[ ˜F(w,x)], (7.21)\\nwhere ˜F(w,x)=m a x\\n(\\n0, −f(x)(w · x)\\n⎡\\nwith f(x) denoting the label of x,a n d\\nˆD is the empirical distribution associated with the sample ( x1,..., xT ). For any\\nt ∈ [1,T ], w ↦→− yt(w · xt) is linear and thus convex. Since the max operator pre-\\nserves convexity, this shows thatF is convex. However,F is not diﬀerentiable. Nev-\\nertheless, the Perceptron algorithm coincides with the application of the stochastic\\ngradient descent technique to F.\\nThe stochastic (or on-line) gradient descent technique examines one point xt at\\na time. For a function ˜F, a generalized version of this technique can be deﬁned by\\nthe execution of the following update for each point xt:\\nwt+1 ←\\n{\\nwt − η∇w ˜F(wt,xt)i f w ↦→ ˜F(w,xt) diﬀerentiable at wt,\\nwt otherwise,\\n(7.22)\\nwhere η> 0 is a learning rate parameter. Figure 7.7 illustrates an example path\\nthe gradient descent follows. In the speciﬁc case we are considering, w ↦→ ˜F(w,xt)\\nis diﬀerentiable at any w such that yt(w · xt) ̸=0w i t h ∇w ˜F(w,xt)= −yxt if\\nyt(w · xt) < 0a n d∇w ˜F(w,xt)=0i f yt(w · xt) > 0. Thus, the stochastic gradient\\ndescent update becomes\\nwt+1 ←\\n⎧\\n⎪⎪⎨\\n⎪⎪\\n⎩\\nw\\nt + ηytxt if yt(w · xt) < 0;\\nwt if yt(w · xt) > 0;\\nwt otherwise,\\n(7.23)\\nwhich coincides exactly with the update of the Perceptron algorithm.\\nThe following theorem gives a margin-based upper bound on the number of\\nmistakes or updates made by the Perceptron algorithm when processing a sequence162 On-Line Learning\\nof T points that can be linearly separated by a hyperplane with margin ρ> 0.\\nTheorem 7.8\\nLet x1,..., xT ∈ RN be a sequence of T points with ∥xt∥≤ r for all t ∈ [1,T ],f o r\\nsome r> 0. Assume that there exist ρ> 0 and v ∈ RN such that for all t ∈ [1,T ],\\nρ ≤ yt(v·xt)\\n∥v∥ . Then, the number of updates made by the Perceptron algorithm when\\nprocessing x1,..., xT is bounded by r2/ρ2.\\nProof Let I be the subset of the T rounds at which there is an update, and let\\nM be the total number of updates, i.e., |I| = M. Summing up the assumption\\ninequalities yields:\\nMρ ≤ v · ∑\\nt∈I ytxt\\n∥v∥ ≤\\n\\ued79\\ued79\\n\\ued79\\n∑\\nt∈I\\nytxt\\n\\ued79\\ued79\\n\\ued79 (Cauchy-Schwarz inequality )\\n=\\n\\ued79\\ued79\\n\\ued79\\n∑\\nt∈I\\n(wt+1 − wt)\\n\\ued79\\ued79\\n\\ued79 (deﬁnition of updates)\\n= ∥w\\nT+1∥ (telescoping sum, w0 =0 )\\n=\\n√∑\\nt∈I\\n∥wt+1∥2 −∥ wt∥2 (telescoping sum, w0 =0 )\\n=\\n√∑\\nt∈I\\n∥wt + ytxt∥2 −∥ wt∥2 (deﬁnition of updates)\\n=\\n\\ued6a\\ued6b\\ued6b\\n√\\n∑\\nt∈I\\n2 ytwt · xt\\ued19 \\ued18\\ued17 \\ued1a\\n≤0\\n+∥xt∥2\\n≤\\n√∑\\nt∈I\\n∥xt∥2 ≤\\n√\\nMr2.\\nComparing the left- and right-hand sides gives\\n√\\nM ≤ r/ρ,t h a ti s ,M ≤ r2/ρ2.\\nBy deﬁnition of the algorithm, the weight vector wT after processing T points is a\\nlinear combination of the vectorsxt at which an update was made:wT = ∑\\nt∈I ytxt.\\nThus, as in the case of SVMs, these vectors can be referred to as support vectors\\nfor the Perceptron algorithm.\\nThe bound of theorem 7.8 is remarkable, since it depends only on the normalized\\nmargin ρ/r and not on the dimension N of the space. This bound can be shown\\nto be tight, that is the number of updates can be equal to r2/ρ2 in some instances\\n(see exercise 7.3 to show the upper bound is tight).\\nThe theorem required no assumption about the sequence of points x1,..., xT .\\nA standard setting for the application of the Perceptron algorithm is one where a\\nﬁnite sample S of size m<T is available and where the algorithm makes multiple7.3 Linear classiﬁcation 163\\npasses over thesem points. The result of the theorem implies that whenS is linearly\\nseparable, the Perceptron algorithm converges after a ﬁnite number of updates and\\nthus passes. For a small margin ρ, the convergence of the algorithm can be quite\\nslow, however. In fact, for some samples, regardless of the order in which the points\\nin S are processed, the number of updates made by the algorithm is in Ω(2\\nN )( s e e\\nexercise 7.1). Of course, ifS is not linearly separable, the Perceptron algorithm does\\nnot converge. In practice, it is stopped after some number of passes over S.\\nThere are many variants of the standard Perceptron algorithm which are used\\nin practice and have been theoretically analyzed. One notable example is thevoted\\nPerceptron algorithm, which predicts according to the rule sgn\\n(\\n(∑\\nt∈I ctwt) · x\\n⎡\\n,\\nwhere ct is a weight proportional to the number of iterations thatwt survives, i.e.,\\nthe number of iterations between wt and wt+1.\\nFor the following theorem, we consider the case where the Perceptron algorithm\\nis trained via multiple passes till convergence over a ﬁnite sample that is linearly\\nseparable. In view of theorem 7.8, convergence occurs after a ﬁnite number of\\nupdates.\\nFor a linearly separable sample S,w ed e n o t eb yr\\nS the radius of the smallest\\nsphere containing all points in S and by ρS the largest margin of a separating\\nhyperplane for S.W ea l s od e n o t eb yM(S) the number of updates made by the\\nalgorithm after training over S.\\nTheorem 7.9\\nAssume that the data is linearly separable. LethS be the hypothesis returned by the\\nPerceptron algorithm after training over a sample S of size m drawn according to\\nsome distribution D. Then, the expected error ofhS is bounded as follows:\\nE\\nS∼Dm\\n[R(hS)] ≤ E\\nS∼Dm+1\\n[min\\n(\\nM(S),r 2\\nS/ρ2\\nS\\n⎡\\nm +1\\n]\\n.\\nProof Let S be a linearly separable sample of size m + 1 drawn i.i.d. according\\nto D and let x be a point in S.I f hS−{x} misclassiﬁes x,t h e nx must be a support\\nvector for hS. Thus, the leave-one-out error of the Perceptron algorithm on sample\\nS is at most M(S)\\nm+1 . The result then follows lemma 4.1, which relates the expected\\nleave-one-out error to the expected error, along with the upper bound on M(S)\\ngiven by theorem 7.8.\\nThis result can be compared with a similar one given for the SVM algorithm (with\\nno oﬀset) in the following theorem, which is an extension of theorem 4.1. We denote\\nby N\\nSV(S) the number of support vectors that deﬁne the hypothesis hS returned\\nb yS V M sw h e nt r a i n e do nas a m p l eS.164 On-Line Learning\\nTheorem 7.10\\nAssume that the data is linearly separable. Let hS be the hypothesis returned by\\nSVMs used with no oﬀset (b =0 ) after training over a sample S of size m drawn\\naccording to some distribution D. Then, the expected error of hS is bounded as\\nfollows:\\nE\\nS∼Dm\\n[R(hS)] ≤ E\\nS∼Dm+1\\n[min\\n(\\nNSV(S),r 2\\nS/ρ2\\nS\\n⎡\\nm +1\\n]\\n.\\nProof The fact that the expected error can be upper bounded by the average\\nfraction of support vectors ( NSV(S)/(m + 1)) was already shown by theorem 4.1.\\nThus, it suﬃces to show that it is also upper bounded by the expected value of\\n(r\\n2\\nS/ρ2\\nS\\n)/(m + 1). To do so, we will bound the leave-one-out error of the SVM\\nalgorithm for a sample S of size m +1b y( r2\\nS/ρ2\\nS\\n)/(m + 1). The result will then\\nfollow by lemma 4.1, which relates the expected leave-one-out error to the expected\\nerror.\\nLet S =( x\\n1,..., xm+1) be a linearly separable sample drawn i.i.d. according to\\nD and let x be a point in S that is misclassiﬁed by hS−{x}. We will analyze the\\ncase where x = xm+1, the analysis of other cases is similar. We denote by S′ the\\nsample (x1,..., xm).\\nFor any q ∈ [1,m + 1], let Gq denote the function deﬁned over Rq by Gq : α ↦→∑q\\ni=1 αi − 1\\n2\\n∑q\\ni,j=1 αiαjyiyj(xi · xj). Then, Gm+1 is the objective function of the\\ndual optimization problem for SVMs associated to the sample S and Gm the one\\nfor the sample S′.L e t α ∈ Rm+1 denote a solution of the dual SVM problem\\nmaxα ≥0 Gm+1(α)a n dα′ ∈ Rm+1 the vector such that ( α′\\n1,...,α ′\\nm)⊤ ∈ Rm is a\\nsolution of max α ≥0 Gm(α)a n d α′\\nm+1 =0 .L e t em+1 denote the ( m +1 ) t hu n i t\\nvector in Rm+1.B yd e ﬁ n i t i o no fα and α′ as maximizers, max β≥0 Gm+1(α′ +\\nβem+1) ≤ Gm+1(α)a n d Gm+1(α − αm+1em+1) ≤ Gm(α′). Thus, the quantity\\nA = Gm+1(α) − Gm(α′) admits the following lower and upper bounds:\\nmax\\nβ≥0\\nGm+1(α′ + βem+1) − Gm(α′) ≤ A ≤ Gm+1(α) − Gm+1(α − αm+1em+1).\\nLet w = ∑m+1\\ni=1 yiαixi denote the weight vector returned by SVMs for the sample\\nS.S i n c ehS′ misclassiﬁes xm+1, xm+1 must be a support vector for hS,t h u s7.3 Linear classiﬁcation 165\\nym+1w · xm+1 = 1. In view of that, the upper bound can be rewritten as follows:\\nGm+1(α) − Gm+1(α − αm+1em+1)\\n= αm+1 −\\nm+1∑\\ni=1\\n(yiαixi) · (ym+1αm+1xm+1)+ 1\\n2α2\\nm+1∥xm+1∥2\\n= αm+1(1 − ym+1w · xm+1)+ 1\\n2α2\\nm+1∥xm+1∥2\\n= 1\\n2α2\\nm+1∥xm+1∥2.\\nSimilarly, let w′ = ∑m\\ni=1 yiα′\\nixi. Then, for any β ≥ 0, the quantity maximized in\\nt h el o w e rb o u n dc a nb ew r i t t e na s\\nGm+1(α′ + βem+1) − Gm(α′)\\n= β\\n(\\n1 − ym+1(w′ + βxm+1) · xm+1\\n⎡\\n+ 1\\n2β2∥xm+1∥2\\n= β(1 − ym+1w′ · xm+1) − 1\\n2β2∥xm+1∥2.\\nThe right-hand side is maximized for the following value of β: 1−ym+1w′·xm+1\\n∥xm+1∥2 .\\nPlugging in this value in the right-hand side gives 1\\n2\\n(1−ym+1w′ ·xm+1)\\n∥xm+1∥2 .T h u s ,\\nA ≥ 1\\n2\\n(1 − ym+1w′ · xm+1)\\n∥xm+1∥2 ≥ 1\\n2∥xm+1∥2 ,\\nusing the fact thatym+1w′ ·xm+1 < 0, sincexm+1 is misclassiﬁed by w′. Comparing\\nthis lower bound onA with the upper bound previously derived leads to 1\\n2∥xm+1∥2 ≤\\n1\\n2 α2\\nm+1∥xm+1∥2,t h a ti s\\nαm+1 ≥ 1\\n∥xm+1∥2 ≥ 1\\nr2\\nS\\n.\\nThe analysis carried out in the casex = xm+1 holds similarly for anyxi in S that is\\nmisclassiﬁed by hS−{xi}.L e tI denote the set of such indicesi. Then, we can write:\\n∑\\ni∈I\\nαi ≥ |I|\\nr2\\nS\\n.\\nBy (4.18), the following simple expression holds for the margin: ∑m+1\\ni=1 αi =1 /ρ2\\nS.\\nUsing this identity leads to\\n|I|≤ r2\\nS\\n∑\\ni∈I\\nαi ≤ r2\\nS\\nm+1∑\\ni=1\\nαi = r2\\nS\\nρ2\\nS\\n.166 On-Line Learning\\nSince by deﬁnition |I| is the total number of leave-one-out errors, this concludes the\\nproof.\\nThus, the guarantees given by theorem 7.9 and theorem 7.10 in the separable\\ncase have a similar form. These bounds do not seem suﬃcient to distinguish the\\neﬀectiveness of the SVM and Perceptron algorithms. Note, however, that while the\\nsame margin quantity ρ\\nS appears in both bounds, the radius rS can be replaced by\\na ﬁner quantity that is diﬀerent for the two algorithms: in both cases, instead of the\\nradius of the sphere containing all sample points, rS can be replaced by the radius\\nof the sphere containing the support vectors, as can be seen straightforwardly from\\nthe proof of the theorems. Thus, the position of the support vectors in the case\\nof SVMs can provide a more favorable guarantee than that of the support vectors\\n(update vectors) for the Perceptron algorithm. Finally, the guarantees given by\\nthese theorems are somewhat weak. These are not high probability bounds, they\\nhold only for the expected error of the hypotheses returned by the algorithms and\\nin particular provide no information about the variance of their error.\\nThe following theorem presents a bound on the number of updates or mistakes\\nmade by the Perceptron algorithm in the more general scenario of a non-linearly\\nseparable sample.\\nTheorem 7.11\\nLet x\\n1,..., xT ∈ RN be a sequence of T points with ∥xt∥≤ r for all t ∈ [1,T ],f o r\\nsome r> 0.L e tv ∈ RN be any vector with ∥v∥ =1 and let ρ> 0. Deﬁne the\\ndeviation of xt by dt =m a x{0,ρ − yt(v · xt)},a n dl e tδ =\\n√∑T\\nt=1 d2\\nt . Then, the\\nnumber of updates made by the Perceptron algorithm when processingx1,..., xT is\\nbounded by (r + δ)2/ρ2.\\nProof We ﬁrst reduce the problem to the separable case by mapping each input\\nvector xt ∈ RN to a vector in x′\\nt ∈ RN+T as follows:\\nxt =\\n⎡\\n⎢⎢⎣\\nxt,1\\n..\\n.\\nx\\nt,N\\n⎤\\n⎥⎥⎦ ↦→ x′\\nt =\\n⎡\\n⎣xt,1 ... x t,N 0 ... 0Δ \\ued19\\ued18\\ued17\\ued1a\\n(N + t)th\\ncomponent\\n0 ... 0\\n⎤\\n⎦\\n⊤\\n,\\nwhere the ﬁrst N components of x′\\nt are identical to those of x and the only other\\nnon-zero component is the ( N + t)th component and is equal to Δ. The value of\\nthe parameter Δ will be set later. The vector v is replaced by the vectorv′ deﬁned\\nas follows:\\nv′ =\\n[\\nv1/Z . . . v N /Z y 1d1/(Δ Z) ... y T dT /(Δ Z)\\n]⊤\\n.\\nThe ﬁrst N components ofv′ are equal to the components ofv/Z and the remaining7.3 Linear classiﬁcation 167\\nDualPerceptron(α0)\\n1 α ← α0 ⊿ typically α0 = 0\\n2 for t ← 1 to T do\\n3 Receive(xt)\\n4 ˆyt ← sgn(∑T\\ns=1 αsys(xs · xt))\\n5 Receive(yt)\\n6 if (ˆyt ̸= yt) then\\n7 αt+1 ← αt +1\\n8 else αt+1 ← αt\\n9 return α\\nFigure 7.8 Dual Perceptron algorithm.\\nT components are functions of the labels and deviations. Z is chosen to guarantee\\nthat ∥v′∥ =1 : Z =\\n√\\n1+ δ2\\nΔ 2 . The predictions made by the Perceptron algorithm\\nfor x′\\nt, t ∈ [1,T ] coincide with those made in the original space for xt, t ∈ [1,T ].\\nFurthermore, by deﬁnition of v′ and x′\\nt, we can write for any t ∈ [1,T ]:\\nyt(v′ · x′\\nt)= yt\\n(v · xt\\nZ +Δ ytdt\\nZΔ\\n⎡\\n= ytv · xt\\nZ + dt\\nZ\\n≥ ytv · xt\\nZ + ρ− yt(v · xt)\\nZ = ρ\\nZ ,\\nwhere the inequality results from the deﬁnition of the deviationdt. This shows that\\nthe sample formed by x′\\n1,..., x′\\nT is linearly separable with margin ρ/Z.T h u s ,i n\\nview of theorem 7.8, since ||x′\\nt||2 ≤ r2 +Δ 2, the number of updates made by the\\nPerceptron algorithm is bounded by (r2+Δ 2)(1+δ2/Δ 2)\\nρ2 .C h o o s i n gΔ2 to minimize\\nthis bound leads to Δ 2 = rδ. Plugging in this value yields the statement of the\\ntheorem.\\nThe main idea behind the proof of the theorem just presented is to map input points\\nto a higher-dimensional space where linear separation is possible, which coincides\\nwith the idea of kernel methods. In fact, the particular kernel used in the proof is\\nclose to a straightforward one with a feature mapping that maps each data point\\nto a distinct dimension.\\nThe Perceptron algorithm can in fact be generalized, as in the case of SVMs,168 On-Line Learning\\nKernelPerceptron(α0)\\n1 α ← α0 ⊿ typically α0 = 0\\n2 for t ← 1 to T do\\n3 Receive(xt)\\n4 ˆyt ← sgn(∑T\\ns=1 αsysK(xs,xt))\\n5 Receive(yt)\\n6 if (ˆyt ̸= yt) then\\n7 αt+1 ← αt +1\\n8 else αt+1 ← αt\\n9 return α\\nFigure 7.9 Kernel Perceptron algorithm for PDS kernel K.\\nto deﬁne a linear separation in a high-dimensional space. It admits an equivalent\\ndual form, the dual Perceptron algorithm, which is presented in ﬁgure 7.8. The\\ndual Perceptron algorithm maintains a vector α ∈ R\\nT of coeﬃcients assigned to\\neach point xt, t ∈ [1,T ]. The label of a point xt is predicted according to the rule\\nsgn(w ·xt), where w = ∑T\\ns=1 αsysxs. The coeﬃcient αt is incremented by one when\\nthis prediction does not match the correct label. Thus, an update forxt is equivalent\\nto augmenting the weight vectorw with ytxt, which shows that the dual algorithm\\nmatches exactly the standard Perceptron algorithm. The dual Perceptron algorithm\\ncan be written solely in terms of inner products between training instances. Thus, as\\ni nt h ec a s eo fS V M s ,i n s t e a do ft h ei n n e rp r o d u c tb e t w e e np o i n t si nt h ei n p u ts p a c e ,\\nan arbitrary PDS kernel can be used, which leads to the kernel Perceptron algorithm\\ndetailed in ﬁgure 7.9. The kernel Perceptron algorithm and its average variant,\\ni.e., voted Perceptron with uniform weights c\\nt, are commonly used algorithms in a\\nvariety of applications.\\n7.3.2 Winnow algorithm\\nThis section presents an alternative on-line linear classiﬁcation algorithm, the\\nWinnow algorithm. Thus, it learns a weight vector deﬁning a separating hyperplane\\nby sequentially processing the training points. As suggested by the name, the\\nalgorithm is particularly well suited to cases where a relatively small number of\\ndimensions or experts can be used to deﬁne an accurate weight vector. Many of the\\nother dimensions may then be irrelevant.7.3 Linear classiﬁcation 169\\nWinnow(η)\\n1 w1 ← 1/N\\n2 for t ← 1 to T do\\n3 Receive(xt)\\n4 ˆyt ← sgn(wt · xt)\\n5 Receive(yt)\\n6 if (ˆyt ̸= yt) then\\n7 Zt ← ∑N\\ni=1 wt,i exp(ηytxt,i)\\n8 for i ← 1 to N do\\n9 wt+1,i ← wt,i exp(ηytxt,i)\\nZt\\n10 else wt+1 ← wt\\n11 return wT+1\\nFigure 7.10 Winnow algorithm, with yt ∈{ − 1, +1} for all t ∈ [1,T ].\\nThe Winnow algorithm is similar to the Perceptron algorithm, but, instead of\\nthe additive update of the weight vector in the Perceptron case, Winnow’s update\\nis multiplicative. The pseudocode of the algorithm is given in ﬁgure 7.10. The\\nalgorithm takes as input a learning parameter η> 0. It maintains a non-negative\\nweight vector wt with components summing to one ( ∥wt∥1 =1 )s t a r t i n gw i t ht h e\\nuniform weight vector (line 1). At each round t ∈ [1,T ], if the prediction does not\\nmatch the correct label (line 6), each component wt,i, i ∈ [1,N ], is updated by\\nmultiplying it by exp(ηytxt,i) and dividing by the normalization factorZt to ensure\\nthat the weights sum to one (lines 7–9). Thus, if the labelyt and xt,i share the same\\nsign, then wt,i is increased, while, in the opposite case, it is signiﬁcantly decreased.\\nThe Winnow algorithm is closely related to the WM algorithm: when xt,i ∈\\n{−1,+1}, sgn(wt ·xt) coincides with the majority vote, since multiplying the weight\\nof correct or incorrect experts byeη or e−η is equivalent to multiplying the weight of\\nincorrect ones by β = e−2η. The multiplicative update rule of Winnow is of course\\nalso similar to that of AdaBoost.\\nThe following theorem gives a mistake bound for the Winnow algorithm in\\nthe separable case, which is similar in form to the bound of theorem 7.8 for the\\nPerceptron algorithm.170 On-Line Learning\\nTheorem 7.12\\nLet x1,..., xT ∈ RN be a sequence of T points with ∥xt∥∞ ≤ r∞ for all t ∈ [1,T ],\\nfor some r∞ > 0. Assume that there existv ∈ RN, v ≥ 0,a n dρ∞ > 0 such that for\\nall t ∈ [1,T ], ρ∞ ≤ yt(v·xt)\\n∥v∥1\\n.T h e n ,f o rη= ρ∞\\nr2\\n∞\\n, the number of updates made by the\\nWinnow algorithm when processingx1,..., xT is upper bounded by2(r2\\n∞ /ρ2\\n∞\\n)l o gN.\\nProof Let I ⊆{ 1,...,T } be the set of iterations at which there is an update,\\nand let M be the total number of updates, i.e., |I| = M. The potential function Φt,\\nt ∈ [1,T ], used for this proof is the relative entropy of the distribution deﬁned by the\\nnormalized weights vi/∥v∥1 ≥ 0, i ∈ [1,N ], and the one deﬁned by the components\\nof the weight vector wt,i, i ∈ [1,N ]:\\nΦt =\\nN∑\\ni=1\\nvi\\n∥v∥1\\nlog vi/∥v∥1\\nwt,i\\n.\\nTo derive an upper bound on Φt, we analyze the diﬀerence of the potential functions\\nat two consecutive rounds. For all t ∈ I, this diﬀerence can be expressed and\\nbounded as follows:\\nΦt+1 − Φt =\\nN∑\\ni=1\\nvi\\n∥v∥1\\nlog wt,i\\nwt+1,i\\n=\\nN∑\\ni=1\\nvi\\n∥v∥1\\nlog Zt\\nexp(ηytxt,i)\\n=l o gZt − η\\nN∑\\ni=1\\nvi\\n∥v∥1\\nytxt,i\\n≤ log\\n[ N∑\\ni=1\\nwt,i exp(ηytxt,i)\\n]\\n− ηρ∞\\n= log E\\nwt\\n[\\nexp(ηytxt)\\n]\\n− ηρ∞\\n≤ log\\n[\\nexp(η2(2r∞ )2/8)\\n]\\n− ηρ∞\\n= η2r2\\n∞ /2 − ηρ∞ .\\nThe ﬁrst inequality follows the deﬁnition ρ∞ . The subsequent equality rewrites\\nthe summation as an expectation over the distribution deﬁned by wt.T h en e x t\\ninequality uses Hoeﬀding’s lemma (lemma D.1). Summing up these inequalities\\nover all t ∈ I yields:\\nΦT+1 − Φ1 ≤ M(η2r2\\n∞ /2 − ηρ∞ ).7.4 On-line to batch conversion 171\\nNext, we derive a lower bound by noting that\\nΦ1 =\\nN∑\\ni=1\\nvi\\n∥v∥1\\nlog vi/∥v∥1\\n1/N =l o gN +\\nN∑\\ni=1\\nvi\\n∥v∥1\\nlog vi\\n∥v∥1\\n≤ log N.\\nAdditionally, since the relative entropy is always non-negative, we have ΦT+1 ≥ 0.\\nThis yields the following lower bound:\\nΦT+1 − Φ1 ≥ 0 − log N = − log N.\\nCombining the upper and lower bounds we see that − log N ≤ M(η2r2\\n∞ /2 − ηρ∞ ).\\nSetting η= ρ∞\\nr2\\n∞\\nyields the statement of the theorem.\\nThe margin-based mistake bounds of theorem 7.8 and theorem 7.12 for the Percep-\\ntron and Winnow algorithms have a similar form, but they are based on diﬀerent\\nnorms. For both algorithms, the norm∥·∥\\np used for the input vectorsxt, t ∈ [1,T ],\\nis the dual of the norm ∥·∥ q used for the margin vector v,t h a ti sp and q are\\nconjugate: 1/p +1 /q = 1: in the case of the Perceptron algorithm p = q = 2, while\\nfor Winnow p = ∞ and q =1 .\\nThese bounds imply diﬀerent types of guarantees. The bound for Winnow is\\nfavorable when a sparse set of the experts i ∈ [1,N ] can predict well. For example,\\nif v = e1 where e1 is the unit vector along the ﬁrst axis inRN and ifxt ∈{ −1, +1}N\\nfor all t, then the upper bound on the number of mistakes given for Winnow by\\ntheorem 7.12 is only logN, while the upper bound of theorem 7.8 for the Perceptron\\nalgorithm is N. The guarantee for the Perceptron algorithm is more favorable in\\nthe opposite situation, where sparse solutions are not eﬀective.\\n7.4 On-line to batch conversion\\nThe previous sections presented several algorithms for the scenario of on-line\\nlearning, including the Perceptron and Winnow algorithms, and analyzed their\\nbehavior within the mistake model, where no assumption is made about the way the\\ntraining sequence is generated. Can these algorithms be used to derive hypotheses\\nwith small generalization error in the standard stochastic setting? How can the\\nintermediate hypotheses they generate be combined to form an accurate predictor?\\nThese are the questions addressed in this section.\\nLet H be a hypothesis of functions mapping X to Y\\n′,a n dl e tL: Y′ ×Y → R+\\nbe a bounded loss function, that is L ≤ M for some M ≥ 0. We assume a standard\\nsupervised learning setting where a labeled sample S =( (x1,y1),..., (xT ,y T )) ∈\\n(X× Y )T is drawn i.i.d. according to some ﬁxed but unknown distribution D.T h e\\nsample is sequentially processed by an on-line learning algorithmA. The algorithm172 On-Line Learning\\nstarts with an initial hypothesis h1 ∈ H and generates a new hypothesis hi+1 ∈ H,\\nafter processing pair ( xi,y i), i ∈ [1,m]. The regret of the algorithm is deﬁned as\\nbefore by\\nRT =\\nT∑\\ni=1\\nL(hi(xi),y i) − min\\nh∈H\\nT∑\\ni=1\\nL(h(xi),y i). (7.24)\\nThe generalization error of a hypothesis h ∈ H is its expected loss R(h)=\\nE(x,y)∼D[L(h(x),y )].\\nThe following lemma gives a bound on the average of the generalization errors of\\nthe hypotheses generated by A in terms of its average loss 1\\nT\\n∑T\\ni=1 L(hi(xi),y i).\\nLemma 7.1\\nLet S =( (x1,y1),..., (xT ,y T )) ∈ (X× Y)T be a labeled sample drawn i.i.d. according\\nto D, L a loss bounded byM and h1,...,h T+1 the sequence of hypotheses generated\\nby an on-line algorithm A sequentially processing S.T h e n ,f o ra n yδ> 0,w i t h\\nprobability at least 1 − δ, the following holds:\\n1\\nT\\nT∑\\ni=1\\nR(hi) ≤ 1\\nT\\nT∑\\ni=1\\nL(hi(xi),y i)+ M\\n√\\n2l o g1\\nδ\\nT . (7.25)\\nProof For any i ∈ [1,T ], let Vi be the random variable deﬁned by Vi = R(hi) −\\nL(hi(xi),y i). Observe that for any i ∈ [1,T ],\\nE[Vi|x1,...,x i−1]= R(hi) − E[L(hi(xi),y i)|hi]= R(hi) − R(hi)=0 .\\nSince the loss is bounded by M, Vi takes values in the interval [ −M, +M]f o r\\nall i ∈ [1,T ]. Thus, by Azuma’s inequality (theorem D.2), Pr[ 1\\nT\\n∑T\\ni=1 Vi ≥ ϵ] ≤\\nexp(−2Tϵ2/(2M)2)). Setting the right-hand side to be equal to δ> 0y i e l d st h e\\nstatement of the lemma.\\nW h e nt h el o s sf u n c t i o ni sc o n v e xw i t hr e s p e c tt oi t sﬁ r s ta r g u m e n t ,t h el e m m a\\ncan be used to derive a bound on the generalization error of the average of the\\nhypotheses generated by A, 1\\nT\\n∑T\\nt=1 hi, in terms of the average loss of A on S,o r\\nin terms of the regret RT and the inﬁmum error of hypotheses in H.\\nTheorem 7.13\\nLet S =( ( x1,y1),..., (xT ,y T )) ∈ (X× Y )T be a labeled sample drawn i.i.d.\\naccording toD, L a loss bounded byM and convex with respect to its ﬁrst argument,\\nand h1,...,h T+1 the sequence of hypotheses generated by an on-line algorithm A\\nsequentially processing S.T h e n ,f o ra n yδ> 0, with probability at least 1 − δ,e a c h7.4 On-line to batch conversion 173\\nof the following holds:\\nR\\n( 1\\nT\\nT∑\\ni=1\\nhi\\n⎡\\n≤ 1\\nT\\nT∑\\ni=1\\nL(hi(xi),y i)+ M\\n√\\n2l o g1\\nδ\\nT (7.26)\\nR\\n( 1\\nT\\nT∑\\ni=1\\nhi\\n⎡\\n≤ inf\\nh∈H\\nR(h)+ RT\\nT +2 M\\n√\\n2l o g2\\nδ\\nT . (7.27)\\nProof By the convexity of L with respect to its ﬁrst argument, for any ( x, y) ∈\\nX× Y ,w eh a v eL( 1\\nT\\n∑T\\ni=1 hi(x),y ) ≤ 1\\nT\\n∑T\\ni=1 L(hi(x),y ). Taking the expectation\\ngives R( 1\\nT\\n∑T\\ni=1 hi) ≤ 1\\nT\\n∑T\\ni=1 R(hi). The ﬁrst inequality then follows by lemma 7.1.\\nThus, by deﬁnition of the regret RT , for any δ> 0, the following holds with\\nprobability at least 1 − δ/2:\\nR\\n( 1\\nT\\nT∑\\ni=1\\nhi\\n⎡\\n≤ 1\\nT\\nT∑\\ni=1\\nL(hi(xi),y i)+ M\\n√\\n2l o g2\\nδ\\nT\\n≤ min\\nh∈H\\n1\\nT\\nT∑\\ni=1\\nL(h(xi),y i)+ RT\\nT + M\\n√\\n2l o g2\\nδ\\nT .\\nBy deﬁnition of inf h∈H R(h), for any ϵ> 0, there exists h∗ ∈ H with R(h∗) ≤\\ninfh∈H R(h)+ ϵ. By Hoeﬀding’s inequality, for anyδ> 0, with probability at least\\n1 − δ/2, 1\\nT\\n∑T\\ni=1 L(h∗(xi),y i) ≤ R(h∗)+ M\\n√\\n2 log 2\\nδ\\nT . Thus, for any ϵ> 0, by the\\nunion bound, the following holds with probability at least 1 − δ:\\nR\\n( 1\\nT\\nT∑\\ni=1\\nhi\\n⎡\\n≤ 1\\nT\\nT∑\\ni=1\\nL(h∗(xi),y i)+ RT\\nT + M\\n√\\n2l o g2\\nδ\\nT\\n≤ R(h∗)+ M\\n√\\n2l o g2\\nδ\\nT + RT\\nT + M\\n√\\n2l o g2\\nδ\\nT\\n= R(h∗)+ RT\\nT +2 M\\n√\\n2l o g2\\nδ\\nT\\n≤ inf\\nh∈H\\nR(h)+ ϵ + RT\\nT +2 M\\n√\\n2l o g2\\nδ\\nT .\\nSince this inequality holds for all ϵ> 0, it implies the second statement of the\\ntheorem.\\nThe theorem can be applied to a variety of on-line regret minimization algorithms,\\nfor example when R\\nT /T = O(1/\\n√\\nT). In particular, we can apply the theorem to\\nthe exponential weighted average algorithm. Assuming that the loss L is bounded174 On-Line Learning\\nby M = 1 and that the number of rounds T is known to the algorithm, we can use\\nthe regret bound of theorem 7.6. The doubling trick (used in theorem 7.7) can be\\nused to derive a similar bound if T is not known in advance. Thus, for any δ> 0,\\nwith probability at least 1 − δ, the following holds for the generalization error of\\nthe average of the hypotheses generated by exponential weighted average:\\nR\\n( 1\\nT\\nT∑\\ni=1\\nhi\\n⎡\\n≤ inf\\nh∈H\\nR(h)+\\n√\\nlog N\\n2T +2\\n√\\n2l o g2\\nδ\\nT ,\\nwhere N is the number of experts, or the dimension of the weight vectors.\\n7.5 Game-theoretic connection\\nThe existence of regret minimization algorithms can be used to give a simple proof\\nof von Neumann’s theorem. For any m ≥ 1, we will denote by Δ m the set of all\\ndistributions over {1,...,m },t h a ti sΔm = {p ∈ Rm : p ≥ 0 ∧∥p∥1 =1 }.\\nTheorem 7.14 Von Neumann’s minimax theorem\\nLet m, n ≥ 1. Then, for any two-person zero-sum game deﬁned by matrix M ∈\\nRm×n,\\nmin\\np∈Δ m\\nmax\\nq∈Δ n\\np⊤Mq =m a x\\nq∈Δ n\\nmin\\np∈Δ m\\np⊤Mq. (7.28)\\nProof The inequality maxq minp p⊤Mq ≤ minp maxq p⊤Mq is straightforward,\\nsince by deﬁnition of min, for all p ∈ Δ m,q ∈ Δ n,w eh a v em i np p⊤Mq ≤ p⊤Mq.\\nTaking the maximum overq of both sides gives: maxq minp p⊤Mq ≤ maxq p⊤Mq\\nfor all p, subsequently taking the minimum over p proves the inequality.2\\nTo show the reverse inequality, consider an on-line learning setting where at each\\nround t ∈ [1,T ], algorithm A returns pt and incurs loss Mqt. We can assume that\\nqt is selected in the optimal adversarial way, that is qt ∈ argmaxq∈Δ m p⊤\\nt Mq,\\nand that A is a regret minimization algorithm, that is RT /T → 0, where RT =∑T\\nt=1 p⊤\\nt Mqt − minp∈Δ m\\n∑T\\nt=1 p⊤Mqt. Then, the following holds:\\nmin\\np∈Δ m\\nmax\\nq∈Δ n\\np⊤Mq ≤ max\\nq\\n(1\\nT\\nT∑\\nt=1\\npt\\n⎡⊤\\nMq ≤ 1\\nT\\nT∑\\nt=1\\nmax\\nq\\np⊤\\nt Mq = 1\\nT\\nT∑\\nt=1\\np⊤\\nt Mqt.\\n2. More generally, the maxmin is always upper bounded by the minmax for any function\\nor two arguments and any constraint sets, following the same proof.7.6 Chapter notes 175\\nBy deﬁnition of regret, the right-hand side can be expressed and bounded as follows:\\n1\\nT\\nT∑\\nt=1\\np⊤\\nt Mqt =m i n\\np∈Δ m\\n1\\nT\\nT∑\\nt=1\\np⊤Mqt + RT\\nT =m i n\\np∈Δ m\\np⊤M\\n(1\\nT\\nT∑\\nt=1\\nqt\\n⎡\\n+ RT\\nT\\n≤ max\\nq∈Δ n\\nmin\\np∈Δ m\\np⊤Mq + RT\\nT .\\nThis implies that the following bound holds for the minmax for all T ≥ 1:\\nmin\\np∈Δ m\\nmax\\nq∈Δ n\\np⊤Mq ≤ max\\nq∈Δ n\\nmin\\np∈Δ m\\np⊤Mq + RT\\nT\\nSince limT → +∞\\nRT\\nT = 0, this shows that minp maxq p⊤Mq ≤ maxq minp p⊤Mq.\\n7.6 Chapter notes\\nAlgorithms for regret minimization were initiated with the pioneering work of\\nHannan [1957] who gave an algorithm whose regret decreases asO(\\n√\\nT) as a function\\nof T but whose dependency on N is linear. The weighted majority algorithm and\\nthe randomized weighted majority algorithm, whose regret is only logarithmic inN,\\nare due to Littlestone and Warmuth [1989]. The exponentiated average algorithm\\nand its analysis, which can be viewed as an extension of the WM algorithm to\\nconvex non-zero-one losses is due to the same authors [Littlestone and Warmuth,\\n1989, 1994]. The analysis we presented follows Cesa-Bianchi [1999] and Cesa-Bianchi\\nand Lugosi [2006]. The doubling trick technique appears in Vovk [1990] and Cesa-\\nBianchi et al. [1997]. The algorithm of exercise 7.7 and the analysis leading to a\\nsecond-order bound on the regret are due to Cesa-Bianchi et al. [2005]. The lower\\nbound presented in theorem 7.5 is from Blum and Mansour [2007].\\nWhile the regret bounds presented are logarithmic in the number of the experts\\nN,w h e nN is exponential in the size of the input problem, the computational\\ncomplexity of an expert algorithm could be exponential. For example, in the on-\\nline shortest paths problem, N is the number of paths between two vertices of\\na directed graph. However, several computationally eﬃcient algorithms have been\\npresented for broad classes of such problems by exploiting their structure [Takimoto\\nand Warmuth, 2002, Kalai and Vempala, 2003, Zinkevich, 2003].\\nThe notion of regret (or external regret) presented in this chapter can be gener-\\nalized to that of internal regret or even swap regret, by comparing the loss of the\\nalgorithm not just to that of the best expert in retrospect, but to that of any modi-\\nﬁcation of the actions taken by the algorithm by replacing each occurrence of some\\nspeciﬁc action with another one (internal regret), or even replacing actions via an ar-176 On-Line Learning\\nbitrary mapping (swap regret) [Foster and Vohra, 1997, Hart and Mas-Colell, 2000,\\nLehrer, 2003]. Several algorithms for low internal regret have been given [Foster\\nand Vohra, 1997, 1998, 1999, Hart and Mas-Colell, 2000, Cesa-Bianchi and Lugosi,\\n2001, Stoltz and Lugosi, 2003], including a conversion of low external regret to low\\nswap regret by Blum and Mansour [2005].\\nThe Perceptron algorithm was introduced by Rosenblatt [1958]. The algorithm\\nraised a number of reactions, in particular by Minsky and Papert [1969], who\\nobjected that the algorithm could not be used to recognize the XOR function.\\nOf course, the kernel Perceptron algorithm already given by Aizerman et al. [1964]\\ncould straightforwardly succeed to do so using second-degree polynomial kernels.\\nThe margin bound for the Perceptron algorithm was proven by Novikoﬀ [1962]\\nand is one of the ﬁrst results in learning theory. The leave-one-out analysis for\\nSVMs is described by Vapnik [1998]. The upper bound presented for the Perceptron\\nalgorithm in the non-separable case is by Freund and Schapire [1999a]. The Winnow\\nalgorithm was introduced by Littlestone [1987].\\nThe analysis of the on-line to batch conversion and exercise 7.10 are from Cesa-\\nBianchi et al. [2001, 2004] (see also Littlestone [1989]). Von Neumann’s minimax\\ntheorem admits a number of diﬀerent generalizations. See Sion [1958] for a gener-\\nalization to quasi-concave-convex functions semi-continuous in each argument and\\nthe references therein. The simple proof of von Neumann’s theorem presented here\\nis entirely based on learning-related techniques. A proof of a more general version\\nusing multiplicative updates was presented by Freund and Schapire [1999b].\\nOn-line learning is a very broad and fast-growing research area in machine\\nlearning. The material presented in this chapter should be viewed only as an\\nintroduction to the topic, but the proofs and techniques presented should indicate\\nthe ﬂavor of most results in this area. For a more comprehensive presentation of on-\\nline learning and related game theory algorithms and techniques, the reader could\\nconsult the book of Cesa-Bianchi and Lugosi [2006].\\n7.7 Exercises\\n7.1 Perceptron lower bound. Let S be a labeled sample of m points in RN with\\nxi =( (−1)i,..., (−1)i, (−1)i+1\\n\\ued19 \\ued18\\ued17 \\ued1a\\ni ﬁrst components\\n, 0,..., 0) and yi =( −1)i+1. (7.29)\\nShow that the Perceptron algorithm makes Ω(2 N ) updates before ﬁnding a sepa-\\nrating hyperplane, regardless of the order in which it receives the points.\\n7.2 Generalized mistake bound. Theorem 7.8 presents a margin bound on the7.7 Exercises 177\\nOn-line-SVM(w0)\\n1 w1 ← w0 ⊿ typically w0 = 0\\n2 for t ← 1 to T do\\n3 Receive(xt,y t)\\n4 if yt(wt · xt) < 1 then\\n5 wt+1 ← wt − η(wt − Cytxt)\\n6 elseif yt(wt · xt) > 1 then\\n7 wt+1 ← wt − ηwt\\n8 else wt+1 ← wt\\n9 return wT+1\\nFigure 7.11 On-line SVM algorithm.\\nmaximum number of updates for the Perceptron algorithm for the special case\\nη = 1. Consider now the general Perceptron update wt+1 ← wt + ηytxt,w h e r e\\nη> 0. Prove a bound on the maximum number of mistakes. How does η aﬀect the\\nbound?\\n7.3 Sparse instances. Suppose each input vector xt, t ∈ [1,T ], coincides with the\\ntth unit vector of RT . How many updates are required for the Perceptron algorithm\\nto converge? Show that the number of updates matches the margin bound of\\ntheorem 7.8.\\n7.4 Tightness of lower bound. Is the lower bound of theorem 7.5 tight? Explain why\\nor show a counter-example.\\n7.5 On-line SVM algorithm. Consider the algorithm described in ﬁgure 7.11. Show\\nthat this algorithm corresponds to the stochastic gradient descent technique applied\\nto the SVM problem (4.23) with hinge loss and no oﬀset (i.e., ﬁx p = 1 and b =0 ) .\\n7.6 Margin Perceptron. Given a training sample S that is linearly separable with\\na maximum margin ρ> 0, theorem 7.8 states that the Perceptron algorithm run\\ncyclically over S is guaranteed to converge after at mostR\\n2/ρ2 updates, where R is\\nthe radius of the sphere containing the sample points. However, this theorem does\\nnot guarantee that the hyperplane solution of the Perceptron algorithm achieves\\na margin close to ρ. Suppose we modify the Perceptron algorithm to ensure that178 On-Line Learning\\nMarginPerceptron()\\n1 w1 ← 0\\n2 for t ← 1 to T do\\n3 Receive(xt)\\n4 Receive(yt)\\n5 if\\n(\\n(wt =0 ) or (ytwt·xt\\n∥wt∥ < ρ\\n2 )\\n⎡\\nthen\\n6 wt+1 ← wt + ytxt\\n7 else wt+1 ← wt\\n8 return wT+1\\nFigure 7.12 Margin Perceptron algorithm.\\nthe margin of the hyperplane solution is at least ρ/2. In particular, consider the\\nalgorithm described in ﬁgure 7.12. In this problem we show that this algorithm\\nconverges after at most 16R2/ρ2 updates. Let I denote the set of times t ∈ [1,T ]\\nat which the algorithm makes an update and let M = |I| be the total number of\\nupdates.\\n(a) Using an analysis similar to the one given for the Perceptron algorithm,\\nshow that Mρ ≤∥ wT+1∥. Conclude that if ∥wT+1∥ < 4R2\\nρ ,t h e nM< 4R2/ρ2.\\n(For the remainder of this problem, we will assume that ∥wT+1∥≥ 4R2\\nρ .)\\n(b) Show that for any t ∈ I (including t = 0), the following holds:\\n∥wt+1∥2 ≤ (∥wt∥ + ρ/2)2 + R2.\\n(c) From (b), infer that for any t ∈ I we have\\n∥wt+1∥≤∥ wt∥ + ρ/2+ R2\\n∥wt∥ + ∥wt+1∥ + ρ/2.\\n(d) Using the inequality from (c), show that for any t ∈ I such that either\\n∥wt∥≥ 4R2\\nρ or ∥wt+1∥≥ 4R2\\nρ ,w eh a v e\\n∥wt+1∥≤∥ wt∥ + 3\\n4ρ.\\n(e) Show that ∥w1∥≤ R ≤ 4R2/ρ. Since by assumption we have ∥wT+1∥≥\\n4R2\\nρ , conclude that there must exist a largest timet0 ∈ I such that ∥wt0 ∥≤ 4R2\\nρ7.7 Exercises 179\\nand ∥wt0+1∥≥ 4R2\\nρ .\\n(f) Show that ∥wT+1∥≤∥ wt0 ∥ + 3\\n4 Mρ. Conclude that M ≤ 16R2/ρ2.\\n7.7 Second-order regret bound. Consider the randomized algorithm that diﬀers from\\nthe RWM algorithm only by the weight update, i.e., wt+1,i ← (1 − (1 − β)lt,i)wt,i,\\nt ∈ [1,T ], which is applied to all i ∈ [1,N ]w i t h1/2 ≤ β< 1. This algorithm can\\nbe used in a more general setting than RWM since the losses lt,i are only assumed\\nto be in [0,1]. The objective of this problem is to show that a similar upper bound\\ncan be shown for the regret.\\n(a) Use the same potential Wt as for the RWM algorithm and derive a simple\\nupper bound for log WT+1:\\nlog WT+1 ≤ log N − (1 − β)LT .\\n(Hint:U s et h ei d e n t i t yl o g ( 1− x) ≤− x for x ∈ [0,1/2].)\\n(b) Prove the following lower bound for the potential for all i ∈ [1,N ]:\\nlog WT+1 ≥− (1 − β)LT,i − (1 − β)2\\nT∑\\nt=1\\nl2\\nt,i .\\n(Hint:U s et h ei d e n t i t yl o g ( 1− x) ≥− x − x2, which is valid for allx ∈ [0, 1/2].)\\n(c) Use upper and lower bounds to derive the following regret bound for the\\nalgorithm: RT ≤ 2√T log N.\\n7.8 Polynomial weighted algorithm. The objective of this problem is to show how\\nanother regret minimization algorithm can be deﬁned and studied. Let L be a loss\\nfunction convex in its ﬁrst argument and taking values in [0,M ].\\nWe will assume N>e 2 and then for any expert i ∈ [1,N ], we denote by rt,i the\\ni n s t a n t a n e o u sr e g r e to ft h a te x p e r ta tt i m et ∈ [1,T ], rt,i = L(ˆyt,y t)−L(yt,i,y t), and\\nby Rt,i his cumulative regret up to timet: Rt,i = ∑t\\ns=1 rt,i. For convenience, we also\\ndeﬁne R0,i =0f o ra l li ∈ [1,N ]. For anyx ∈ R,( x)+ denotes max(x,0), that is the\\npositive part of x,a n df o rx =( x1,...,x N )⊤ ∈ RN ,( x)+ =( (x1)+,..., (xN )+)⊤.\\nLet α> 2 and consider the algorithm that predicts at round t ∈ [1,T ]a c c o r d i n g\\nto ˆyt =\\nPn\\ni=1 wt,iyt,iPn\\ni=1 wt,i\\n,w i t ht h ew e i g h twt,i deﬁned based on the αth power of\\nt h er e g r e tu pt ot i m e(t − 1): wt,i =( Rt−1,i)α−1\\n+ .T h ep o t e n t i a lf u n c t i o nw e\\nuse to analyze the algorithm is based on the function Φ deﬁned over RN by\\nΦ: x ↦→∥ (x)+∥2\\nα =\\n[∑N\\ni=1(xi)α\\n+\\n]2\\nα .180 On-Line Learning\\n(a) Show that Φ is twice diﬀerentiable over RN − B,w h e r eB is deﬁned as\\nfollows:\\nB = {u ∈ RN :( u)+ =0 }.\\n(b) For any t ∈ [1,T ], let rt denote the vector of instantaneous regrets,\\nrt =( rt,1,...,r t,N )⊤,a n ds i m i l a r l yRt =( Rt,1,...,R t,N )⊤.W ed e ﬁ n et h e\\npotential function as Φ( Rt)= ∥(Rt)+∥2\\nα.C o m p u t e∇Φ(Rt−1)f o rRt−1 ̸∈ B\\nand show that ∇Φ(Rt−1) · rt ≤ 0( Hint: use the convexity of the loss with\\nrespect to the ﬁrst argument).\\n(c) Prove the inequality r⊤[∇2Φ(u)]r ≤ 2(α − 1)∥r∥2\\nα valid for all r ∈ RN and\\nu ∈ RN − B (Hint: write the Hessian ∇2Φ(u) as a sum of a diagonal matrix\\nand a positive semi-deﬁnite matrix multiplied by (2 − α). Also, use H¨older’s\\ninequality generalizing Cauchy-Schwarz : for anyp> 1a n dq> 1w i t h1\\np +1\\nq =1\\nand u,v ∈ RN , |u · v|≤∥ u∥p∥v∥q).\\n(d) Using the answers to the two previous questions and Taylor’s formula, show\\nthat for all t ≥ 1, Φ(Rt) − Φ(Rt−1) ≤ (α − 1)∥rt∥2\\nα,i f γRt−1 +( 1− γ)Rt ̸∈ B\\nfor all γ ∈ [0, 1].\\n(e) Suppose there exists γ ∈ [0,1] such that (1− γ)Rt−1 +γRt ∈ B. Show that\\nΦ(Rt) ≤ (α − 1)∥rt∥2\\nα.\\n(f) Using the two previous questions, derive an upper bound on Φ( RT )e x -\\npressed in terms of T, N,a n dM.\\n(g) Show that Φ( RT )a d m i t sa sal o w e rb o u n dt h es q u a r eo ft h er e g r e tRT of\\nthe algorithm.\\n(h) Using the two previous questions give an upper bound on the regret RT .\\nFor what value ofα is the bound the most favorable? Give a simple expression\\nof the upper bound on the regret for a suitable approximation of that optimal\\nvalue.\\n7.9 General inequality. In this exercise we generalize the result of exercise 7.7 by\\nusing a more general inequality: log(1 − x) ≥− x −\\nx2\\nα for some 0 <α< 2.\\n(a) First prove that the inequality is true for x ∈ [0, 1 − α\\n2 ]. What does this\\nimply about the valid range of β?\\n(b) Give a generalized version of the regret bound derived in exercise 7.7 in\\nterms of α,w h i c hs h o w s :\\nRT ≤ log N\\n1 − β + 1 − β\\nα T.\\nWhat is the optimal choice of β and the resulting bound in this case?7.7 Exercises 181\\n(c) Explain how α may act as a regularization parameter. What is the optimal\\nchoice of α?\\n7.10 On-line to batch. Consider the margin loss (4.3), which is convex. Our goal is\\nto apply theorem 7.13 to the kernel Perceptron algorithm using the margin loss.\\n(a) Show that the regret RT can be bounded as RT ≤\\n√\\nTr[K]/ρ2 where ρ is\\nthe margin and K is the kernel matrix associated to the sequence x1,...,x T .\\n(b) Apply theorem 7.13. How does this result compare with the margin bounds\\nfor kernel-based hypotheses given by corollary 5.1?\\n7.11 On-line to batch — non-convex loss. The on-line to batch result of theorem 7.13\\nheavily relies on the fact that the loss in convex in order to provide a generalization\\nguarantee for the uniformly averaged hypothesis 1\\nT\\n∑T\\ni=1 hi. For general losses,\\ninstead of using the averaged hypothesis we will use a diﬀerent strategy and try\\nto estimate the best single base hypothesis and show the expected loss of this\\nhypothesis is bounded.\\nLet m\\ni denote the number of errors of hypothesis hi makes on the points\\n(xi,...,x T ), i.e. the subset of points in the sequence that are not used to train\\nhi. Then we deﬁne the penalized risk estimate of hypothesis hi as,\\nmi\\nT − i +1 + cδ(T − i +1 ) w h e r ecδ(x)=\\n√\\n1\\n2x log T(T +1 )\\nδ .\\nThe term cδ penalizes the empirical error when the test sample is small. Deﬁne\\nˆh = hi∗ where i∗=a r g m i ni mi/(T − i)+ cδ(T − i+1). We will then show under the\\nsame conditions of theorem 7.13 (withM = 1 for simplicity), but without requiring\\nthe convexity of L, that the following holds with probability at least 1 − δ:\\nR(ˆh) ≤ 1\\nT\\nT∑\\ni=1\\nL(hi(xi),y i)+6\\n√\\n1\\nT log 2(T +1 )\\nδ . (7.30)\\n(a) Prove the following inequality:\\nmin\\ni∈[1,T]\\n(R(hi)+2 cδ(T − i +1 ) )≤ 1\\nT\\nT∑\\ni=1\\nR(hi)+4\\n√\\n1\\nT log T +1\\nδ .182 On-Line Learning\\n(b) Use part (a) to show that with probability at least 1 − δ,\\nmin\\ni∈[1,T]\\n(R(hi)+2 cδ(T − i +1 ) )\\n<\\nT∑\\ni=1\\nL(hi(xi),y i)+\\n√\\n2\\nT log 1\\nδ +4\\n√\\n1\\nT log T +1\\nδ .\\n(c) By design, the deﬁnition of cδ ensures that with probability at least 1 − δ\\nR(ˆh) ≤ min\\ni∈[1,T]\\n(R(hi)+2 cδ(T − i +1 ) ).\\nUse this property to complete the proof of (7.30).8 Multi-Class Classiﬁcation\\nThe classiﬁcation problems we examined in the previous chapters were all binary.\\nHowever, in most real-world classiﬁcation problems the number of classes is greater\\nthan two. The problem may consist of assigning a topic to a text document, a\\ncategory to a speech utterance or a function to a biological sequence. In all of these\\ntasks, the number of classes may be on the order of several hundred or more.\\nIn this chapter, we analyze the problem of multi-class classiﬁcation. We ﬁrst in-\\ntroduce the multi-class classiﬁcation learning problem and discuss its multiple set-\\ntings, and then derive generalization bounds for it using the notion of Rademacher\\ncomplexity. Next, we describe and analyze a series of algorithms for tackling the\\nmulti-class classiﬁcation problem. We will distinguish between two broad classes\\nof algorithms: uncombined algorithms that are speciﬁcally designed for the multi-\\nclass setting such as multi-class SVMs, decision trees, or multi-class boosting, and\\naggregated algorithms that are based on a reduction to binary classiﬁcation and re-\\nquire training multiple binary classiﬁers. We will also brieﬂy discuss the problem of\\nstructured prediction, which is a related problem arising in a variety of applications.\\n8.1 Multi-class classiﬁcation problem\\nLet X denote the input space and Y denote the output space, and let D be an\\nunknown distribution over X according to which input points are drawn. We will\\ndistinguish between two cases: themono-label case,w h e r eY is a ﬁnite set of classes\\nthat we mark with numbers for convenience, Y = {1,...,k },a n dt h emulti-label\\ncase where Y = {−1, +1}k. In the mono-label case, each example is labeled with a\\nsingle class, while in the multi-label case it can be labeled with several. The latter\\ncan be illustrated by the case of text documents, which can be labeled with several\\ndiﬀerent relevant topics, e.g.,sports, business,a n dsociety. The positive components\\nof a vector in {−1, +1}k indicate the classes associated with an example.\\nIn either case, the learner receives a labeled sampleS =\\n(\\n(x1,y1),..., (xm,y m)\\n⎡\\n∈\\n(X× Y )m with x1,...,x m drawn i.i.d. according to D,a n d yi = f(xi) for all\\ni ∈ [1,m ], where f : X→ Y is the target labeling function. Thus, we consider a184 Multi-Class Classiﬁcation\\ndeterministic scenario, which, as discussed in section 2.4.1, can be straightforwardly\\nextended to a stochastic one where we have a distribution over X× Y .\\nGiven a hypothesis set H of functions mapping X to Y, the multi-class classiﬁ-\\ncation problem consists of using the labeled sample S to ﬁnd a hypothesis h ∈ H\\nwith small generalization error R(h) with respect to the target f:\\nR(h)= E\\nx∼D\\n[1h(x)̸=f(x)] mono-label case (8.1)\\nR(h)= E\\nx∼D\\n[ k∑\\nl=1\\n1[h(x)]l ̸=[f(x)]l\\n]\\nmulti-label case. (8.2)\\nThe notion of Hamming distance dH, that is, the number of corresponding compo-\\nnents in two vectors that diﬀer, can be used to give a common formulation for both\\nerrors:\\nR(h)= E\\nx∼D\\n[\\ndH(h(x),f (x))\\n]\\n. (8.3)\\nThe empirical error of h ∈ H is denoted by ˆR(h) and deﬁned by\\nˆR(h)= 1\\nm\\nm∑\\ni=1\\ndH(h(xi),y i) . (8.4)\\nSeveral issues, both computational and learning-related, often arise in the multi-\\nclass setting. Computationally, dealing with a large number of classes can be\\nproblematic. The number of classes k directly enters the time complexity of the\\nalgorithms we will present. Even for a relatively small number of classes such as\\nk = 100 or k =1 ,000, some techniques may become prohibitive to use in practice.\\nThis dependency is even more critical in the case where k is very large or even\\ninﬁnite as in the case of some structured prediction problems.\\nA learning-related issue that commonly appears in the multi-class setting is the\\nexistence of unbalanced classes. Some classes may be represented by less than 5\\npercent of the labeled sample, while others may dominate a very large fraction\\nof the data. When separate binary classiﬁers are used to deﬁne the multi-class\\nsolution, we may need to train a classiﬁer distinguishing between two classes with\\nonly a small representation in the training sample. This implies training on a small\\nsample, with poor performance guarantees. Alternatively, when a large fraction\\nof the training instances belong to one class, it may be tempting to propose a\\nhypothesis always returning that class, since its generalization error as deﬁned\\nearlier is likely to be relatively low. However, this trivial solution is typically not the\\none intended. Instead, the loss function may need to be reformulated by assigning\\ndiﬀerent misclassiﬁcation weights to each pair of classes.\\nAnother learning-related issue is the relationship between classes, which can8.2 Generalization bounds 185\\nbe hierarchical. For example, in the case of document classiﬁcation, the error of\\nmisclassifying a document dealing with world politics as one dealing with real\\nestate should naturally be penalized more than the error of labeling a document\\nwith sports instead of the more speciﬁc label baseball. Thus, a more complex and\\nmore useful multi-class classiﬁcation formulation would take into consideration the\\nhierarchical relationships between classes and deﬁne the loss function in accordance\\nwith this hierarchy. More generally, there may be a graph relationship between\\nclasses as in the case of the GO ontology in computational biology. The use of\\nhierarchical relationships between classes leads to a richer and more complex multi-\\nclass classiﬁcation problem.\\n8.2 Generalization bounds\\nIn this section, we present margin-based generalization bounds for multi-class\\nclassiﬁcation in the mono-label case. In the binary setting, classiﬁers are often\\ndeﬁned based on the sign of a scoring function. In the multi-class setting, a\\nhypothesis is deﬁned based on a scoring functionh: X× Y → R.T h el a b e la s s o c i a t e d\\nto point x is the one resulting in the largest scoreh(x, y), which deﬁnes the following\\nmapping from X to Y:\\nx ↦→ argmax\\ny∈Y\\nh(x, y).\\nThis naturally leads to the following deﬁnition of themargin ρh(x, y) of the function\\nh at a labeled example (x, y):\\nρh(x, y)= h(x, y) − max\\ny′ ̸=y\\nh(x, y′).\\nThus, h misclassiﬁes (x, y)i ﬀρh(x, y) ≤ 0. For anyρ> 0, we can deﬁne theempirical\\nmargin loss of a hypothesis h for multi-class classiﬁcation as\\nˆRρ(h)= 1\\nm\\nm∑\\ni=1\\nΦρ(ρh(xi,y i)), (8.5)\\nwhere Φρ is the margin loss function (deﬁnition 4.3). Thus, the empirical margin\\nloss for multi-class classiﬁcation is upper bounded by the fraction of the training\\npoints misclassiﬁed by h or correctly classiﬁed but with conﬁdence less than or equal\\nto ρ:\\nˆR\\nρ(h) ≤ 1\\nm\\nm∑\\ni=1\\n1ρh(xi,yi)≤ρ. (8.6)186 Multi-Class Classiﬁcation\\nThe following lemma will be used in the proof of the main result of this section.\\nLemma 8.1\\nLet F1,..., Fl be l hypothesis sets in RX , l ≥ 1,a n dl e tG = {max{h1,...,h l}: hi ∈\\nFi,i ∈ [1,l ]}. Then, for any sample S of size m, the empirical Rademacher\\ncomplexity of G can be upper bounded as follows:\\nˆRS(G) ≤\\nl∑\\nj=1\\nˆRS(Fj). (8.7)\\nProof Let S =( x1,...,x m) be a sample of size m. We ﬁrst prove the result in\\nthe case l = 2. By deﬁnition of the max operator, for any h1 ∈F 1 and h2 ∈F 2,\\nmax{h1,h2} = 1\\n2[h1 + h2 + |h1 − h2|].\\nThus, we can write:\\nˆRS(G)= 1\\nm E\\nσ\\n[\\nsup\\nh1∈F1\\nh2∈F2\\nm∑\\ni=1\\nσi max{h1(xi),h2(xi)}\\n]\\n= 1\\n2m E\\nσ\\n[\\nsup\\nh1∈F1\\nh2∈F2\\nm∑\\ni=1\\nσi\\n(\\nh1(xi)+ h2(xi)+ |(h1 − h2)(xi)|\\n⎡]\\n≤ 1\\n2\\nˆRS(F1)+ 1\\n2\\nˆRS(F2)+ 1\\n2m E\\nσ\\n[\\nsup\\nh1∈F1\\nh2∈F2\\nm∑\\ni=1\\nσi|(h1 − h2)(xi)|\\n]\\n, (8.8)\\nusing the sub-additivity of sup. Since x ↦→| x| is 1-Lipschitz, by Talagrand’s lemma\\n(lemma 4.2), the last term can be bounded as follows\\n1\\n2m E\\nσ\\n[\\nsup\\nh1∈F1\\nh2∈F2\\nm∑\\ni=1\\nσi|(h1 − h2)(xi)|\\n]\\n≤ 1\\n2m E\\nσ\\n[\\nsup\\nh1∈F1\\nh2∈F2\\nm∑\\ni=1\\nσi(h1 − h2)(xi)\\n]\\n≤ 1\\n2\\nˆRS(F1)+ 1\\n2m E\\nσ\\n[\\nsup\\nh2∈F2\\nm∑\\ni=1\\n−σih2(xi)\\n]\\n= 1\\n2\\nˆRS(F1)+ 1\\n2\\nˆRS(F2), (8.9)\\nwhere we again use the sub-additivity of sup for the second inequality and the fact\\nthat σi and −σi have the same distribution for any i ∈ [1,m] for the last equality.\\nCombining (8.8) and (8.9) yields ˆRS(G) ≤ ˆRS(F1)+ ˆRS(F2). The general case can\\nbe derived from the case l =2u s i n gm a x{h1,...,h l} =m a x{h1, max{h2,...,h l}}\\nand an immediate recurrence.8.2 Generalization bounds 187\\nFor any family of hypotheses mapping X× Y to R,w ed e ﬁ n eΠ1(H)b y\\nΠ1(H)= {x ↦→ h(x, y): y ∈Y ,h ∈ H}.\\nThe following theorem gives a general margin bound for multi-class classiﬁcation.\\nTheorem 8.1 Margin bound for multi-class classiﬁcation\\nLet H ⊆ RX× Y be a hypothesis set with Y = {1,...,k }.F i x ρ> 0.T h e n ,f o r\\nany δ> 0, with probability at least 1 − δ, the following multi-class classiﬁcation\\ngeneralization bound holds for all h ∈ H:\\nR(h) ≤ ˆRρ(h)+ 2k2\\nρ Rm(Π1(H)) +\\n√\\nlog 1\\nδ\\n2m . (8.10)\\nProof The ﬁrst part of the proof is similar to that of theorem 4.4. Let ˜H be\\nthe family of hypotheses mapping X× Y to R deﬁned by ˜H = {z =( x, y) ↦→\\nρh(x, y): h ∈ H}. Consider the family of functions ˜H = {Φρ ◦r: r ∈ ˜H} derived\\nfrom ˜H, which take values in [0, 1]. By theorem 3.1, with probability at least 1− δ,\\nfor all h ∈ H,\\nE\\n[\\nΦρ(ρh(x, y))\\n]\\n≤ ˆRρ(h)+2 Rm\\n(\\nΦρ ◦ ˜H\\n⎡\\n+\\n√\\nlog 1\\nδ\\n2m .\\nSince 1u≤0 ≤ Φρ(u) for all u ∈ R, the generalization error R(h) is a lower bound on\\nthe left-hand side, R(h)=E [ 1y[h(x′)−h(x)]≤0] ≤ E\\n[\\nΦρ(ρh(x, y))\\n]\\n,a n dw ec a nw r i t e :\\nR(h) ≤ ˆRρ(h)+2 Rm\\n(\\nΦρ ◦ ˜H\\n⎡\\n+\\n√\\nlog 1\\nδ\\n2m .\\nAs in the proof of theorem 4.4, we can show that Rm\\n(\\nΦρ ◦ ˜H\\n⎡\\n≤ 1\\nρRm( ˜H)u s i n g188 Multi-Class Classiﬁcation\\nthe (1/ρ)-Lipschitzness of Φρ. Here, Rm( ˜H) can be upper bounded as follows:\\nRm( ˜H)= 1\\nm E\\nS,σ\\n[\\nsup\\nh∈H\\nm∑\\ni=1\\nσiρh(xi,y i)\\n]\\n= 1\\nm E\\nS,σ\\n[\\nsup\\nh∈H\\nm∑\\ni=1\\n∑\\ny∈Y\\nσiρh(xi,y )1y=yi\\n]\\n≤ 1\\nm\\n∑\\ny∈Y\\nE\\nS,σ\\n[\\nsup\\nh∈H\\nm∑\\ni=1\\nσiρh(xi,y )1y=yi\\n]\\n(sub-additivity of sup)\\n= 1\\nm\\n∑\\ny∈Y\\nE\\nS,σ\\n[\\nsup\\nh∈H\\nm∑\\ni=1\\nσiρh(xi,y )\\n(2(1y=yi )−1\\n2 + 1\\n2\\n⎡]\\n≤ 1\\n2m\\n∑\\ny∈Y\\nE\\nS,σ\\n[\\nsup\\nh∈H\\nm∑\\ni=1\\nσiϵiρh(xi,y )\\n]\\n+\\n(\\nϵi =2 ( 1y=yi ) − 1\\n⎡\\n1\\n2m\\n∑\\ny∈Y\\nE\\nS,σ\\n[\\nsup\\nh∈H\\nm∑\\ni=1\\nσiρh(xi,y )\\n]\\n(sub-additivity of sup)\\n= 1\\nm\\n∑\\ny∈Y\\nE\\nS,σ\\n[\\nsup\\nh∈H\\nm∑\\ni=1\\nσiρh(xi,y )\\n]\\n,\\nwhere by deﬁnition ϵi ∈{ − 1, +1} and we use the fact that σi and σiϵi have the\\nsame distribution.\\nLet Π1(H)(k−1) = {max{h1,...,h l}: hi ∈ Π1(H),i ∈ [1,k − 1]}.N o w ,r e w r i t i n g\\nρh(xi,y ) explicitly, using again the sub-additivity of sup, observing that −σi and\\nσi are distributed in the same way, and using lemma 8.1 leads to\\nRm( ˜H) ≤ 1\\nm\\n∑\\ny∈Y\\nE\\nS,σ\\n[\\nsup\\nh∈H\\nm∑\\ni=1\\nσi\\n(\\nh(xi,y ) − max\\ny′ ̸=y\\nh(xi,y ′)\\n⎡]\\n≤\\n∑\\ny∈Y\\n[\\n1\\nm E\\nS,σ\\n[\\nsup\\nh∈H\\nm∑\\ni=1\\nσih(xi,y )\\n]\\n+ 1\\nm E\\nS,σ\\n[\\nsup\\nh∈H\\nm∑\\ni=1\\n−σi max\\ny′ ̸=y\\nh(xi,y ′)\\n]]\\n=\\n∑\\ny∈Y\\n[\\n1\\nm E\\nS,σ\\n[\\nsup\\nh∈H\\nm∑\\ni=1\\nσih(xi,y )\\n]\\n+ 1\\nm E\\nS,σ\\n[\\nsup\\nh∈H\\nm∑\\ni=1\\nσi max\\ny′ ̸=y\\nh(xi,y ′)\\n]]\\n≤\\n∑\\ny∈Y\\n[\\n1\\nm E\\nS,σ\\n[\\nsup\\nh∈Π1(H)\\nm∑\\ni=1\\nσih(xi)\\n]\\n+ 1\\nm E\\nS,σ\\n[\\nsup\\nh∈Π1(H)(k− 1)\\nm∑\\ni=1\\nσih(xi)\\n]]\\n≤ k\\n[\\nk\\nm E\\nS,σ\\n[\\nsup\\nh∈Π1(H)\\nm∑\\ni=1\\nσih(xi)\\n]]\\n= k2Rm(Π1(H)).\\nThis concludes the proof.8.2 Generalization bounds 189\\nThese bounds can be generalized to hold uniformly for all ρ> 0a tt h ec o s to f\\nan additional term\\n√\\n(log log2(2/ρ))/m, as in theorem 4.5 and exercise 4.2. As for\\nother margin bounds presented in previous sections, they show the conﬂict between\\ntwo terms: the larger the desired pairwise ranking marginρ, the smaller the middle\\nterm, at the price of a larger empirical multi-class classiﬁcation margin lossˆR\\nρ. Note,\\nhowever, that here there is additionally a quadratic dependency on the number of\\nclasses k.T h i ss u g g e s t sw e a k e rg u a r a n t e e sw h e nl e a r n i n gw i t hal a r g en u m b e ro f\\nclasses or the need for even larger margins ρ for which the empirical margin loss\\nwould be small.\\nFor some hypothesis sets, a simple upper bound can be derived for the\\nRademacher complexity of Π1(H), thereby making theorem 8.1 more explicit. We\\nwill show this for kernel-based hypotheses. LetK : X× X → R be a PDS kernel and\\nlet Φ : X→ H be a feature mapping associated to K. In multi-class classiﬁcation, a\\nkernel-based hypothesis is based on k weight vectors w1,..., wk ∈ H. Each weight\\nvector wl, l ∈ [1,k ], deﬁnes a scoring functionx ↦→ wl ·Φ(x) and the class associated\\nto point x ∈X is given by\\nargmax\\ny∈Y\\nwy · Φ(x).\\nWe denote byW the matrix formed by these weight vectors:W =( w⊤\\n1 ,..., w⊤\\nk )⊤\\nand for any p ≥ 1 denote by ∥W∥H,p the LH,p group norm of W deﬁned by\\n∥W∥H,p =\\n( k∑\\nl=1\\n∥wl∥p\\nH\\n⎡1/p\\n.\\nFor any p ≥ 1, the family of kernel-based hypotheses we will consider is1\\nHK,p = {(x, y) ∈X×{ 1,...,k } ↦→ wy · Φ(x): W =( w1,..., wk)⊤, ∥W∥H,p ≤ Λ}.\\nProposition 8.1 Rademacher complexity of multi-class kernel-based hy-\\npotheses\\nLet K : X× X → R be a PDS kernel and let Φ : X→ H be a feature mapping\\nassociated to K. Assume that there exists r> 0 such that K(x, x) ≤ r\\n2 for all\\nx ∈X .T h e n ,f o ra n ym ≥ 1, Rm(Π1(HK,p)) can be bounded as follows:\\nRm(Π1(HK,p)) ≤\\n√\\nr2Λ2\\nm .\\nProof Let S =( x1,...,x m) denote a sample of size m. Observe that for all\\n1. The hypothesis set H can also be deﬁned via H = {h ∈ RX× Y : h(·,y ) ∈ H ∧∥ h∥K,p ≤\\nΛ}, where ∥h∥K,p =\\n` Pk\\ny=1 ∥h(·,y )∥p\\nH\\n´1/p\\n, without referring to a feature mapping for K.190 Multi-Class Classiﬁcation\\nl ∈ [1,k ], the inequality ∥wl∥H ≤\\n(∑k\\nl=1 ∥wl∥p\\nH\\n⎡1/p\\n= ∥W∥H,p holds. Thus, the\\ncondition ∥W∥H,p ≤ Λi m p l i e st h a t∥wl∥H ≤ Λ for all l ∈ [1,k ]. In view of that,\\nthe Rademacher complexity of the hypothesis set Π 1(HK,p) can be expressed and\\nbounded as follows:\\nˆRS(Π1(HK,p)) = 1\\nm E\\nS,σ\\n[\\nsup\\ny∈Y\\n∥W∥≤Λ\\n⣨\\nwy,\\nm∑\\ni=1\\nσiΦ(xi)\\n⟩]\\n≤ 1\\nm E\\nS,σ\\n[\\nsup\\ny∈Y\\n∥W∥≤Λ\\n∥wy ∥H\\n\\ued79\\ued79\\ued79\\nm∑\\ni=1\\nσiΦ(xi)\\n\\ued79\\ued79\\ued79\\nH\\n]\\n(Cauchy-Schwarz ineq. )\\n≤ Λ\\nm E\\nS,σ\\n[\\ued79\\ued79\\ued79\\nm∑\\ni=1\\nσiΦ(xi)\\n\\ued79\\ued79\\ued79\\nH\\n]\\n≤ Λ\\nm\\n[\\nE\\nS,σ\\n[\\ued79\\ued79\\n\\ued79\\nm∑\\ni=1\\nσiΦ(xi)\\n\\ued79\\ued79\\n\\ued79\\n2\\nH\\n]]1/2\\n(Jensen’s inequality)\\n= Λ\\nm\\n[\\nE\\nS,σ\\n[ m∑\\ni=1\\n∥Φ(xi)∥2\\nH\\n]]1/2\\n(i ̸= j ⇒ E\\nσ\\n[σiσj]=0 )\\n= Λ\\nm\\n[\\nE\\nS,σ\\n[ m∑\\ni=1\\nK(xi,xi)\\n]]1/2\\n≤ Λ\\n√\\nmr2\\nm =\\n√\\nr2Λ2\\nm ,\\nwhich concludes the proof.\\nCombining theorem 8.1 and proposition 8.1 yields directly the following result.\\nCorollary 8.1 Margin bound for multi-class classiﬁcation with kernel-\\nbased hypotheses\\nLet K : X× X → R be a PDS kernel and let Φ : X→ H be a feature mapping\\nassociated to K. Assume that there exists r> 0 such that K(x, x) ≤ r\\n2 for all\\nx ∈X .F i xρ> 0.T h e n ,f o ra n yδ> 0, with probability at least1 − δ, the following\\nmulti-class classiﬁcation generalization bound holds for all h ∈ HK,p:\\nR(h) ≤ ˆRρ(h)+2 k2\\n√\\nr2Λ2/ρ2\\nm +\\n√\\nlog 1\\nδ\\n2m . (8.11)\\nIn the next two sections, we describe multi-class classiﬁcation algorithms that\\nbelong to two distinct families:uncombined algorithms, which are deﬁned by a single\\noptimization problem, and aggregated algorithms, which are obtained by training\\nmultiple binary classiﬁcations and by combining their outputs.8.3 Uncombined multi-class algorithms 191\\n8.3 Uncombined multi-class algorithms\\nIn this section, we describe three algorithms designed speciﬁcally for multi-class\\nclassiﬁcation. We start with a multi-class version of SVMs, then describe a boosting-\\ntype multi-class algorithm, and conclude with decision trees, which are often used\\nas base learners in boosting.\\n8.3.1 Multi-class SVMs\\nWe describe an algorithm that can be derived directly from the theoretical guar-\\nantees presented in the previous section. Proceeding as in section 4.4 for classiﬁ-\\ncation, the guarantee of corollary 8.1 can be expressed as follows: for any δ> 0,\\nwith probability at least 1 − δ, for all h ∈ H\\nK,2 = {(x, y) → wy · Φ(x): W =\\n(w1,..., wk)⊤, ∑k\\nl=1 ∥wl∥2 ≤ Λ2},\\nR(h) ≤ 1\\nm\\nm∑\\ni=1\\nξi +4 k2\\n√\\nr2Λ2\\nm +\\n√\\nlog 1\\nδ\\n2m , (8.12)\\nwhere ξi =m a x\\n(\\n1 − [wyi · Φ(xi) − maxy′ ̸=yi wy′ · Φ(xi)], 0\\n⎡\\nfor all i ∈ [1,m].\\nAn algorithm based on this theoretical guarantee consists of minimizing the\\nright-hand side of (8.12), that is, minimizing an objective function with a term\\ncorresponding to the sum of the slack variables ξ\\ni, and another one minimizing\\n∥W∥H,2 or equivalently ∑k\\nl=1 ∥wl∥2.T h i si sp r e c i s e l yt h eo p t i m i z a t i o np r o b l e m\\ndeﬁning the multi-class SVM algorithm:\\nmin\\nW,ξ\\n1\\n2\\nk∑\\nl=1\\n∥wl∥2 + C\\nm∑\\ni=1\\nξi\\nsubject to: ∀i ∈ [1,m], ∀l ∈Y−{ yi},\\nwyi · Φ(xi) ≥ wl · Φ(xi)+1 − ξi.\\nThe decision function learned is of the form x ↦→ argmaxl∈Y wl · Φ(x). As with\\nthe primal problem of SVMs, this is a convex optimization problem: the objective\\nfunction is convex, since it is a sum of convex functions, and the constraints are\\naﬃne and thus qualiﬁed. The objective and constraint functions are diﬀerentiable,\\nand the KKT conditions hold at the optimum. Deﬁning the Lagrangian and applying\\nthese conditions leads to the equivalent dual optimization problem, which can be192 Multi-Class Classiﬁcation\\nexpressed in terms of the kernel function K alone:\\nmax\\nα ∈Rm×k\\nm∑\\ni=1\\nαi · eyi − 1\\n2\\nm∑\\ni=1\\n(αi · αj)K(xi,xj)\\nsubject to: 0 ≤ αi ≤ C ∧ αi · 1 =0 , ∀i ∈ [1,m].\\nHere, α ∈ Rm×k is a matrix, αi denotes the ith row of α,a n del the lth unit vector\\nin Rk, l ∈ [1,k ]. Both the primal and dual problems are simple QPs generalizing\\nthose of the standard SVM algorithm. However, the size of the solution and the\\nnumber of constraints for both problems is in Ω(mk), which, for a large number of\\nclasses k, can make it diﬃcult to solve. However, there exist speciﬁc optimization\\nsolutions designed for this problem based on a decomposition of the problem into\\nm disjoint sets of constraints.\\n8.3.2 Multi-class boosting algorithms\\nWe describe a boosting algorithm for multi-class classiﬁcation calledAdaBoost.MH,\\nwhich in fact coincides with a special instance of AdaBoost. An alternative multi-\\nclass classiﬁcation algorithm based on similar boosting ideas, AdaBoost.MR, is\\ndescribed and analyzed in exercise 9.5. AdaBoost.MH applies to the multi-label\\nsetting whereY = {−1, +1}\\nk. As in the binary case, it returns a convex combination\\nof base classiﬁers selected from a hypothesis setH.L e tF be the following objective\\nfunction deﬁned for all samples S =( ( x1,y1),..., (xm,y m)) ∈ (X× Y )m and\\nα =( α1,...,α n) ∈ Rn, n ≥ 1, by\\nF(α)=\\nm∑\\ni=1\\nk∑\\nl=1\\ne−yi[l]gn(xi,l) =\\nm∑\\ni=1\\nk∑\\nl=1\\ne−yi[l] Pn\\nt=1 αtht(xi,l), (8.13)\\nwhere gn = ∑n\\nt=1 αtht and where yi[l] denotes the lth coordinate of yi for any\\ni ∈ [1,m]a n d l ∈ [1,k ]. F is a convex and diﬀerentiable upper bound on the\\nmulti-class multi-label loss:\\nm∑\\ni=1\\nk∑\\nl=1\\n1yi[l]̸=gn(xi,l) ≤\\nm∑\\ni=1\\nk∑\\nl=1\\ne−yi[l]gn(xi,l), (8.14)\\nsince for any x ∈X with label y = f(x) and any l ∈ [1,k ], the inequality\\n1y[l]̸=gn(x,l) ≤ e−y[l]gn(x,l) holds. AdaBoost.MH coincides exactly with the appli-\\ncation of coordinate descent to the objective function F.F i g u r e8 . 1g i v e st h e\\npseudocode of the algorithm in the case where the base classiﬁers are functions\\nmapping from X× Y to {−1, +1}. The algorithm takes as input a labeled sam-\\nple S =( (x\\n1,y1),..., (xm,y m)) ∈ (X× Y )m and maintains a distribution Dt over\\n{1,...,m }× Y . The remaining details of the algorithm are similar to AdaBoost. In8.3 Uncombined multi-class algorithms 193\\nAdaBoost.MH(S =( (x1,y1),..., (xm,y m)))\\n1 for i ← 1 to m do\\n2 for l ← 1 to k do\\n3 D1(i, l) ← 1\\nmk\\n4 for t ← 1 to T do\\n5 ht ← base classiﬁer in H with small error ϵt =P r(i,l)∼Dt [ht(xi,l ) ̸= yi[l]]\\n6 αt ← 1\\n2 log 1−ϵt\\nϵt\\n7 Zt ← 2[ϵt(1 − ϵt)]\\n1\\n2 ⊿ normalization factor\\n8 for i ← 1 to m do\\n9 for l ← 1 to k do\\n10 Dt+1(i, l) ← Dt(i,l)e x p (−αtyi[l]ht(xi,l))\\nZt\\n11 g ← ∑T\\nt=1 αtht\\n12 return h = sgn(g)\\nFigure 8.1 AdaBoost.MH algorithm, for H ⊆ ({−1, +1}k)X× Y.\\nfact, AdaBoost.MH exactly coincides with AdaBoost applied to the training sam-\\nple derived from S by splitting each labeled point (xi,y i)i n t ok labeled examples\\n((xi,l ),y i[l]), with each example (xi,l )i n X× Y and its label in {−1, +1}:\\n(xi,y i) → ((xi, 1),y i[1]),..., ((xi,k ),y i[k]),i ∈ [1,m].\\nLet S′ denote the resulting sample, then S′ =( (x1,1),y1[1]),..., (xm,k ),y m[k])).\\nS′ contains mk examples and the expression of the objective function F in (8.13)\\ncoincides exactly with that of the objective function of AdaBoost for the sampleS′.\\nIn view of this connection, the theoretical analysis along with the other observations\\nwe presented for AdaBoost in chapter 6 also apply here. Hence, we will focus on\\naspects related to the computational eﬃciency and to the weak learning condition\\nthat are speciﬁc to the multi-class scenario.\\nThe complexity of the algorithm is that of AdaBoost applied to a sample of\\nsize mk.F o rX⊆ RN , using boosting stumps as base classiﬁers, the complexity of\\nthe algorithm is therefore in O((mk) log(mk)+ mkNT ). Thus, for a large number\\nof classes k, the algorithm may become impractical using a single processor. The\\nweak learning condition for the application of AdaBoost in this scenario requires\\nthat at each round there exists a base classiﬁer h\\nt : X× Y→{ − 1,+1} such that\\nPr(i,l)∼Dt [ht(xi,l ) ̸= yi[l]] < 1/2. This may be hard to achieve if classes are close194 Multi-Class Classiﬁcation\\nX1 < a1\\nX1 < a2 X2 < a3\\nX2 < a4 R3 R4 R5\\nR1 R2\\nX1\\nX2\\na4\\na2 a1\\na3\\nR2\\nR1\\nR3\\nR5\\nR4\\nFigure 8.2 Left: example of a decision tree with numerical questions based on two\\nvariables X1 and X2. Here, each leaf is marked with the region it deﬁnes. The class\\nlabeling for a leaf is obtained via majority vote based on the training points falling\\nin the region it deﬁnes. Right: Partition of the two-dimensional space induced by\\nthat decision tree.\\nand it is diﬃcult to distinguish between them. It is also more diﬃcult in this context\\nto come up with “rules of thumb” h\\nt deﬁned over X× Y .\\n8.3.3 Decision trees\\nWe present and discuss the general learning method of decision trees that can\\nbe used in multi-class classiﬁcation, but also in other learning problems such as\\nregression (chapter 10) and clustering. Although the empirical performance of\\ndecision trees often is not state-of-the-art, decision trees can be used as weak learners\\nwith boosting to deﬁne eﬀective learning algorithms. Decision trees are also typically\\nfast to train and evaluate and relatively easy to interpret.\\nDeﬁnition 8.1 Binary decision tree\\nA binary decision tree is a tree representation of a partition of the feature space.\\nFigure 8.2 shows a simple example in the case of a two-dimensional space based\\non two features X\\n1 and X2, as well as the partition it represents. Each interior\\nnode of a decision tree corresponds to a question related to features. It can be a\\nnumerical question of the form X\\ni ≤ a for a feature variable Xi, i ∈ [1,N ],a n d\\nsome threshold a ∈ R, as in the example of ﬁgure 8.2, or a categorical question\\nsuch as Xi ∈{ blue,white,red},w h e nf e a t u r eXi takes a categorical value such as a\\ncolor. Each leaf is labeled with a labell ∈Y .\\nDecision trees can be deﬁned using more complex node questions, resulting in\\npartitions based on more complex decision surfaces. For example, binary space8.3 Uncombined multi-class algorithms 195\\nGreedyDecisionTrees(S =( (x1,y1),..., (xm,y m)))\\n1 tree ←{ n0} ⊿ root node.\\n2 for t ← 1 to T do\\n3( nt, qt) ← argmin(n,q) ˜F(n, q)\\n4 Split(tree, nt, qt)\\n5 return tree\\nFigure 8.3 Greedy algorithm for building a decision tree from a labeled sampleS.\\nThe procedure Split(tree, nt, qt) splits node nt by making it an internal node with\\nquestion qt and leaf children n− (n, q) and n+(n, q), each labeled with the dominating\\nclass of the region it deﬁnes, with ties broken arbitrarily.\\npartition (BSP) trees partition the space with convex polyhedral regions, based\\non questions of the form ∑n\\ni=1 αiXi ≤ a,a n d sphere trees partition with pieces\\nof spheres based on questions of the form ∥X − a0∥≤ a,w h e r eX is a feature\\nvector, a0 a ﬁxed vector, and a is a ﬁxed positive real number. More complex\\ntree questions lead to richer partitions and thus hypothesis sets, which can cause\\noverﬁtting in the absence of a suﬃciently large training sample. They also increase\\nthe computational complexity of prediction and training. Decision trees can also\\nbe generalized to branching factors greater than two, but binary trees are most\\ncommonly used due to computational considerations.\\nPrediction/partitioning: To predict the label of any point x ∈X we start\\nat the root node of the decision tree and go down the tree until a leaf is found,\\nby moving to the right child of a node when the response to the node question is\\npositive, and to the left child otherwise. When we reach a leaf, we associatex with\\nthe label of this leaf.\\nT h u s ,e a c hl e a fd e ﬁ n e saregion of X formed by the set of points corresponding\\nexactly to the same node responses and thus the same traversal of the tree. By\\ndeﬁnition, no two regions intersect and all points belong to exactly one region.\\nThus, leaf regions deﬁne a partition of X, as shown in the example of ﬁgure 8.2. In\\nmulti-class classiﬁcation, the label of a leaf is determined using the training sample:\\nthe class with the majority representation among the training points falling in a\\nleaf region deﬁnes the label of that leaf, with ties broken arbitrarily.\\nLearning: We will discuss two diﬀerent methods for learning a decision tree\\nusing a labeled sample. The ﬁrst method is a greedy technique. This is motivated\\nby the fact that the general problem of ﬁnding a decision tree with the smallest\\nerror is NP-hard. The method consists of starting with a tree reduced to a single196 Multi-Class Classiﬁcation\\n(root) node, which is a leaf whose label is the class that has majority over the\\nentire sample. Next, at each round, a node nt is split based on some question\\nqt. The pair ( nt, qt)i sc h o s e ns ot h a tt h enode impurity is maximally decreased\\naccording to some measure of impurity F. We denote by F(n) the impurity of n.\\nThe decrease in node impurity after a split of noden based on question q is deﬁned\\nas follows. Let n+(n, q) denote the right child of n after the split, n− (q, n)t h e\\nleft child, and η(n, q) the fraction of the points in the region deﬁned by n that are\\nmoved to n− (n, q). The total impurity of the leavesn− (n, q)a n dn+(n, q) is therefore\\nη(n, q)F(n− (n, q))+(1 − η(n, q))F(n+(n, q)). Thus, the decrease in impurity ˜F(n, q)\\nby that split is given by\\n˜F(n, q)= F(n) − [η(n, q)F(n− (n, q)) + (1− η(n, q))F(n+(n, q))].\\nFigure 8.3 shows the pseudocode of this greedy construction based on˜F.I np r a c t i c e ,\\nthe algorithm is stopped once all nodes have reached a suﬃcient level of purity, when\\nthe number of points per leaf has become too small for further splitting or based\\non some other similar heuristic.\\nFor any noden and class l ∈ [1,k ], let p\\nl(n) denote the fraction of points atn that\\nbelong to class l. Then, the three most commonly used measures of node impurity\\nF are deﬁned as follows:\\nF(n)=\\n⎧\\n⎪⎪⎨\\n⎪⎪\\n⎩\\n1 − max\\nl∈[1,k] pl(n) misclassiﬁcation ;\\n− ∑k\\nl=1 pl(n)l o g2 pl(n) entropy;\\n∑k\\nl=1 pl(n)(1 − pl(n)) Gini index.\\nFigure 8.4 illustrates these deﬁnitions in the special cases of two classes (k =2 ) .T h e\\nentropy and Gini index impurity functions are upper bounds on the misclassiﬁcation\\nimpurity function. All three functions are convex, which ensures that\\nF(n) − [η(n, q)F(n− (n, q)) + (1− η(n, q))F(n+(n, q))] ≥ 0.\\nHowever, the misclassiﬁcation function is piecewise linear, so ˜F(n, q)i sz e r oi ft h e\\nfraction of positive points remains less than (or more than) half after a split. In\\nsome cases, the impurity cannot be decreased by any split using that criterion. In\\ncontrast, the entropy and Gini functions are strictly convex, which guarantees a\\nstrict decrease in impurity. Furthermore, they are diﬀerentiable which is a useful\\nfeature for numerical optimization. Thus, the Gini index and the entropy criteria\\nare typically preferred in practice.\\nThe greedy method just described faces some issues. One issue relates to the\\ngreedy nature of the algorithm: a seemingly bad split may dominate subsequent\\nuseful splits, which could lead to trees with less impurity overall. This can be\\naddressed to a certain extent by using a look-ahead of some depth d to determine8.3 Uncombined multi-class algorithms 197\\n0 0.2 0.4 0.6 0.8 10\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\np\\nimpurity\\nFigure 8.4 Node impurity plotted as a function of the fraction of positive examples\\nin the binary case: misclassiﬁcation (in black), entropy (in green, scaled by.5 to set\\nthe maximum to the same value for all three functions), and the Gini index (in red).\\nthe splitting decisions, but such look-aheads can be computationally very costly.\\nAnother issue relates to the size of the resulting tree. To achieve some desired level\\nof impurity, trees of relatively large sizes may be needed. But larger trees deﬁne\\noverly complex hypotheses with high VC-dimensions (see exercise 9.6) and thus\\ncould overﬁt.\\nAn alternative method for learning decision trees using a labeled training sample\\nis based on the so-called grow-then-prune strategy. First a very large tree is grown\\nuntil it fully ﬁts the training sample or until no more than a very small number of\\npoints are left at each leaf. Then, the resulting tree, denoted astree, is pruned back\\nto minimize an objective function deﬁned based on generalization bounds as the\\nsum of an empirical error and a complexity term that can be expressed in terms of\\nt h es i z eo f˜tree, the set of leaves of tree:\\nG\\nλ(tree)=\\n∑\\nn∈gtree\\n|n|F(n)+ λ|˜tree|. (8.15)\\nλ ≥ 0 is a regularization parameter determining the trade-oﬀ between misclassiﬁ-\\ncation, or more generally impurity, versus tree complexity. For any tree tree′,w e\\ndenote by ˆR(tree′) the total empirical error ∑\\nn∈gtree′ |n|F(n). We seek a sub-tree\\ntreeλ of tree that minimizes Gλ and that has the smallest size. treeλ can be shown\\nto be unique. To determine treeλ, the following pruning method is used, which de-\\nﬁnes a ﬁnite sequence of nested sub-trees tree(0),..., tree(n). We start with the full\\ntree tree(0) = tree and for anyi ∈ [0,n − 1], deﬁne tree(i+1) from tree(i) by collapsing\\nan internal node n′ of tree(i),t h a ti sb yr e p l a c i n gt h es u b - t r e er o o t e da tn′ with a\\nleaf, or equivalently by combining the regions of all the leaves dominated by n′. n′\\nis chosen so that collapsing it causes the smallest per node increase in ˆR(tree(i)),198 Multi-Class Classiﬁcation\\nthat is the smallest r(tree(i), n′)d e ﬁ n e db y\\nr(tree(i), n′)= |n′|F(n′) − ˆR(tree′)\\n|˜tree\\n′\\n|− 1\\n,\\nwhere n′ is an internal node of tree(i). If several nodes n′ in tree(i) cause the same\\nsmallest increase per noder(tree(i), n′), then all of them are pruned to deﬁnetree(i+1)\\nfrom tree(i). This procedure continues until the tree tree(n) obtained has a single\\nnode. The sub-tree treeλ can be shown to be among the elements of the sequence\\ntree(0),..., tree(n).T h ep a r a m e t e rλ is determined via n-fold cross-validation.\\nDecision trees seem relatively easy to interpret, and this is often underlined as\\none of their most useful features. However, such interpretations should be carried\\nout with care since decision trees are unstable: small changes in the training data\\nmay lead to very diﬀerent splits and thus entirely diﬀerent trees, as a result of their\\nhierarchical nature. Decision trees can also be used in a natural manner to deal\\nwith the problem ofmissing features , which often appears in learning applications;\\nin practice, some features values may be missing because the proper measurements\\nwere not taken or because of some noise source causing their systematic absence. In\\nsuch cases, only those variables available at a node can be used in prediction. Finally,\\ndecision trees can be used and learned from data in a similar way inregression (see\\nchapter 10).\\n2\\n8.4 Aggregated multi-class algorithms\\nIn this section, we discuss a diﬀerent approach to multi-class classiﬁcation that\\nreduces the problem to that of multiple binary classiﬁcation tasks. A binary clas-\\nsiﬁcation algorithm is then trained for each of these tasks independently, and the\\nmulti-class predictor is deﬁned as a combination of the hypotheses returned by each\\nof these algorithms. We ﬁrst discuss two standard techniques for the reduction of\\nmulti-class classiﬁcation to binary classiﬁcation, and then show that they are both\\nspecial instances of a more general framework.\\n8.4.1 One-versus-all\\nLet S =( ( x\\n1,y1),...,x m,y m)) ∈ (X× Y )m be a labeled training sample. A\\nstraightforward reduction of the multi-class classiﬁcation to binary classiﬁcation\\n2. The only changes to the description for classiﬁcation are the following. For prediction,\\nthe label of a leaf is deﬁned as the mean squared average of the labels of the points falling\\nin that region. For learning, the impurity function is the mean squared error.8.4 Aggregated multi-class algorithms 199\\nis based on the so-called one-versus-all (OVA) or one-versus-the-rest technique .\\nThis technique consists of learning k binary classiﬁers hl : X→ { − 1, +1}, l ∈Y ,\\neach seeking to discriminate one class l ∈Y from all the others. For any l ∈Y , hl\\nis obtained by training a binary classiﬁcation algorithm on the full sample S after\\nrelabeling points in class l with 1 and all others with −1. For l ∈Y , assume that\\nhl is derived from the sign of a scoring function fl : X→ R,t h a ti shl = sgn(fl), as\\nin the case of many of the binary classiﬁcation algorithms discussed in the previous\\nchapters. Then, the multi-class hypothesish: X→ Y deﬁned by the OVA technique\\nis given by:\\n∀x ∈X ,h (x) = argmax\\nl∈Y\\nfl(x). (8.16)\\nThis formula may seem similar to those deﬁning a multi-class classiﬁcation hypoth-\\nesis in the case of uncombined algorithms. Note, however, that for uncombined\\nalgorithms the functions f\\nl are learned together, while here they are learned in-\\ndependently. Formula (8.16) is well-founded when the scores given by functions fl\\ncan be interpreted as conﬁdence scores, that is when fl(x) is learned as an esti-\\nmate of the probability of x conditioned on class l. However, in general, the scores\\ngiven by functions fl, l ∈Y , are not comparable and the OVA technique based\\non (8.16) admits no principled justiﬁcation. This is sometimes referred to as a cal-\\nibration problem.C l e a r l y ,t h i sp r o b l e mc a n n o tb ec o r r e c t e db ys i m p l yn o r m a l i z i n g\\nthe scores of each function to make their magnitudes uniform, or by applying other\\nsimilar heuristics. When it is justiﬁable, the OVA technique is simple and its com-\\nputational cost is k times that of training a binary classiﬁcation algorithm, which\\nis similar to the computation costs for many uncombined algorithms.\\n8.4.2 One-versus-one\\nAn alternative technique, known as the one-versus-one (OVO) technique, consists\\nof using the training data to learn (independently), for each pair of distinct classes\\n(l,l\\n′) ∈Y 2, l ̸= l′, a binary classiﬁer hll′ : X→ { − 1, 1} discriminating between\\nclasses l and l′. For any (l,l ′) ∈Y 2, hll′ is obtained by training a binary classiﬁcation\\nalgorithm on the sub-sample containing exactly the points labeled with l or l′,\\nwith the value +1 returned for class l′ and −1 for class l.T h i sr e q u i r e st r a i n i n g(k\\n2\\n⎡\\n= k(k − 1)/2 classiﬁers, which are combined to deﬁne a multi-class classiﬁcation\\nhypothesis h via majority vote:\\n∀x ∈X ,h (x) = argmax\\nl′ ∈Y\\n⏐⏐{l: hll′ (x)=1 }\\n⏐⏐. (8.17)\\nThus, for a ﬁxed point x ∈X , if we describe the prediction values hll′ (x)a st h e\\nresults of the matches in a tournament between two playersl and l′,w i t hhll′ (x)=1200 Multi-Class Classiﬁcation\\nTraining Testing\\nOVA O(kmα) O(kct)\\nOVO O(k2−αmα) O(k2ct)\\nT able 8.1 Comparison of the time complexity the OVA and OVO techniques for\\nboth training and testing. The table assumes a full training sample of size m with\\neach class represented by m/k points. The time for training a binary classiﬁcation\\nalgorithm on a sample of size n is assumed to be in O(nα ). Thus, the training time\\nfor the OVO technique is inO(k2(m/k)α )= O(k2−α mα ). ct denotes the cost of testing\\na single classiﬁer.\\nindicating l′ winning over l, then the class predicted by h can be interpreted as the\\no n ew i t ht h el a r g e s tn u m b e ro fw i n si nt h a tt o u r n a m e n t .\\nLet x ∈X b eap o i n tb e l o n g i n gt oc l a s sl′.B yd e ﬁ n i t i o no ft h eO V Ot e c h n i q u e ,\\nif hll′ (x) = 1 for all l ̸= l′, then the class associated to x by OVO is the correct\\nclass l′ since\\n⏐⏐{l: hll′ (x)=1 }\\n⏐⏐ = k − 1 and no other class can reach (k − 1) wins. By\\ncontraposition, if the OVO hypothesis misclassiﬁesx, then at least one of the (k −1)\\nbinary classiﬁers hll′ , l ̸= l′, incorrectly classiﬁes x. Assume that the generalization\\nerror of all binary classiﬁers hll′ used by OVO is at most r, then, in view of this\\ndiscussion, the generalization error of the hypothesis returned by OVO is at most\\n(k − 1)r.\\nThe OVO technique is not subject to the calibration problem pointed out in the\\ncase of the OVA technique. However, when the size of the sub-sample containing\\nmembers of the classes l and l\\n′ is relatively small, hll′ may be learned without\\nsuﬃcient data or with increased risk of overﬁtting. Another concern often raised for\\nthe use of this technique is the computational cost of training k(k − 1)/2 binary\\nclassiﬁers versus that of the OVA technique.\\nTaking a closer look at the computational requirements of these two methods\\nreveals, however, that the disparity may not be so great and that in fact under\\nsome assumptions the time complexity of training for OVO could be less than that\\nof OVA. Table 8.1 compares the computational complexity of these methods both\\nfor training and testing assuming that the complexity of training a binary classiﬁer\\non a sample of size m is in O(mα) and that each class is equally represented in\\nt h et r a i n i n gs e t ,t h a ti sb ym/k points. Under these assumptions, if α ∈ [2, 3) as in\\nthe case of some algorithms solving a QP problem, such as SVMs, then the time\\ncomplexity of training for the OVO technique is in fact more favorable than that\\nof OVA. Forα = 1, the two are comparable and it is only for sub-linear algorithms\\nthat the OVA technique would beneﬁt from a better complexity. In all cases, at test\\ntime, OVO requiresk(k−1)/2 classiﬁer evaluations, which is (k−1) times more than8.4 Aggregated multi-class algorithms 201\\nOVA. However, for some algorithms the evaluation time for each classiﬁer could be\\nmuch smaller for OVO. For example, in the case of SVMs, the average number of\\nsupport vectors may be signiﬁcantly smaller for OVO, since each classiﬁer is trained\\non a signiﬁcantly smaller sample. If the number of support vectors isk times smaller\\nand if sparse feature representations are used, then the time complexities of both\\ntechniques for testing are comparable.\\n8.4.3 Error-correction codes\\nA more general method for the reduction of multi-class to binary classiﬁcation is\\nbased on the idea of error-correction codes (ECOC).T h i st e c h n i q u ec o n s i s t so f\\nassigning to each classl ∈Y a code word of length c ≥ 1, which in the simplest case\\nis a binary vectorM\\nl ∈{ −1, +1}c. Ml serves as a signature for classl, and together\\nthese vectors deﬁne a matrix M ∈{ −1, +1}k×c whose lth row is Ml, as illustrated\\nby ﬁgure 8.5. Next, for each column j ∈ [1,c], a binary classiﬁer hj : X→ { − 1, +1}\\nis learned using the full training sample S, after relabeling points that belong to\\na class of column l labeled with +1, and all others with −1. For any x ∈X ,l e t\\nh(x) denote the vectorh(x)=( h1(x),...,h c(x))⊤. Then, the multi-class hypothesis\\nh: X→ Y is deﬁned by\\n∀x ∈X ,h (x) = argmax\\nl∈Y\\ndH\\n(\\nMl,h(x)\\n⎡\\n. (8.18)\\nT h u s ,t h ec l a s sp r e d i c t e di st h eo n ew h o s es i g n a t u r e si st h ec l o s e s tt oh(x)i n\\nHamming distance. Figure 8.5 illustrates this deﬁnition: no row of matrix M\\nmatches the vector of predictions h(x) in that case, but the third row shares the\\nlargest number of components with h(x).\\nThe success of the ECOC technique depends on the minimal Hamming distance\\nbetween the class code words. Let d denote that distance, then up to r0 =\\n⌊d−1\\n2\\n⌋\\nbinary classiﬁcation errors can be corrected by this technique: by deﬁnition of d,\\neven if r<r 0 binary classiﬁers hl misclassify x ∈X , h(x) is closest to the code\\nword of the correct class of x. For a ﬁxed c, the design of error-correction matrix\\nM is subject to a trade-oﬀ, since larger d values may imply substantially more\\ndiﬃcult binary classiﬁcation tasks. In practice, each column may correspond to a\\nclass feature determined based on domain knowledge.\\nThe ECOC technique just described can be extended in two ways. First, instead\\nof using only the label predicted by each classiﬁer hl the magnitude of the scores\\ndeﬁning hl is used. Thus, if hl = sgn(fl) for some function fl whose values can\\nbe interpreted as conﬁdence scores, then the multi-class hypothesis h: X→ Y is202 Multi-Class Classiﬁcation\\n123456\\n1 000100\\n2 100000\\n3 011010\\n4 110000\\n5 110010\\n6 001101\\n7 001000\\n8 010100\\n011011\\nclasses\\ncodes\\nnew example    x\\nf1(x) f2(x) f3(x) f4(x) f5(x) f6(x)\\nFigure 8.5 Illustration of error-correction codes for multi-class classiﬁcation. Left:\\nbinary code matrix M, with each row representing the code word of length c =6\\nof a class l ∈ [1, 8].R i g h t :v e c t o ro fp r e d i c t i o n sh(x) for a test point x.T h eE C O C\\nclassiﬁer assigns label 3 to x, since the binary code for the third class yields the\\nminimal Hamming distance with h(x) (distance of 1).\\ndeﬁned by\\n∀x ∈X ,h (x) = argmin\\nl∈Y\\nc∑\\nj=1\\nL(mljfj(x)), (8.19)\\nwhere (mlj) are the entries ofM and where L: R → R+ is a loss function. When L\\nis deﬁned by L(x)= 1−sgn(x)\\n2 for all x ∈X and hl = fl,w ec a nw r i t e :\\nc∑\\nj=1\\nL(mljfj(x)) =\\nc∑\\nj=1\\n1 − sgn(mljhj(x))\\n2 = dh(Ml,h(x)),\\nand (8.19) coincides with (8.18). Furthermore, ternary codes can be used with ma-\\ntrix entries in {−1, 0, +1} so that examples in classes labeled with 0 are disregarded\\nwhen training a binary classiﬁer for each column. With these extensions, both OVA\\nand OVO become special instances of the ECOC technique. The matrix M for\\nOVA is a square matrix, that is c = k, with all terms equal to −1e x c e p tf r o mt h e\\ndiagonal ones which are all equal to +1. The matrixM for OVO hasc = k(k − 1)/2\\ncolumns. Each column corresponds to a pair of distinct classes ( l,l ′), l ̸= l′,w i t h\\nall entries equal to 0 except from the one with rowl,w h i c hi s−1, and the one with\\nrow l′, which is +1.\\nSince the values of the scoring functions are assumed to be conﬁdence scores,\\nmljfj(x) can be interpreted as the margin of classiﬁer j on point x and (8.19) is\\nthus based on some loss L deﬁned with respect to the binary classiﬁer’s margin.\\nA further extension of ECOC consists of extending discrete codes to continuous8.5 Structured prediction algorithms 203\\nones by letting the matrix entries take arbitrary real values and by using the training\\nsample to learn matrix M. Starting with a discrete version ofM, c binary classiﬁers\\nwith scoring functions fl, l ∈ [1,c], are ﬁrst learned as described previously. We will\\ndenote by F(x) the vector (f1(x),...,f c(x))⊤ for any x ∈X . Next, the entries of\\nM are relaxed to take real values and learned from the training sample with the\\nobjective of making the row of M corresponding to the class of any point x ∈X\\nmore similar to F(x) than other rows. The similarity can be measured using any\\nPDS kernel K. An example of an algorithm for learning M using a PDS kernel K\\nand the idea just discussed is in fact multi-class SVMs, which, in this context, can\\nbe formulated as follows:\\nmin\\nM,ξ\\n∥M∥2\\nF + C\\nm∑\\ni=1\\nξi\\nsubject to: ∀(i, l) ∈ [1,m] ×Y ,\\nK(f(xi),Myi ) ≥ K(f(xi),Ml)+1 − ξi.\\nSimilar algorithms can be deﬁned using other matrix norms. The resulting multi-\\nclass classiﬁcation decision function has the following form:\\nh: x ↦→ argmax\\nl∈{1,...,k}\\nK(f(x),Ml).\\n8.5 Structured prediction algorithms\\nIn this section, we brieﬂy discuss an important class of problems related to multi-\\nclass classiﬁcation that frequently arises in computer vision, computational biology,\\nand natural language processing. These include all sequence labeling problems and\\ncomplex problems such as parsing, machine translation, and speech recognition.\\nIn these applications, the output labels have a rich internal structure. For exam-\\nple, in part-of-speech tagging the problem consists of assigning a part-of-speech tag\\nsuch as N (noun), V (verb), or A (adjective), to every word of a sentence. Thus, the\\nlabel of the sentence ω\\n1 ...ω n made of the words ωi is a sequence of part-of-speech\\ntags t1 ...t n. This can be viewed as a multi-class classiﬁcation problem where each\\nsequence of tags is a possible label. However, several critical aspects common to\\nsuch structured output problems make them distinct from the standard multi-class\\nclassiﬁcation.\\nFirst, the label set is exponentially large as a function of the size of the output.\\nFor example, if Σ denotes the alphabet of part-of-speech tags, for a sentence of\\nlength n there are |Σ|\\nn possible tag sequences. Second, there are dependencies204 Multi-Class Classiﬁcation\\nbetween the substructures of a label that are important to take into account for\\nan accurate prediction. For example, in part-of-speech tagging, some tag sequences\\nmay be ungrammatical or unlikely. Finally, the loss function used is typically not a\\nzero-one loss but one that depends on the substructures. LetL: Y×Y → R denote\\nal o s sf u n c t i o ns u c ht h a tL(y\\n′,y ) measures the penalty of predicting the labely′ ∈Y\\ninstead of the correct label y ∈Y .3 In part-of-speech tagging, L(y′,y ) could be for\\nexample the Hamming distance between y′ and y.\\nThe relevant features in structured output problems often depend on both the\\ninput and the output. Thus, we will denote by Φ(x, y) ∈ RN the feature vector\\nassociated to a pair (x, y) ∈X×Y .\\nTo model the label structures and their dependency, the label set Y is typically\\nassumed to be endowed with a graphical model structure, that is, a graph giving a\\nprobabilistic model of the conditional dependence between the substructures. It is\\nalso assumed that both the feature vectorΦ(x, y) associated to an inputx ∈X and\\noutput y ∈Y and the lossL(y′,y ) factorize according to the cliques of that graphical\\nmodel.4 A detailed treatment of this topic would require a further background in\\ngraphical models, and is thus beyond the scope of this section.\\nThe hypothesis set used by most structured prediction algorithms is then deﬁned\\nas the set of functions h: X→ Y such that\\n∀x ∈X ,h (x) = argmax\\ny∈Y\\nw · Φ(x, y), (8.20)\\nfor some vector w ∈ RN .L e tS =( (x1,y1),...,x m,y m)) ∈ (X× Y )m be an i.i.d.\\nlabeled sample. Since the hypothesis set is linear, we can seek to deﬁne an algorithm\\nsimilar to multi-class SVMs. The optimization problem for multi-class SVMs can\\nbe rewritten equivalently as follows:\\nmin\\nw\\n1\\n2 ∥w∥2+C\\nm∑\\ni=1\\nmax\\ny̸=yi\\nmax\\n(\\n0, 1 − w · [Φ(xi,y i)−Φ(xi,y )]\\n⎡\\n, (8.21)\\nHowever, here we need to take into account the loss functionL,t h a ti sL(y,y i)f o r\\neach i ∈ [1,m]a n dy ∈Y , and there are multiple ways to proceed. One possible way\\nis to let the margin violation be penalized additively with L(y,y i). Thus, in that\\ncase L(y,y i) is added to the margin violation. Another natural method consists of\\npenalizing the margin violation by multiplying it with L(y,y i). A margin violation\\nwith a larger loss is then penalized more than one with a smaller one.\\n3. More generally, in some applications, the loss function could also depend on the input.\\nThus, L is then a function mapping L: X× Y× Y→ R,w i t hL(x, y′,y ) measuring the\\npenalty of predicting the label y′ instead of y given the input x.\\n4. In an undirected graph, a clique is a set of fully connected vertices.8.5 Structured prediction algorithms 205\\nThe additive penalization leads to the following algorithm known as Maximum\\nMargin Markov Networks (M3N):\\nmin\\nw\\n1\\n2 ∥w∥2+C\\nm∑\\ni=1\\nmax\\ny̸=yi\\nmax\\n(\\n0,L(yi,y ) − w · [Φ(xi,y i)−Φ(xi,y )]\\n⎡\\n. (8.22)\\nAn advantage of this algorithm is that, as in the case of SVMs, it admits a natural\\nuse of PDS kernels. As already indicated, the label setY is assumed to be endowed\\nwith a graph structure with a Markov property, typically a chain or a tree, and\\nthe loss function is assumed to be decomposable in the same way. Under these\\nassumptions, by exploiting the graphical model structure of the labels, a polynomial-\\ntime algorithm can be given to determine its solution.\\nA multiplicative combination of the loss with the margin leads to the following\\nalgorithm known as SVMStruct:\\nmin\\nw\\n1\\n2 ∥w∥2+C\\nm∑\\ni=1\\nmax\\ny̸=yi\\nL(yi,y )m a x\\n(\\n0,1 − w · [Φ(xi,y i)−Φ(xi,y )]\\n⎡\\n. (8.23)\\nThis problem can be equivalently written as a QP with an inﬁnite number of\\nconstraints. In practice, it is solved iteratively by augmenting at each round the\\nﬁnite set of constraints of the previous round with the most violating constraint.\\nThis method can be applied in fact under very general assumptions and for arbitrary\\nloss deﬁnitions. As in the case of M\\n3N, SVMStruct naturally admits the use of PDS\\nkernels and thus an extension to non-linear models for the solution.\\nAnother standard algorithm for structured prediction problems is Conditional\\nRandom Fields (CRFs). We will not describe this algorithm in detail, but point\\nout its similarity with the algorithms just described, in particular M 3N. The\\noptimization problem for CRFs can be written as\\nmin\\nw\\n1\\n2 ∥w∥2+C\\nm∑\\ni=1\\nlog\\n∑\\ny∈Y\\nexp\\n(\\nL(yi,y ) − w · [Φ(xi,y i)−Φ(xi,y )]\\n⎡\\n. (8.24)\\nAssume for simplicity that Y is ﬁnite and has cardinality k and let f denote the\\nfunction (x1,...,x k) ↦→ log(∑k\\nj=1 exj ). f is a convex function known as the soft-\\nmax, since it provides a smooth approximation of ( x1,...,x k) ↦→ max(x1,...,x k).\\nThen, problem (8.24) is similar to (8.22) modulo the replacement of the max\\noperator with the soft-max function just described.206 Multi-Class Classiﬁcation\\n8.6 Chapter notes\\nThe margin-based generalization for multi-class classiﬁcation presented in theo-\\nrem 8.1 is based on an adaptation of the result and proof due to Koltchinskii and\\nPanchenko [2002]. Proposition 8.1 bounding the Rademacher complexity of multi-\\nclass kernel-based hypotheses and corollary 8.1 are new.\\nAn algorithm generalizing SVMs to the multi-class classiﬁcation setting was ﬁrst\\nintroduced by Weston and Watkins [1999]. The optimization problem for that\\nalgorithm was based on k(k − 1)/2 slack variables for a problem withk classes and\\nthus could be ineﬃcient for a relatively large number of classes. A simpliﬁcation of\\nthat algorithm by replacing the sum of the slack variables∑\\nj̸=i ξij related to point\\nxi by its maximum ξi =m a xj̸=i ξij considerably reduces the number of variables\\nand leads to the multi-class SVM algorithm presented in this chapter [Crammer\\nand Singer, 2001, 2002].\\nThe AdaBoost.MH algorithm is presented and discussed by Schapire and Singer\\n[1999, 2000]. As we showed in this chapter, the algorithm is a special instance\\nof AdaBoost. Another boosting-type algorithm for multi-class classiﬁcation, Ad-\\naBoost.MR, is presented by Schapire and Singer [1999, 2000]. That algorithm is\\nalso a special instance of the RankBoost algorithm presented in chapter 9. See ex-\\nercise 9.5 for a detailed analysis of this algorithm, including generalization bounds.\\nThe most commonly used tools for learning decision trees are CART (classiﬁcation\\nand regression tree) [Breiman et al., 1984] and C4.5 [Quinlan, 1986, 1993]. The\\ngreedy technique we described for learning decision trees beneﬁts in fact from an\\ninteresting analysis: remarkably, it has been shown by Kearns and Mansour [1999],\\nMansour and McAllester [1999] that, under a weak learner hypothesis assumption,\\nsuch decision tree algorithms produce a strong hypothesis. The grow-then-prune\\nmethod is from CART. It has been analyzed by a variety of diﬀerent studies, in\\nparticular by Kearns and Mansour [1998] and Mansour and McAllester [2000], who\\ngive generalization bounds for the resulting decision trees with respect to the error\\nand size of the best sub-tree of the original tree pruned.\\nThe idea of the ECOC framework for multi-class classiﬁcation is due to Dietterich\\nand Bakiri [1995]. Allwein et al. [2000] further extended and analyzed this method\\nto margin-based losses, for which they presented a bound on the empirical error\\nand a generalization bound in the more speciﬁc case of boosting. While the OVA\\ntechnique is in general subject to a calibration issue and does not have any\\njustiﬁcation, it is very commonly used in practice. Rifkin [2002] reports the results\\nof extensive experiments with several multi-class classiﬁcation algorithms that are\\nrather favorable to the OVA technique, with performances often very close or better\\nthan for those of several uncombined algorithms, unlike what has been claimed by\\nsome authors (see also Rifkin and Klautau [2004]).8.7 Exercises 207\\nThe CRFs algorithm was introduced by Laﬀerty, McCallum, and Pereira [2001].\\nM3N is due to Taskar, Guestrin, and Koller [2003] and StructSVM was presented by\\nTsochantaridis, Joachims, Hofmann, and Altun [2005]. An alternative technique for\\ntackling structured prediction as a regression problem was presented and analyzed\\nby Cortes, Mohri, and Weston [2007c].\\n8.7 Exercises\\n8.1 Generalization bounds for multi-label case. Use similar techniques to those used\\nin the proof of theorem 8.1 to derive a margin-based learning bound in the multi-\\nlabel case.\\n8.2 Multi-class classiﬁcation with kernel-based hypotheses constrained by an L\\np\\nnorm. Use corollary 8.1 to deﬁne alternative multi-class classiﬁcation algorithms\\nwith kernel-based hypotheses constrained by an Lp norm with p ̸= 2. For which\\nvalue ofp ≥ 1 is the bound of proposition 8.1 tightest? Derive the dual optimization\\nof the multi-class classiﬁcation algorithm deﬁned with p = ∞ .\\n8.3 Alternative multi-class boosting algorithm. Consider the objective function\\nG deﬁned for any sample S =( ( x1,y1),..., (xm,y m)) ∈ (X× Y )m and α =\\n(α1,...,α n) ∈ Rn, n ≥ 1, by\\nG(α)=\\nm∑\\ni=1\\ne− 1\\nk\\nPk\\nl=1 yi[l]gn(xi,l) =\\nm∑\\ni=1\\ne− 1\\nk\\nPk\\nl=1 yi[l] Pn\\nt=1 αtht(xi,l). (8.25)\\nUse the convexity of the exponential function to compareG with the objective func-\\ntion F deﬁning AdaBoost.MH. Show that G is a convex function upper bounding\\nthe multi-label multi-class error. Discuss the properties ofG and derive an algorithm\\ndeﬁned by the application of coordinate descent to G. Give theoretical guarantees\\nfor the performance of the algorithm and analyze its running-time complexity when\\nusing boosting stumps.\\n8.4 Multi-class algorithm based on RankBoost. This problem requires familiarity\\nwith the material presented both in this chapter and in chapter 9. An alternative\\nboosting-type multi-class classiﬁcation algorithm is one based on a ranking criterion.\\nWe will deﬁne and examine that algorithm in the mono-label setting. Let H be a\\nfamily of base hypothesis mapping X× Y to {−1, +1}.L e t F be the following\\nobjective function deﬁned for all samples S =( (x\\n1,y1),..., (xm,y m)) ∈ (X× Y )m208 Multi-Class Classiﬁcation\\nand α =( α1,...,α n) ∈ Rn, n ≥ 1, by\\nF(α)=\\nm∑\\ni=1\\n∑\\nl̸=yi\\ne−(gn(xi,yi)−gn(xi,l)) =\\nm∑\\ni=1\\n∑\\nl̸=yi\\ne− Pn\\nt=1 αt(ht(xi,yi)−ht(xi,l)). (8.26)\\nwhere gn = ∑n\\nt=1 αtht.\\n(a) Show that F is convex and diﬀerentiable.\\n(b) Show that 1\\nm\\n∑m\\ni=1 1ρgn (xi,y i) ≤ 1\\nk−1 F(α), where gn = ∑n\\nt=1 αtht.\\n(c) Give the pseudocode of the algorithm obtained by applying coordinate\\ndescent to F. The resulting algorithm is known as AdaBoost.MR. Show that\\nAdaBoost.MR exactly coincides with the RankBoost algorithm applied to the\\nproblem of ranking pairs (x, y) ∈X×Y . Describe exactly the ranking target\\nfor these pairs.\\n(d) Use question (8.4b) and the learning bounds of this chapter to derive\\nmargin-based generalization bounds for this algorithm.\\n(e) Use the connection of the algorithm with RankBoost and the learning\\nbounds of chapter 9 to derive alternative generalization bounds for this al-\\ngorithm. Compare these bounds with those of the previous question.\\n8.5 Decision trees. Show that VC-dimension of a binary decision tree with n nodes\\nin dimension N is in O(n log N).\\n8.6 Give an example where the generalization error of each of thek(k − 1)/2 binary\\nclassiﬁers h\\nll′ , l ̸= l′, used in the deﬁnition of the OVO technique is r and that of\\nthe OVO hypothesis (k − 1)r.9R a n k i n g\\nThe learning problem of ranking arises in many modern applications, including\\nthe design of search engines, information extraction platforms, and movie recom-\\nmendation systems. In these applications, the ordering of the documents or movies\\nreturned is a critical aspect of the system. The main motivation for ranking over\\nclassiﬁcation in the binary case is the limitation of resources: for very large data\\nsets, it may be impractical or even impossible to display or process all items labeled\\nas relevant by a classiﬁer. A standard user of a search engine is not willing to con-\\nsult all the documents returned in response to a query, but only the top ten or so.\\nSimilarly, a member of the fraud detection department of a credit card company\\ncannot investigate thousands of transactions classiﬁed as potentially fraudulent, but\\nonly a few dozens of the most suspicious ones.\\nIn this chapter, we study in depth the learning problem of ranking. We distinguish\\ntwo general settings for this problem: the score-based and the preference-based set-\\ntings. For the score-based setting, which is the most widely explored one, we present\\nmargin-based generalization bounds using the notion of Rademacher complexity.\\nWe then describe an SVM-based ranking algorithm that can be derived from these\\nbounds and describe and analyze RankBoost, a boosting algorithm for ranking.\\nWe further study speciﬁcally the bipartite setting of the ranking problem where,\\nas in binary classiﬁcation, each point belongs to one of two classes. We discuss an\\neﬃcient implementation of RankBoost in that setting and point out its connec-\\ntions with AdaBoost. We also introduce the notions of ROC curves and area under\\nthe ROC curves (AUC) which are directly relevant to bipartite ranking. For the\\npreference-based setting, we present a series of results, in particular regret-based\\nguarantees for both a deterministic and a randomized algorithm, as well as a lower\\nb o u n di nt h ed e t e r m i n i s t i cc a s e .\\n9.1 The problem of ranking\\nWe ﬁrst introduce the most commonly studied scenario of the ranking problem in\\nmachine learning. We will refer to this scenario as the score-based setting of the210 Ranking\\nranking problem. In section 9.6, we present and analyze an alternative setting, the\\npreference-based setting.\\nThe general supervised learning problem of ranking consists of using labeled\\ninformation to deﬁne an accurate ranking prediction function for all points. In the\\nscenario examined here, the labeled information is supplied only for pairs of points\\nand the quality of a predictor is similarly measured in terms of its average pairwise\\nmisranking. The predictor is a real-valued function, a scoring function: the scores\\nassigned to input points by this function determine their ranking.\\nLet X denote the input space. We denote by D an unknown distribution over\\nX× X according to which pairs of points are drawn and byf : X× X →{ − 1,0, +1}\\na target labeling function or preference function. The three values assigned by f\\nare interpreted as follows: f(x, x\\n′) = +1 if x′ is preferred to x or ranked higher\\nthan x, f(x, x′)= −1i f x is preferred to x′,a n df(x, x′) = 0 if both x and x′ have\\nthe same preference or ranking, or if there is no information about their respective\\nranking. This formulation corresponds to a deterministic scenario which we adopt\\nfor simpliﬁcation. As discussed in section 2.4.1, it can be straightforwardly extended\\nto a stochastic scenario where we have a distribution over X× X× { − 1,0, +1}.\\nNote that in general no particular assumption is made about the transitivity of the\\norder induced by f:w em a yh a v ef(x, x\\n′)=1a n d f(x′,x ′′)=1b u t f(x, x”) = −1\\nfor three points x, x′,a n d x′′. While this may contradict an intuitive notion of\\npreference, such preference orders are in fact commonly encountered in practice, in\\nparticular when they are based on human judgments. This is sometimes because the\\npreference between two items are decided based on diﬀerent features: for example,\\nan individual may prefer movie x\\n′ to x because x′ is an action movie and x a\\nmusical, and prefer x′′ to x′ because x′′ is an action movie with more active scenes\\nthan x′. Nevertheless, he may prefer x to x′′ because the cost of renting a DVD\\nfor x′′ is prohibitive. Thus, in this example, two features, the genre and the price,\\nare invoked, each aﬀecting the decision for diﬀerent pairs. In fact, in general, no\\nassumption is made about the preference function, not even the antisymmetry of\\nthe order induced; thus, we may have f(x, x′)=1a n d f(x′,x)=1a n dy e t x ̸= x′.\\nThe learner receives a labeled sample S =\\n(\\n(x1,x ′\\n1,y1),..., (xm,x ′\\nm,y m)\\n⎡\\n∈\\nX× X× { − 1, 0, +1} with (x1,x ′\\n1),..., (xm,x ′\\nm) drawn i.i.d. according to D and\\nyi = f(xi,x ′\\ni) for all i ∈ [1,m]. Given a hypothesis setH of functions mapping X to\\nR, the ranking problem consists of selecting a hypothesish ∈ H with small expected\\npairwise misranking or generalization error R(h) with respect to the target f:\\nR(h)= P r\\n(x,x′)∼D\\n[(\\nf(x, x′) ̸=0\\n⎡\\n∧\\n(\\nf(x, x′)(h(x′) − h(x)) ≤ 0\\n⎡]\\n. (9.1)\\nThe empirical pairwise misranking or empirical error of h is denoted by ˆR(h)a n d9.2 Generalization bound 211\\ndeﬁned by\\nˆR(h)= 1\\nm\\nm∑\\ni=1\\n1(yi̸=0)∧(yi(h(x′\\ni)−h(xi))≤0) . (9.2)\\nNote that while the target preference function f is in general not transitive, the\\nlinear ordering induced by a scoring functionh ∈ H is by deﬁnition transitive. This\\nis a drawback of the score-based setting for the ranking problem since, regardless of\\nthe complexity of the hypothesis set H, if the preference function is not transitive,\\nno hypothesis h ∈ H can faultlessly predict the target pairwise ranking.\\n9.2 Generalization bound\\nIn this section, we present margin-based generalization bounds for ranking. To\\nsimplify the presentation, we will assume for the results of this section that the\\npairwise labels are in {−1, +1}.T h u s ,i fap a i r(x, x\\n′) is drawn according toD,t h e n\\neither x is preferred to x′ or the opposite. The learning bounds for the general case\\nhave a very similar form but require more details. As in the case of classiﬁcation,\\nfor any ρ> 0, we can deﬁne the empirical margin loss of a hypothesish for pairwise\\nranking as\\nˆRρ(h)= 1\\nm\\nm∑\\ni=1\\nΦρ(yi(h(x′\\ni) − h(xi)), (9.3)\\nwhere Φρ is the margin loss function (deﬁnition 4.3). Thus, the empirical margin\\nloss for ranking is upper bounded by the fraction of the pairs ( xi,x ′\\ni)t h a th is\\nmisranking or correctly ranking but with conﬁdence less than ρ:\\nˆRρ(h) ≤ 1\\nm\\nm∑\\ni=1\\n1yi(h(x′\\ni)−h(xi))≤ρ. (9.4)\\nWe denote byD1 t h em a r g i n a ld i s t r i b u t i o no ft h eﬁ r s te l e m e n to ft h ep a i r si nX× X\\nderived from D,a n db y D2 the marginal distribution with respect to the second\\nelement of the pairs. Similarly,S1 is the sample derived fromS by keeping only the\\nﬁrst element of each pair: S1 =\\n(\\n(x1,y1),..., (xm,y m)\\n⎡\\nand S2 the one obtained by\\nkeeping only the second element: S2 =\\n(\\n(x′\\n1,y1),..., (x′\\nm,y m)\\n⎡\\n.W ea l s od e n o t eb y\\nRD1\\nm (H) the Rademacher complexity ofH with respect to the marginal distribution\\nD1,t h a ti sRD1\\nm (H)=E [ ˆRS1 (H)], and similarly RD2\\nm (H)=E [ ˆRS2 (H) ] .C l e a r l y ,i f\\nthe distribution D is symmetric, the marginal distributionsD1 and D2 coincide and\\nRD1\\nm (H)= RD2\\nm (H).212 Ranking\\nTheorem 9.1 Margin bound for ranking\\nLet H be a set of real-valued functions. Fix ρ> 0;t h e n ,f o ra n yδ> 0,w i t h\\nprobability at least 1 − δ over the choice of a sample S of size m,e a c ho ft h e\\nfollowing holds for all h ∈ H:\\nR(h) ≤ ˆRρ(h)+ 2\\nρ\\n(\\nRD1\\nm (H)+ RD2\\nm (H)\\n⎡\\n+\\n√\\nlog 1\\nδ\\n2m (9.5)\\nR(h) ≤ ˆRρ(h)+ 2\\nρ\\n(ˆRS1 (H)+ ˆRS2 (H)\\n⎡\\n+3\\n√\\nlog 2\\nδ\\n2m . (9.6)\\nProof The proof is similar to that of theorem 4.4. Let ˜H be the family of\\nhypotheses mapping (X× X ) ×{ −1,+1} to R deﬁned by ˜H = {z =( (x, x′),y ) ↦→\\ny[h(x′) − h(x)]: h ∈ H}. Consider the family of functions ˜H = {Φρ ◦f : f ∈ ˜H}\\nderived from ˜H which are taking values in [0, 1]. By theorem 3.1, for anyδ> 0w i t h\\nprobability at least 1 − δ, for all h ∈ H,\\nE\\n[\\nΦρ(y[h(x′) − h(x)])\\n]\\n≤ ˆRρ(h)+2 Rm\\n(\\nΦρ ◦ ˜H\\n⎡\\n+\\n√\\nlog 1\\nδ\\n2m .\\nSince 1u≤0 ≤ Φρ(u) for all u ∈ R, the generalization error R(h)i sal o w e rb o u n d\\non left-hand side, R(h)=E [ 1y[h(x′)−h(x)]≤0] ≤ E\\n[\\nΦρ(y[h(x′) − h(x)])\\n]\\n,a n dw ec a n\\nwrite:\\nR(h) ≤ ˆRρ(h)+2 Rm\\n(\\nΦρ ◦ ˜H\\n⎡\\n+\\n√\\nlog 1\\nδ\\n2m .\\nExactly as in the proof of theorem 4.4, we can show that Rm\\n(\\nΦρ ◦ ˜H\\n⎡\\n≤ 1\\nρRm( ˜H)\\nusing the (1/ρ)-Lipschitzness of Φρ. Here, Rm( ˜H) can be upper bounded as follows:\\nRm( ˜H)= 1\\nm E\\nS,σ\\n[\\nsup\\nh∈H\\nm∑\\ni=1\\nσiyi(h(x′\\ni) − h(xi))\\n]\\n= 1\\nm E\\nS,σ\\n[\\nsup\\nh∈H\\nm∑\\ni=1\\nσi(h(x′\\ni) − h(xi))\\n]\\n(yiσi and σi: same distrib.)\\n≤ 1\\nm E\\nS,σ\\n[\\nsup\\nh∈H\\nm∑\\ni=1\\nσih(x′\\ni)+s u p\\nh∈H\\nm∑\\ni=1\\nσih(xi)\\n]\\n(by sub-additivity of sup)\\n=E\\nS\\n[\\nRS2 (H)+ RS1 (H)\\n]\\n(deﬁnition of S1 and S2)\\n= RD2\\nm (H)+ RD1\\nm (H) ,\\nwhich proves (9.5). The second inequality, (9.6), can be derived in the same way by\\nusing the second inequality of theorem 3.1, (3.4), instead of (3.3).9.3 Ranking with SVMs 213\\nThese bounds can be generalized to hold uniformly for all ρ> 0 at the cost of an\\nadditional term\\n√\\n(log log2(2/ρ))/m, as in theorem 4.5 and exercise 4.2. As for other\\nmargin bounds presented in previous sections, they show the conﬂict between two\\nterms: the larger the desired pairwise ranking marginρ, the smaller the middle term.\\nHowever, the ﬁrst term, the empirical pairwise ranking margin loss ˆR\\nρ, increases as\\naf u n c t i o no fρ.\\nKnown upper bounds for the Rademacher complexity of a hypothesisH, including\\nbounds in terms of VC-dimension, can be used directly to make theorem 9.1 more\\nexplicit. In particular, using theorem 9.1, we obtain immediately the following\\nmargin bound for pairwise ranking using kernel-based hypotheses.\\nCorollary 9.1 Margin bounds for ranking with kernel-based hypotheses\\nLet K : X× X→ R be a PDS kernel with r =s u px∈X K(x, x).L e tΦ: X→ H be a\\nfeature mapping associated toK and let H = {x ↦→ w · Φ(x): ∥w∥H ≤ Λ} for some\\nΛ ≥ 0.F i xρ> 0.T h e n ,f o ra n yδ> 0, the following pairwise margin bound holds\\nwith probability at least 1 − δfor any h ∈ H:\\nR(h) ≤ ˆRρ(h)+4\\n√\\nr2Λ2/ρ2\\nm +\\n√\\nlog 1\\nδ\\n2m . (9.7)\\nAs with theorem 4.4, the bound of this corollary can be generalized to hold\\nuniformly for all ρ> 0 at the cost of an additional term\\n√\\n(log log2(2/ρ))/m.T h i s\\ngeneralization bound for kernel-based hypotheses is remarkable, since it does not\\ndepend directly on the dimension of the feature space, but only on the pairwise\\nranking margin. It suggests that a small generalization error can be achieved when\\nρ/r is large (small second term) while the empirical margin loss is relatively small\\n(ﬁrst term). The latter occurs when few points are either classiﬁed incorrectly or\\ncorrectly but with margin less than ρ.\\n9.3 Ranking with SVMs\\nIn this section, we discuss an algorithm that is derived directly from the theoretical\\nguarantees just presented. The algorithm turns out to be a special instance of the\\nSVM algorithm.\\nProceeding as in section 4.4 for classiﬁcation, the guarantee of corollary 9.1 can\\nbe expressed as follows: for any δ> 0, with probability at least 1 − δ, for all\\nh ∈ H = {x ↦→ w · Φ(x): ∥w∥≤ Λ},\\nR(h) ≤ 1\\nm\\nm∑\\ni=1\\nξi +4\\n√\\nr2Λ2\\nm +\\n√\\nlog 1\\nδ\\n2m , (9.8)214 Ranking\\nwhere ξi =m a x\\n(\\n1 − yi\\n[\\nw ·\\n(\\nΦ(x′\\ni) − Φ(xi)\\n⎡]\\n, 0\\n⎡\\nfor all i ∈ [1,m], and where\\nΦ : X→ H is a feature mapping associated to a PDS kernelK. An algorithm based\\non this theoretical guarantee consists of minimizing the right-hand side of (9.8),\\nthat is minimizing an objective function with a term corresponding to the sum of\\nthe slack variables ξ\\ni, and another one minimizing ∥w∥ or equivalently ∥w∥2.I t s\\noptimization problem can thus be formulated as\\nmin\\nw,ξ\\n1\\n2 ∥w∥2 + C\\nm∑\\ni=1\\nξi (9.9)\\nsubject to: yi\\n[\\nw ·\\n(\\nΦ(x′\\ni) − Φ(xi)\\n⎡]\\n≥ 1 − ξi\\nξi ≥ 0, ∀i ∈ [1,m] .\\nThis coincides exactly with the primal optimization problem of SVMs, with a feature\\nmapping Ψ : X× X → H deﬁned by Ψ(x, x′)= Φ(x′) −Φ(x) for all (x, x′) ∈X× X ,\\nand with a hypothesis set of functions of the form ( x, x′) ↦→ w · Ψ(x, x′). Thus,\\nclearly, all the properties already presented for SVMs apply in this instance. In\\nparticular, the algorithm can beneﬁt from the use of PDS kernels. Problem (9.9)\\nadmits an equivalent dual that can be expressed in terms of the kernel matrix K\\n′\\ndeﬁned by\\nK′\\nij = Ψ(xi,x ′\\ni) ·Ψ(xj,x ′\\nj)= K(xi,xj)+ K(x′\\ni,x ′\\nj\\n) − K(x′\\ni,xj) − K(xi,x ′\\nj), (9.10)\\nfor all i, j ∈ [1,m]. This algorithm can provide an eﬀective solution for pairwise\\nranking in practice. The algorithm can also be used and extended to the case where\\nthe labels are in {−1, 0, +1}. The next section presents an alternative algorithm for\\nranking in the score-based setting.\\n9.4 RankBoost\\nThis section presents a boosting algorithm for pairwise ranking,RankBoost,s i m i l a r\\nto the AdaBoost algorithm for binary classiﬁcation. RankBoost is based on ideas\\nanalogous to those discussed for classiﬁcation: it consists of combining diﬀerent\\nbase rankers to create a more accurate predictor. The base rankers are hypotheses\\nreturned by a weak learning algorithm for ranking. As for classiﬁcation, these\\nbase hypotheses must satisfy a minimal accuracy condition that will be described\\nprecisely later.\\nLet H denote the hypothesis set from which the base rankers are selected.\\nAlgorithm 9.1 gives the pseudocode of the RankBoost algorithm whenH is a set of9.4 RankBoost 215\\nRankBoost(S =( (x1,x ′\\n1,y1) ..., (xm,x ′\\nm,y m)))\\n1 for i ← 1 to m do\\n2 D1(i) ← 1\\nm\\n3 for t ← 1 to T do\\n4 ht ← base ranker in H with smallest ϵ−\\nt − ϵ+\\nt = − E\\ni∼Dt\\n[\\nyi\\n(\\nht(x′\\ni) − ht(xi)\\n⎡]\\n5 αt ← 1\\n2 log ϵ+\\nt\\nϵ−\\nt\\n6 Zt ← ϵ0\\nt +2 [ϵ+\\nt ϵ−\\nt\\n]\\n1\\n2 ⊿ normalization factor\\n7 for i ← 1 to m do\\n8 Dt+1(i) ←\\nDt(i)e x p\\n[\\n−αtyi\\n(\\nht(x′\\ni)−ht(xi)\\n⎡]\\nZt\\n9 g ← ∑T\\nt=1 αtht\\n10 return g\\nFigure 9.1 RankBoost algorithm for H ⊆{ 0, 1}X .\\nfunctions mapping from X to {0, 1}. For any s ∈{ −1, 0, +1},w ed e ﬁ n eϵs\\nt by\\nϵs\\nt =\\nm∑\\ni=1\\nDt(i)1yi(ht(x′\\ni)−ht(xi))=s =E\\ni∼Dt\\n[1yi(ht(x′\\ni)−ht(xi))=s], (9.11)\\nand simplify the notation ϵ+1\\nt into ϵ+\\nt and similarly write ϵ−\\nt instead of ϵ−1\\nt .W i t h\\nthese deﬁnitions, clearly the following equality holds: ϵ0\\nt + ϵ+\\nt + ϵ−\\nt =1 .\\nThe algorithm takes as input a labeled sampleS =\\n(\\n(x1,x ′\\n1,y1),..., (xm,x ′\\nm,y m)\\n⎡\\nwith elements in X× X× { − 1,0, +1}, and maintains a distribution over the subset\\nof the indices i ∈{ 1,...,m } for which yi ̸= 0. To simplify the presentation, we will\\nassume that yi ̸=0f o ra l l i ∈{ 1,...,m } and consider distributions deﬁned over\\n{1,...,m }. This can be guaranteed by simply ﬁrst removing from the sample the\\npairs labeled with zero.\\nInitially (lines 1–2), the distribution is uniform (D1) .A te a c hr o u n do fb o o s t i n g ,\\nthat is at each iteration t ∈ [1,T ] of the loop 3–8, a new base ranker ht ∈ H is\\nselected with the smallest diﬀerenceϵ−\\nt − ϵ+\\nt , that is one with the smallest pairwise\\nmisranking error and largest correct pairwise ranking accuracy for the distribution\\nDt:\\nht ∈ argmin\\nh∈H\\n{\\n− E\\ni∼Dt\\n[\\nyi\\n(\\nh(x′\\ni) − h(xi)\\n⎡]}\\n.\\nNote that ϵ−\\nt − ϵ+\\nt = ϵ−\\nt − (1 − ϵ−\\nt − ϵ0\\nt )=2 ϵ−\\nt + ϵ0\\nt − 1. Thus, ﬁnding the smallest216 Ranking\\ndiﬀerence ϵ−\\nt −ϵ+\\nt is equivalent to seeking the smallest 2ϵ−\\nt +ϵ0\\nt , which itself coincides\\nwith seeking the smallest ϵ−\\nt when ϵ0\\nt =0 . Zt is simply a normalization factor to\\nensure that the weights Dt+1(i) sum to one. RankBoost relies on the assumption\\nthat at each roundt ∈ [1,T ], for the hypothesisht found, the inequalityϵ+\\nt − ϵ−\\nt > 0\\nholds; thus, the probability mass of the pairs correctly ranked byht (ignoring pairs\\nwith label zero) is larger than that of misranked pairs. We denote byγt the edge of\\nthe base ranker ht: γt = ϵ+\\nt −ϵ−\\nt\\n2 .\\nThe precise reason for the deﬁnition of the coeﬃcient αt (line 5) will become\\nclear later. For now, observe that if ϵ+\\nt − ϵ−\\nt > 0, then ϵ+\\nt /ϵ−\\nt\\n> 1a n d αt > 0.\\nThus, the new distribution Dt+1 is deﬁned from Dt by increasing the weight on\\ni if the pair ( xi,x ′\\ni)i sm i s r a n k e d(yi(ht(x′\\ni) − ht(xi) < 0), and, on the contrary,\\ndecreasing it if ( xi,x ′\\ni) is ranked correctly ( yi(ht(x′\\ni) − ht(xi) > 0). The relative\\nweight is unchanged for a pair with ht(x′\\ni) − ht(xi) = 0. This distribution update\\nhas the eﬀect of focusing more on misranked points at the next round of boosting.\\nAfter T rounds of boosting, the hypothesis returned by RankBoost is g,w h i c hi s\\na linear combination of the base classiﬁers ht.T h ew e i g h tαt assigned to ht in that\\nsum is a logarithmic function of the ratio of ϵ+\\nt and ϵ−\\nt . Thus, more accurate base\\nrankers are assigned a larger weight in that sum.\\nFor anyt ∈ [1,T ], we will denote bygt the linear combination of the base rankers\\nafter t rounds of boosting: gt = ∑t\\ns=1 αtht.I np a r t i c u l a r ,w eh a v egT = g.T h e\\ndistribution Dt+1 can be expressed in terms ofgt and the normalization factors Zs,\\ns ∈ [1,t ], as follows:\\n∀i ∈ [1,m],D t+1(i)= e−yi(gt(x′\\ni))−gt(xi))\\nm ∏t\\ns=1 Zs\\n. (9.12)\\nWe will make use of this identity several times in the proofs of the following sections.\\nIt can be shown straightforwardly by repeatedly expanding the deﬁnition of the\\ndistribution over the point x\\ni:\\nDt+1(i)= Dt(i)e−αtyi(ht(x′\\ni)−ht(xi))\\nZt\\n= Dt−1(i)e−αt− 1yi(ht− 1(x′\\ni)−ht− 1(xi))e−αtyi(ht(x′\\ni)−ht(xi))\\nZt−1Zt\\n= e−yi\\nPt\\ns=1 αs(hs(x′\\ni)−hs(xi))\\nm ∏t\\ns=1 Zs\\n.\\n9.4.1 Bound on the empirical error\\nWe ﬁrst show that the empirical error of RankBoost decreases exponentially fast\\nas a function of the number of rounds of boosting when the edge γt of each base9.4 RankBoost 217\\nranker ht is lower bounded by some positive value γ> 0.\\nTheorem 9.2\\nThe empirical error of the hypothesish: X→ { 0, 1} returned by RankBoost veriﬁes:\\nˆR(h) ≤ exp\\n[\\n− 2\\nT∑\\nt=1\\n(ϵ+\\nt − ϵ−\\nt\\n2\\n⎡2]\\n. (9.13)\\nFurthermore, if there exists γ such that for all t ∈ [1,T ], 0 <γ ≤ ϵ+\\nt −ϵ−\\nt\\n2 ,t h e n\\nˆR(h) ≤ exp(−2γ2T) . (9.14)\\nProof Using the general inequality 1 u≤0 ≤ exp(−u) valid for all u ∈ R and\\nidentity 9.12, we can write:\\nˆR(h)= 1\\nm\\nm∑\\ni=1\\n1yi(g(x′\\ni)−g(xi))≤0 ≤ 1\\nm\\nm∑\\ni=1\\ne−yi(g(x′\\ni)−g(xi))\\n≤ 1\\nm\\nm∑\\ni=1\\n[\\nm\\nT∏\\nt=1\\nZt\\n]\\nDT+1(i)=\\nT∏\\nt=1\\nZt.\\nBy the deﬁnition of normalization factor, for all t ∈ [1,T ], we have Zt =∑m\\ni=1 Dt(i)e−αtyi(ht(x′\\ni)−ht(xi)).B yg r o u p i n gt o g e t h e rt h ei n d i c e si for which\\nyi(ht(x′\\ni) − ht(xi)) takes the values in +1, −1, or 0, Zt c a nb er e w r i t t e na s\\nZt = ϵ+\\nt e−αt + ϵ−\\nt eαt + ϵ0\\nt = ϵ+\\nt\\n√\\nϵ−\\nt\\nϵ+\\nt\\n+ ϵ−\\nt\\n√\\nϵ+\\nt\\nϵ−\\nt\\n+ ϵ0\\nt =2\\n√\\nϵ+\\nt ϵ−\\nt\\n+ ϵ0\\nt .\\nSince ϵ+\\nt =1 − ϵ−\\nt − ϵ0\\nt ,w eh a v e\\n4ϵ+\\nt ϵ−\\nt\\n=( ϵ+\\nt + ϵ−\\nt )2 − (ϵ+\\nt − ϵ−\\nt )2 =( 1 − ϵ0\\nt )2 − (ϵ+\\nt − ϵ−\\nt )2.\\nThus, assuming that ϵ0\\nt < 1, Zt can be upper bounded as follows:\\nZt =\\n√\\n(1 − ϵ0\\nt )2 − (ϵ+\\nt − ϵ−\\nt )2 + ϵ0\\nt\\n=( 1 − ϵ0\\nt )\\n√\\n1 − (ϵ+\\nt − ϵ−\\nt )2\\n(1 − ϵ0\\nt )2 + ϵ0\\nt\\n≤ (1 − ϵ0\\nt )e x p\\n(\\n− (ϵ+\\nt − ϵ−\\nt )2\\n2(1 − ϵ0\\nt )2\\n⎡\\n+ ϵ0\\nt\\n≤ exp\\n(\\n− (ϵ+\\nt − ϵ−\\nt )2\\n2(1 − ϵ0\\nt )\\n⎡\\n≤ exp\\n(\\n− (ϵ+\\nt − ϵ−\\nt )2\\n2\\n⎡\\n≤ exp\\n(\\n−2[(ϵ+\\nt − ϵ−\\nt )/2]2⎡\\n,\\nwhere we used for the ﬁrst inequality the identity 1 − x ≤ e−x valid for all x ∈ R218 Ranking\\nand for the second inequality the convexity of the exponential function and the fact\\nthat 0 < 1 − ϵ0\\nt ≤ 1. This upper bound on Zt also trivially holds when ϵ0\\nt =1s i n c e\\nin that case ϵ+\\nt = ϵ−\\nt = 0. This concludes the proof.\\nA sc a nb es e e nf r o mt h ep r o o fo ft h et h e o r e m ,t h ew e a kr a n k i n ga s s u m p t i o n\\nγ ≤ ϵ+\\nt −ϵ−\\nt\\n2 with γ> 0 can be replaced with the somewhat weaker requirement\\nγ ≤ ϵ+\\nt −ϵ−\\nt\\n2\\n√\\n1−ϵ0\\nt\\n,w i t hϵ0\\nt ̸=1 ,w h i c hc a nb er e w r i t t e na sγ ≤ 1\\n2\\nϵ+\\nt −ϵ−\\nt√\\nϵ+\\nt\\n+ϵ−\\nt\\n,w i t hϵ+\\nt +ϵ−\\nt ̸=0 ,\\nwhere the quantity ϵ+\\nt −ϵ−\\nt√\\nϵ+\\nt\\n+ϵ−\\nt\\ncan be interpreted as a (normalized) relative diﬀerence\\nbetween ϵ+\\nt and ϵ−\\nt .\\nThe proof of the theorem also shows that the coeﬃcientαt is selected to minimize\\nZt. Thus, overall, these coeﬃcients are chosen to minimize the upper bound on\\nthe empirical error ∏T\\nt=1 Zt, as for AdaBoost. The RankBoost algorithm can be\\ngeneralized in several ways:\\ninstead of a hypothesis with minimal diﬀerenceϵ−\\nt − ϵ+\\nt , ht can be more generally\\na base ranker returned by a weak ranking algorithm trained on Dt with ϵ+\\nt >ϵ −\\nt\\n;\\nthe range of the base rankers could be [0, +1], or more generallyR. The coeﬃcients\\nαt can then be diﬀerent and may not even admit a closed form. However, in general,\\nthey are chosen to minimize the upper bound ∏T\\nt=1 Zt on the empirical error.\\n9.4.2 Relationship with coordinate descent\\nRankBoost coincides with the application of the coordinate descent technique\\nto a convex and diﬀerentiable objective function F deﬁned for all samples S =(\\n(x\\n1,x ′\\n1,y1),..., (xm,x ′\\nm,y m)\\n⎡\\n∈X×X×{ − 1,0, +1} and α =( α1,...,α n) ∈ Rn,\\nn ≥ 1b y\\nF(α)=\\nm∑\\ni=1\\ne−yi[gn(x′\\ni)−gn(xi)] =\\nm∑\\ni=1\\ne−yi\\nPn\\nt=1 αt[ht(x′\\ni)−ht(xi)] , (9.15)\\nwhere gn = ∑n\\nt=1 αtht. This loss function is a convex upper bound on the zero-one\\npairwise loss function α ↦→ ∑m\\ni=1 1yi[gn(x′\\ni)−gn(xi)]≤0, which is not convex. Let et\\ndenote the unit vector corresponding to thetth coordinate inRn and letαt−1 denote\\nthe vector based on the (t − 1) ﬁrst coeﬃcients, i.e. αt−1 =( α1,...,α t−1, 0,..., 0)⊤\\nif t − 1 > 0, αt−1 = 0 otherwise. At each iteration t ≥ 1, the direction et selected\\nby coordinate descent is the one minimizing the directional derivative:\\net =a r g m i n\\nt\\ndF(αt−1 + ηet)\\ndη\\n⏐⏐⏐\\n⏐\\nη=0\\n.9.4 RankBoost 219\\nSince F(αt−1 + ηet)= ∑m\\ni=1 e−yi\\nPt− 1\\ns=1 αs(hs(x′\\ni)−hs(xi))−ηyi(ht(x′\\ni)−ht(xi)), the direc-\\ntional derivative along et can be expressed as follows:\\ndF(αt−1 + ηet)\\ndη\\n⏐⏐\\n⏐\\n⏐\\nη=0\\n= −\\nm∑\\ni=1\\nyi(ht(x′\\ni) − ht(xi)) exp\\n[\\n− yi\\nt−1∑\\ns=1\\nαs(hs(x′\\ni) − hs(xi))\\n]\\n= −\\nm∑\\ni=1\\nyi(ht(x′\\ni) − ht(xi))Dt(i)\\n[\\nm\\nt−1∏\\ns=1\\nZs\\n]\\n= −\\n[ m∑\\ni=1\\nDt(i)1yi(ht(x′\\ni)−ht(xi))=+1 −\\nm∑\\ni=1\\nDt(i)1yi(ht(x′\\ni)−ht(xi))=−1\\n][\\nm\\nt−1∏\\ns=1\\nZs\\n]\\n= −[ϵ+\\nt − ϵ−\\nt ]\\n[\\nm\\nt−1∏\\ns=1\\nZs\\n]\\n.\\nThe ﬁrst equality holds by diﬀerentiation and evaluation at η = 0 and the second\\none follows from (9.12). In view of the ﬁnal equality, since m ∏t−1\\ns=1 Zs is ﬁxed,\\nthe direction et selected by coordinate descent is the one minimizing ϵt,w h i c h\\ncorresponds exactly to the base ranker ht selected by RankBoost.\\nThe step size η is identiﬁed by setting the derivative to zero in order to minimize\\nthe function in the chosen directionet. Thus, using identity 9.12 and the deﬁnition\\nof ϵt,w ec a nw r i t e :\\ndF(αt−1 + ηet)\\ndη =0\\n⇔−\\nm∑\\ni=1\\nyi(ht(x′\\ni) − ht(xi))e−yi\\nPt− 1\\ns=1 αs(hs(x′\\ni)−hs(xi))e−ηyi(ht(x′\\ni)−ht(xi)) =0\\n⇔−\\nm∑\\ni=1\\nyi(ht(x′\\ni) − ht(xi))Dt(i)\\n[\\nm\\nt−1∏\\ns=1\\nZs\\n]\\ne−ηyi(ht(x′\\ni)−ht(xi)) =0\\n⇔−\\nm∑\\ni=1\\nyi(ht(x′\\ni) − ht(xi))Dt(i)e−ηyi(ht(x′\\ni)−ht(xi)) =0\\n⇔− [ϵ+\\nt e−η − ϵ−\\nt eη]=0\\n⇔ η= 1\\n2 log ϵ+\\nt\\nϵ−\\nt\\n.\\nThis proves that the step size chosen by coordinate descent matches the base\\nranker weight αt of RankBoost. Thus, coordinate descent applied to F precisely\\ncoincides with the RankBoost algorithm. As in the classiﬁcation case, other convex\\nloss functions upper bounding the zero-one pairwise misranking loss can be used.220 Ranking\\nIn particular, the following objective function based on the logistic loss can be\\nused: α ↦→ ∑m\\ni=1 log(1 + e−yi[gn(x′\\ni)−gn(xi)]) to derive an alternative boosting-type\\nalgorithm.\\n9.4.3 Margin bound for ensemble methods in ranking\\nTo simplify the presentation, we will assume for the results of this section, as\\nin section 9.2, that the pairwise labels are in {−1, +1}. By theorem 6.2, the\\nempirical Rademacher complexity of the convex hull conv( H)e q u a l st h a to fH.\\nThus, theorem 9.1 immediately implies the following guarantee for ensembles of\\nhypotheses in ranking.\\nCorollary 9.2\\nLet H be a set of real-valued functions. Fix ρ> 0;t h e n ,f o ra n yδ> 0,w i t h\\nprobability at least 1 − δ over the choice of a sample S of size m,e a c ho ft h e\\nfollowing ranking guarantees holds for all h ∈ conv(H):\\nR(h) ≤ ˆR\\nρ(h)+ 2\\nρ\\n(\\nRD1\\nm (H)+ RD2\\nm (H)\\n⎡\\n+\\n√\\nlog 1\\nδ\\n2m (9.16)\\nR(h) ≤ ˆRρ(h)+ 2\\nρ\\n(ˆRS1 (H)+ ˆRS2 (H)\\n⎡\\n+3\\n√\\nlog 2\\nδ\\n2m . (9.17)\\nFor RankBoost, these bounds apply to g/∥α∥1,w h e r eg is the hypothesis returned\\nby the algorithm. Since g and g/∥α∥1 induce the same ordering of the points, for\\nany δ> 0, the following holds with probability at least 1 − δ:\\nR(g) ≤ ˆRρ(g/∥α∥1)+ 2\\nρ\\n(\\nRD1\\nm (H)+ RD2\\nm (H)\\n⎡\\n+\\n√\\nlog 1\\nδ\\n2m (9.18)\\nRemarkably, the number of rounds of boosting T does not appear in this bound.\\nThe bound depends only on the margin ρ, the sample size m, and the Rademacher\\ncomplexity of the family of base classiﬁers H. Thus, the bound guarantees an eﬀec-\\ntive generalization if the pairwise margin loss ˆRρ(g/∥α∥1) is small for a relatively\\nlarge ρ. A bound similar to that of theorem 6.3 for AdaBoost can be derived for the\\nempirical pairwise ranking margin loss of RankBoost (see exercise 9.3) and similar\\ncomments on that result apply here.\\nThese results provide a margin-based analysis in support of ensemble methods\\nin ranking and RankBoost in particular. As in the case of AdaBoost, however,\\nRankBoost in general does not achieve a maximum margin. But, in practice, it has\\nbeen observed to obtain excellent pairwise ranking performances.9.5 Bipartite ranking 221\\n9.5 Bipartite ranking\\nThis section examines an important ranking scenario within the score-based setting,\\nthe bipartite ranking problem.I nt h i ss c e n a r i o ,t h es e to fp o i n t sX is partitioned\\ninto two classes: X+ the class of positive points, and X− that of negative ones. The\\nproblem consists of ranking positive points higher than negative ones. For example,\\nfor a ﬁxed search engine query, the task consists of ranking relevant (positive)\\ndocuments higher than irrelevant (negative) ones.\\nThe bipartite problem could be treated in the way already discussed in the\\nprevious sections with exactly the same theory and algorithms. However, the setup\\ntypically adopted for this problem is diﬀerent: instead of assuming that the learner\\nreceives a sample of random pairs, here pairs of positive and negative elements, it\\nis assumed that he receives a sample of positive points from some distribution and\\na sample of negative points from another. This leads to the set of all pairs made of\\na positive point of the ﬁrst sample and a negative point of the second.\\nMore formally, the learner receives a sample S\\n+ =( x′\\n1,...,x ′\\nm) drawn i.i.d.\\naccording to some distribution D+ over X+, and a sample S− =( x1,...,x n)d r a w n\\ni.i.d. according to some distribution D− over X− .1 Given a hypothesis set H of\\nfunctions mapping X to R, the learning problem consists of selecting a hypothesis\\nh ∈ H with small expected bipartite misranking or generalization error R(h):\\nR(h)= P r\\nx∼D−\\nx′∼D+\\n[h(x′) <h (x)]. (9.19)\\nThe empirical pairwise misranking or empirical error of h is denoted by ˆR(h)a n d\\ndeﬁned by\\nˆR(h)= 1\\nmn\\nm∑\\ni=1\\nn∑\\nj=1\\n1h(x′\\ni)<h(xj) . (9.20)\\nNote that while the bipartite ranking problem bears some similarity with binary\\nclassiﬁcation, in particular, the presence of two classes, they are distinct problems,\\nsince their objectives and measures of success clearly diﬀer.\\n1. This two-distribution formulation also avoids a potential dependency issue that can\\narise for some modeling of the problem: if pairs are drawn according to some distribution\\nD over X\\n− ×X + and the learner makes use of this information to augment his training\\nsample, then the resulting sample is in general not i.i.d. This is because if ( x1,x ′\\n1)a n d\\n(x2,x ′\\n2) are in the sample, then so are the pairs (x1,x ′\\n2) and (x2,x ′\\n1) and thus the pairs are\\nnot independent. However, without sample augmentation, the points are i.i.d., and this\\nissue does not arise.222 Ranking\\nBy the deﬁnition of the formulation of the bipartite ranking just presented, the\\nlearning algorithm must typically deal withmn pairs. For example, the application\\nof SVMs to ranking in this scenario leads to an optimization withmn slack variables\\nor constraints. With just a thousand positive and a thousand negative points,\\none million pairs would need to be considered. This can lead to a prohibitive\\ncomputational cost for some learning algorithms. The next section shows that\\nRankBoost admits an eﬃcient implementation in the bipartite scenario.\\n9.5.1 Boosting in bipartite ranking\\nThis section shows the eﬃciency of RankBoost in the bipartite scenario and dis-\\ncusses the connection between AdaBoost and RankBoost in this context.\\nThe key property of RankBoost leading to an eﬃcient algorithm in the bipartite\\nsetting is the fact that its objective function is based on the exponential function.\\nAs a result, it can be decomposed into the product of two functions, one depending\\non only the positive and the other on only the negative points. Similarly, the\\ndistribution D\\nt maintained by the algorithm can be factored as the product of\\ntwo distributions D+\\nt and D−\\nt . This is clear for the uniform distribution D1 at the\\nﬁrst round as for any i ∈ [1,m]a n dj ∈ [1,n ], D1(i, j)=1 /(mn)= D+\\n1 (i)D−\\n1 (j)\\nwith D+\\n1 (i)=1 /m and D−\\n1 (j)=1 /n. This property is recursively preserved since,\\nin view of the following, the decomposition of Dt implies that of Dt+1 for any\\nt ∈ [1,T ]. For any i ∈ [1,m]a n d j ∈ [1,n ], by deﬁnition of the update, we can\\nwrite:\\nDt+1(i, j)= Dt(i, j)e−αt[ht(x′\\ni)−ht(xj)]\\nZt\\n= D+\\nt (i)e−αtht(x′\\ni)\\nZt,+\\nD−\\nt (j)eαtht(xj)\\nZt,−\\n,\\nsince the normalization factor Zt can also be decomposed as Zt = Z−\\nt Z+\\nt ,w i t h\\nZ+\\nt = ∑m\\ni=1 D+\\nt (i)e−αtht(x′\\ni) and Z−\\nt = ∑n\\nj=1 D−\\nt (j)eαtht(xj).F u r t h e r m o r e ,t h e\\npairwise misranking of a hypothesis h ∈ H based on the distribution Dt used to\\ndetermine ht can also be computed as the diﬀerence of two quantities, one depending\\nonly on positive points, the other only on negative ones:\\nE\\n(i,j)∼Dt\\n[h(x′\\ni) − h(xj)] = E\\ni∼D+\\nt\\n[E\\nj∼D−\\nt\\n[h(x′\\ni) − h(xj)]] = E\\ni∼D+\\nt\\n[h(x′\\ni)] − E\\nj∼D−\\nt\\n[h(xj)].\\nThus, the time and space complexity of RankBoost depends only on the total\\nnumber of pointsm+n and not the number of pairsmn. More speciﬁcally, ignoring\\nthe call to the weak ranker or the cost of determining ht, the time and space\\ncomplexity of each round is linear, that is, in O(m + n). Furthermore, the cost of\\ndetermining ht depends only on O(m + n) and not on O(mn). Figure 9.2 gives the\\npseudocode of the algorithm adapted to the bipartite scenario.\\nIn the bipartite scenario, a connection can be made between the classiﬁcation algo-9.5 Bipartite ranking 223\\nBipartiteRankBoost(S =( x′\\n1,...,x ′\\nm,x1,...,x n))\\n1 for j ← 1 to m do\\n2 D+\\n1 (j) ← 1\\nm\\n3 for i ← 1 to n do\\n4 D−\\n1 (i) ← 1\\nn\\n5 for t ← 1 to T do\\n6 ht ← base ranker in H with smallest ϵ−\\nt − ϵ+\\nt =E\\nj∼D−\\nt\\n[h(xj)] − E\\ni∼D+\\nt\\n[h(x′\\ni)]\\n7 αt ← 1\\n2 log ϵ+\\nt\\nϵ−\\nt\\n8 Z+\\nt ← 1 − ϵ+\\nt +\\n√\\nϵ+\\nt ϵ−\\nt\\n9 for i ← 1 to m do\\n10 D+\\nt+1(i) ←\\nD+\\nt (i)e x p\\n[\\n−αtht(x′\\ni)\\n]\\nZ+\\nt\\n11 Z−\\nt ← 1 − ϵ−\\nt +\\n√\\nϵ+\\nt ϵ−\\nt\\n12 for j ← 1 to n do\\n13 D−\\nt+1(j) ←\\nD−\\nt (j)e x p\\n[\\n+αtht(xj)\\n]\\nZt\\n14 g ← ∑T\\nt=1 αtht\\n15 return g\\nFigure 9.2 Pseudocode of RankBoost in a bipartite setting, with H ⊆{ 0, 1}X ,\\nϵ+\\nt =E i∼D+\\nt\\n[h(x′\\ni)] and ϵ−\\nt =E j∼D−\\nt\\n[h(xj)].\\nrithm AdaBoost and the ranking algorithm RankBoost. In particular, the objective\\nfunction of RankBoost can be expressed as follows for any α =( α1,...,α T ) ∈ RT ,\\nT ≥ 1:\\nFRankBoost(α)=\\nm∑\\nj=1\\nn∑\\ni=1\\nexp(−[g(x′\\ni) − g(xj)])\\n=\\n( m∑\\ni=1\\ne− PT\\nt=1 αtht(x′\\ni)\\n⎡( n∑\\nj=1\\ne+ PT\\nt=1 αtht(xj)\\n⎡\\n= F+(α)F− (α),\\nwhere F+ denotes the function deﬁned by the sum over the positive points and F−\\nthe function deﬁned over the negative points. The objective function of AdaBoost224 Ranking\\ncan be deﬁned in terms of these same two functions as follows:\\nFAdaBoost(α)=\\nm∑\\ni=1\\nexp(−y′\\nig(x′\\ni)) +\\nn∑\\nj=1\\nexp(−yjg(xj))\\n=\\nm∑\\ni=1\\ne− PT\\nt=1 αtht(x′\\ni) +\\nn∑\\nj=1\\ne+ PT\\nt=1 αtht(xj)\\n= F+(α)+ F− (α).\\nNote that the gradient of the objective function of RankBoost can be expressed in\\nterms of AdaBoost as follows:\\n∇α FRankBoost(α)= F− (α)∇α F+(α)+ F+(α)∇α F− (α) (9.21)\\n= F− (α)(∇α F+(α)+ ∇α F− (α)) + (F+(α) − F− (α))∇α F− (α)\\n= F− (α)∇α FAdaBoost(α)+( F+(α) − F− (α))∇α F− (α).\\nIf α is a minimizer ofFAdaBoost,t h e n∇α FAdaBoost(α)=0a n di tc a nb es h o w nt h a t\\nthe equality F+(α) − F− (α)=0a l s oh o l d sf o rα, provided that the family of base\\nhypotheses H used for AdaBoost includes the constant hypothesish0 : x ↦→ 1, which\\noften is the case in practice. Then, by (9.21), this implies that∇α FRankBoost(α)=0\\nand therefore that α is also a minimizer of the convex function FRankBoost.I n\\ngeneral, FAdaBoost does not admit a minimizer. Nevertheless, it can be shown that\\nif limk→∞ FAdaBoost(αk)=i n f α FAdaBoost(α) for some sequence ( αk)k∈N, then,\\nunder the same assumption on the use of a constant base hypothesis and for\\na non-linearly separable dataset, the following holds: lim k→∞ FRankBoost(αk)=\\ninfα FRankBoost(α).\\nThe connections between AdaBoost and RankBoost just mentioned suggest that\\nAdaBoost could achieve a good ranking performance as well. This is often observed\\nempirically, a fact that brings strong support to the use of AdaBoost both as a\\nclassiﬁer and a ranking algorithm. Nevertheless, RankBoost may converge faster\\nand achieve a good ranking faster than AdaBoost.\\n9.5.2 Area under the ROC curve\\nThe performance of a bipartite ranking algorithm is typically reported in terms of\\nthe area under the receiver operating characteristic (ROC) curve,o rt h earea under\\nthe curve (AUC) for short.\\nLet U be a test sample used to evaluate the performance of h (or a training\\nsample) with m positive points z\\n′\\n1,...,z ′\\nm and n negative points z1,...,z n. For any\\nh ∈ H,l e t ˆR(h, U) denote the average pairwise misranking of h over U. Then, the\\nAUC of h for the sample U is precisely 1 − ˆR(h, U), that is, its average pairwise9.5 Bipartite ranking 225\\nFalse positive rate\\nTrue positive rate\\nAUC\\n.2\\n.4\\n0 .2 .4 .6 .8 1\\n.6\\n.8\\n1\\n0\\nFigure 9.3 The AUC (area under the ROC curve) is a measure of the performance\\nof a bipartite ranking.\\nranking accuracy on U:\\nAUC(h, U)= 1\\nmn\\nm∑\\ni=1\\nn∑\\nj=1\\n1h(z′\\ni)≥h(zj) =P r\\nz∼ bD−\\nU\\nz′∼ bD+\\nU\\n[h(z′) ≥ h(z)].\\nHere, ˆD+\\nU denotes the empirical distribution corresponding to the positive points in\\nU and ˆD+\\nU the empirical distribution corresponding to the negative ones. AUC(h, U)\\nis thus an empirical estimate of the pairwise ranking accuracy based on the sample\\nU, and by deﬁnition it is in [0, 1]. Higher AUC values correspond to a better ranking\\nperformance. In particular, an AUC of one indicates that the points ofU are ranked\\nperfectly using h. AUC(h, U) can be computed in linear time from a sorted array\\ncontaining them+n elements h(z\\n′\\ni)a n dh(zj), fori ∈ [1,m]a n dj ∈ [1,n ]. Assuming\\nthat the array is sorted in increasing order (with a positive point placed higher than a\\nnegative one if they both have the same scores) the total number of correctly ranked\\npairs r can be computed as follows. Starting with r =0 ,t h ea r r a yi si n s p e c t e di n\\nincreasing order of the indices while maintaining at any time the number of negative\\npoints seen n and incrementing the current value of r with n whenever a positive\\npoint is found. After full inspection of the array, the AUC is given byr/(mn). Thus,\\nassuming that a comparison-based sorting algorithm is used, the complexity of the\\nc o m p u t a t i o no ft h eA U Ci si nO((m + n) log(m + n)).\\nAs indicated by its name, the AUC coincides with the area under the ROC curve\\n(ﬁgure 9.3). An ROC curve plots the true positive rate, that is, the percentage of\\npositive points correctly predicted as positive as a function of the false positive\\nrate, that is, the percentage of negative points incorrectly predicted as positive.\\nFigure 9.4 illustrates the deﬁnition and construction of an ROC curve.226 Ranking\\nFalse positive rate\\nTrue positive rate\\n.2\\n.4\\n0 .2 .4 .6 .8 1\\n.6\\n.8\\n1\\n0 h(x3) h(x1)h(x14) h(x23)h(x5)\\nθ +-\\nsorted scores\\nFigure 9.4 An example ROC curve and illustrated threshold. Varying the value\\nof θ from one extreme to the other generates points on the curve.\\nPoints are generated along the curve by varying a threshold value θ as in the\\nright panel of ﬁgure 9.4, from higher values to lower ones. The threshold is used to\\ndetermine the label of any point x (positive or negative) based on sgn( h(x) − θ).\\nAt one extreme, all points are predicted as negative; thus, the false positive rate is\\nz e r o ,b u tt h et r u ep o s i t i v er a t ei sz e r oa sw e l l .T h i sg i v e st h eﬁ r s tp o i n t( 0, 0) of\\nthe plot. At the other extreme, all points are predicted as positive; thus, both the\\ntrue and the false positive rates are equal to one, which gives the point (1 , 1). In\\nt h ei d e a lc a s e ,a sa l r e a d yd i s c u s s e d ,t h eA U Cv a l u ei so n e ,a n d ,w i t ht h ee x c e p t i o n\\nof (0,0), the curve coincides with a horizontal line reaching (1, 1).\\n9.6 Preference-based setting\\nThis section examines a diﬀerent setting for the problem of learning to rank: the\\npreference-based setting.I nt h i ss e t t i n g ,t h eo b j e c t i v ei st or a n ka sa c c u r a t e l ya s\\npossible any test subsetX ⊆X , typically a ﬁnite set that we refer to as aﬁnite query\\nsubset. This is close to the query-based scenario of search engines or information\\nextraction systems and the terminology stems from the fact that X could be a\\nset of items needed to rank in response to a particular query. The advantage of\\nthis setting over the score-based setting is that here the learning algorithm is not\\nrequired to return a linear ordering of all points of X, which may be impossible\\nto achieve faultlessly in accordance with a general possibly non-transitive pairwise\\npreference labeling. Supplying a correct linear ordering for a query subset is more\\nlikely to be achievable exactly or at least with a better approximation.\\nThe preference-based setting consists of two stages. In the ﬁrst stage, a sample of\\nlabeled pairs S, exactly as in the score-based setting, is used to learn a preference\\nfunction h : X× X ↦→ [0,1], that is, a function that assigns a higher value to a\\npair (u, v)w h e nu is preferred to v or is to be ranked higher than v, and smaller9.6 Preference-based setting 227\\nvalues in the opposite case. This preference function can be obtained as the output\\nof a standard classiﬁcation algorithm trained on S. A crucial diﬀerence with the\\nscore-based setting is that, in general, the preference function h is not required to\\ninduce a linear ordering. The relation it induces may be non-transitive; thus, we\\nmay have, for example, h(u, v)= h(v,w )= h(w,u ) = 1 for three distinct points u,\\nv,a n dw.\\nIn the second stage, given a query subsetX ∈X , the preference functionh is used\\nto determine a ranking of X.H o wc a nh be used to generate an accurate ranking?\\nThis will be the main focus of this section. The computational complexity of the\\nalgorithm determining the ranking is also crucial. Here, we will measure its running\\ntime complexity in terms of the number of calls to h.\\nWhen the preference function is obtained as the output of a binary classiﬁcation\\nalgorithm, the preference-based setting can be viewed as a reduction of ranking to\\nclassiﬁcation: the second stage speciﬁes how a ranking is obtained from a classiﬁer’s\\noutput.\\n9.6.1 Second-stage ranking problem\\nThe ranking problem of the second stage is modeled as follows. We assume that a\\npreference function h is given. From the point of view of this stage, the way the\\nfunction h has been determined is immaterial, it can be viewed as a black box. As\\nalready discussed, h is not assumed to be transitive. But, we will assume that it is\\npairwise consistent,t h a ti sh(u, v)+ h(v,u )=1 ,f o ra l lu, v ∈X .\\nLet D be an unknown distribution according to which pairs ( X,σ\\n∗)a r ed r a w n\\nwhere X ⊆X is a query subset and σ∗ a target ranking or permutation of X,\\nthat is, a bijective function from X to {1,..., |X|}. Thus, we consider a stochastic\\nscenario, and σ∗ is a random variable. The objective of a second-stage algorithmA\\nconsists of using the preference function h to return an accurate ranking A(X)f o r\\nany query subset X. The algorithm may be deterministic, in which case A(X)i s\\nuniquely determined from X o ri tm a yb er a n d o m i z e d ,i nw h i c hc a s ew ed e n o t eb y\\ns the randomization seed it may depend on.\\nThe following loss function L can be used to measure the disagreement between\\nar a n k i n gσ and a desired one σ∗ over a set X of n ≥ 1e l e m e n t s :\\nL(σ, σ∗)= 2\\nn(n − 1)\\n∑\\nu̸=v\\n1σ(u)<σ(v)1σ∗(v)<σ∗(u), (9.22)\\nwhere the sum runs over all pairs (u, v)w i t hu and v distinct elements ofX.A l lt h e\\nresults presented in the following hold for a broader set of loss functions described\\nlater. Abusing the notation, we also deﬁne the loss of the preference functionh with228 Ranking\\nrespect to a ranking σ∗ of a set X of n ≥ 1e l e m e n t sb y\\nL(h, σ∗)= 2\\nn(n − 1)\\n∑\\nu̸=v\\nh(u, v)1σ∗(v)<σ∗(u). (9.23)\\nThe expected loss for a deterministic algorithm A is thus E(X,σ∗)∼D[L(A(X),σ ∗)].\\nThe regret of algorithm A is then deﬁned as the diﬀerence between its loss and that\\nof the best ﬁxed global ranking. This can be written as follows:\\nReg(A)= E\\n(X,σ∗)∼D\\n[L(A(X),σ ∗)] − min\\nσ′\\nE\\n(X,σ∗)∼D\\n[L(σ′\\n|X,σ ∗)], (9.24)\\nwhere σ′\\n|X denotes the ranking induced onX by a global rankingσ′ of X. Similarly,\\nwe deﬁne the regret of the preference function as follows\\nReg(h)= E\\n(X,σ∗)∼D\\n[L(h|X,σ ∗)] − min\\nh′\\nE\\n(X,σ∗)∼D\\n[L(h′\\n|X,σ ∗)], (9.25)\\nwhere h|X denotes the restriction of h to X × X,a n ds i m i l a r l yw i t hh′.T h er e g r e t\\nresults presented in this section hold assuming the followingpairwise independence\\non irrelevant alternatives property:\\nE\\nσ∗|X1\\n[1σ∗(v)<σ∗(u)]= E\\nσ∗|X2\\n[1σ∗(v)<σ∗(u)], (9.26)\\nfor any u, v ∈X and any two sets X1 and X2 containing u and v.2 Similar\\nregret deﬁnitions can be given for a randomized algorithm additionally taking the\\nexpectation over s.\\nClearly, the quality of the ranking output by the second-stage algorithm inti-\\nmately depends on that of the preference functionh. In the next sections, we discuss\\nboth a deterministic and a randomized second-stage algorithm for which the regret\\ncan be upper bounded in terms of the regret of the preference function.\\n2. More generally, they hold without that assumption using the following weaker notions\\nof regret:\\nReg′(A)= E\\n(X,σ∗)∼D\\n[L(A(X),σ ∗)] − E\\nX\\nh\\nmin\\nσ′\\nE\\nσ∗|X\\n[L(σ′,σ ∗)]\\ni\\n(9.27)\\nReg′(h)= E\\n(X,σ∗)∼D\\n[L(h|X,σ ∗)] − E\\nX\\nh\\nmin\\nh′\\nE\\nσ∗|X\\n[L(h′,σ ∗)]\\ni\\n, (9.28)\\nwhere σ′ denotes a ranking of X, h′ a preference function deﬁned over X × X,a n dσ∗|X\\nthe random variable σ∗ conditioned on X.9.6 Preference-based setting 229\\n9.6.2 Deterministic algorithm\\nA natural deterministic algorithm for the second-stage is based on thesort-by-degree\\nalgorithm. This consists of ranking each element ofX based on the number of other\\nelements it is preferred to according to the preference functionh.L e tAsort-by-degree\\ndenote this algorithm. In the bipartite setting, the following bounds can be proven\\nfor the expected loss of this algorithm and its regret:\\nE\\nX,σ∗\\n[L(Asort-by-degree(X),σ ∗)] ≤ 2E\\nX,σ∗\\n[L(h, σ∗)] (9.29)\\nReg(Asort-by-degree(X)) ≤ 2R e g(h) . (9.30)\\nThese results show that the sort-by-degree algorithm can achieve an accurate\\nranking when the loss or the regret of the preference function h is small. They\\nalso bound the ranking loss or regret of the algorithm in terms of the classiﬁcation\\nloss or regret of h, which can be viewed as a guarantee for the reduction of ranking\\nto classiﬁcation using the sort-by-degree algorithm.\\nNevertheless, in some cases, the guarantee given by these results is weak or unin-\\nformative owing to the presence of the factor of two. Consider the case of a binary\\nclassiﬁer h with an error rate of just 25 percent, which is quite reasonable in many\\napplications. Assume that the Bayes error is close to zero for the classiﬁcation prob-\\nlem and, similarly, that for the ranking problem the regret and loss approximately\\ncoincide. Then, using the bound in (9.29) guarantees a worst-case pairwise mis-\\nranking error of at most 50 percent for the ranking algorithm, which is the pairwise\\nmisranking error of random ranking.\\nFurthermore, the running time complexity of the algorithm quadratic, that is in\\nΩ(|X|\\n2)o faq u e r ys e tX, since it requires calling the preference function for every\\npair (u, v)w i t hu and v in X.\\nAs shown by the following theorem, no deterministic algorithm can improve upon\\nthe factor of two appearing in the regret guarantee of the sort-by-degree algorithm.\\nTheorem 9.3 Lower bound for deterministic algorithms\\nFor any deterministic algorithm A, there is a bipartite distribution for which\\nReg(A) ≥ 2R e g(h). (9.31)\\nProof C o n s i d e rt h es i m p l ec a s ew h e r eX = X = {u, v, w} and where the\\npreference function induces a cycle as illustrated by ﬁgure 9.5a. An arrow from\\nu to v indicates that v is preferred to u according to h. The proof is based on an\\nadversarial choice of the target σ\\n∗.\\nWithout loss of generality, either A returns the ranking u, v, w(ﬁgure 9.5b) or\\nw,v,u (ﬁgure 9.5c). In the ﬁrst case, let σ∗ be deﬁned by the labeling indicated in\\nthe ﬁgure. In that case, we have L(h, σ∗)=1 /3, since u is preferred to w according230 Ranking\\nu\\nv w\\nh\\nu, v w\\n+- +-\\nw,v u\\n(a) (b) (c)\\nFigure 9.5 Illustration of the proof of theorem 9.3.\\nto h while w is labeled positively and u negatively. The loss of the algorithm is\\nL(A,σ ∗)=2 /3, since both u and v are ranked higher than the positively labeledw\\nby the algorithm. Similarly, σ∗ can be deﬁned as in ﬁgure 9.5c in the second case,\\nand we ﬁnd again that L(h, σ∗)=1 /3a n d L(A,σ ∗)=2 /3. This concludes the\\nproof.\\nThe theorem suggests that randomization is necessary in order to achieve a better\\nguarantee. In the next section, we present a randomized algorithm that beneﬁts\\nboth from better guarantees and a better time complexity.\\n9.6.3 Randomized algorithm\\nThe general idea of the algorithm described in this section is to use a straightforward\\nextension of the randomized QuickSort algorithm in the second stage. Unlike in\\nthe standard version of QuickSort , here the comparison function is based on the\\npreference function, which in general is not transitive. Nevertheless, it can be shown\\nhere, too, that the expected time complexity of the algorithm is inO(n log n)w h e n\\na p p l i e dt oa na r r a yo fs i z en.\\nThe algorithm works as follows, as illustrated by ﬁgure 9.6. At each recursive\\nstep, a pivot element u is selected uniformly at random from X.F o re a c hv ̸= u, v\\nis placed on the left ofu with probability h(v,u ) and to its right with the remaining\\nprobability h(u, v). The algorithm proceeds recursively with the array to the left of\\nu and the one to its right and returns the concatenation of the permutation returned\\nby the left recursion, u, and the permutation returned by the right recursion.\\nLet A\\nQuickSort denote this algorithm. In the bipartite setting, the following\\nguarantees can be proven:\\nE\\nX,σ∗,s\\n[L(AQuickSort(X,s ),σ ∗)] = E\\nX,σ∗\\n[L(h, σ∗)] (9.32)\\nReg(AQuickSort) ≤ Reg(h) . (9.33)\\nThus, here, the factor of two of the bounds in the deterministic case has vanished,9.6 Preference-based setting 231\\nleft recursion right recursion\\nrandom \\npivot\\nu\\nv\\nh(v,u ) h(u, v)\\nFigure 9.6 Illustration of randomized QuickSort based on a preference functionh\\n(not necessarily transitive).\\nwhich is substantially more favorable. Furthermore, the guarantee for the loss is\\nan equality. Moreover, the expected time complexity of the algorithm is only in\\nO(n log n), and, if only the top k items are needed to be ranked, as in many\\napplications, the time complexity is reduced to O(n + k log k).\\nFor the QuickSort algorithm, the following guarantee can also be proven in the\\ncase of general ranking setting (not necessarily bipartite setting):\\nE\\nX,σ∗,s\\n[L(AQuickSort(X,s ),σ ∗)] ≤ 2E\\nX,σ∗\\n[L(h, σ∗)]. (9.34)\\n9.6.4 Extension to other loss functions\\nAll of the results just presented hold for a broader class of loss functionsLω deﬁned\\nin terms of a weight function or emphasis function ω. Lω is similar to (9.22), but\\nmeasures the weighted disagreement between a rankingσ a n dad e s i r e do n eσ∗ over\\nas e tX of n ≥ 1e l e m e n t sa sf o l l o w s :\\nLω(σ, σ∗)= 2\\nn(n − 1)\\n∑\\nu̸=v\\nω(σ∗(v),σ ∗(u)) 1σ(u)<σ(v) 1σ∗(v)<σ∗(u), (9.35)\\nwhere the sum runs over all pairs ( u, v)w i t hu and v distinct elements of X,a n d\\nwhere ω is a symmetric function whose properties are described below. Thus, the loss\\ncounts the number of pairwise misrankings of σ with respect to σ∗, each weighted\\nby ω.F u n c t i o nω is assumed to satisfy the following three natural axioms:\\nsymmetry: ω(i, j)= ω(j, i) for all i, j;\\nmonotonicity: ω(i, j) ≤ ω(i, k)i fe i t h e ri<j<k or i>j>k ;\\ntriangle inequality: ω(i, j) ≤ ω(i, k)+ ω(k,j ).232 Ranking\\nThe motivation for this last property stems from the following: if correctly ordering\\nitems in positions (i, k)a n d(k,j ) is not of great importance, then the same should\\nhold for items in positions (i, j).\\nUsing diﬀerent functions ω, the family of functions Lω can cover several familiar\\nand important losses. Here are some examples. Setting ω(i, j) = 1 for all i ̸= j\\nyields the unweighted pairwise misranking measure. For a ﬁxed integer k ≥ 1,\\nthe function ω deﬁned by ω(i, j)=1 ((i≤k)∨(j≤k))∧(i̸=j) for all ( i, j) can be used\\nto emphasize ranking at the top k elements. Misranking of pairs with at least\\none element ranked among the top k is penalized by this function. This can be\\nof interest in applications such as information extraction or search engines where\\nthe ranking of the top documents matters more. For this emphasis function, all\\nelements ranked below k are in a tie. Any tie relation can be encoded using ω.\\nFinally, in a bipartite ranking scenario with m\\n+ positive and m− negative points\\nand m+ + m− = n, choosing ω(i, j)= n(n−1)\\n2m− m+ yields the standard loss function\\ncoinciding with 1 − AUC.\\n9.7 Discussion\\nThe objective function for the ranking problems discussed in this chapter were\\nall based on pairwise misranking. Other ranking criteria have been introduced in\\ninformation retrieval and used to derive alternative ranking algorithms. Here, we\\nbrieﬂy present several of these criteria.\\nPrecision, precision@n, average precision, recall. All of these criteria assume that\\npoints are partitioned into two classes (positives and negatives), as in the bipar-\\ntite ranking setting. Precision is the fraction of positively predicted points that\\nare in fact positive. Whereas precision takes into account all positive predictions,\\nprecision@n only considers the top n predictions. For example, precision@5 consid-\\ners only the top 5 positively predicted points.Average precisioninvolves computing\\nprecision@n for each value ofn, and averaging across these values. Each precision@n\\ncomputation can be interpreted as computing precision for a ﬁxed value of recall,\\nor the fraction of positive points that are predicted to be positive (recall coincides\\nwith the notion of true positive rate).\\nDCG, NDCG. These criteria assume the existence of relevance scores associated\\nwith the points to be ranked, e.g., given a web search query, each website returned\\nby a search engine has an associated relevance score. Moreover, these criteria\\nmeasure the extent to which points with large relevance scores appear at or near the\\nbeginning of a ranking. Deﬁne ( c\\ni)i∈N as a predeﬁned sequence of non-increasing\\nand non-negative discount factors, e.g., ci = log(i)−1. Then, given a ranking of m\\npoints and deﬁning ri as the relevance score of the ith point in this ranking, the9.8 Chapter notes 233\\ndiscounted cumulative gain (DCG)is deﬁned as DCG = ∑m\\ni=1 ciri. Note that DCG\\nis an increasing function of m. In contrast, the normalized discounted cumulative\\ngain (NDCG) normalizes the DCG across values of m by dividing the DCG by the\\nIDCG, or the ideal DCG that would result from an optimal ordering of the points.\\n9.8 Chapter notes\\nThe problem of learning to rank is distinct from the purely algorithmic one of rank\\naggregation, which, as shown by Dwork, Kumar, Naor, and Sivakumar [2001], is\\nNP-hard even for k = 4 rankings. The Rademacher complexity and margin-based\\ngeneralization bounds for pairwise ranking given in theorem 9.1 and corollary 5.1 are\\nnovel. Margin bounds based on covering numbers were also given by Rudin, Cortes,\\nMohri, and Schapire [2005]. Other learning bounds in the score-based setting of\\nranking, including VC-dimension and stability-based learning bounds, have been\\ngiven by Agarwal and Niyogi [2005], Agarwal et al. [2005] and Cortes et al. [2007b].\\nThe ranking algorithm based on SVMs presented in section 9.3 has been used and\\ndiscussed by several researchers. One early and speciﬁc discussion of its use can be\\nfound in Joachims [2002]. The fact that the algorithm is simply a special instance of\\nSVMs seems not to be clearly stated in the literature. The theoretical justiﬁcation\\npresented here for its use in ranking is novel.\\nRankBoost was introduced by Freund et al. [2003]. The version of the algorithm\\npresented here is the coordinate descent RankBoost from Rudin et al. [2005].\\nRankBoost in general does not achieve a maximum margin and may not increase\\nthe margin at each iteration. A Smooth Margin ranking algorithm [Rudin et al.,\\n2005] based on a modiﬁed version of the objective function of RankBoost can be\\nshown to increase the smooth margin at every iteration, but the comparison of\\nits empirical performance with that of RankBoost has not been reported. For the\\nempirical ranking quality of AdaBoost and the connections between AdaBoost and\\nRankBoost in the bipartite, setting see Cortes and Mohri [2003] and Rudin et al.\\n[2005].\\nThe Receiver Operating Characteristics (ROC) curves were originally developed\\nin signal detection theory [Egan, 1975] in connection with radio signals during\\nWorld War II. They also had applications to psychophysics [Green and Swets, 1966]\\nand have been used since then in a variety of other applications, in particular for\\nmedical decision making. The area under an ROC curve (AUC) is equivalent to the\\nWilcoxon-Mann-Whitney statistic [Hanley and McNeil, 1982] and is closely related\\nto the Gini index [Breiman et al., 1984] (see also chapter 8). For a statistical analysis\\nof the AUC and conﬁdence intervals depending on the error rate, see Cortes and\\nMohri [2003, 2005]. The deterministic algorithm in the preference-based setting234 Ranking\\ndiscussed in this chapter was presented and analyzed by Balcan et al. [2008]. The\\nrandomized algorithm as well as much of the results presented in section 9.6 are\\ndue to Ailon and Mohri [2008].\\nA somewhat related problem of ordinal regression has been studied by some\\nauthors [McCullagh, 1980, McCullagh and Nelder, 1983, Herbrich et al., 2000] which\\nconsists of predicting the correct label of each item out of a ﬁnite set, as in multi-\\nclass classiﬁcation, with the additional assumption of an ordering among the labels.\\nThis problem is distinct, however, from the pairwise ranking problem discussed in\\nthis chapter.\\nThe DCG ranking criterion was introduced by J¨arvelin and Kek¨al¨ainen [2000],\\nand has been used and discussed in a number of subsequent studies, in particular\\nCossock and Zhang [2008] who consider a subset ranking problem formulated in\\nterms of DCG, for which they consider a regression-based solution.\\n9.9 Exercises\\n9.1 Uniform margin-bound for ranking. Use theorem 9.1 to derive a margin-based\\nlearning bound for ranking that holds uniformly for all ρ> 0 (see similar binary\\nclassiﬁcation bounds of theorem 4.5 and exercise 4.2).\\n9.2 On-line ranking. Give an on-line version of the SVM-based ranking algorithm\\npresented in section 9.3.\\n9.3 Empirical margin loss of RankBoost. Derive an upper bound on the empirical\\npairwise ranking margin loss of RankBoost similar to that of theorem 6.3 for\\nAdaBoost.\\n9.4 Margin maximization and RankBoost. Give an example showing that Rank-\\nBoost does not achieve the maximum margin, as in the case of AdaBoost.\\n9.5 RankPerceptron. Adapt the Perceptron algorithm to derive a pairwise ranking\\nalgorithm based on a linear scoring function. Assume that the training sample is\\nlinear separable for pairwise ranking. Give an upper bound on the number of updates\\nmade by the algorithm in terms of the ranking margin.\\n9.6 Margin-maximization ranking. Give a linear programming (LP) algorithm re-\\nturning a linear hypothesis for pairwise ranking based on margin maximization.\\n9.7 Bipartite ranking. Suppose that we use a binary classiﬁer for ranking in the9.9 Exercises 235\\nbipartite setting. Prove that if the error of the binary classiﬁer isϵ, then that of the\\nranking it induces is also at most ϵ. Show that the converse does not hold.\\n9.8 Multipartite ranking. Consider the ranking scenario in a k-partite setting\\nwhere X is partitioned into k subsets X1,..., Xk with k ≥ 1. The bipartite case\\n(k = 2) is already speciﬁcally examined in the chapter. Give a precise formulation\\nof the problem in terms of k distributions. Does RankBoost admit an eﬃcient\\nimplementation in this case? Give the pseudocode of the algorithm.\\n9.9 Deviation bound for the AUC. Let h be a ﬁxed scoring function used to rank\\nt h ep o i n t so fX. Use Hoeﬀding’s bound to show that with high probability the AUC\\nof h for a ﬁnite sample is close to its average.\\n9.10 k-partite weight function. Show how the weight function ω c a nb ed e ﬁ n e ds o\\nthat Lω encodes the natural loss function associated to ak-partite ranking scenario.10 Regression\\nThis chapter discusses in depth the learning problem of regression,w h i c hc o n s i s t s\\nof using data to predict, as closely as possible, the correct real-valued labels of the\\npoints or items considered. Regression is a common task in machine learning with a\\nvariety of applications, which justiﬁes the speciﬁc chapter we reserve to its analysis.\\nThe learning guarantees presented in the previous sections focused largely on\\nclassiﬁcation problems. Here we present generalization bounds for regression, both\\nfor ﬁnite and inﬁnite hypothesis sets. Several of these learning bounds are based on\\nthe familiar notion of Rademacher complexity, which is useful for characterizing\\nthe complexity of hypothesis sets in regression as well. Others are based on a\\ncombinatorial notion of complexity tailored to regression that we will introduce,\\npseudo-dimension,w h i c hc a nb ev i e w e da sa ne x t e n s i o no ft h eV C - d i m e n s i o nt o\\nregression. We describe a general technique for reducing regression problems to\\nclassiﬁcation and deriving generalization bounds based on the notion of pseudo-\\ndimension. We present and analyze several regression algorithms, including linear\\nregression, kernel ridge regression , support-vector regression, Lasso,a n ds e v e r a l\\non-line versions of these algorithms. We discuss in detail the properties of these\\nalgorithms, including the corresponding learning guarantees.\\n10.1 The problem of regression\\nWe ﬁrst introduce the learning problem of regression. Let X denote the input space\\nand Y a measurable subset of R.W ed e n o t eb yD an unknown distribution over X\\naccording to which input points are drawn and by f : X→ Y the target labeling\\nfunction. This corresponds to a deterministic learning scenario that we adopt to\\nsimplify the presentation. As discussed in section 2.4.1, the deterministic scenario\\ncan be straightforwardly extended to a stochastic one where we have instead a\\ndistribution over the pairs (x, y) ∈X×Y .\\nAs in all supervised learning problems, the learner receives a labeled sample\\nS =\\n(\\n(x1,y1),..., (xm,y m)\\n⎡\\n∈ (X× Y )m with x1,...,x m drawn i.i.d. according\\nto D,a n d yi = f(xi) for all i ∈ [1,m]. Since the labels are real numbers, it is238 Regression\\nnot reasonable to hope that the learner could predict precisely the correct label.\\nInstead, we can require that his predictions be close to the correct ones. This is\\nthe key diﬀerence between regression and classiﬁcation: in regression, the measure\\nof error is based on the magnitude of the diﬀerence between the real-valued label\\npredicted and the true or correct one, and not based on the equality or inequality\\nof these two values.\\nWe denote by L: Y×Y→ R\\n+ the loss function used to measure the magnitude\\nof error. The most common loss function used in regression is the squared loss L2\\ndeﬁned by L(y,y ′)= |y′ − y|2 for all y,y ′ ∈Y , or, more generally, anLp loss deﬁned\\nby L(y,y ′)= |y′ − y|p,f o rs o m ep ≥ 1 and all y,y ′ ∈Y .\\nGiven a hypothesis set H of functions mapping X to Y, the regression problem\\nconsists of using the labeled sample S to ﬁnd a hypothesis h ∈ H with small\\nexpected loss or generalization error R(h) with respect to the target f:\\nR(h)= E\\nx∼D\\n[\\nL\\n(\\nh(x),f (x)\\n⎡]\\n. (10.1)\\nAs in the previous chapters, the empirical loss or error ofh ∈ H is denoted by ˆR(h)\\nand deﬁned by\\nˆR(h)= 1\\nm\\nm∑\\ni=1\\nL\\n(\\nh(xi),y i\\n⎡\\n. (10.2)\\nIn the common case where L is the squared loss, this represents the mean squared\\nerror of h on the sample S.\\nW h e nt h el o s sf u n c t i o nL is bounded by some M> 0, that is L(y′,y ) ≤ M for all\\ny,y ′ ∈Y or, more strictly,L(h(x),f (x)) ≤ M for all h ∈ H and x ∈X , the problem\\nis referred to as a bounded regression problem. Much of the theoretical results\\npresented in the following sections are based on that assumption. The analysis of\\nunbounded regression problemsis technically more elaborate and typically requires\\nsome other types of assumptions.\\n10.2 Generalization bounds\\nThis section presents learning guarantees for bounded regression problems. We start\\nw i t ht h es i m p l ec a s eo faﬁ n i t eh y p o t h e s i ss e t .\\n10.2.1 Finite hypothesis sets\\nIn the case of a ﬁnite hypothesis, we can derive a generalization bound for regression\\nby a straightforward application of Hoeﬀding’s inequality and the union bound.10.2 Generalization bounds 239\\nTheorem 10.1\\nLet L be a bounded loss function. Assume that the hypothesis setH is ﬁnite. Then,\\nfor any δ> 0, with probability at least 1 − δ, the following inequality holds for all\\nh ∈ H:\\nR(h) ≤ ˆR(h)+ M\\n√\\nlog |H| +l o g1\\nδ\\n2m .\\nProof By Hoeﬀding’s inequality, since L takes values in [0,M ], for any h ∈ H,\\nthe following holds:\\nPr\\n[\\nR(h) − ˆR(h) >ϵ\\n]\\n≤ e− 2mϵ2\\nM 2 .\\nT h u s ,b yt h eu n i o nb o u n d ,w ec a nw r i t e\\nPr\\n[\\n∃h ∈ H : R(h) − ˆR(h) >ϵ\\n]\\n≤\\n∑\\nh∈H\\nPr\\n[\\nR(h) − ˆR(h) >ϵ\\n]\\n≤| H|e− 2mϵ2\\nM 2 .\\nSetting the right-hand side to be equal toδyields the statement of the theorem.\\nWith the same assumptions and using the same proof, a two-sided bound can be\\nderived: with probability at least 1 − δ, for all h ∈ H,\\n|R(h) − ˆR(h)|≤ M\\n√\\nlog |H| +l o g2\\nδ\\n2m .\\nThese learning bounds are similar to those derived for classiﬁcation. In fact, they\\ncoincide with the classiﬁcation bounds given in the inconsistent case when M =1 .\\nThus, all the remarks made in that context apply identically here. In particular,\\na larger sample size m guarantees better generalization; the bound increases as a\\nfunction of log |H| and suggests selecting, for the same empirical error, a smaller\\nhypothesis set. This is an instance of Occam’s razor principle for regression. In the\\nnext sections, we present other instances of this principle for the general case of\\ninﬁnite hypothesis sets using the notions of Rademacher complexity and pseudo-\\ndimension.\\n10.2.2 Rademacher complexity bounds\\nHere, we show how the Rademacher complexity bounds of theorem 3.1 can be\\nused to derive generalization bounds for regression in the case of the family of L\\np\\nloss functions. We ﬁrst show an upper bound for the Rademacher complexity of a\\nrelevant family of functions.240 Regression\\nTheorem 10.2 Rademacher complexity of Lp loss functions\\nLet p ≥ 1 and Hp = {x ↦→| h(x) − f(x)|p : h ∈ H}. Assume that |h(x) − f(x)|≤ M\\nfor all x ∈X and h ∈ H.T h e n ,f o ra n ys a m p l eS of size m, the following inequality\\nholds:\\nˆRS(Hp) ≤ pMp−1 ˆRS(H) .\\nProof Let φp : x ↦→| x|p, then, Hp c a nb er e w r i t t e na sHp = {φp ◦h: h ∈ H′},\\nwhere H′ = {x ↦→ h(x) − f(x): h ∈ H}.S i n c eφp is pMp−1-Lipschitz over [−M,M ],\\nwe can apply Talagrand’s lemma (lemma 4.2):\\nˆRS(Hp) ≤ pMp−1 ˆRS(H′) . (10.3)\\nNow, ˆRS(H′) can be expressed as follows:\\nˆRS(H′)= 1\\nm E\\nσ\\n[\\nsup\\nh∈H\\nm∑\\ni=1\\n(\\nσih(xi)+ σif(xi)\\n⎡]\\n= 1\\nm E\\nσ\\n[\\nsup\\nh∈H\\nm∑\\ni=1\\nσih(xi)\\n]\\n+E\\nσ\\n[ m∑\\ni=1\\nσif(xi)\\n]\\n= ˆRS(H) ,\\nsince Eσ\\n[ ∑m\\ni=1 σif(xi)\\n]\\n= ∑m\\ni=1 Eσ [σi]f(xi)=0 .\\nCombining this result with the general Rademacher complexity learning bound\\nof theorem 3.1 yields directly the following Rademacher complexity bounds for\\nregression with L\\np losses.\\nTheorem 10.3 Rademacher complexity regression bounds\\nLet p ≥ 1 and assume that ∥h − f ∥∞ ≤ M for all h ∈ H.T h e n ,f o ra n yδ> 0,w i t h\\nprobability at least1 − δover a sample S of size m, each of the following inequalities\\nholds for all h ∈ H:\\nE\\n[⏐⏐h(x) − f(x)\\n⏐⏐p]\\n≤ 1\\nm\\nm∑\\ni=1\\n⏐⏐h(xi) − f(xi)\\n⏐⏐p\\n+2 pMp−1Rm(H)+ Mp\\n√\\nlog 1\\nδ\\n2m\\nE\\n[⏐⏐h(x) − f(x)\\n⏐⏐p]\\n≤ 1\\nm\\nm∑\\ni=1\\n⏐⏐h(xi) − f(xi)\\n⏐⏐p\\n+2 pMp−1 ˆRS(H)+3 Mp\\n√\\nlog 2\\nδ\\n2m .\\nAs in the case of classiﬁcation, these generalization bounds suggest a trade-oﬀ\\nb e t w e e nr e d u c i n gt h ee m p i r i c a le r r o r ,w h i c hm a yr e q u i r em o r ec o m p l e xh y p o t h e s i s\\nsets, and controlling the Rademacher complexity of H, which may increase the\\nempirical error. An important beneﬁt of the last learning bound is that it is data-\\ndependent. This can lead to more accurate learning guarantees. The upper bounds\\non R\\nm(H)o r RS(H) for kernel-based hypotheses (theorem 5.5) can be used directly10.2 Generalization bounds 241\\nx1 x2\\nt2\\nt1\\nFigure 10.1 Illustration of the shattering of a set of two points {x1,x 2} with\\nwitnesses t1 and t2.\\nhere to derive generalization bounds in terms of the trace of the kernel matrix or\\nthe maximum diagonal entry.\\n10.2.3 Pseudo-dimension bounds\\nAs previously discussed in the case of classiﬁcation, it is sometimes computationally\\nhard to estimate the empirical Rademacher complexity of a hypothesis set. In\\nchapter 3, we introduce other measures of the complexity of a hypothesis set\\nsuch as the VC-dimension, which are purely combinatorial and typically easier\\nto compute or upper bound. However, the notion of shattering or that of VC-\\ndimension introduced for binary classiﬁcation are not readily applicable to real-\\nvalued hypothesis classes.\\nWe ﬁrst introduce a new notion ofshattering for families of real-valued functions.\\nAs in previous chapters, we will use the notation G for a family of functions,\\nwhenever we intend to later interpret it (at least in some cases) as the family of loss\\nfunctions associated to some hypothesis set H: G = {x ↦→ L(h(x),f (x)): h ∈ H}.\\nDeﬁnition 10.1 Shattering\\nLet G be a family of functions from X to R.As e t {x\\n1,...,x m}⊆X is said to be\\nshattered by G if there exist t1,...,t m ∈ R such that,\\n⏐⏐⏐\\n⏐⏐⏐\\n⏐⏐\\n⎧\\n⎪⎪\\n⎨\\n⎪⎪\\n⎩\\n⎡\\n⎢⎢⎣\\nsgn\\n(\\ng(x\\n1) − t1\\n⎡\\n..\\n.\\nsgn\\n(\\ng(x\\nm) − tm\\n⎡\\n⎤\\n⎥⎥⎦: g ∈ G\\n⎫\\n⎪⎪\\n⎬\\n⎪⎪\\n⎭\\n⏐⏐⏐⏐\\n⏐⏐⏐⏐\\n=2\\nm .\\nWhen they exist, the threshold values t1,...,t m are said to witness the shattering.\\nThus, {x1,...,x m} is shattered if for some witnesses t1,...,t m, the family of\\nfunctions G is rich enough to contain a function going above a subset A of the242 Regression\\nLoss\\n-2 -1 0 1 2\\n0.0\\n0.5\\n1.0\\n1.5\\nL(h(x),f (x))\\nt\\nI(L(h(x),f (x)) >t )\\nx\\nFigure 10.2 Af u n c t i o ng: x ↦→ L(h(x),f (x)) (in blue) deﬁned as the loss of some\\nﬁxed hypothesis h ∈ H, and its thresholded version x ↦→ 1L(h(x),f (x))>t (in red) with\\nrespect to the threshold t (in yellow).\\nset of points I = {(xi,t i): i ∈ [1,m]} and below the others (I − A), for any choice\\nof the subset A. Figure 10.1 illustrates this shattering in a simple case. The notion\\nof shattering naturally leads to the following deﬁnition.\\nDeﬁnition 10.2 Pseudo-dimension\\nLet G be a family of functions mapping from X to R. Then, the pseudo-dimension\\nof G, denoted by Pdim(G), is the size of the largest set shattered by G.\\nBy deﬁnition of the shattering just introduced, the notion of pseudo-dimension of\\na family of real-valued functions G coincides with that of the VC-dimension of the\\ncorresponding thresholded functions mapping X to {0,1}:\\nPdim(G)=V C d i m\\n({\\n(x, t) ↦→ 1(g(x)−t)>0 : g ∈ G\\n}⎡\\n. (10.4)\\nFigure 10.2 illustrates this interpretation. In view of this interpretation, the follow-\\ning two results follow directly the properties of the VC-dimension.\\nTheorem 10.4\\nThe pseudo-dimension of hyperplanes in RN is given by\\nPdim({x ↦→ w · x + b: w ∈ RN ,b ∈ R})= N +1 .\\nTheorem 10.5\\nThe pseudo-dimension of a vector space of real-valued functions H is equal to the\\ndimension of the vector space:\\nPdim(H)=d i m (H) .\\nThe following theorem gives a generalization bound for bounded regression10.2 Generalization bounds 243\\nin terms of the pseudo-dimension of a family of loss function G = {x ↦→\\nL(h(x),f (x)): h ∈ H} associated to a hypothesis set H. The key technique to\\nderive these bounds consists of reducing the problem to that of classiﬁcation by\\nmaking use of the following general identity for the expectation of a random variable\\nX:\\nE[X]= −\\n∫\\n0\\n−∞\\nPr[X<t ]dt +\\n∫ +∞\\n0\\nPr[X>t ]dt , (10.5)\\nwhich holds by deﬁnition of the Lebesgue integral. In particular, for any distribution\\nD and any non-negative measurable function f, we can write\\nE\\nx∼D\\n[f(x)] =\\n∫ ∞\\n0\\nPr\\nx∼D\\n[f(x) >t ]dt . (10.6)\\nTheorem 10.6\\nLet H be a family of real-valued functions and letG = {x ↦→ L(h(x),f (x)): h ∈ H}\\nbe the family of loss functions associated toH. Assume that Pdim(G)= d and that\\nthe loss function L is bounded by M.T h e n ,f o ra n yδ> 0, with probability at least\\n1 − δ over the choice of a sample of size m, the following inequality holds for all\\nh ∈ H:\\nR(h) ≤ ˆR(h)+ M\\n√\\n2d log em\\nd\\nm + M\\n√\\nlog 1\\nδ\\n2m . (10.7)\\nProof Let S be a sample of size m drawn i.i.d. according to D and let ˆD denote\\nthe empirical distribution deﬁned by S. For any h ∈ H and t ≥ 0, we denote by\\nc(h, t) the classiﬁer deﬁned by c(h, t): x ↦→ 1L(h(x),f(x))>t.T h ee r r o ro fc(h, t)c a n\\nbe deﬁned by\\nR(c(h, t)) = Pr\\nx∼D\\n[c(h, t)(x) = 1] = Pr\\nx∼D\\n[L(h(x),f (x)) >t ],\\nand, similarly, its empirical error is ˆR(c(h, t)) = Prx∼ bD[L(h(x),f (x)) >t ].\\nNow, in view of the identity (10.6) and the fact that the loss functionL is bounded244 Regression\\nby M,w ec a nw r i t e :\\n|R(h) − ˆR(h)| =\\n⏐⏐\\n⏐ E\\nx∼D\\n[L(h(x),f (x))] − E\\nx∼ bD\\n[L(h(x),f (x))]\\n⏐⏐\\n⏐\\n=\\n⏐⏐\\n⏐\\n⏐\\n⏐\\n∫\\nM\\n0\\n(\\nPr\\nx∈D\\n[L(h(x),f (x)) >t ] − Pr\\nx∈ bD\\n[L(h(x),f (x)) >t ]\\n⎡\\ndt\\n⏐⏐\\n⏐\\n⏐\\n⏐\\n≤ M sup\\nt∈[0,M]\\n⏐⏐\\n⏐⏐ Pr\\nx∈D\\n[L(h(x),f (x)) >t ] − Pr\\nx∈ bD\\n[L(h(x),f (x)) >t ]\\n⏐⏐⏐⏐\\n= M sup\\nt∈[0,M]\\n⏐⏐⏐R(c(h, t)) − ˆR(c(h, t))\\n⏐⏐⏐ .\\nThis implies the following inequality:\\nPr\\n[\\n|R(h) − ˆR(h)| >ϵ\\n]\\n≤ Pr\\n[\\nsup\\nh∈H\\nt∈[0,M]\\n⏐⏐⏐R(c(h, t)) − ˆR(c(h, t))\\n⏐⏐⏐ > ϵ\\nM\\n]\\n.\\nThe right-hand side can be bounded using a standard generalization bound for clas-\\ns i ﬁ c a t i o n( c o r o l l a r y3 . 4 )i nt e r m so ft h eV C - d i m e n s i o no ft h ef a m i l yo fh y p o t h e s e s\\n{c(h, t): h ∈ H,t ∈ [0,M ]}, which, by deﬁnition of the pseudo-dimension, is pre-\\ncisely Pdim(G)= d. The resulting bound coincides with (10.7).\\nThe notion of pseudo-dimension is suited to the analysis of regression as demon-\\nstrated by the previous theorem; however, it is not a scale-sensitive notion. There\\nexists an alternative complexity measure, thefat-shattering dimension,t h a ti ss c a l e -\\nsensitive and that can be viewed as a natural extension of the pseudo-dimension.\\nIts deﬁnition is based on the notion of γ-shattering.\\nDeﬁnition 10.3 γ-shattering\\nLet G be a family of functions from X to R and let γ> 0.As e t {x\\n1,...,x m}⊆X\\nis said to be γ-shattered by G if there exist t1,...,t m ∈ R such that for all\\ny ∈{ −1,+1}m,t h e r ee x i s t sg ∈ G such that:\\n∀i ∈ [1,m],y i(g(xi) − ti) ≥ γ.\\nThus, {x1,...,x m} is γ-shattered if for some witnesses t1,...,t m, the family of\\nfunctions G is rich enough to contain a function going at least γ above a subset A\\nof the set of pointsI = {(xi,t i): i ∈ [1,m]} and at least γ below the others (I − A),\\nfor any choice of the subset A.\\nDeﬁnition 10.4 γ-fat-dimension\\nThe γ-fat-dimension of G, fatγ(G), is the size of the largest set that is γ-shattered\\nby G.\\nFiner generalization bounds than those based on the pseudo-dimension can be10.3 Regression algorithms 245\\nΦ(x)\\ny\\nFigure 10.3 For N =1 , linear regression consists of ﬁnding the line of best ﬁt,\\nmeasured in terms of the squared loss.\\nderived in terms of the γ-fat-dimension. However, the resulting learning bounds,\\nare not more informative than those based on the Rademacher complexity, which\\nis also a scale-sensitive complexity measure. Thus, we will not detail an analysis\\nbased on the γ-fat-dimension.\\n10.3 Regression algorithms\\nT h er e s u l t so ft h ep r e v i o u ss e c t i o n ss h o wt h a t ,f o rt h es a m ee m p i r i c a le r r o r ,\\nhypothesis sets with smaller complexity measured in terms of the Rademacher\\ncomplexity or in terms of pseudo-dimension beneﬁt from better generalization\\nguarantees. One family of functions with relatively small complexity is that of linear\\nhypotheses. In this section, we describe and analyze several algorithms based on\\nthat hypothesis set:linear regression, kernel ridge regression(KRR), support vector\\nregression (SVR), and Lasso. These algorithms, in particular the last three, are\\nextensively used in practice and often lead to state-of-the-art performance results.\\n10.3.1 Linear regression\\nWe start with the simplest algorithm for regression known aslinear regression.L e t\\nΦ : X→ R\\nN b eaf e a t u r em a p p i n gf r o mt h ei n p u ts p a c eX to RN and consider the\\nfamily of linear hypotheses\\nH = {x ↦→ w · Φ(x)+ b: w ∈ RN ,b ∈ R} . (10.8)\\nLinear regression consists of seeking a hypothesis in H with the smallest empirical\\nmean squared error. Thus, for a sample S =\\n(\\n(x1,y1),..., (xm,y m)\\n⎡\\n∈ (X× Y )m,\\nthe following is the corresponding optimization problem:\\nmin\\nw,b\\n1\\nm\\nm∑\\ni=1\\n(w · Φ(xi)+ b − yi)2 . (10.9)246 Regression\\nFigure 10.3 illustrates the algorithm in the simple case whereN = 1. The optimiza-\\ntion problem admits the simpler formulation:\\nmin\\nW\\nF(W)= 1\\nm ∥X⊤W − Y∥2, (10.10)\\nusing the notationX =\\n[Φ(x1) ... Φ(xm)\\n1 ... 1\\n]\\n, W =\\n[ w1..\\n.\\nwN\\n1\\n]\\nand Y =\\n[ y1..\\n.\\nym\\n]\\n. The objective\\nfunction F is convex, by composition of the convex function u ↦→∥ u∥2 with the\\naﬃne function W ↦→ X⊤W − Y, and it is diﬀerentiable. Thus, F admits a global\\nminimum at W if and only if ∇F(W) = 0, that is if and only if\\n2\\nmX(X⊤W − Y)=0 ⇔ XX⊤W = XY . (10.11)\\nWhen XX⊤ is invertible, this equation admits a unique solution. Otherwise, the\\nequation admits a family of solutions that can be given in terms of the pseudo-inverse\\nof matrix XX\\n⊤ (see appendix A) byW =( XX⊤)† XY +(I − (XX⊤)† (XX⊤))W0,\\nwhere W0 is an arbitrary matrix in RN ×N . Among these, the solution W =\\n(XX⊤)† XY is the one with the minimal norm and is often preferred for that reason.\\nThus, we will write the solutions as\\nW =\\n{\\n(XX⊤)−1XY if XX⊤ is invertible,\\n(XX⊤)† XY otherwise.\\n(10.12)\\nThe matrix XX⊤ can be computed in O(mN2). The cost of its inversion or that of\\ncomputing its pseudo-inverse is in O(N3).1 Finally, the multiplication with X and\\nY takes O(mN2). Therefore, the overall complexity of computing the solution W\\nis in O(mN2 + N3). Thus, when the dimension of the feature space N is not too\\nlarge, the solution can be computed eﬃciently.\\nWhile linear regression is simple and admits a straightforward implementation,\\nit does not beneﬁt from a strong generalization guarantee, since it is limited to\\nminimizing the empirical error without controlling the norm of the weight vector\\nand without any other regularization. Its performance is also typically poor in most\\napplications. The next sections describe algorithms with both better theoretical\\nguarantees and improved performance in practice.\\n1. In the analysis of the computational complexity of the algorithms discussed in this\\nchapter, the cubic-time complexity of matrix inversion can be replaced by a more favorable\\ncomplexity O(N\\n2+ω ), with ω = .376 using asymptotically faster matrix inversion methods\\nsuch as that of Coppersmith and Winograd.10.3 Regression algorithms 247\\n10.3.2 Kernel ridge regression\\nWe ﬁrst present a learning guarantee for regression with bounded linear hypotheses\\nin a feature space deﬁned by a PDS kernel. This will provide a strong theoretical\\nsupport for the kernel ridge regression algorithm presented in this section. The\\nlearning bounds of this section are given for the squared loss. Thus, in particular,\\nthe generalization error of a hypothesis h is deﬁned by R(h)=E\\n[\\n(h(x) − f(x))\\n2]\\nwhen the target function is f.\\nTheorem 10.7\\nLet K : X× X → R be a PDS kernel, Φ: X→ H a feature mapping associated to\\nK,a n dH = {x ↦→ w · Φ(x): ∥w∥H ≤ Λ}. Assume that there exists r> 0 such that\\nK(x, x) ≤ r2 and |f(x)|≤ Λr for all x ∈X .T h e n ,f o ra n yδ> 0,w i t hp r o b a b i l i t y\\nat least 1 − δ, each of the following inequalities holds for all h ∈ H:\\nR(h) ≤ ˆR(h)+ 8r2Λ2\\n√m\\n⎛\\n⎝1+ 1\\n2\\n√\\nlog 1\\nδ\\n2\\n⎞\\n⎠ (10.13)\\nR(h) ≤ ˆR(h)+ 8r2Λ2\\n√m\\n⎛\\n⎝\\n√\\nTr[K]\\nmr2 + 3\\n4\\n√\\nlog 2\\nδ\\n2\\n⎞\\n⎠ . (10.14)\\nProof For all x ∈X ,w eh a v e|w · Φ(x)|≤ Λ∥Φ(x)∥≤ Λr,t h u s ,f o ra l lx ∈X and\\nh ∈ H, |h(x) − f(x)|≤ 2Λr. By the bound on the empirical Rademacher complexity\\nof kernel-based hypotheses (theorem 5.5), the following holds for any sample S of\\nsize m:\\nˆRS(H) ≤ Λ\\n√\\nTr[K]\\nm ≤\\n√\\nr2Λ2\\nm ,\\nwhich implies that Rm(H) ≤\\n√\\nr2Λ2\\nm . Plugging in this inequality in the ﬁrst bound\\nof theorem 10.3 with M =2 Λr gives\\nR(h) ≤ ˆR(h)+4 M Rm(H)+ M2\\n√\\nlog 1\\nδ\\n2m = ˆR(h)+ 8r2Λ2\\n√m\\n(\\n1+ 1\\n2\\n√\\nlog 1\\nδ\\n2\\n⎡\\n.\\nThe second generalization bound is shown in a similar way by using the second\\nbound of theorem 10.3.\\nThe ﬁrst bound of the theorem just presented has the form R(h) ≤ ˆR(h)+ λΛ2,\\nwith λ = 8r2\\n√m\\n(\\n1+ 1\\n2\\n√\\nlog 1\\nδ\\n2\\n⎡\\n= O( 1√m ). Kernel ridge regression is deﬁned by the\\nminimization of an objective function that has precisely this form and thus is directly248 Regression\\nmotivated by the theoretical analysis just presented:\\nmin\\nw\\nF(w)= λ∥w∥2 +\\nm∑\\ni=1\\n(w · Φ(xi) − yi)2 . (10.15)\\nHere, λ is a positive parameter determining the trade-oﬀ between the regularization\\nterm ∥w∥2 and the empirical mean squared error. The objective function diﬀers from\\nthat of linear regression only by the ﬁrst term, which controls the norm ofw.A si n\\nthe case of linear regression, the problem can be rewritten in a more compact form\\nas\\nmin\\nW\\nF(W)= λ∥W∥2 + ∥X⊤W − Y∥2, (10.16)\\nwhere X ∈ RN ×m is the matrix formed by the feature vectors,X =[ Φ(x1) ... Φ(xm) ],\\nW = w,a n d Y =( y1,...,y m)⊤.H e r et o o ,F is convex, by the convexity of\\nw ↦→∥ w∥2 and that of the sum of two convex functions, and is diﬀerentiable.\\nThus F admits a global minimum at W if and only if\\n∇F(W)=0 ⇔ (XX⊤ + λI)W = XY ⇔ W =( XX⊤ + λI)−1XY. (10.17)\\nNote that the matrix XX⊤ + λI is always invertible, since its eigenvalues are the\\nsum of the non-negative eigenvalues of the symmetric positive semideﬁnite matrix\\nXX\\n⊤ and λ> 0. Thus, kernel ridge regression admits a closed-form solution.\\nAn alternative formulation of the optimization problem for kernel ridge regression\\nequivalent to (10.15) is\\nmin\\nw\\nm∑\\ni=1\\n(w · Φ(xi) − yi)2 subject to: ∥w∥2 ≤ Λ2.\\nThis makes the connection with the bounded linear hypothesis set of theorem 10.7\\neven more evident. Using slack variables ξi, for all i ∈ [1,m], the problem can be\\nequivalently written as\\nmin\\nw\\nm∑\\ni=1\\nξ2\\ni subject to: (∥w∥2 ≤ Λ2) ∧\\n(\\n∀i ∈ [1,m],ξ i = yi − w · Φ(xi)\\n⎡\\n.\\nThis is a convex optimization problem with diﬀerentiable objective function and\\nconstraints. To derive the equivalent dual problem, we introduce the LagrangianL,\\nwhich is deﬁned for all ξ,w, α′,a n dλ ≥ 0b y\\nL(ξ,w,α′,λ)=\\nm∑\\ni=1\\nξ2\\ni +\\nm∑\\ni=1\\nα′\\ni(yi − ξi − w · Φ(xi)) +λ(∥w∥2 − Λ2) .10.3 Regression algorithms 249\\nThe KKT conditions lead to the following equalities:\\n∇wL = −\\nm∑\\ni=1\\nα′\\niΦ(xi)+2 λw =0 = ⇒ w = 1\\n2λ\\nm∑\\ni=1\\nα′\\niΦ(xi)\\n∇ξi L =2 ξi − α′\\ni =0 = ⇒ ξi = α′\\ni/2\\n∀i ∈ [1,m],α ′\\ni(yi − ξi − w · Φ(xi)) = 0\\nλ(∥w∥2 − Λ2)=0 .\\nPlugging in the expressions of w and ξisi nt h a to fL gives\\nL =\\nm∑\\ni=1\\nα′2\\ni\\n4 +\\nm∑\\ni=1\\nα′\\niyi −\\nm∑\\ni=1\\nα′\\ni\\n2\\n2 − 1\\n2λ\\nm∑\\ni,j=1\\nα′\\niα′\\njΦ(xi)⊤Φ(xj)\\n+ λ\\n( 1\\n4λ2 ∥\\nm∑\\ni=1\\nα′\\niΦ(xi)∥2 − Λ2\\n⎡\\n= − 1\\n4\\nm∑\\ni=1\\nα′2\\ni +\\nm∑\\ni=1\\nα′\\niyi − 1\\n4λ\\nm∑\\ni,j=1\\nα′\\niα′\\njΦ(xi)⊤Φ(xj) − λΛ2\\n= −λ\\nm∑\\ni=1\\nα2\\ni +2\\nm∑\\ni=1\\nαiyi −\\nm∑\\ni,j=1\\nαiαjΦ(xi)⊤Φ(xj) − λΛ2,\\nwith α′\\ni =2 λαi. Thus, the equivalent dual optimization problem for KRR can be\\nwritten as follows:\\nmax\\nα ∈Rm\\n−λα⊤α +2 α⊤Y − α⊤(X⊤X)α , (10.18)\\nor, more compactly, as\\nmax\\nα ∈Rm\\nG(α)= −α⊤(K + λI)α +2 α⊤Y , (10.19)\\nwhere K = X⊤X is the kernel matrix associated to the training sample. The\\nobjective function G is concave and diﬀerentiable. The optimal solution is obtained\\nby diﬀerentiating the function and setting it to zero:\\n∇G(α)=0 ⇐⇒ 2(K + λI)α =2 Y ⇐⇒ α =( K + λI)−1Y . (10.20)\\nNote that (K+λI) is invertible, since its eigenvalues are the sum of the eigenvalues\\nof the SPSD matrixK and λ> 0. Thus, as in the primal case, the dual optimization\\nproblem admits a closed-form solution. By the ﬁrst KKT equation, w can be\\ndetermined from α by\\nw =\\nm∑\\ni=1\\nαiΦ(xi)= Xα = X(K + λI)−1Y. (10.21)250 Regression\\nThe hypothesis h solution can be given as follows in terms of α:\\n∀x ∈X ,h (x)= w · Φ(x)=\\nm∑\\ni=1\\nαiK(xi,x) . (10.22)\\nNote that the form of the solution, h = ∑m\\ni=1 αiK(xi, ·), could be immediately\\npredicted using the Representer theorem, since the objective function minimized by\\nKRR falls within the general framework of theorem 5.4. This also could show thatw\\nc o u l db ew r i t t e na sw = Xα. This fact, combined with the following simple lemma,\\ncan be used to determine α in a straightforward manner, without the intermediate\\nderivation of the dual problem.\\nLemma 10.1\\nThe following identity holds for any matrix X:\\n(XX\\n⊤ + λI)−1X = X(X⊤X + λI)−1 .\\nProof Observe that (XX⊤ +λI)X = X(X⊤X+λI). Left-multiplying by (XX⊤ +\\nλI)−1 this equality and right-multiplying it by (X⊤X + λI)−1 yields the statement\\nof the lemma.\\nNow, using this lemma, the primal solution of w c a nb er e w r i t t e na sf o l l o w s :\\nw =( XX⊤ + λI)−1XY = X(X⊤X + λI)−1Y = X(K + λI)−1Y.\\nComparing with w = Xα gives immediately α =( K + λI)−1Y.\\nOur presentation of the KRR algorithm was given for linear hypotheses with no\\noﬀset, that is we implicitly assumed b = 0. It is common to use this formulation\\nand to extend it to the general case by augmenting the feature vectorΦ(x)w i t ha n\\ne x t r ac o m p o n e n te q u a lt oo n ef o ra l lx ∈X and the weight vector w with an extra\\ncomponent b ∈ R. For the augmented feature vector Φ′(x) ∈ RN+1 and weight\\nvector w′ ∈ RN+1,w eh a v ew′ ·Φ′(x)= w ·Φ(x)+ b. Nevertheless, this formulation\\ndoes not coincide with the general KRR algorithm where a solution of the form\\nx ↦→ w ·Φ(x)+ b is sought. This is because for the general KRR, the regularization\\nterm is λ∥w∥, while for the extension just described it is λ∥w\\n′∥.\\nIn both the primal and dual cases, KRR admits a closed-form solution. Table 10.1\\ngives the time complexity of the algorithm for computing the solution and the one\\nfor determining the prediction value of a point in both cases. In the primal case,\\ndetermining the solutionw requires computing matrixXX\\n⊤,w h i c ht a k e sO(mN2),\\nthe inversion of (XX⊤ + λI), which is in O(N3), and multiplication with X,w h i c h\\nis in O(mN2). Prediction requires computing the inner product ofw with a feature\\nvector of the same dimension that can be achieved inO(N). The dual solution ﬁrst\\nrequires computing the kernel matrix K.L e tκ be the maximum cost of computing10.3 Regression algorithms 251\\nSolution Prediction\\nPrimal O(mN2 + N3) O(N)\\nDual O(κm2 + m3) O(κm)\\nT able 10.1 Comparison of the running-time complexity of KRR for computing\\nthe solution or the prediction value of a point in both the primal and the dual\\ncase. κ denotes the time complexity of computing a kernel value; for polynomial\\nand Gaussian kernels, κ = O(N).\\nK(x, x′) for all pairs (x, x′) ∈X×X . Then, K can be computed in O(κm2). The\\ninversion of matrix K + λI can be achieved in O(m3) and multiplication with Y\\ntakes O(m2). Prediction requires computing the vector ( K(x1,x),...,K (xm,x))⊤\\nfor some x ∈X ,w h i c hr e q u i r e sO(κm), and the inner product with α, which is in\\nO(m).\\nThus, in both cases, the main step for computing the solution is a matrix inversion,\\nwhich takes O(N3) in the primal case,O(m3) in the dual case. When the dimension\\nof the feature space is relatively small, solving the primal problem is advantageous,\\nwhile for high-dimensional spaces and medium-sized training sets, solving the dual\\nis preferable. Note that for relatively large matrices, the space complexity could also\\nbe an issue: the size of relatively large matrices could be prohibitive for memory\\nstorage and the use of external memory could signiﬁcantly aﬀect the running time\\nof the algorithm.\\nFor sparse matrices, there exist several techniques for faster computations of the\\nmatrix inversion. This can be useful in the primal case where the features can be\\nrelatively sparse. On the other hand, the kernel matrix K is typically dense; thus,\\nthere is less hope for beneﬁting from such techniques in the dual case. In such cases,\\nor, more generally, to deal with the time and space complexity issues arising when\\nm and N are large, approximation methods using low-rank approximations via the\\nNystr¨om method or the partial Cholesky decomposition can be used very eﬀectively.\\nThe KRR algorithm admits several advantages: it beneﬁts from favorable theo-\\nretical guarantees since it can be derived directly from the generalization bound we\\npresented; it admits a closed-form solution, which can make the analysis of many\\nof its properties convenient; and it can be used with PDS kernels, which extends its\\nuse to non-linear regression solutions and more general features spaces. KRR also\\nadmits favorable stability properties that we discuss in chapter 11.\\nThe algorithm can be generalized to learning a mapping from X to R\\np, p> 1.\\nT h i sc a nb ed o n eb yf o r m u l a t i n gt h ep r o b l e ma sp independent regression problems,\\neach consisting of predicting one of the p target components. Remarkably, the\\ncomputation of the solution for this generalized algorithm requires only a single252 Regression\\nΦ(x)\\ny\\nϵ\\nw·Φ(x)+b\\nFigure 10.4 SVR attempts to ﬁt a “tube” with width ϵ to the data. Training data\\nwithin the “epsilon tube” (blue points) incur no loss.\\nmatrix inversion, e.g., (K + λI)−1 in the dual case, regardless of the value of p.\\nOne drawback of the KRR algorithm, in addition to the computational issues for\\ndetermining the solution for relatively large matrices, is the fact that the solution it\\nreturns is typically not sparse. The next two sections present two sparse algorithms\\nf o rl i n e a rr e g r e s s i o n .\\n10.3.3 Support vector regression\\nIn this section, we present the support vector regression (SVR) algorithm, which\\nis inspired by the SVM algorithm presented for classiﬁcation in chapter 4. The\\nmain idea of the algorithm consists of ﬁtting a tube of width ϵ> 0t ot h ed a t a ,a s\\nillustrated by ﬁgure 10.4. As in binary classiﬁcation, this deﬁnes two sets of points:\\nthose falling inside the tube, which are ϵ-close to the function predicted and thus\\nnot penalized, and those falling outside, which are penalized based on their distance\\nto the predicted function, in a way that is similar to the penalization used by SVMs\\nin classiﬁcation.\\nUsing a hypothesis set H of linear functions: H = {x ↦→ w · Φ(x)+ b: w ∈\\nR\\nN ,b ∈ R},w h e r eΦ is the feature mapping corresponding some PDS kernel K,\\nthe optimization problem for SVR can be written as follows:\\nmin\\nw,b\\n1\\n2 ∥w∥2 + C\\nm∑\\ni=1\\n⏐⏐yi − (w · Φ(xi)+ b)\\n⏐⏐\\nϵ , (10.23)\\nwhere |·| ϵ denotes the ϵ-insensitive loss:\\n∀y,y ′ ∈Y , |y′ − y|ϵ =m a x ( 0, |y′ − y|− ϵ). (10.24)\\nThe use of this loss function leads to sparse solutions with a relatively small\\nnumber of support vectors. Using slack variables ξi ≥ 0a n d ξ′\\ni ≥ 0, i ∈ [1,m],10.3 Regression algorithms 253\\nthe optimization problem can be equivalently written as\\nmin\\nw,b,ξ,ξ′\\n1\\n2 ∥w∥2 + C\\nm∑\\ni=1\\n(ξi + ξ′\\ni) (10.25)\\nsubject to (w · Φ(xi)+ b) − yi ≤ ϵ + ξi\\nyi − (w · Φ(xi)+ b) ≤ ϵ + ξ′\\ni\\nξi ≥ 0,ξ′\\ni ≥ 0, ∀i ∈ [1,m].\\nThis is a convex quadratic program (QP) with aﬃne constraints. Introducing the\\nLagrangian and applying the KKT conditions leads to the following equivalent dual\\nproblem in terms of the kernel matrix K:\\nmax\\nα ,α ′\\n− ϵ(α′ + α)⊤1 +( α′ − α)⊤y − 1\\n2(α′ − α)⊤K(α′ − α) (10.26)\\nsubject to: (0 ≤ α ≤ C) ∧ (0 ≤ α′ ≤ C) ∧ ((α′ − α)⊤1 =0 ).\\nAny PDS kernelK can be used with SVR, which extends the algorithm to non-linear\\nregression solutions. Problem (10.26) is a convex QP similar to the dual problem\\nof SVMs and can be solved using similar optimization techniques. The solutions α\\nand α′ deﬁne the hypothesis h returned by SVR as follows:\\n∀x ∈X ,h (x)=\\nm∑\\ni=1\\n(α′\\ni − αi)K(xi,x)+ b, (10.27)\\nwhere the oﬀset b c a nb eo b t a i n e df r o map o i n txj with 0 <α j <C by\\nb = −\\nm∑\\ni=1\\n(α′\\ni − αi)K(xi,xj)+ yj + ϵ, (10.28)\\nor from a point xj with 0 <α ′\\nj <C via\\nb = −\\nm∑\\ni=1\\n(α′\\ni − αi)K(xi,xj)+ yj − ϵ. (10.29)\\nBy the complementarity conditions, for all i ∈ [1,m], the following equalities hold:\\nαi\\n(\\n(w · Φ(xi)+ b) − yi − ϵ − ξi\\n⎡\\n=0\\nα′\\ni\\n(\\n(w · Φ(xi)+ b) − yi + ϵ + ξ′\\ni\\n⎡\\n=0 .\\nThus, if αi ̸=0o r α′\\ni ̸=0 ,t h a ti si fxi is a support vector, then, either (w ·Φ(xi)+\\nb) − yi − ϵ = ξi holds or yi − (w ·Φ(xi)+ b) − ϵ = ξ′\\ni. This shows that support vectors\\npoints lying outside the ϵ-tube. Of course, at most one of αi or α′\\ni is non-zero for\\nany point xi: the hypothesis either overestimates or underestimates the true label254 Regression\\nby more than ϵ. For the points within the ϵ-tube, we have αj = α′\\nj =0 ;t h u s ,\\nthese points do not contribute to the deﬁnition of the hypothesis returned by SVR.\\nThus, when the number of points inside the tube is relatively large, the hypothesis\\nreturned by SVR is relatively sparse. The choice of the parameter ϵ determines a\\ntrade-oﬀ between sparsity and accuracy: larger ϵ values provide sparser solutions,\\nsince more points can fall within the ϵ-tube, but may ignore too many key points\\nfor determining an accurate solution.\\nThe following generalization bounds hold for the ϵ-insensitive loss and kernel-\\nbased hypotheses and thus for the SVR algorithm. We denote byD the distribution\\naccording to which sample points are drawn and by ˆD the empirical distribution\\ndeﬁned by a training sample of size m.\\nTheorem 10.8\\nLet K : X× X → R be a PDS kernel, letΦ: X→ H be a feature mapping associated\\nto K and let H = {x ↦→ w · Φ(x): ∥w∥\\nH ≤ Λ}. Assume that there exists r> 0 such\\nthat K(x, x) ≤ r2 and |f(x)|≤ Λr for all x ∈X .F i xϵ> 0.T h e n ,f o ra n yδ> 0,\\nwith probability at least1 − δ, each of the following inequalities holds for all h ∈ H,\\nE\\nx∼D\\n[|h(x) − f(x)|ϵ] ≤ E\\nx∼ bD\\n[|h(x) − f(x)|ϵ]+ 2rΛ√m\\n(\\n1+\\n√\\nlog 1\\nδ\\n2\\n⎡\\nE\\nx∼D\\n[|h(x) − f(x)|ϵ] ≤ E\\nx∼ bD\\n[|h(x) − f(x)|ϵ]+ 2rΛ√m\\n(√\\nTr[K]\\nmr2 +3\\n√\\nlog 2\\nδ\\n2\\n⎡\\n.\\nProof Let Hϵ = {x ↦→| h(x)− f(x)|ϵ : h ∈ H} and let H′ = {x ↦→ h(x)− f(x): h ∈\\nH}. Note that the function Φϵ : x ↦→| x|ϵ is 1-Lipschitz. Thus, by Talagrand’s lemma\\n(lemma 4.2), we have ˆRS(Hϵ) ≤ ˆRS(H′). By the proof of theorem 10.2, the equality\\nˆRS(H′)= ˆRS(H) holds, thus ˆRS(Hϵ) ≤ ˆRS(H).\\nAs in the proof of theorem 10.7, for allx ∈X and h ∈ H,w eh a v e|h(x) − f(x)|≤\\n2Λr and Rm(H) ≤\\n√\\nr2Λ2\\nm . By the general Rademacher complexity learning bound\\nof theorem 3.1, for any δ> 0, with with probability at least 1 − δ, the following\\nlearning bound holds with M =2 Λr:\\nE[|h(x) − f(x)|ϵ] ≤ ˆE[|h(x) − f(x)|ϵ]+2 Rm(H)+ M\\n√\\nlog 1\\nδ\\n2m .\\nUsing Rm(H) ≤\\n√\\nr2Λ2\\nm yields the ﬁrst statement of the theorem. The second\\nstatement is shown in a similar way.\\nThese results provide strong theoretical guarantees for the SVR algorithm. Note,\\nhowever, that the theorem does not provide guarantees for the expected loss of the\\nhypotheses in terms of the squared loss. For 0 <ϵ< 1/4, the inequality |x|\\n2 ≤| x|ϵ10.3 Regression algorithms 255\\nholds for all x in [−η′\\nϵ, −ηϵ] ∪ [ηϵ,η′\\nϵ]w i t hηϵ = 1− √1−4ϵ\\n2 and η′\\nϵ = 1+√1−4ϵ\\n2 .F o r\\nsmall values of ϵ, ηϵ ≈ 0a n dη′\\nϵ ≈ 1, thus, if M =2 rλ ≤ 1, then, the squared loss\\ncan be upper bounded by theϵ-insensitive loss for almost all values of (h(x) − f(x))\\nin [−1,1] and the theorem can be used to derive a useful generalization bound for\\nthe squared loss.\\nMore generally, if the objective is to achieve a small squared loss, then, SVR\\ncan be modiﬁed by using the quadratic ϵ-insensitive loss, that is the square of the\\nϵ-insensitive loss, which also leads to a convex QP. We will refer byquadratic SVR\\nto this version of the algorithm. Introducing the Lagrangian and applying the KKT\\nconditions leads to the following equivalent dual optimization problem for quadratic\\nSVR in terms of the kernel matrix K:\\nmax\\nα ,α ′\\n− ϵ(α′ + α)⊤1 +( α′ − α)⊤y − 1\\n2(α′ − α)⊤\\n(\\nK + 1\\nCI\\n⎡\\n(α′ − α)\\n(10.30)\\nsubject to: (α ≥ 0) ∧ (α ≥ 0) ∧ (α′ − α)⊤1 =0 ).\\nAny PDS kernelK can be used with quadratic SVR, which extends the algorithm to\\nnon-linear regression solutions. Problem (10.30) is a convex QP similar to the dual\\nproblem of SVMs in the separable case and can be solved using similar optimization\\ntechniques. The solutions α and α\\n′ deﬁne the hypothesis h returned by SVR as\\nfollows:\\nh(x)=\\nm∑\\ni=1\\n(α′\\ni − αi)K(xi,x)+ b, (10.31)\\nwhere the oﬀset b can be obtained from a point xj with 0 <α j <C or 0 <α ′\\nj <C\\nexactly as in the case of SVR with (non-quadratic) ϵ-insensitive loss. Note that for\\nϵ = 0, the quadratic SVR algorithm coincides with KRR as can be seen from the\\ndual optimization problem (the additional constraint (α′ − α)⊤1 = 0 appears here\\ndue to use of an oﬀset b). The following generalization bound holds for quadratic\\nSVR. It can be shown in a way that is similar to the proof of theorem 10.8 using\\nthe fact that the quadratic ϵ-insensitive function x ↦→| x|2\\nϵ is 2-Lipschitz.\\nTheorem 10.9\\nLet K : X× X → R be a PDS kernel, Φ: X→ H a feature mapping associated to\\nK,a n dH = {x ↦→ w · Φ(x): ∥w∥H ≤ Λ}. Assume that there exists r> 0 such that\\nK(x, x) ≤ r2 and |f(x)|≤ Λr for all x ∈X .F i xϵ> 0.T h e n ,f o ra n yδ> 0,w i t h256 Regression\\n-4 -2 0 2 4\\n0\\n2\\n4\\n6\\n8loss\\nx\\nx ↦→ max(0, |x|− ϵ)2\\nquadratic ε-insensitive\\nHuber\\nx ↦→\\n{\\nx2 if |x|≤ c\\n2c|x|− c2 otherwise.\\nε-insensitive\\nx ↦→ max(0, |x|− ϵ)\\nFigure 10.5 Alternative loss functions that can be used in conjunction with SVR.\\nprobability at least 1 − δ, each of the following inequalities holds for all h ∈ H:\\nE\\nx∼D\\n[|h(x) − f(x)|2\\nϵ] ≤ E\\nx∼ bD\\n[|h(x) − f(x)|2\\nϵ]+ 8r2Λ2\\n√m\\n⎛\\n⎝1+ 1\\n2\\n√\\nlog 1\\nδ\\n2\\n⎞\\n⎠\\nE\\nx∼D\\n[|h(x) − f(x)|2\\nϵ] ≤ E\\nx∼ bD\\n[|h(x) − f(x)|2\\nϵ]+ 8r2Λ2\\n√m\\n⎛\\n⎝\\n√\\nTr[K]\\nmr2 + 3\\n4\\n√\\nlog 2\\nδ\\n2\\n⎞\\n⎠ .\\nThis theorem provides a strong justiﬁcation for the quadratic SVR algorithm. Alter-\\nnative convex loss functions can be used to deﬁne regression algorithms, in particular\\nthe Huber loss (see ﬁgure 10.5), which penalizes smaller errors quadratically and\\nlarger ones only linearly.\\nSVR admits several advantages: the algorithm is based on solid theoretical\\nguarantees, the solution returned is sparse, and it allows a natural use of PDS\\nkernels, which extend the algorithm to non-linear regression solutions. SVR also\\nadmits favorable stability properties that we discuss in chapter 11. However, one\\ndrawback of the algorithm is that it requires the selection of two parameters, C\\nand ϵ. These can be selected via cross-validation, as in the case of SVMs, but this\\nrequires a relatively larger validation set. Some heuristics are often used to guide\\nthe search for their values: C is searched near the maximum value of the labels in\\nthe absence of an oﬀset (b = 0) and for a normalized kernel, andϵ is chosen close to\\nthe average diﬀerence of the labels. As already discussed, the value ofϵ determines\\nthe number of support vectors and the sparsity of the solution. Another drawback of\\nSVR is that, as in the case of SVMs or KRR, it may be computationally expensive\\nwhen dealing with large training sets. One eﬀective solution in such cases, as for\\nKRR, consists of approximating the kernel matrix using low-rank approximations\\nvia the Nystr¨om method or the partial Cholesky decomposition. In the next section,10.3 Regression algorithms 257\\nwe discuss an alternative sparse algorithm for regression.\\n10.3.4 Lasso\\nUnlike the KRR and SVR algorithms, the Lasso (least absolute shrinkage and\\nselection operator) algorithm does not admit a natural use of PDS kernels. Thus,\\nhere, we assume that the input space X is a subset of R\\nN and consider a family of\\nlinear hypotheses H = {x ↦→ w · x + b: w ∈ RN ,b ∈ R}.\\nLet S =\\n(\\n(x1,y1),..., (xm,y m)\\n⎡\\n∈ (X× Y )m be a labeled training sample.\\nLasso is based on the minimization of the empirical squared error on S with a\\nregularization term depending on the norm of the weight vector, as in the case of\\nthe ridge regression, but using the L\\n1 n o r mi n s t e a do ft h eL2 norm and without\\nsquaring the norm:\\nmin\\nw,b\\nF(w,b )= λ∥w∥1 +\\nm∑\\ni=1\\n(w · xi + b − yi)2 . (10.32)\\nHere λ denotes a positive parameter as for ridge regression. This is a convex\\noptimization problem, since ∥·∥1 is convex as with all norms and since the empirical\\nerror term is convex, as already discussed for linear regression. The optimization\\nfor Lasso can be written equivalently as\\nmin\\nw,b\\nm∑\\ni=1\\n(w · xi + b − yi)2 subject to: ∥w∥1 ≤ Λ1, (10.33)\\nwhere Λ1 is a positive parameter.\\nThe key property of Lasso as in the case of other algorithms using the L1\\nnorm constraint is that it leads to a sparse solution w, that is one with few\\nnon-zero components. Figure 10.6 illustrates the diﬀerence between the L1 and L2\\nregularizations in dimension two. The objective function of (10.33) is a quadratic\\nfunction, thus its contours are ellipsoids, as illustrated by the ﬁgure (in blue). The\\nareas corresponding to L1 and L2 balls of a ﬁxed radius Λ 1 are also shown in the\\nleft and right panel (in red). The Lasso solution is the point of intersection of the\\ncontours with the L1 ball. As can be seen form the ﬁgure, this can typically occur\\nat a corner of the L1 ball where some coordinates are zero. In contrast, the ridge\\nregression solution is at the point of intersection of the contours and the L2 ball,\\nw h e r en o n eo ft h ec o o r d i n a t e si st y p i c a l l yz e r o .\\nThe following results show that Lasso also beneﬁts from strong theoretical guar-\\nantees. We ﬁrst give a general upper bound on the empirical Rademacher complexity\\nof L\\n1 norm-constrained linear hypotheses .\\nTheorem 10.10 Rademacher complexity of linear hypotheses with bounded258 Regression\\nL1 regularization L2 regularization\\nFigure 10.6 Comparison of the Lasso and ridge regression solutions.\\nL1 norm\\nLet X⊆ RN and let S =\\n(\\n(x1,y1),..., (xm,y m)\\n⎡\\n∈ (X× Y )m be a sample of\\nsize m. Assume that for all i ∈ [1,m], ∥xi∥∞ ≤ r∞ for some r∞ > 0,a n dl e t\\nH = {x ∈X ↦→ w · x: ∥w∥1 ≤ Λ1}. Then, the empirical Rademacher complexity of\\nH can be bounded as follows:\\nˆRS(H) ≤\\n√\\n2r2∞ Λ2\\n1 log(2N)\\nm . (10.34)\\nProof For any i ∈ [1,m] we denote by xij the jth component of xi.\\nˆRS(H)= 1\\nm E\\nσ\\n[\\nsup\\n∥w∥1≤Λ1\\nm∑\\ni=1\\nσiw · xi\\n]\\n= Λ1\\nm E\\nσ\\n[\\ued79\\ued79\\ued79\\nm∑\\ni=1\\nσixi\\n\\ued79\\ued79\\ued79\\n∞\\n]\\n(by deﬁnition of the dual norm)\\n= Λ1\\nm E\\nσ\\n[\\nmax\\nj∈[1,N]\\n⏐⏐\\n⏐\\n⏐\\n⏐\\nm∑\\ni=1\\nσixij\\n⏐⏐\\n⏐\\n⏐\\n⏐\\n]\\n(by deﬁnition of ∥·∥\\n∞ )\\n= Λ1\\nm E\\nσ\\n[\\nmax\\nj∈[1,N]\\nmax\\ns∈{−1,+1}\\ns\\nm∑\\ni=1\\nσixij\\n]\\n(by deﬁnition of ∥·∥ ∞ )\\n= Λ1\\nm E\\nσ\\n[\\nsup\\nz∈A\\nm∑\\ni=1\\nσizi\\n]\\n,\\nwhere A denotes the set of N vectors {s(x1j,...,x mj)⊤ : j ∈ [1,N ],s ∈{ −1, +1}}.\\nFor any z ∈ A,w eh a v e∥z∥2 ≤\\n√\\nmr2∞ = r∞\\n√m. Thus, by Massart’s lemma10.3 Regression algorithms 259\\n(theorem 3.3), sinceA contains at most 2N elements, the following inequality holds:\\nˆRS(H) ≤ Λ1r∞\\n√m\\n√\\n2l o g( 2N)\\nm = r∞ Λ1\\n√\\n2l o g( 2N)\\nm ,\\nwhich concludes the proof.\\nNote that dependence of the bound on the dimension N is only logarithmic, which\\nsuggests that using very high-dimensional feature spaces does not signiﬁcantly aﬀect\\ngeneralization.\\nUsing the Rademacher complexity bound just proven and the general result of\\ntheorem 10.3, the following generalization bound can be shown to hold for the\\nhypothesis set used by Lasso, using the squared loss.\\nTheorem 10.11\\nLet X⊆ R\\nN and H = {x ∈X ↦→ w · x: ∥w∥1 ≤ Λ1}.A s s u m et h a tt h e r ee x i s t s\\nr∞ > 0 such for all x ∈X , ∥x∥∞ ≤ r∞ and |f(x)|≤ Λ1r∞ .T h e n ,f o ra n yδ> 0,\\nwith probability at least1 − δ, each of the following inequalities holds for all h ∈ H:\\nR(h) ≤ ˆR(h)+ 8r2\\n∞ Λ2\\n1√m\\n⎛\\n⎝√\\nlog(2N)+ 1\\n2\\n√\\nlog 1\\nδ\\n2\\n⎞\\n⎠ . (10.35)\\nProof For allx ∈X ,b yH ¨older’s inequality, we have|w·x|≤∥ w∥1∥x∥∞ ≤ Λ1r∞ ,\\nthus, for all h ∈ H, |h(x) − f(x)|≤ 2r∞ Λ1. Plugging in the inequality of\\ntheorem 10.10 in the bound of theorem 10.3 with M =2 r∞ Λ1 gives\\nR(h) ≤ ˆR(h)+8 r2\\n∞ Λ2\\n1\\n√\\n2l o g( 2N)\\nm +( 2r∞ Λ1)2\\n√\\nlog 1\\nδ\\n2m ,\\nwhich can be simpliﬁed and written as (10.35).\\nAs in the case of ridge regression, we observe that the objective function minimized\\nby Lasso has the same form as the right-hand side of this generalization bound.\\nThere exist a variety of diﬀerent methods for solving the optimization problem of\\nLasso, including an eﬃcient algorithm (Lars) for computing the entireregularization\\npath of solutions, that is, the Lasso solutions for all values of the regularization\\nparameter λ, and other on-line solutions that apply more generally to optimization\\nproblems with an L1 norm constraint.\\nHere, we show that the Lasso problems (10.32) or (10.33) are equivalent to a\\nquadratic program (QP), and therefore that any QP solver can be used to compute\\nthe solution. Observe that any weight vector w c a nb ew r i t t e na sw = w+ − w− ,\\nwith w+ ≥ 0, w− ≥ 0, and w+\\nj =0o r w−\\nj =0f o ra n y j ∈ [1,N ], which implies\\n∥w∥1 = ∑N\\nj=1 w+\\nj + w−\\nj . This can be done by deﬁning the jth component of w+ as\\nwj if wj ≥ 0, 0 otherwise, and similarly thejth component of w− as −wj if wj ≤ 0,260 Regression\\n0 otherwise, for any j ∈ [1,N ]. With the replacementw = w+ − w− ,w i t hw+ ≥ 0,\\nw− ≥ 0, and ∥w∥1 = ∑N\\nj=1 w+\\nj + w−\\nj , the Lasso problem (10.32) becomes\\nmin\\nw+≥0,w− ≥0,b\\nλ\\nN∑\\nj=1\\n(w+\\nj + w−\\nj )+\\nm∑\\ni=1\\n(\\n(w+ − w− ) · xi + b − yi\\n⎡2\\n. (10.36)\\nConversely, a solution w = w+ − w− of (10.36) veriﬁes the condition w+\\nj =0o r\\nw−\\nj =0f o ra n yj ∈ [1,N ], thus wj = w+\\nj when wj ≥ 0a n dwj = −w−\\nj when wj ≤ 0.\\nThis is because if δj =m i n (w+\\nj ,w −\\nj ) > 0f o rs o m ej ∈ [1,N ], replacing w+\\nj with\\n(w+\\nj − δj)a n dw−\\nj with (w−\\nj − δj)w o u l dn o ta ﬀ e c tw+\\nj − w−\\nj =( w+\\nj − δ) − (w−\\nj − δ),\\nbut would reduce the term ( w+\\nj + w−\\nj ) in the objective function by 2 δj > 0a n d\\nprovide a better solution. In view of this analysis, problems (10.32) and (10.36)\\nadmit the same optimal solution and are equivalent. Problem (10.36) is a QP since\\nthe objective function is quadratic in w\\n+, w− ,a n db, and since the constraints are\\naﬃne. With this formulation, the problem can be straightforwardly shown to admit\\na natural online algorithmic solution (exercise 10.10).\\n2\\nThus, Lasso has several advantages: it beneﬁts from strong theoretical guarantees\\nand returns a sparse solution, which is advantageous when there are accurate\\nsolutions based on few features. The sparsity of the solution is also computationally\\nattractive; sparse feature representations of the weight vector can be used to make\\nthe inner product with a new vector more eﬃcient. The algorithm’s sparsity can also\\nbe used for feature selection. The main drawback of the algorithm is that it does not\\nadmit a natural use of PDS kernels and thus an extension to non-linear regression,\\nunlike KRR and SVR. One solution is then to use empirical kernel maps, as discussed\\nin chapter 5. Also, Lasso’s solution does not admit a closed-form solution. This is\\nnot a critical property from the optimization point of view but one that can make\\nsome mathematical analyses very convenient.\\n10.3.5 Group norm regression algorithms\\nOther types of regularization aside from the L\\n1 or L2 norm can be used to deﬁne\\nregression algorithms. For instance, in some situations, the feature space may be\\nnaturally partitioned into subsets, and it may be desirable to ﬁnd a sparse solution\\nthat selects or omits entire subsets of features. A natural norm in this setting is\\nthe group or mixed norm L\\n2,1, which is a combination of the L1 and L2 norms.\\nImagine that we partition w ∈ RN as w1,..., wk,w h e r ewj ∈ RNj for 1 ≤ j ≤ k\\nand ∑\\nj Nj = N, and deﬁne W =( w⊤\\n1 ,..., w⊤\\nk )⊤.T h e nt h eL2,1 norm of W is\\n2. The technique we described to avoid absolute values in the objective function can be\\nused similarly in other optimization problems.10.3 Regression algorithms 261\\nWidrowHoff(w0)\\n1 w1 ← w0 ⊿ typically w0 = 0\\n2 for t ← 1 to T do\\n3 Receive(xt)\\n4 ˆyt ← wt · xt\\n5 Receive(yt)\\n6 wt+1 ← wt +2 η(wt · xt − yt)xt ⊿ learning rate η> 0.\\n7 return wT+1\\nFigure 10.7 The Widrow-Hoﬀ algorithm.\\ndeﬁned as\\n∥W∥2,1 =\\nk∑\\nj=1\\n∥wj ∥ .\\nCombining the L2,1 norm with the empirical mean squared error leads to theGroup\\nLasso formulation. More generally, an Lq,p group norm regularization can be used\\nfor q,p ≥ 1 (see appendix A for the deﬁnition of group norms).\\n10.3.6 On-line regression algorithms\\nThe regression algorithms presented in the previous sections admit natural on-\\nline versions. Here, we brieﬂy present two examples of these algorithms. These\\nalgorithms are particularly useful for applications to very large data sets for which\\na batch solution can be computationally too costly to derive and more generally in\\nall of the on-line learning settings discussed in chapter 7.\\nOur ﬁrst example is known as the Widrow-Hoﬀ algorithm and coincides with\\nthe application of stochastic gradient descent techniques to the linear regression\\nobjective function. Figure 10.7 gives the pseudocode of the algorithm. A similar\\nalgorithm can be derived by applying the stochastic gradient technique to ridge\\nregression. At each round, the weight vector is augmented with a quantity that\\ndepends on the prediction error (w\\nt · xt − yt).\\nOur second example is an online version of the SVR algorithm, which is obtained\\nby application of stochastic gradient descent to the dual objective function of SVR.\\nFigure 10.8 gives the pseudocode of the algorithm for an arbitrary PDS kernel K\\nin the absence of any oﬀset (b = 0). Another on-line regression algorithm is given262 Regression\\nOnLineDualSVR()\\n1 α ← 0\\n2 α′ ← 0\\n3 for t ← 1 to T do\\n4 Receive(xt)\\n5 ˆyt ← ∑T\\ns=1(α′\\ns − αs)K(xs,xt)\\n6 Receive(yt)\\n7 α′\\nt+1 ← α′\\nt + min(max(η(yt − ˆyt − ϵ), −α′\\nt),C − α′\\nt)\\n8 αt+1 ← αt + min(max(η(ˆyt − yt − ϵ), −αt),C − αt)\\n9 return ∑T\\nt=1 αtK(xt, ·)\\nFigure 10.8 An on-line version of dual SVR.\\nby exercise 10.10 for Lasso.\\n10.4 Chapter notes\\nThe generalization bounds presented in this chapter are for bounded regression\\nproblems. When {x ↦→ L(h(x),f (x)): h ∈ H}, the family of losses of the hypotheses,\\nis not bounded, a single function can take arbitrarily large values with arbitrarily\\nsmall probabilities. This is the main issue for deriving uniform convergence bounds\\nfor unbounded losses. This problem can be avoided either by assuming the existence\\nof an envelope, that is a single non-negative function with a ﬁnite expectation lying\\nabove the absolute value of the loss of every function in the hypothesis set [Dudley,\\n1984, Pollard, 1984, Dudley, 1987, Pollard, 1989, Haussler, 1992], or by assuming\\nthat some moment of the loss functions is bounded [Vapnik, 1998, 2006]. Cortes,\\nMansour, and Mohri [2010a] give two-sided generalization bounds for unbounded\\nlosses with ﬁnite second moments. The one-sided version of their bounds coincides\\nwith that of Vapnik [1998, 2006] modulo a constant factor, but the proofs given by\\nVapnik in both books seem to be incorrect.\\nThe Rademacher complexity bounds given for regression in this chapter (theo-\\nrem 10.2) are novel. The notion of pseudo-dimension is due to Pollard [1984]. Its\\nequivalent deﬁnition in terms of VC-dimension is discussed by Vapnik [2000]. The\\nnotion of fat-shattering was introduced by Kearns and Schapire [1990]. The linear\\nregression algorithm is a classical algorithm in statistics that dates back at least to10.5 Exercises 263\\nthe nineteenth century. The ridge regression algorithm is due to Hoerl and Kennard\\n[1970]. Its kernelized version (KRR) was introduced and discussed by Saunders,\\nGammerman, and Vovk [1998]. An extension of KRR to outputs in R\\np with p> 1\\nwith possible constraints on the regression is presented and analyzed by Cortes,\\nMohri, and Weston [2007c]. The support vector regression (SVR) algorithm is dis-\\ncussed in Vapnik [2000]. Lasso was introduced by Tibshirani [1996]. The LARS\\nalgorithm for solving its optimization problem was later presented by Efron et al.\\n[2004]. The Widrow-Hoﬀ on-line algorithm is due to Widrow and Hoﬀ [1988]. The\\ndual on-line SVR algorithm was ﬁrst introduced and analyzed by Vijayakumar and\\nWu [1999]. The kernel stability analysis of exercise 9.3 is from Cortes et al. [2010b].\\nFor large-scale problems where a straightforward batch optimization of a primal or\\ndual objective function is intractable, general iterative stochastic gradient descent\\nmethods similar to those presented in section 10.3.6, or quasi-Newton methods\\nsuch as the limited-memory BFGS (Broyden-Fletcher-Goldfard-Shanno) algorithm\\n[Nocedal, 1980] can be practical alternatives in practice.\\nIn addition to the linear regression algorithms presented in this chapter and their\\nkernel-based non-linear extensions, there exist many other algorithms for regression,\\nincluding decision trees for regression (see chapter 8), boosting trees for regression,\\nand artiﬁcial neural networks.\\n10.5 Exercises\\n10.1 Pseudo-dimension and monotonic functions.\\nAssume that φ is a strictly monotonic function and let φ ◦H be the family of\\nfunctions deﬁned by φ ◦H = {φ(h(·)) : h ∈ H},w h e r eH is some set of real-valued\\nfunctions. Show that Pdim(φ ◦H)=P d i m (H).\\n10.2 Pseudo-dimension of linear functions. Let H be the set of all linear functions\\nin dimension d, i.e. h(x)= w⊤x for some w ∈ Rd. Show that Pdim(H)= d.\\n10.3 Linear regression.\\n(a) What condition is required on the data X in order to guarantee thatXX⊤\\nis invertible?\\n(b) Assume the problem is under-determined. Then, we can choose a solution\\nw such that the equality X⊤w = X⊤(XX⊤)† Xy (which can be shown to\\nequal X† Xy) holds. One particular choice that satisﬁes this equality is w∗ =\\n(XX⊤)† Xy. However, this is not the unique solution. As a function of w∗,\\ncharacterize all choices of w that satisfy X⊤w = X† Xy (Hint: use the fact264 Regression\\nthat XX† [X = X).\\n10.4 Perturbed kernels. Suppose two diﬀerent kernel matrices,K and K′,a r eu s e dt o\\ntrain two kernel ridge regression hypothesis with the same regularization parameter\\nλ. In this problem, we will show that the diﬀerence in the optimal dual variables,\\nα and α\\n′ respectively, is bounded by a quantity that depends on ∥K′ − K∥2.\\n(a) Show α′ − α =\\n(\\n(K′ + λI)−1(K′ − K)(K + λI)−1⎡\\ny.( Hint:S h o wt h a tf o r\\nany invertible matrix M, M′1 − M1 = −M′−1(M′ − M)M−1.)\\n(b) Assuming ∀y ∈Y , |y|≤ M, show that\\n∥α′ − α∥≤\\n√mM ∥K′ − K∥2\\nλ2 .\\n10.5 Huber loss. Derive the primal and dual optimization problem used to solve the\\nSVR problem with the Huber loss:\\nLc(ξi)=\\n{\\n1\\n2 ξ2\\ni , if |ξi|≤ c\\ncξi − 1\\n2 c2, otherwise\\n,\\nwhere ξi = w · Φ(xi)+ b − yi.\\n10.6 SVR and squared loss. Assuming that 2 rΛ ≤ 1, use theorem 10.8 to derive a\\ngeneralization bound for the squared loss.\\n10.7 SVR dual formulations. Give a detailed and carefully justiﬁed derivation of\\nthe dual formulations of the SVR algorithm both for the ϵ-insensitive loss and the\\nquadratic ϵ-insensitive loss.\\n10.8 Optimal kernel matrix. Suppose in addition to optimizing the dual variables\\nα ∈ Rm, as in (10.19), we also wish to optimize over the entries of the PDS kernel\\nmatrix K ∈ Rm×m.\\nmin\\nK⪰0\\nmax\\nα\\n−λα⊤α − α⊤Kα +2 α⊤y , s.t. ∥K∥2 ≤ 1\\n(a) What is the closed-form solution for the optimal K for the joint optimiza-\\ntion?\\n(b) Optimizing over the choice of kernel matrix will provide a better value of\\nthe objective function. Explain, however, why the resulting kernel matrix is not\\nuseful in practice.10.5 Exercises 265\\nOnLineLasso(w+\\n0 ,w−\\n0 )\\n1 w+\\n1 ← w+\\n0 ⊿ w+\\n0 ≥ 0\\n2 w−\\n1 ← w−\\n0 ⊿ w−\\n0 ≥ 0\\n3 for t ← 1 to T do\\n4 Receive(xt,y t)\\n5 for j ← 1 to N do\\n6 w+\\nt+1j ← max\\n(\\n0,w +\\ntj − η\\n[\\nλ −\\n[\\nyt − (w+\\nt − w−\\nt ) · xt\\n]\\nxtj\\n]⎡\\n7 w−\\nt+1j ← max\\n(\\n0,w −\\ntj − η\\n[\\nλ+\\n[\\nyt − (w+\\nt − w−\\nt ) · xt\\n]\\nxtj\\n]⎡\\n8 return w+\\nT+1 − w−\\nT+1\\nFigure 10.9 On-line algorithm for Lasso.\\n10.9 Leave-one-out error. In general, the computation of the leave-one-out error\\ncan be very costly since, for a sample of size m, it requires training the algorithm\\nm times. The objective of this problem is to show that, remarkably, in the case\\nof kernel ridge regression, the leave-one-out error can be computed eﬃciently by\\ntraining the algorithm only once.\\nLet S =( (x1,y1),..., (xm,y m)) denote a training sample of size m and for any\\ni ∈ [1,m], let Si denote the sample of size m − 1 obtained from S by removing\\n(xi,y i): Si = S −{ (xi,y i)}. For any sampleT,l e thT denote a hypothesis obtained\\nby training T. By deﬁnition (see deﬁnition 4.1), for the squared loss, the leave-one-\\nout error with respect to S is deﬁned by\\nˆRLOO(KRR) = 1\\nm\\nm∑\\ni=1\\n(hSi (xi) − yi)2 .\\n(a) Let S′\\ni =( (x1,y1),..., (xi,h Si (yi)),..., (xm,y m)). Show that hSi = hS′\\ni\\n.\\n(b) Deﬁne yi = y − yiei + hSi (xi)ei, that is the vector of labels with the\\nith component replaced with hSi (xi). Prove that for KRR hSi (xi)= y⊤\\ni (K +\\nλI)−1Kei.\\n(c) Prove that the leave-one-out error admits the following simple expression\\nin terms of hS:\\nˆRLOO(KRR) = 1\\nm\\nm∑\\ni=1\\n[ hS(xi) − yi\\ne⊤\\ni (K + λI)−1Kei\\n]2\\n. (10.37)266 Regression\\n(d) Suppose that the diagonal entries of matrixM =( K+λI)−1K are all equal\\nto γ. How do the empirical error ˆR of the algorithm and the leave-one-out error\\nˆRLOO relate? Is there any value of γ for which the two errors coincide?\\n10.10 On-line Lasso. Use the formulation (10.36) of the optimization problem of\\nLasso and stochastic gradient descent (see section 7.3.1) to show that the problem\\ncan be solved using the on-line algorithm of ﬁgure 10.9.\\n10.11 On-line quadratic SVR. Derive an on-line algorithm for the quadratic SVR\\nalgorithm (provide the full pseudocode).11 Algorithmic Stability\\nIn chapters 2–4 and several subsequent chapters, we presented a variety of general-\\nization bounds based on diﬀerent measures of the complexity of the hypothesis set\\nH used for learning, including the Rademacher complexity, the growth function,\\nand the VC-dimension. These bounds ignore the speciﬁc algorithm used, that is,\\nthey hold for any algorithm using H as a hypothesis set.\\nOne may ask if an analysis of the properties of a speciﬁc algorithm could lead\\nto ﬁner guarantees. Such an algorithm-dependent analysis could have the beneﬁt\\nof a more informative guarantee. On the other hand, it could be inapplicable to\\no t h e ra l g o r i t h m su s i n gt h es a m eh y p o t h e s i ss e t .A l t e r n a t i v e l y ,a sw es h a l ls e ei n\\nthis chapter, a more general property of the learning algorithm could be used to\\nincorporate algorithm-speciﬁc properties while extending the applicability of the\\nanalysis to other learning algorithms with similar properties.\\nThis chapter uses the property of algorithmic stability to derive algorithm-\\ndependent learning guarantees. We ﬁrst present a generalization bound for any\\nalgorithm that is suﬃciently stable. Then, we show that the wide class of kernel-\\nbased regularization algorithms enjoys this property and derive a general upper\\nbound on their stability coeﬃcient. Finally, we illustrate the application of these\\nresults to the analysis of several algorithms both in the regression and classiﬁcation\\nsettings, including kernel ridge regression (KRR), SVR, and SVMs.\\n11.1 Deﬁnitions\\nWe start by introducing the notation and deﬁnitions relevant to our analysis of\\nalgorithmic stability. We denote by z a labeled example ( x, y) ∈X× Y .T h e\\nhypotheses h we consider map X to a set Y\\n′ sometimes diﬀerent from Y.I n\\nparticular, for classiﬁcation, we may have Y = {−1, +1} while the hypothesis h\\nlearned takes values in R. The loss functions L we consider are therefore deﬁned\\nover Y′ ×Y ,w i t hY′ = Y in most cases. For a loss function L: Y′ ×Y → R+,w e\\ndenote the loss of a hypothesis h at point z by Lz(h)= L(h(x),y ). We denote by\\nD the distribution according to which samples are drawn and byH the hypothesis268 Algorithmic Stability\\nset. The empirical error or loss of h ∈ H on a sample S =( z1,...,z m)a n di t s\\ngeneralization error are deﬁned, respectively, by\\nˆR(h)= 1\\nm\\nm∑\\ni=1\\nLzi (h)a n d R(h)= E\\nz∼D\\n[Lz(h)].\\nG i v e na na l g o r i t h mA,w ed e n o t eb yhS the hypothesis hS ∈ H returned by A when\\ntrained on sample S.W ew i l ls a yt h a tt h el o s sf u n c t i o nL is bounded by M ≥ 0i f\\nfor all h ∈ H and z ∈X×Y , Lz(h) ≤ M. For the results presented this chapter, a\\nweaker condition suﬃces, namely that Lz(hS) ≤ M for all hypotheses hS returned\\nby the algorithm A considered.\\nWe are now able to deﬁne the notion of uniform stability, the algorithmic property\\nu s e di nt h ea n a l y s e so ft h i sc h a p t e r .\\nDeﬁnition 11.1 Uniform stability\\nLet S and S′ be any two training samples that diﬀer by a single point. Then, a\\nlearning algorithm A is uniformly β-stable if the hypotheses it returns when trained\\non any such samples S and S′ satisfy\\n∀z ∈Z , |Lz(hS) − Lz(hS′ )|≤ β.\\nThe smallest such β satisfying this inequality is called the stability coeﬃcient of A.\\nIn other words, whenA is trained on two similar training sets, the losses incurred by\\nthe corresponding hypotheses returned byA should not diﬀer by more thanβ. Note\\nthat a uniformlyβ-stable algorithm is often referred to as beingβ-stable or even just\\nstable (for some unspeciﬁed β). In general, the coeﬃcient β depends on the sample\\nsize m. We will see in section 11.2 thatβ = o(1/√m) is necessary for the convergence\\nof the stability-based learning bounds presented in this chapter. In section 11.3, we\\nwill show that a more favorable condition holds, that is, β = O(1/m), for a wide\\nfamily of algorithms.\\n11.2 Stability-based generalization guarantee\\nIn this section, we show that exponential bounds can be derived for the generaliza-\\ntion error of stable learning algorithms. The main result is presented in theorem 11.1.\\nTheorem 11.1\\nAssume that the loss function L is bounded by M ≥ 0.L e tA be a β-stable learning\\nalgorithm and letS be a sample ofm points drawn i.i.d. according to distributionD.11.2 Stability-based generalization guarantee 269\\nThen, with probability at least 1 − δover the sample S drawn, the following holds:\\nR(hS) ≤ ˆR(hS)+ β +( 2mβ + M)\\n√\\nlog 1\\nδ\\n2m .\\nProof The proof is based on the application of McDiarmid’s inequality (theo-\\nrem D.3) to the function Φ deﬁned for all samples S by Φ(S)= R(hS) − ˆR(hS). Let\\nS′ be another sample of size m with points drawn i.i.d. according to D that diﬀers\\nfrom S by exactly one point. We denote that point by zm in S, z′\\nm in S′, i.e.,\\nS =( z1,...,z m−1,z m)a n d S′ =( z1,...,z m−1,z ′\\nm).\\nBy deﬁnition of Φ, the following inequality holds:\\n|Φ(S′) − Φ(S)|≤| R(hS′ ) − R(hS)| + | ˆR(hS′ ) − ˆR(hS)|. (11.1)\\nWe bound each of these two terms separately. By theβ-stability of A,w eh a v e\\n|R(hS) − R(hS′ )| = | E\\nz\\n[Lz(hS)] − E\\nz\\n[Lz(hS′ )]|≤ E\\nz\\n[|Lz(hS) − Lz(hS′ )|] ≤ β.\\nUsing the boundedness of L along with β-stability of A,w ea l s oh a v e\\n| ˆR(hS) − ˆR(hS′ )| = 1\\nm\\n⏐⏐⏐⏐\\n⏐\\n(\\nm−1∑\\ni=1\\nLzi (hS) − Lzi (hS′ )\\n⎡\\n+ Lzm (hS) − Lz′\\nm (hS′ )\\n⏐⏐⏐⏐\\n⏐\\n≤ 1\\nm\\n[( m−1∑\\ni=1\\n|Lzi (hS) − Lzi (hS′ )|\\n⎡\\n+ |Lzm (hS) − Lz′\\nm (hS′ )|\\n]\\n≤ m − 1\\nm β + M\\nm ≤ β + M\\nm .\\nThus, in view of (11.1), Φ satisﬁes the condition |Φ(S) − Φ(S′)|≤ 2β + M\\nm .B y\\napplying McDiarmid’s inequality to Φ( S), we can bound the deviation of Φ from\\nits mean as\\nPr\\n[\\nΦ(S) ≥ ϵ +E\\nS\\n[Φ(S)]\\n]\\n≤ exp\\n( −2mϵ2\\n(2mβ + M)2\\n⎡\\n,\\nor, equivalently, with probability 1− δ,\\nΦ(S) <ϵ +E\\nS\\n[Φ(S)], (11.2)\\nwhere δ=e x p\\n(\\n−2mϵ2\\n(2mβ+M)2\\n⎡\\n. If we solve forϵ in this expression forδ, plug into (11.2)270 Algorithmic Stability\\nand rearrange terms, then, with probability 1 − δ,w eh a v e\\nΦ(S) ≤ E\\nS∼Dm\\n[Φ(S) ]+( 2mβ + M)\\n√\\nlog 1\\nδ\\n2m . (11.3)\\nWe now bound the expectation term, ﬁrst noting that by linearity of expectation\\nES[Φ(S)] = ES[R(hS)] − ES[ ˆR(hS)]. By deﬁnition of the generalization error,\\nE\\nS∼Dm\\n[R(hS)] = E\\nS∼Dm\\n[\\nE\\nz∼D\\n[Lz(hS)]\\n]\\n=E\\nS,z∼Dm+1\\n[Lz(hS)]. (11.4)\\nBy the linearity of expectation,\\nE\\nS∼Dm\\n[ ˆR(hS)] = 1\\nm\\nm∑\\ni=1\\nE\\nS∼Dm\\n[Lzi (hS)] = E\\nS∼Dm\\n[Lz1 (hS)], (11.5)\\nwhere the second equality follows from the fact that thezi are drawn i.i.d. and thus\\nthe expectations ES∼Dm [Lzi (hS)], i ∈ [1,m], are all equal. The last expression in\\n(11.5) is the expected loss of a hypothesis on one of its training points. We can\\nrewrite it as E S∼Dm [Lz1 (hS) ]=E S,z∼Dm+1 [Lz(hS′ )], where S′ is a sample of m\\npoints containing z extracted from the m +1p o i n t sf o r m e db yS and z.T h u s ,i n\\nview of (11.4) and by the β-stability of A, it follows that\\n| E\\nS∼Dm\\n[Φ(S)]| =\\n⏐⏐ E\\nS,z∼Dm+1\\n[Lz(hS)] − E\\nS,z∼Dm+1\\n[Lz(hS′ )]\\n⏐⏐\\n≤ E\\nS,z∼Dm+1\\n[\\n|Lz(hS) − Lz(hS′ )|\\n]\\n≤ E\\nS,z∼Dm+1\\n[β]= β.\\nWe can thus replace ES[Φ(S)] by β in (11.3), which completes the proof.\\nThe bound of the theorem converges for (mβ)/√m = o(1), that is β = o(1/√m). In\\nparticular, when the stability coeﬃcientβ is inO(1/m), the theorem guarantees that\\nR(hS)− ˆR(hS)= O(1/√m) with high probability. In the next section, we show that\\nkernel-based regularization algorithms precisely admit this property under some\\ngeneral assumptions.\\n11.3 Stability of kernel-based regularization algorithms\\nLet K be a positive deﬁnite symmetric kernel, H the reproducing kernel Hilbert\\nspace associated to K,a n d ∥·∥ K the norm induced by K in H.Ak e r n e l - b a s e d\\nregularization algorithm is deﬁned by the minimization over H of an objective\\nfunction FS based on a training sample S =( z1,...,z m) and deﬁned for all h ∈ H11.3 Stability of kernel-based regularization algorithms 271\\nx y\\n}\\nF(y)\\nBF (y||x)\\nF(x)+( y − x) ·∇ F(x)\\nFigure 11.1 Illustration of the quantity measured by the Bregman divergence\\ndeﬁned based on a convex and diﬀerentiable function F. The divergence measures\\nthe distance between F(y) and the hyperplane tangent to the curve at point x.\\nby:\\nFS(h)= ˆRS(h)+ λ∥h∥2\\nK. (11.6)\\nIn this equation, ˆRS(h)= 1\\nm\\n∑m\\ni=1 Lzi (h) is the empirical error of hypothesish with\\nrespect to a loss functionL and λ ≥ 0 a trade-oﬀ parameter balancing the emphasis\\non the empirical error versus the regularization term ∥h∥2\\nK. The hypothesis set H\\nis the subset of H formed by the hypotheses possibly returned by the algorithm.\\nAlgorithms such as KRR, SVR and SVMs all fall under this general model.\\nWe ﬁrst introduce some deﬁnitions and tools needed for a general proof of an\\nupper bound on the stability coeﬃcient of kernel-based regularization algorithms.\\nOur analysis will assume that the loss function L i sc o n v e xa n dt h a ti tf u r t h e r\\nveriﬁes the following Lipschitz-like smoothness condition.\\nDeﬁnition 11.2 σ-admissibility\\nA loss function L is σ-admissible with respect to the hypothesis class H if there\\nexists σ ∈ R\\n+ such that for any two hypothesesh, h′ ∈ H and for all (x, y) ∈X× Y ,\\n|L(h′(x),y ) − L(h(x),y )|≤ σ|h′(x) − h(x)|. (11.7)\\nThis assumption holds for the quadratic loss and most other loss functions where\\nthe hypothesis set and the set of output labels are bounded by some M ∈ R+:\\n∀h ∈ H, ∀x ∈X , |h(x)|≤ M and ∀y ∈Y , |y|≤ M.\\nWe will use the notion of Bregman divergence, BF which can be deﬁned for any\\nconvex and diﬀerentiable functionF : H → R as follows: for all f,g ∈ H,\\nBF (f ∥g)= F(f) − F(g) −⟨ f − g, ∇F(g)⟩ .\\nFigure 11.1 illustrates the geometric interpretation of the Bregman divergence. We\\ngeneralize this deﬁnition to cover the case of convex but non-diﬀerentiable loss272 Algorithmic Stability\\n}\\nδF(h)\\nF(h)\\nFigure 11.2 Illustration of the notion of sub-gradient: elements of the subgradient\\nset ∂F(h) are shown in red at point h, for the function F s h o w ni nb l u e .\\nfunctions F by using the notion of subgradient. For a convex function F : H → R,\\nwe denote by ∂F(h) the subgradient of F at h, which is deﬁned as follows:\\n∂F(h)= {g ∈ H: ∀h′ ∈ H,F (h′) − F(h) ≥⟨ h′ − h, g⟩}.\\nThus, ∂F(h)i st h es e to fv e c t o r sg deﬁning a hyperplane supporting function F at\\npoint h (see ﬁgure 11.2). ∂F(h)c o i n c i d e sw i t h∇F(h)w h e nF is diﬀerentiable ath,\\ni.e. ∂F(h)= {∇F(h)}. Note that at a point h where F is minimal, 0 is an element\\nof ∂F(h). Furthermore, the subgradient is additive, that is, for two convex function\\nF1 and F2, ∂(F1 + F2)(h)= {g1 + g2 : g1 ∈ ∂F1(h),g2 ∈ ∂F2(h)}. For any h ∈ H,\\nwe ﬁx δF(h) to be an (arbitrary) element of ∂F(h). For any such choice of δF,w e\\ncan deﬁne the generalized Bregman divergenceassociated to F by:\\n∀h′,h ∈ H,B F (h′∥h)= F(h′) − F(h) −⟨ h′ − h, δF(h)⟩ . (11.8)\\nNote that by deﬁnition of the subgradient, BF (h′∥h) ≥ 0 for all h′,h ∈ H.\\nStarting from (11.6), we can now deﬁne the generalized Bregman divergence\\nof FS.L e t N denote the convex function h →∥ h∥2\\nK.S i n c eN is diﬀerentiable,\\nδN(h)= ∇N(h) for all h ∈ H,a n d δN and thus BN is uniquely deﬁned. To\\nmake the deﬁnition of the Bregman divergences for FS and ˆRS compatible so that\\nBFS = B bRS\\n+λBN ,w ed e ﬁ n eδˆRS in terms of δFS by: δˆRS(h)= δFS(h) − λ∇N(h)\\nfor all h ∈ H.F u r t h e r m o r e ,w ec h o o s eδFS(h)t ob e0f o ra n yp o i n th where FS is\\nminimal and let δFS(h) be an arbitrary element of ∂FS(h) for all other h ∈ H.W e\\nproceed in a similar way to deﬁne the Bregman divergences forFS′ and ˆRS′ so that\\nBFS′ = B bRS′ + λBN .\\nWe will use the notion of generalized Bregman divergence for the proof of the fol-\\nlowing general upper bound on the stability coeﬃcient of kernel-based regularization\\nalgorithms.11.3 Stability of kernel-based regularization algorithms 273\\nProposition 11.1\\nLet K be a positive deﬁnite symmetric kernel such that for all x ∈X , K(x, x) ≤ r2\\nfor some r ∈ R+ and let L be a convex and σ-admissible loss function. Then, the\\nkernel-based regularization algorithm deﬁned by the minimization (11.6) is β-stable\\nwith the following upper bound on β:\\nβ ≤ σ2r2\\nmλ .\\nProof Let h be a minimizer ofFS and h′ a minimizer ofFS′ ,w h e r es a m p l e sS and\\nS′ diﬀer exactly by one point,zm in S and z′\\nm in S′. Since the generalized Bregman\\ndivergence is non-negative and since BFS = B bRS\\n+ λBN and BFS′ = B bRS′ + λBN ,\\nwe can write\\nBFS (h′∥h)+ BFS′ (h∥h′) ≥ λ\\n(\\nBN (h′∥h)+ BN (h∥h′)\\n⎡\\n.\\nObserve that BN (h′∥h)+ BN (h∥h′)= −⟨ h′ − h, 2h⟩−⟨ h − h′, 2h′⟩ =2 ∥h′ − h∥2\\nK.\\nLet Δ h denote h′ − h, then we can write\\n2λ||Δ h∥2\\nK\\n≤ BFS (h′||h)+ BFS′ (h||h′)\\n= FS(h′) − FS(h) −⟨ h′ − h, δFS(h)⟩ + FS′ (h) − FS′ (h′) −⟨ h − h′,δFS′ (h′)⟩\\n= FS(h′) − FS(h)+ FS′ (h) − FS′ (h′)\\n= ˆRS(h′) − ˆRS(h)+ ˆRS′ (h) − ˆRS′ (h′).\\nThe second equality follows from the deﬁnition of h′ and h as minimizers and our\\nchoice of the subgradients for minimal points which together imply δFS′ (h′)=0\\nand δFS(h) = 0. The last equality follows from the deﬁnitions ofFS and FS′ .N e x t ,\\nwe express the resulting inequality in terms of the loss function L and use the fact\\nthat S and S′ diﬀer by only one point along with the σ-admissibility of L to get\\n2λ∥Δ h∥2\\nK ≤ 1\\nm[Lzm (h′) − Lzm (h)+ Lz′m (h) − Lz′m (h′)]\\n≤ σ\\nm[|Δ h(xm)| + |Δ h(x′\\nm)|]. (11.9)\\nBy the reproducing kernel property and the Cauchy-Schwarz inequality , for all\\nx ∈X ,\\nΔ h(x)= ⟨Δ h, K(x, ·)⟩≤∥ Δ h∥K ∥K(x, ·)∥K =\\n√\\nK(x, x)∥Δ h∥K ≤ r∥Δ h∥K.\\nIn view of (11.9), this implies ∥Δ h∥K ≤ σr\\nλm .B yt h eσ-admissibility of L and the\\nreproducing property, the following holds:\\n∀z ∈ X × Y, |Lz(h′) − Lz(h)|≤ σ|Δ h(x)|≤ rσ∥Δ h∥K,274 Algorithmic Stability\\nwhich gives\\n∀z ∈ X × Y, |Lz(h′) − Lz(h)|≤ σ2r2\\nmλ ,\\nand concludes the proof.\\nThus, under the assumptions of the proposition, for a ﬁxedλ, the stability coeﬃcient\\nof kernel-based regularization algorithms is in O(1/m).\\n11.3.1 Application to regression algorithms: SVR and KRR\\nHere, we analyze more speciﬁcally two widely used regression algorithms, Support\\nVector Regression (SVR) and Kernel Ridge Regression (KRR), which are both\\nspecial instances of the family of kernel-based regularization algorithms.\\nSVR is based on the ϵ-insensitive loss L\\nϵ deﬁned for all ( y,y ′) ∈Y×Y by:\\nLϵ(y′,y )=\\n{\\n0i f |y′ − y|≤ ϵ;\\n|y′ − y|− ϵ otherwise.\\n(11.10)\\nWe now present a stability-based bound for SVR assuming that Lϵ is bounded for\\nthe hypotheses returned by SVR (which, as we shall later see in lemma 11.1, is\\ni n d e e dt h ec a s ew h e nt h el a b e ls e tY is bounded).\\nCorollary 11.1 Stability-based learning bound for SVR\\nAssume that K(x, x) ≤ r\\n2 for all x ∈X for some r ≥ 0 and that Lϵ is bounded\\nby M ≥ 0.L e t hS denote the hypothesis returned by SVR when trained on an\\ni.i.d. sample S of size m.T h e n ,f o ra n yδ> 0, the following inequality holds with\\nprobability at least 1 − δ:\\nR(hS) ≤ ˆR(hS)+ r2\\nmλ +\\n(2r2\\nλ + M\\n⎡\\n√\\nlog 1\\nδ\\n2m .\\nProof We ﬁrst show that Lϵ(·)= Lϵ(·,y )i s1 - L i p s c h i t zf o ra n yy ∈Y . For any\\ny′,y ′′ ∈Y , we must consider four cases. First, if |y′ − y|≤ ϵ and |y′′ − y|≤ ϵ,\\nthen |Lϵ(y′′) − Lϵ(y′)| =0 .S e c o n d ,i f|y′ − y| >ϵ and |y′′ − y| >ϵ ,t h e n\\n|Lϵ(y′′) − Lϵ(y′)| = ||y′′ − y|−| y′ − y| |≤| y′′ − y′|, by the triangle inequality.\\nThird, if |y′ − y|≤ ϵ and |y′′ − y| >ϵ ,t h e n|Lϵ(y′′) − Lϵ(y′)| = ||y′′ − y|− ϵ| =\\n|y′′ − y|− ϵ ≤| y′′ − y|−| y′ − y|≤| y′′ − y′|.F o u r t h ,i f|y′′ − y|≤ ϵ and |y′ − y| >ϵ ,\\nby symmetry the same inequality is obtained as in the previous case.\\nThus, in all cases,|Lϵ(y′′,y )−Lϵ(y′,y )|≤| y′′ −y′|. This implies in particular that\\nLϵ is σ-admissible with σ = 1 for any hypothesis setH. By proposition 11.1, under\\nthe assumptions made, SVR is β-stable with β ≤ r2\\nmλ . Plugging this expression into\\nthe bound of theorem 11.1 yields the result.11.3 Stability of kernel-based regularization algorithms 275\\nWe next present a stability-based bound for KRR, which is based on the square\\nloss L2 deﬁned for all y′,y ∈Y by:\\nL2(y′,y )=( y′ − y)2. (11.11)\\nAs in the SVR setting, we assume in our analysis that L2 is bounded for the\\nhypotheses returned by KRR (which, as we shall later see again in lemma 11.1,\\nis indeed the case when the label set Y is bounded).\\nCorollary 11.2 Stability-based learning bound for KRR\\nAssume that K(x, x) ≤ r2 for all x ∈X for some r ≥ 0 and that L2 is bounded\\nby M ≥ 0.L e t hS denote the hypothesis returned by KRR when trained on an\\ni.i.d. sample S of size m.T h e n ,f o ra n yδ> 0, the following inequality holds with\\nprobability at least 1 − δ:\\nR(hS) ≤ ˆR(hS)+ 4Mr2\\nλm +\\n(8Mr2\\nλ + M\\n⎡\\n√\\nlog 1\\nδ\\n2m .\\nProof For any (x, y) ∈X×Y and h, h′ ∈ H,\\n|L2(h′(x),y ) − L2(h(x),y )| =\\n⏐⏐(h′(x) − y)2 − (h(x) − y)2⏐⏐\\n=\\n⏐⏐\\n⏐\\n[\\nh\\n′(x) − h(x)][(h′(x) − y)+( h(x) − y)\\n]⏐⏐\\n⏐\\n≤ (|h\\n′(x) − y| + |h(x) − y|)|h(x) − h′(x)|\\n≤ 2\\n√\\nM |h(x) − h′(x)|,\\nw h e r ew eu s e dt h eM-boundedness of the loss. Thus, L2 is σ-admissible with\\nσ =2\\n√\\nM. Therefore, by proposition 11.1, KRR isβ-stable withβ ≤ 4r2M\\nmλ . Plugging\\nthis expression into the bound of theorem 11.1 yields the result.\\nThe previous two corollaries assumed bounded loss functions. We now present a\\nlemma that implies in particular that the loss functions used by SVR and KRR are\\nbounded when the label set is bounded.\\nLemma 11.1\\nAssume that K(x, x) ≤ r\\n2 for all x ∈X for some r ≥ 0 and that for all y ∈ Y ,\\nL(0,y ) ≤ B for some B ≥ 0. Then, the hypothesis hS returned by a kernel-based\\nregularization algorithm trained on a sampleS is bounded as follows:\\n∀x ∈ X, |hS(x)|≤ r\\n√\\nB/λ.\\nProof By the reproducing kernel property and the Cauchy-Schwarz inequality ,\\nwe can write\\n∀x ∈ X, |hS(x)| = ⟨hS,K (x, ·)⟩≤∥ hS ∥K\\n√\\nK(x, x) ≤ r∥hS ∥K. (11.12)276 Algorithmic Stability\\nThe minimization (11.6) is over H, which includes 0. Thus, by deﬁnition of FS and\\nhS, the following inequality holds:\\nFS(hS) ≤ FS(0) = 1\\nm\\nm∑\\ni=1\\nL(0,y i) ≤ B.\\nSince the loss L is non-negative, we haveλ∥hS ∥2\\nK ≤ FS(hS)a n dt h u sλ∥hS ∥2\\nK ≤ B.\\nCombining this inequality with (11.12) yields the result.\\n11.3.2 Application to classiﬁcation algorithms: SVMs\\nThis section presents a generalization bound for SVMs, when using the standard\\nhinge loss deﬁned for all y ∈Y = {−1, +1} and y′ ∈ R by\\nLhinge(y′,y )=\\n{\\n0i f 1 − yy′ ≤ 0;\\n1 − yy′ otherwise.\\n(11.13)\\nCorollary 11.3 Stability-based learning bound for SVMs\\nAssume that K(x, x) ≤ r2 for all x ∈X for some r ≥ 0.L e thS denote the hypothesis\\nreturned by SVMs when trained on an i.i.d. sampleS of size m.T h e n ,f o ra n yδ> 0,\\nthe following inequality holds with probability at least 1 − δ:\\nR(hS) ≤ ˆR(hS)+ r2\\nmλ +\\n(2r2\\nλ + r√\\nλ\\n+1\\n⎡\\n√\\nlog 1\\nδ\\n2m .\\nProof It is straightforward to verify that Lhinge(·,y ) is 1-Lipschitz for any y ∈Y\\nand therefore that it is σ-admissible with σ =1 .T h e r e f o r e ,b yp r o p o s i t i o n1 1 . 1 ,\\nSVMs isβ-stable withβ ≤ r2\\nmλ .S i n c e|Lhinge(0,y )|≤ 1 for anyy ∈Y , by lemma 11.1,\\n∀x ∈X , |hS(x)|≤ r/\\n√\\nλ. Thus, for any sample S and any x ∈X and y ∈Y ,t h e\\nloss is bounded as follows: Lhinge(hS(x),y ) ≤ r/\\n√\\nλ+ 1. Plugging this value of M\\nand the one found for β into the bound of theorem 11.1 yields the result.\\nSince the hinge loss upper bounds the binary loss, the bound of the corollary 11.3\\nalso applies to the generalization error of hS measured in terms of the standard\\nbinary loss used in classiﬁcation.\\n11.3.3 Discussion\\nNote that the learning bounds presented for kernel-based regularization algorithms\\nare of the form R(hS) − ˆR(hS) ≤ O\\n( 1\\nλ√m\\n⎡\\n. Thus, these bounds are informative\\nonly when λ ≫ 1/√m. The regularization parameter λ i saf u n c t i o no ft h es a m p l e\\nsize m: for larger values of m, it is expected to be smaller, decreasing the emphasis\\non regularization. The magnitude of λ aﬀects the norm of the linear hypotheses11.4 Chapter notes 277\\nused for prediction, with a larger value ofλ implying a smaller hypothesis norm. In\\nthis sense, λ is a measure of the complexity of the hypothesis set and the condition\\nrequired for λ can be interpreted as stating that a less complex hypothesis set\\nguarantees better generalization.\\nNote also that our analysis of stability in this chapter assumed a ﬁxed λ:t h e\\nregularization parameter is assumed to be invariant to the change of one point of\\nthe training sample. While this is a mild assumption, it may not hold in general.\\n11.4 Chapter notes\\nThe notion of algorithmic stability was ﬁrst used by Devroye, Rogers and Wagner\\n[Rogers and Wagner, 1978, Devroye and Wagner, 1979a,b] for thek-nearest neighbor\\nalgorithm and other k-local rules. Kearns and Ron [1999] later gave a formal deﬁni-\\ntion of stability and used it to provide an analysis of the leave-one-out error. Much\\nof the material presented in this chapter is based on Bousquet and Elisseeﬀ [2002].\\nOur proof of proposition 11.1 is novel and generalizes the results of Bousquet and\\nElisseeﬀ [2002] to the case of non-diﬀerentiable convex losses. Moreover, stability-\\nbased generalization bounds have been extended to ranking algorithms [Agarwal\\nand Niyogi, 2005, Cortes et al., 2007b], as well as to the non-i.i.d. scenario of sta-\\ntionary Φ- and β-mixing processes [Mohri and Rostamizadeh, 2010], and to the\\ntransductive setting [Cortes et al., 2008a]. Additionally, exercise 11.5 is based on\\nCortes et al. [2010b], which introduces and analyzes stability with respect to the\\nchoice of the kernel function or kernel matrix.\\nNote that while, as shown in this chapter, uniform stability is suﬃcient for\\nderiving generalization bounds, it is not a necessary condition. Some algorithms may\\ngeneralize well in the supervised learning scenario but may not be uniformly stable,\\nfor example, the Lasso algorithm [Xu et al., 2008]. Shalev-Shwartz et al. [2009]\\nhave used the notion of stability to provide necessary and suﬃcient conditions for a\\ntechnical condition of learnability related to PAC-learning, even in general scenarios\\nwhere learning is possible only by using non-ERM rules.\\n11.5 Exercises\\n11.1 Tighter stability bounds\\n(a) Assuming the conditions of theorem 11.1 hold, can one hope to guarantee\\na generalization with slack better than O(1/√m) even if the algorithm is very\\nstable, i.e. β → 0?278 Algorithmic Stability\\n(b) Can you show an O(1/m) generalization guarantee if L is bounded by\\nC/√m (a very strong condition)? If so, how stable does the learning algorithm\\nneed to be?\\n11.2 Quadratic hinge loss stability. Let L denote the quadratic hinge loss function\\ndeﬁned for all y ∈{ +1, −1} and y′ ∈ R by\\nL(y′,y )=\\n{\\n0i f 1 − y′y ≤ 0;\\n(1 − y′y)2 otherwise.\\nAssume that L(h(x),y )i sb o u n d e db yM,1 ≤ M< ∞ , for all h ∈ H, x ∈X ,a n d\\ny ∈{ +1, −1}, which also implies a bound on|h(x)| for all h ∈ H and x ∈X .D e r i v e\\na stability-based generalization bound for SVMs with the quadratic hinge loss.\\n11.3 Stability of linear regression.\\n(a) How does the stability bound in corollary 11.2 for ridge regression (i.e.\\nkernel ridge regression with a linear kernel) behave as λ → 0?\\n(b) Can you show a stability bound for linear regression (i.e. ridge regression\\nwith λ = 0)? If not, show a counter-example.\\n11.4 Kernel stability. Suppose an approximation of the kernel matrix K, denoted\\nK′, is used to train the hypothesish′ (and leth denote the non-approximate hypoth-\\nesis). At test time, no approximation is made, so if we letkx =\\n[\\nK(x, x1),...,K (x, xm)\\n]⊤\\nwe can writeh(x)= α⊤kx and h′(x)= α′⊤kx.S h o wt h a ti f∀x, x′ ∈X ,K (x, x′) ≤ r\\nthen\\n|h′(x) − h(x)|≤ rmM\\nλ2 ∥K′ − K∥2 .\\n(Hint: Use exercise 9.3)\\n11.5 Stability of relative-entropy regularization.\\n(a) Consider an algorithm that selects a distribution g over a hypothesis class\\nwhich is parameterized by θ ∈ Θ. Given a point z =( x, y) the expected loss is\\ndeﬁned as\\nH(g,z )=\\n∫\\nΘ\\nL(hθ(x),y )g(θ) dθ ,\\nwith respect to a base loss function L. Assuming the loss function L is\\nbounded by M, show that the expected loss H is M-admissible, i.e. show\\n|H(g,z ) − H(g′,z )|≤ M\\n∫\\nΘ |g(θ) − g′(θ)| dθ.11.5 Exercises 279\\n(b) Consider an algorithm that minimizes the entropy regularized objective\\nover the choice of distribution g:\\nFS(g)= 1\\nm\\nm∑\\ni=1\\nH(g,z i)\\n\\ued19 \\ued18\\ued17 \\ued1a\\nbRS(g)\\n+λK(g,f 0) .\\nHere, K is the Kullback-Leibler divergence (or relative entropy) between two\\ndistributions,\\nK(g,f 0)=\\n∫\\nΘ\\ng(θ)l o gg(θ)\\nf0(θ) dθ , (11.14)\\nand f0 is some ﬁxed distribution. Show that such an algorithm is stable by\\nperforming the following steps:\\ni. First use the fact 1\\n2 (\\n∫\\nΘ |g(θ) − g′(θ)|dθ)2 ≤ K(g,g ′) (Pinsker’s inequal-\\nity), to show\\n(∫\\nΘ\\n|gS(θ) − gS′ (θ)| dθ\\n⎡2\\n≤ BK(.,f0)(g∥g′)+ BK(.,f0)(g′∥g) .\\nii. Next, let g be the minimizer of FS and g′ the minimizer of FS′ ,w h e r e\\nS and S′ diﬀer only at the index m. Show that\\nBK(.,f0)(g∥g′)+ BK(.,f0)(g′∥g)\\n≤ 1\\nmλ\\n⏐⏐H(g′,zm) − H(g,z m)+ H(g,z ′\\nm) − H(g′,z ′\\nm)\\n⏐⏐\\n≤ 2M\\nmλ\\n∫\\nΘ\\n|g(θ) − g′(θ)|dθ .\\niii. Finally, combine the results above to show that the entropy regularized\\nalgorithm is 2M 2\\nmλ -stable.12 Dimensionality Reduction\\nIn settings where the data has a large number of features, it is often desirable\\nto reduce its dimension, or to ﬁnd a lower-dimensional representation preserving\\nsome of its properties. The key arguments for dimensionality reduction (or manifold\\nlearning) techniques are:\\nComputational: to compress the initial data as a preprocessing step to speed up\\nsubsequent operations on the data.\\nVisualization: to visualize the data for exploratory analysis by mapping the input\\ndata into two- or three-dimensional spaces.\\nFeature extraction: to hopefully generate a smaller and more eﬀective or useful\\nset of features.\\nThe beneﬁts of dimensionality reduction are often illustrated via simulated data,\\nsuch as the Swiss roll dataset. In this example, the input data, depicted in ﬁg-\\nure 12.1a, is three-dimensional, but it lies on a two-dimensional manifold that\\nis “unfolded” in two-dimensional space as shown in ﬁgure 12.1b. It is important\\nto note, however, that exact low-dimensional manifolds are rarely encountered in\\npractice. Hence, this idealized example is more useful to illustrate the concept of\\ndimensionality reduction than to verify the eﬀectiveness of dimensionality reduction\\nalgorithms.\\nDimensionality reduction can be formalized as follows. Consider a sample S =\\n(x\\n1,...,x m), a feature mapping Φ : X→ RN and the data matrix X ∈ RN ×m\\ndeﬁned as (Φ(x1),..., Φ(xm)). The ith data point is represented byxi = Φ(xi), or\\nthe ith column of X,w h i c hi sa nN-dimensional vector. Dimensionality reduction\\ntechniques broadly aim to ﬁnd, for k ≪ N,a k-dimensional representation of the\\ndata, Y ∈ Rk×m, that is in some way faithful to the original representation X.\\nIn this chapter we will discuss various techniques that address this problem.\\nWe ﬁrst present the most commonly used dimensionality reduction technique called\\nprincipal component analysis(PCA). We then introduce a kernelized version of PCA\\n(KPCA) and show the connection between KPCA and manifold learning algorithms.\\nWe conclude with a presentation of the Johnson-Lindenstrauss lemma, a classical\\ntheoretical result that has inspired a variety of dimensionality reduction methods282 Dimensionality Reduction\\n(a) (b)\\nFigure 12.1 The “Swiss roll” dataset. (a) high-dimensional representation. (b)\\nlower-dimensional representation.\\nbased on the concept of random projections. The discussion in this chapter relies\\non basic matrix properties that are reviewed in appendix A.\\n12.1 Principal Component Analysis\\nFix k ∈ [1,N ]a n dl e tX be a mean-centered data matrix, that is, ∑m\\ni=1 xi = 0.\\nDeﬁne Pk as the set of N-dimensional rank- k orthogonal projection matrices.\\nPCA consists of projecting the N-dimensional input data onto the k-dimensional\\nlinear subspace that minimizes reconstruction error, that is the sum of the squared\\nL2-distances between the original data and the projected data. Thus, the PCA\\nalgorithm is completely deﬁned by the orthogonal projection matrix solution P∗ of\\nthe following minimization problem:\\nmin\\nP∈Pk\\n∥PX − X∥2\\nF . (12.1)\\nThe following theorem shows that PCA coincides with the projection of each\\ndata point onto the k top singular vectors of the sample covariance matrix, i.e.,\\nC = 1\\nmXX⊤ for the mean-centered data matrix X. Figure 12.2 illustrates the\\nbasic intuition behind PCA, showing how two-dimensional data points with highly\\ncorrelated features can be more succinctly represented with a one-dimensional\\nrepresentation that captures most of the variance in the data.\\nTheorem 12.1\\nLet P\\n∗ ∈P k be the PCA solution, i.e., the orthogonal projection matrix solution of\\n(12.1).T h e n ,P∗ = UkU⊤\\nk ,w h e r eUk ∈ RN ×k is the matrix formed by the top k\\nsingular vectors of C = 1\\nmXX⊤, the sample covariance matrix corresponding toX.12.2 Kernel Principal Component Analysis (KPCA) 283\\nMoreover, the associatedk-dimensional representation ofX is given by Y = U⊤\\nk X.\\nProof Let P = P⊤ be an orthogonal projection matrix. By the deﬁnition of\\nthe Frobenius norm, the linearity of the trace operator and the fact that P is\\nidempotent, i.e., P2 = P,w eo b s e r v et h a t\\n∥PX − X∥2\\nF =T r [ (PX − X)⊤(PX − X)] = Tr[X⊤P2X − 2X⊤PX + X⊤X]\\n= − Tr[X⊤PX]+T r [X⊤X] .\\nSince Tr[X⊤X] is a constant with respect to P,w eh a v e\\nmin\\nP∈Pk\\n∥PX − X∥2\\nF =m a x\\nP∈Pk\\nTr[X⊤PX] . (12.2)\\nBy deﬁnition of orthogonal projections in Pk, P = UU⊤ for some U ∈ RN ×k\\ncontaining orthogonal columns. Using the invariance of the trace operator under\\ncyclic permutations and the orthogonality of the columns of U,w eh a v e\\nTr[X⊤PX]= U⊤XX⊤U =\\nk∑\\ni=1\\nu⊤\\ni XX⊤ui ,\\nwhere ui is the ith column ofU. By the Rayleigh quotient (section A.2.3), it is clear\\nthat the largestk singular vectors ofXX⊤ maximize the rightmost sum above. Since\\nXX⊤ and C diﬀer only by a scaling factor, they have the same singular vectors,\\nand thus Uk maximizes this sum, which proves the ﬁrst statement of the theorem.\\nFinally, since PX = UkU⊤\\nk X, Y = U⊤\\nk X is a k-dimensional representation of X\\nwith Uk as the basis vectors.\\nBy deﬁnition of the covariance matrix, the top singular vectors of C are the\\ndirections of maximal variance in the data, and the associated singular values\\nare equal to these variances. Hence, PCA can also be viewed as projecting onto\\nthe subspace of maximal variance. Under this interpretation, the ﬁrst principal\\ncomponent is derived from projection onto the direction of maximal variance, given\\nby the top singular vector ofC. Similarly, theith principal component, for 1≤ i ≤ k,\\nis derived from projection onto the ith direction of maximal variance, subject to\\northogonality constraints to the previous i − 1 directions of maximal variance (see\\nexercise 12.1 for more details).\\n12.2 Kernel Principal Component Analysis (KPCA)\\nIn the previous section, we presented the PCA algorithm, which involved projecting\\nonto the singular vectors of the sample covariance matrix C.I nt h i ss e c t i o n ,w e284 Dimensionality Reduction\\n7 8 9 10 11 12 1340\\n41\\n42\\n43\\n44\\n45\\n46\\nUS shoe size\\nEuropean shoe size\\n−5 0 5−6\\n−4\\n−2\\n0\\n2\\n4\\n6\\nUS shoe size (mean centered)\\nEuropean shoe size (mean centered)\\n(a) (b)\\nFigure 12.2 Example of PCA. (a) Two-dimensional data points with features cap-\\nturing shoe size measured with diﬀerent units. (b) One-dimensional representation\\n(blue squares) that captures the most variance in the data, generated by projecting\\nonto largest principal component (red line) of the mean-centered data points.\\npresent a kernelized version of PCA, called KPCA. In the KPCA setting, Φ is\\na feature mapping to an arbitrary RKHS (not necessarily to R\\nN )a n dw ew o r k\\nexclusively with a kernel function K corresponding to the inner product in this\\nRKHS. The KPCA algorithm can thus be deﬁned as a generalization of PCA in\\nw h i c ht h ei n p u td a t ai sp r o j e c t e do n t ot h et o pp r i n c i p l ec o m p o n e n t si nt h i sR K H S .\\nWe will show the relationship between PCA and KPCA by drawing upon the deep\\nconnections among the SVDs of X, C and K. We then illustrate how various\\nmanifold learning algorithms can be interpreted as special instances of KPCA.\\nLet K be a PDS kernel deﬁned over X× X and deﬁne the kernel matrix as K =\\nX\\n⊤X.S i n c eX admits the following singular value decomposition: X = UΣV⊤, C\\nand K c a nb er e w r i t t e na sf o l l o w s :\\nC = 1\\nmUΛU⊤ K = VΛV⊤ , (12.3)\\nwhere Λ = Σ2 is the diagonal matrix of the singular values of mC and U is the\\nmatrix of the singular vectors of C (and mC).\\nStarting with the SVD of X, note that right multiplying byVΣ−1 and using the\\nrelationship between Λ and Σ yields U = XVΛ−1/2. Thus, the singular vectoru of\\nC associated to the singular value λ/m coincides with Xv√\\nλ ,w h e r ev is the singular\\nvector of K associated to λ. Now ﬁx an arbitrary feature vector x = Φ(x)f o r\\nx ∈X . Then, following the expression for Y in theorem 12.1, the one-dimensional12.3 KPCA and manifold learning 285\\nrepresentation of x derived by projection onto Pu = uu⊤ is deﬁned by\\nx⊤u = x⊤ Xv√\\nλ\\n= k⊤\\nx v√\\nλ\\n, (12.4)\\nwhere kx =( K(x1,x),...,K (xm,x))⊤.I f x is one of the data points, i.e.,x = xi for\\n1 ≤ i ≤ m,t h e nkx is the ith column of K and (12.4) can be simpliﬁed as follows:\\nx⊤u = k⊤\\nx v√\\nλ\\n= λvi√\\nλ\\n=\\n√\\nλvi , (12.5)\\nwhere vi is theith component ofv. More generally, the PCA solution of theorem 12.1\\ncan be fully deﬁned by the top k singular vectors of K, v1,..., vk,a n dt h e\\ncorresponding singular values. This alternative derivation of the PCA solution in\\nterms of K precisely deﬁnes the KPCA solution, providing a generalization of PCA\\nvia the use of PDS kernels (see chapter 5 for more details on kernel methods).\\n12.3 KPCA and manifold learning\\nSeveral manifold learning techniques have been proposed as non-linear methods for\\ndimensionality reduction. These algorithms implicitly assume that high-dimensional\\ndata lie on or near a low-dimensional non-linear manifold embedded in the input\\nspace. They aim to learn this manifold structure by ﬁnding a low-dimensional\\nspace that in some way preserves the local structure of high-dimensional input\\ndata. For instance, the Isomap algorithm aims to preserve approximate geodesic\\ndistances, or distances along the manifold, between all pairs of data points. Other\\nalgorithms, such as Laplacian eigenmaps and locally linear embedding, focus only\\non preserving local neighborhood relationships in the high-dimensional space. We\\nwill next describe these classical manifold learning algorithms and then interpret\\nthem as speciﬁc instances of KPCA.\\n12.3.1 Isomap\\nIsomap aims to extract a low-dimensional data representation that best preserves\\nall pairwise distances between input points, as measured by their geodesic distances\\nalong the underlying manifold. It approximates geodesic distance assuming thatL\\n2\\ndistance provides good approximations for nearby points, and for faraway points\\nit estimates distance as a series of hops between neighboring points. The Isomap\\nalgorithm works as follows:\\n1. Find the t nearest neighbors for each data point based on L\\n2 distance and\\nconstruct an undirected neighborhood graph, denoted by G,w i t hp o i n t sa sn o d e s286 Dimensionality Reduction\\nand links between neighbors as edges.\\n2. Compute the approximate geodesic distances, Δ ij, between all pairs of nodes\\n(i, j) by computing all-pairs shortest distances in G using, for instance, the Floyd-\\nWarshall algorithm.\\n3. Convert the squared distance matrix into am×m similarity matrix by performing\\ndouble centering, i.e., compute KIso = −1\\n2HΔH, where Δ is the squared distance\\nmatrix, H = Im − 1\\nm11⊤ is the centering matrix, Im is the m × m identity matrix\\nand 1 is a column vector of all ones (for more details on double centering see\\nexercise 12.2).\\n4. Find the optimal k-dimensional representation, Y = {yi}n\\ni=1,s u c ht h a tY =\\nargminY′\\n∑\\ni,j\\n(\\n∥y′\\ni − y′\\nj ∥2\\n2 − Δ2\\nij\\n⎡\\n. The solution is given by,\\nY =( ΣIso,k)1/2U⊤\\nIso,k (12.6)\\nwhere ΣIso,k is the diagonal matrix of the top k singular values of KIso and UIso,k\\nare the associated singular vectors.\\nKIso can naturally be viewed as a kernel matrix, thus providing a simple connection\\nbetween Isomap and KPCA. Note, however, that this interpretation is valid only\\nwhen K\\nIso is in fact positive semideﬁnite, which is indeed the case in the continuum\\nlimit for a smooth manifold.\\n12.3.2 Laplacian eigenmaps\\nThe Laplacian eigenmaps algorithm aims to ﬁnd a low-dimensional representation\\nthat best preserves neighborhood relations as measured by a weight matrixW.T h e\\nalgorithm works as follows:\\n1. Find t nearest neighbors for each point.\\n2. Construct W,as p a r s e ,s y m m e t r i cm × m matrix, where W\\nij =e x p\\n(\\n−∥ xi −\\nxj ∥2\\n2/σ2⎡\\nif (xi,xj) are neighbors, 0 otherwise, and σ is a scaling parameter.\\n3. Construct the diagonal matrix D, such that Dii = ∑\\nj Wij.\\n4. Find the k-dimensional representation by minimizing the weighted distance\\nbetween neighbors as,\\nY =a r g m i n\\nY′\\n∑\\ni,j\\nWij ∥y′\\ni − y′\\nj ∥2\\n2\\n. (12.7)\\nThis objective function penalizes nearby inputs for being mapped to faraway\\noutputs, with “nearness” measured by the weight matrix W. The solution to the\\nminimization in (12.7) is Y = U⊤\\nL,k,w h e r eL = D − W is the graph Laplacian\\nand U⊤\\nL,k are the bottom k singular vectors of L, excluding the last singular vector12.3 KPCA and manifold learning 287\\ncorresponding to the singular value 0 (assuming that the underlying neighborhood\\ngraph is connected).\\nThe solution to (12.7) can also be interpreted as ﬁnding the largest singular\\nvectors ofL† , the pseudo-inverse ofL.D e ﬁ n i n gKL = L† we can thus view Laplacian\\nE i g e n m a p sa sa ni n s t a n c eo fK P C Ai nw h i c ht h eo u t p u td i m e n s i o n sa r en o r m a l i z e d\\nto have unit variance, which corresponds to setting λ = 1 in (12.5). Moreover, it\\ncan be shown that K\\nL is the kernel matrix associated with the commute times of\\ndiﬀusion on the underlying neighborhood graph, where the commute time between\\nnodes i and j in a graph is the expected time taken for a random walk to start at\\nnode i,r e a c hn o d ej and then return to i.\\n12.3.3 Locally linear embedding (LLE)\\nThe Locally linear embedding (LLE) algorithm also aims to ﬁnd a low-dimensional\\nrepresentation that preserves neighborhood relations as measured by a weight\\nmatrix W. The algorithm works as follows:\\n1. Find t nearest neighbors for each point.\\n2. Construct W,as p a r s e ,s y m m e t r i cm×m matrix, whose ith row sums to one and\\ncontains the linear coeﬃcients that optimally reconstruct x\\ni from its t neighbors.\\nMore speciﬁcally, if we assume that the ith row of W sums to one, then the\\nreconstruction error is\\n(\\nxi −\\n∑\\nj∈Ni\\nWijxj\\n⎡2\\n=\\n( ∑\\nj∈Ni\\nWij(xi − xj)\\n⎡2\\n=\\n∑\\nj,k∈Ni\\nWijWikC′\\njk (12.8)\\nwhere Ni is the set of indices of the neighbors of pointxi and C′\\njk =( xi −xj)⊤(xi −\\nxk) the local covariance matrix. Minimizing this expression with the constraint∑\\nj Wij = 1 gives the solution\\nWij =\\n∑\\nk(C′−1)jk∑\\nst(C′−1)st\\n. (12.9)\\nNote that the solution can be equivalently obtained by ﬁrst solving the system of\\nlinear equations ∑\\nj C′\\nkjWij =1 ,f o r k ∈N i, and then normalizing so that the\\nweights sum to one.\\n3. Find the k-dimensional representation that best obeys neighborhood relations as\\nspeciﬁed by W, i.e.,\\nY =a r g m i n\\nY′\\n∑\\ni\\n(\\ny′\\ni −\\n∑\\nj\\nWijy′\\nj\\n⎡2\\n. (12.10)288 Dimensionality Reduction\\nThe solution to the minimization in (12.10) isY = U⊤\\nM,k,w h e r eM =( I−W⊤)(I−\\nW⊤)a n dU⊤\\nM,k are the bottom k singular vectors ofM, excluding the last singular\\nvector corresponding to the singular value 0.\\nAs discussed in exercise 12.5, LLE coincides with KPCA used with a particular\\nkernel matrix KLLE whereby the output dimensions are normalized to have unit\\nvariance (as in the case of Laplacian Eigenmaps).\\n12.4 Johnson-Lindenstrauss lemma\\nThe Johnson-Lindenstrauss lemma is a fundamental result in dimensionality reduc-\\ntion that states that any m points in high-dimensional space can be mapped to a\\nmuch lower dimension, k ≥ O(\\nlog m\\nϵ2 ), without distorting pairwise distance between\\nany two points by more than a factor of (1 ± ϵ). In fact, such a mapping can be\\nfound in randomized polynomial time by projecting the high-dimensional points\\nonto randomly chosen k-dimensional linear subspaces. The Johnson-Lindenstrauss\\nlemma is formally presented in lemma 12.3. The proof of this lemma hinges on\\nlemma 12.1 and lemma 12.2, and it is an example of the “probabilistic method”,\\nin which probabilistic arguments lead to a deterministic statement. Moreover, as\\nwe will see, the Johnson-Lindenstrauss lemma follows by showing that the squared\\nlength of a random vector is sharply concentrated around its mean when the vector\\nis projected onto a k-dimensional random subspace.\\nFirst, we prove the following property of the χ\\n2-squared distribution (see deﬁni-\\ntion C.6 in appendix), which will be used in lemma 12.2.\\nLemma 12.1\\nLet Q be a random variable following a χ2-squared distribution with k degrees of\\nfreedom. Then, for any 0 <ϵ< 1/2, the following inequality holds:\\nPr[(1 − ϵ)k ≤ Q ≤ (1 +ϵ)k] ≥ 1 − 2e−(ϵ2−ϵ3)k/4 . (12.11)\\nProof By Markov’s inequality, we can write\\nPr[Q ≥ (1 +ϵ)k]=P r [ e x p (λQ) ≥ exp(λ(1 +ϵ)k)] ≤ E[exp(λQ)]\\nexp(λ(1 +ϵ)k)\\n= (1 − 2λ)−k/2\\nexp(λ(1 +ϵ)k) ,\\nwhere we used for the ﬁnal equality the expression of the moment-generating\\nfunction of a χ2-squared distribution, E[exp(λQ)], for λ< 1/2 (equation C.14).\\nChoosing λ = ϵ\\n2(1+ϵ) < 1/2, which minimizes the right-hand side of the ﬁnal12.4 Johnson-Lindenstrauss lemma 289\\nequality, and using the identity 1 +ϵ ≤ exp(ϵ − (ϵ2 − ϵ3)/2) yield\\nPr[Q ≥ (1 +ϵ)k] ≤\\n( 1+ ϵ\\nexp(ϵ)\\n⎡k/2\\n≤\\n(exp\\n(\\nϵ − ϵ2−ϵ3\\n2\\n⎡\\nexp(ϵ)\\n⎡k/2\\n=e x p\\n(\\n− k\\n4(ϵ2 − ϵ3)\\n⎡\\n.\\nThe statement of the lemma follows by using similar techniques to bound Pr[ Q ≤\\n(1 − ϵ)k] and by applying the union bound.\\nLemma 12.2\\nLet x ∈ RN,d e ﬁ n ek<N and assume that entries in A ∈ Rk×N are sampled\\nindependently from the standard normal distribution, N(0, 1).T h e n ,f o ra n y0 <\\nϵ< 1/2,\\nPr\\n[\\n(1 − ϵ)∥x∥2 ≤∥ 1√\\nk\\nAx∥2 ≤ (1 +ϵ)∥x∥2\\n]\\n≥ 1 − 2e−(ϵ2−ϵ3)k/4 . (12.12)\\nProof Let ˆx = Ax and observe that\\nE[ˆx2\\nj ]=E\\n[( N∑\\ni=1\\nAjixi\\n⎡2]\\n=E\\n[ N∑\\ni=1\\nA2\\njix2\\ni\\n]\\n=\\nN∑\\ni=1\\nx2\\ni = ∥x∥2 .\\nThe second and third equalities follow from the independence and unit variance,\\nrespectively, of the Aij.N o w ,d e ﬁ n eTj = ˆxj/∥x∥ and note that the Tjsa r e\\nindependent standard normal random variables since the Aij are i.i.d. standard\\nnormal random variables and E[ ˆx2\\nj ]= ∥x∥2. Thus, the variable Q deﬁned by\\nQ = ∑k\\nj=1 T2\\nj follows a χ2-squared distribution with k degrees of freedom and\\nwe have\\nPr\\n[\\n(1 − ϵ)∥x∥2 ≤ ∥ˆx∥2\\nk ≤ (1 +ϵ)∥x∥2\\n]\\n=P r\\n[\\n(1 − ϵ)k ≤\\nk∑\\nj=1\\nT2\\nj ≤ (1 +ϵ)k\\n]\\n=P r\\n[\\n(1 − ϵ)k ≤ Q ≤ (1 +ϵ)k\\n]\\n≥ 1 − 2e−(ϵ2−ϵ3)k/4 ,\\nwhere the ﬁnal inequality holds by lemma 12.1, thus proving the statement of the\\nlemma.\\nLemma 12.3 Johnson-Lindenstrauss\\nFor any 0 <ϵ< 1/2 and any integer m> 4,l e tk = 20 logm\\nϵ2 . Then for any set V of\\nm points in RN, there exists a map f : RN → Rk such that for all u,v ∈ V ,\\n(1 − ϵ)∥u − v∥2 ≤∥ f(u) − f(v)∥2 ≤ (1 +ϵ)∥u − v∥2. (12.13)\\nProof Let f = 1√\\nk A where k<N and entries in A ∈ Rk×N are sampled290 Dimensionality Reduction\\nindependently from the standard normal distribution, N(0, 1). For ﬁxed u,v ∈ V ,\\nwe can apply lemma 12.2, with x = u − v, to lower bound the success probability\\nby 1 − 2e−(ϵ2−ϵ3)k/4. Applying the union bound over the O(m2)p a i r si nV , setting\\nk = 20\\nϵ2 log m and upper bounding ϵ by 1/2, we have\\nPr[success] ≥ 1 − 2m2e−(ϵ2−ϵ3)k/4 =1 − 2m5ϵ−3 > 1 − 2m−1/2 > 0 .\\nSince the success probability is strictly greater than zero, a map that satisﬁes the\\ndesired conditions must exist, thus proving the statement of the lemma.\\n12.5 Chapter notes\\nPCA was introduced in the early 1900s by Pearson [1901]. KPCA was introduced\\nroughly a century later, and our presentation of KPCA is a more concise derivation\\nof results given by Mika et al. [1999]. Isomap and LLE were pioneering works on\\nnon-linear dimensionality reduction introduced byTenenbaum et al. [2000], Roweis\\nand Saul [2000]. Isomap itself is a generalization of a standard linear dimensionality\\nreduction technique called Multidimensional Scaling [Cox and Cox, 2000]. Isomap\\nand LLE led to the development of several related algorithms for manifold learning,\\ne.g., Laplacian Eigenmaps and Maximum Variance Unfolding [Belkin and Niyogi,\\n2001, Weinberger and Saul, 2006]. As shown in this chapter, classical manifold\\nlearning algorithms are special instances of KPCA [Ham et al., 2004]. The Johnson-\\nLindenstrauss lemma was introduced by Johnson and Lindenstrauss [1984], though\\nour proof of the lemma follows Vempala [2004]. Other simpliﬁed proofs of this lemma\\nhave also been presented, including Dasgupta and Gupta [2003].\\n12.6 Exercises\\n12.1 PCA and maximal variance. Let X be an uncentered data matrix and let\\n¯x = 1\\nm\\n∑\\ni xi b et h es a m p l em e a no ft h ec o l u m n so fX.\\n(a) Show that the variance of one-dimensional projections of the data onto an\\narbitrary vector u equals u⊤Cu,w h e r eC = 1\\nm\\n∑\\ni(xi − ¯x)(xi − ¯x)⊤ is the\\nsample covariance matrix.\\n(b) Show that PCA with k = 1 projects the data onto the direction (i.e.,\\nu⊤u = 1) of maximal variance.\\n12.2 Double centering. In this problem we will prove the correctness of the double12.6 Exercises 291\\ncentering step in Isomap when working with Euclidean distances. DeﬁneX and ¯x as\\nin exercise 12.1, and deﬁne X∗ as the centered version ofX,t h a ti s ,l e tx∗\\ni = xi − ¯x\\nbe the ith column of X∗.L e tK = X⊤X,a n dl e tD denote the Euclidean distance\\nmatrix, i.e., Dij = ∥xi − xj ∥.\\n(a) Show that Kij = 1\\n2 (Kii + Kjj + D2\\nij).\\n(b) Show that K∗ = X∗⊤X∗ = K − 1\\nmK11⊤ − 1\\nm11⊤K + 1\\nm2 11⊤K11⊤.\\n(c) Using the results from (a) and (b) show that\\nK∗\\nij = − 1\\n2\\n[\\nD2\\nij − 1\\nm\\nm∑\\nk=1\\nD2\\nik − 1\\nm\\nm∑\\nk=1\\nD2\\nkj + ¯D\\n]\\n,\\nwhere ¯D = 1\\nm2\\n∑\\nu\\n∑\\nv D2\\nu,v is the mean of the m2 entries in D.\\n(d) Show that K∗ = −1\\n2HDH.\\n12.3 Laplacian eigenmaps. Assume k = 1 and we seek a one-dimensional represen-\\ntation y. Show that (12.7) is equivalent to y =a r g m i ny′ y′⊤Ly′,w h e r eL is the\\ngraph Laplacian.\\n12.4 Nystr¨om method. Deﬁne the following block representation of a kernel matrix:\\nK =\\n[\\nWK ⊤\\n21\\nK21 K22\\n]\\nand C =\\n[\\nW\\nK21\\n]\\n.\\nThe Nystr¨om method uses W ∈ Rl×l and C ∈ Rm×l to generate the approximation\\n˜K = CW† C⊤ ≈ K.\\n(a) Show that W is SPSD and that ∥K − ˜K∥F = ∥K22 − K21W† K⊤\\n21∥F .\\n(b) Let K = X⊤X for some X ∈ RN ×m,a n dl e tX′ ∈ RN ×l be the ﬁrst\\nl columns of X. Show that ˜K = X⊤PUX′ X,w h e r ePUX′ is the orthogonal\\nprojection onto the span of the left singular vectors of X′.\\n(c) Is ˜K SPSD?\\n(d) If rank(K)=r a n k (W)= r ≪ m,s h o wt h a t˜K = K. Note: this statement\\nholds whenever rank(K)=r a n k (W), but is of interest mainly in the low-rank\\nsetting.\\n(e) If m = 20M andK is a dense matrix, how much space is required to storeK\\ni fe a c he n t r yi ss t o r e da sad o u b l e ?H o wm u c hs p a c ei sr e q u i r e db yt h eN y s t r ¨om\\nmethod if l = 10K?292 Dimensionality Reduction\\n12.5 Expression for KLLE. Show the connection between LLE and KPCA by\\nderiving the expression for KLLE.\\n12.6 Random projection, PCA, and nearest neighbors.\\n(a) Download the MNIST test set of handwritten digits at:\\nhttp://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz.\\nCreate a data matrix X ∈ RN ×m from the ﬁrst m =2 ,000 instances of this\\ndataset (the dimension of each instance should be N = 784).\\n(b) Find the ten nearest neighbors for each point in X, that is, compute Ni,10\\nfor 1 ≤ i ≤ m,w h e r eNi,t denotes the set of the t nearest neighbors for the ith\\ndatapoint and nearest neighbors are deﬁned with respect to the L2 norm. Also\\ncompute Ni,50 for all i.\\n(c) Generate ˜X = AX,w h e r eA ∈ Rk×N , k = 100 and entries of A are\\nsampled independently from the standard normal distribution. Find the ten\\nnearest neighbors for each point in ˜X, that is, compute ˜Ni,10 for 1 ≤ i ≤ m.\\n(d) Report the quality of approximation by computing score10 = 1\\nm\\n∑m\\ni=1 |Ni,10∩\\n˜Ni,10|. Similarly, compute score50 = 1\\nm\\n∑m\\ni=1 |Ni,50 ∩ ˜Ni,10|.\\n(e) Generate two plots that show score 10 and score50 as functions of k (i.e.,\\nperform steps (c) and (d) for k = {1,10, 50, 100,250, 500}). Provide a one- or\\ntwo-sentence explanation of these plots.\\n(f) Generate similar plots as in (e) using PCA (with various values ofk)t og e n -\\nerate ˜X and subsequently compute nearest neighbors. Are the nearest neighbor\\napproximations generated via PCA better or worse than those generated via\\nrandom projections? Explain why.13 Learning Automata and Languages\\nThis chapter presents an introduction to the problem of learning languages. This\\nis a classical problem explored since the early days of formal language theory and\\ncomputer science, and there is a very large body of literature dealing with related\\nmathematical questions. In this chapter, we present a brief introduction to this\\nproblem and concentrate speciﬁcally on the question of learning ﬁnite automata,\\nwhich, by itself, has been a topic investigated in multiple forms by thousands of\\ntechnical papers. We will examine two broad frameworks for learning automata,\\nand for each, we will present an algorithm. In particular, we describe an algorithm\\nfor learning automata in which the learner has access to several types of query, and\\nwe discuss an algorithm for identifying a sub-class of the family of automata in the\\nlimit.\\n13.1 Introduction\\nLearning languages is one of the earliest problems discussed in linguistics and\\ncomputer science. It has been prompted by the remarkable faculty of humans to\\nlearn natural languages. Humans are capable of uttering well-formed new sentences\\nat an early age, after having been exposed only to ﬁnitely many sentences. Moreover,\\neven at an early age, they can make accurate judgments of grammaticality for new\\nsentences.\\nIn computer science, the problem of learning languages is directly related to that\\nof learning the representation of the computational device generating a language.\\nThus, for example, learning regular languages is equivalent to learning ﬁnite au-\\ntomata, or learning context-free languages or context-free grammars is equivalent\\nto learning pushdown automata.\\nThere are several reasons for examining speciﬁcally the problem of learning\\nﬁnite automata. Automata provide natural modeling representations in a variety\\nof diﬀerent domains including systems, networking, image processing, text and\\nspeech processing, logic and many others. Automata can also serve as simple or\\neﬃcient approximations for more complex devices. For example, in natural language294 Learning Automata and Languages\\n0\\nb\\n1a\\n3\\na\\n2a\\nε\\nb\\nb\\n0\\nb\\n1a 2a 3b\\na\\nb\\n(a) (b)\\nFigure 13.1 (a) A graphical representation of a ﬁnite automaton. (b) Equivalent\\n(minimal) deterministic automaton.\\nprocessing, they can be used to approximate context-free languages. When it is\\npossible, learning automata is often eﬃcient, though, as we shall see, the problem\\nis hard in a number of natural scenarios. Thus, learning more complex devices or\\nlanguages is even harder.\\nWe consider two general learning frameworks: the model ofeﬃcient exact learning\\nand the model of identiﬁcation in the limit . For each of these models, we brieﬂy\\ndiscuss the problem of learning automata and describe an algorithm.\\nWe ﬁrst give a brief review of some basic automata deﬁnitions and algorithms,\\nthen discuss the problem of eﬃcient exact learning of automata and that of the\\nidentiﬁcation in the limit.\\n13.2 Finite automata\\nWe will denote by Σ a ﬁnite alphabet. The length of a string x ∈ Σ∗ over that\\nalphabet is denoted by |x|.T h eempty string is denoted by ϵ,t h u s|ϵ| = 0. For any\\nstring x = x1 ··· xk ∈ Σ∗ of length k ≥ 0, we denote by x[j]= x1 ··· xj its preﬁx of\\nlength j ≤ k and deﬁne x[0] as ϵ.\\nFinite automata are labeled directed graphs equipped with initial and ﬁnal states.\\nThe following gives a formal deﬁnition of these devices.\\nDeﬁnition 13.1 Finite automata\\nA ﬁnite automaton A is a 5-tuple (Σ,Q ,I,F,E ) where Σ is a ﬁnite alphabet, Q a\\nﬁnite set of states, I ⊆ Q a set of initial states, F ⊆ Q a set of ﬁnal states, and\\nE ⊆ Q × (Σ ∪{ ϵ}) × Q a ﬁnite set of transitions.\\nFigure 13.1a shows a simple example of a ﬁnite automaton. States are represented\\nby circles. A bold circle indicates an initial state, a double circle a ﬁnal state. Each\\ntransition is represented by an arrow from its origin state to its destination state\\nwith its label in Σ ∪{ ϵ}.\\nA path from an initial state to a ﬁnal state is said to be an accepting path.A n13.3 Eﬃcient exact learning 295\\nautomaton is said to be trim if all of its states are accessible from an initial state\\nand admit a path to a ﬁnal state, that is, if all of its states lie on an accepting\\npath. A string x ∈ Σ\\n∗ is accepted by an automaton A iﬀ x labels an accepting path.\\nFor convenience, we will say that x ∈ Σ∗ is rejected by A when it is not accepted.\\nThe set of all strings accepted by A deﬁnes the language accepted by A denoted by\\nL(A). The class of languages accepted by ﬁnite automata coincides with the family\\nof regular languages, that is, languages that can be described byregular expressions.\\nAny ﬁnite automaton admits an equivalent automaton with no ϵ-transition,t h a t\\nis, no transition labeled with the empty string: there exists a general ϵ-removal\\nalgorithm that takes as input an automaton and returns an equivalent automaton\\nwith no ϵ-transition.\\nAn automaton with noϵ-transition is said to bedeterministic if it admits a unique\\ninitial state and if no two transitions sharing the same label leave any given state.\\nA deterministic ﬁnite automaton is often referred to by the acronym DFA, while\\nthe acronym NFA is used for arbitrary automata, that is, non-deterministic ﬁnite\\nautomata. Any NFA admits an equivalent DFA: there exists a general (exponential-\\ntime) determinization algorithm that takes as input an NFA with no ϵ-transition\\nand returns an equivalent DFA. Thus, the class of languages accepted by DFAs\\ncoincides with that of the languages accepted by NFAs, that is regular languages.\\nFor any string x ∈ Σ\\n∗ and DFA A,w ed e n o t eb yA(x) the state reached in A when\\nreading x from its unique initial state.\\nA DFA is said to be minimal if it admits no equivalent deterministic automaton\\nwith a smaller number of states. There exists a general minimization algorithm\\ntaking as input a deterministic automaton and returning a minimal one that runs\\nin O(|E| log |Q|). When the input DFA is acyclic, that is when it admits no path\\nforming a cycle, it can be minimized in linear timeO(|Q|+ |E|). Figure 13.1b shows\\nthe minimal DFA equivalent to the NFA of ﬁgure 13.1a.\\n13.3 Eﬃcient exact learning\\nIn the eﬃcient exact learning framework, the problem consists of identifying a\\ntarget concept c from a ﬁnite set of examples in time polynomial in the size of the\\nrepresentation of the concept and in an upper bound on the size of the representation\\nof an example. Unlike the PAC-learning framework, in this model, there is no\\nstochastic assumption, instances are not assumed to be drawn according to some\\nunknown distribution. Furthermore, the objective is to identify the target concept\\nexactly, without any approximation. A concept class C is said to be eﬃciently\\nexactly learnable if there is an algorithm for eﬃcient exact learning of any c ∈ C.\\nWe will consider two diﬀerent scenarios within the framework of eﬃciently exact296 Learning Automata and Languages\\nlearning: a passive and an active learning scenario. The passive learning scenario is\\nsimilar to the standard supervised learning scenario discussed in previous chapters\\nbut without any stochastic assumption: the learning algorithm passively receives\\ndata instances as in the PAC model and returns a hypothesis, but here, instances\\nare not assumed to be drawn from any distribution. In the active learning scenario,\\nthe learner actively participates in the selection of the training samples by using\\nvarious types of queries that we will describe. In both cases, we will focus more\\nspeciﬁcally on the problem of learning automata.\\n13.3.1 Passive learning\\nThe problem of learning ﬁnite automata in this scenario is known as the minimum\\nconsistent DFA learning problem . It can be formulated as follows: the learner\\nreceives a ﬁnite sample S =( (x\\n1,y1),..., (xm,y m)) with xi ∈ Σ∗ and yi ∈{ −1, +1}\\nfor any i ∈ [1,m]. If yi =+ 1 ,t h e nxi is an accepted string, otherwise it is rejected.\\nThe problem consists of using this sample to learn the smallest DFA A consistent\\nwith S, that is the automaton with the smallest number of states that accepts the\\nstrings of S with label +1 and rejects those with label −1. Note that seeking the\\nsmallest DFA consistent withS can be viewed as following Occam’s razor principle.\\nThe problem just described is distinct from the standard minimization of DFAs. A\\nminimal DFA accepting exactly the strings ofS labeled positively may not have the\\nsmallest number of states: in general there may be DFAs with fewer states accepting\\na superset of these strings and rejecting the negatively labeled sample strings.\\nFor example, in the simple case S =( ( a,+1), (b, −1)), a minimal deterministic\\nautomaton accepting the unique positively labeled stringa or the unique negatively\\nlabeled string b admits two states. However, the deterministic automaton accepting\\nthe language a∗ accepts a and rejects b and has only one state.\\nPassive learning of ﬁnite automata turns out to be a computationally hard\\nproblem. The following theorems present several negative results known for this\\nproblem.\\nTheorem 13.1\\nThe problem of ﬁnding the smallest deterministic automaton consistent with a set\\nof accepted or rejected strings is NP-complete.\\nHardness results are known even for a polynomial approximation, as stated by the\\nfollowing theorem.\\nTheorem 13.2\\nIf P ̸= NP, then, no polynomial-time algorithm can be guaranteed to ﬁnd a DFA\\nconsistent with a set of accepted or rejected strings of size smaller than a polynomial\\nfunction of the smallest consistent DFA, even when the alphabet is reduced to just13.3 Eﬃcient exact learning 297\\ntwo elements.\\nOther strong negative results are known for passive learning of ﬁnite automata\\nunder various cryptographic assumptions.\\nThese negative results for passive learning invite us to consider alternative\\nlearning scenarios for ﬁnite automata. The next section describes a scenario leading\\nto more positive results where the learner can actively participate in the data\\nselection process using various types of queries.\\n13.3.2 Learning with queries\\nThe model of learning with queries corresponds to that of a (minimal) teacher or\\noracle and an active learner. In this model, the learner can make the following two\\nt y p e so fq u e r i e st ow h i c ha no r a c l er e s p o n d s :\\nmembership queries: the learner requests the target label f(x) ∈{ −1, +1} of an\\ninstance x and receives that label;\\nequivalence queries: the learner conjectures hypothesish; he receives the response\\nyes if h = f, a counter-example otherwise.\\nWe will say that a concept class C is eﬃciently exactly learnable with membership\\nand equivalence queries when it is eﬃciently exactly learnable within this model.\\nThis model is not realistic, since no such oracle is typically available in practice.\\nNevertheless, it provides a natural framework, which, as we shall see, leads to\\npositive results. Note also that for this model to be signiﬁcant, equivalence must be\\ncomputationally testable. This would not be the case for some concept classes such\\nas that of context-free grammars, for example, for which the equivalence problem is\\nundecidable. In fact, equivalence must be further eﬃciently testable, otherwise the\\nresponse to the learner cannot be supplied in a reasonable amount of time.\\n1\\nEﬃcient exact learning within this model of learning with queries implies the\\nfollowing variant of PAC-learning: we will say that a concept class C is PAC-\\nlearnable with membership queries if it is PAC-learnable by an algorithm that has\\naccess to a polynomial number of membership queries.\\nTheorem 13.3\\nLet C be a concept class that is eﬃciently exactly learnable with membership and\\nequivalence queries, then C is PAC-learnable using membership queries.\\n1. For a human oracle, answering membership queries may also become very hard in some\\ncases when the queries are near the class boundaries. This may also make the model\\ndiﬃcult to adopt in practice.298 Learning Automata and Languages\\nProof Let A be an algorithm for eﬃciently exactly learning C using membership\\nand equivalence queries. Fix ϵ, δ >0. We replace in the execution of A for learning\\ntarget c ∈ C, each equivalence query by a test of the current hypothesis on a\\npolynomial number of labeled examples. Let D be the distribution according to\\nwhich points are drawn. To simulate the tth equivalence query, we draw mt =\\n1\\nϵ (log 1\\nδ + t log 2) points i.i.d. according to D to test the current hypothesis ht.I f\\nht is consistent with all of these points, then the algorithm stops and returns ht.\\nOtherwise, one of the points drawn does not belong toht, which provides a counter-\\nexample.\\nSince A learns c exactly, it makes at most T equivalence queries, where T is\\npolynomial in the size of the representation of the target concept and in an upper\\nbound on the size of the representation of an example. Thus, if no equivalence\\nquery is positively responded by the simulation, the algorithm will terminate afterT\\nequivalence queries and return the correct conceptc. Otherwise, the algorithm stops\\nat the ﬁrst equivalence query positively responded by the simulation. The hypothesis\\nit returns is not an ϵ-approximation only if the equivalence query stopping the\\nalgorithm is incorrectly responded positively. By the union bound, since for any\\nﬁxed t ∈ [1,T ], Pr[R(h\\nt) >ϵ ] ≤ (1 − ϵ)mt , the probability that for some t ∈ [1,T ],\\nR(ht) >ϵ can be bounded as follows:\\nPr[∃t ∈ [1,T ]: R(ht) >ϵ ] ≤\\nT∑\\ni=1\\nPr[R(ht) >ϵ ]\\n≤\\nT∑\\ni=1\\n(1 − ϵ)mt ≤\\nT∑\\ni=1\\ne−mtϵ ≤\\nT∑\\ni=1\\nδ\\n2t ≤\\n+∞∑\\ni=1\\nδ\\n2t = δ.\\nThus, with probability at least 1 − δ, the hypothesis returned by the algorithm is\\nan ϵ-approximation. Finally, the maximum number of points drawn is ∑T\\nt=1 mt =\\n1\\nϵ (T log 1\\nδ + T(T+1)\\n2 log 2), which is polynomial in 1 /ϵ,1 /δ,a n d T. Since the rest\\nof the computational cost of A is also polynomial by assumption, this proves the\\nPAC-learning of C.\\n13.3.3 Learning automata with queries\\nIn this section, we describe an algorithm for eﬃcient exact learning of DFAs with\\nmembership and equivalence queries. We will denote by A the target DFA and by\\nˆA the DFA that is the current hypothesis of the algorithm. For the discussion of\\nthe algorithm, we assume without loss of generality that A is a minimal DFA.\\nThe algorithm uses two sets of strings, U and V . U is a set of access strings:\\nreading an access string u ∈ U from the initial state ofA leads to a state A(u). The\\nalgorithm ensures that the states A(u), u ∈ U,a r ea l ld i s t i n c t .T od os o ,i tu s e sa13.3 Eﬃcient exact learning 299\\na ba\\nb\\n/gid1\\n/gid1 ε\\na\\nbb\\nb\\nbaa\\na\\nb\\n0\\na\\n1b\\nb\\n2a\\nb\\n3a\\na\\nb\\n(a) (b) (c)\\nFigure 13.2 (a) Classiﬁcation tree T,w i t hU = {ϵ, b, ba} and V = {ϵ, a}.( b )C u r r e n t\\nautomaton bA constructed using T. (c) Target automaton A.\\nset V of distinguishing strings.S i n c eA is minimal, for two distinct states q and q′\\nof A, there must exist at least one string that leads to a ﬁnal state from q and not\\nfrom q′, or vice versa. That string helps distinguish q and q′.T h es e to fs t r i n g sV\\nhelp distinguish any pair of access strings in U. They deﬁne in fact a partition of\\nall strings of Σ∗.\\nThe objective of the algorithm is to ﬁnd at each iteration a new access string\\ndistinguished from all previous ones, ultimately obtaining a number of access strings\\nequal to the number of states of A. It can then identify each state A(u)o f A with\\ni t sa c c e s ss t r i n gu. To ﬁnd the destination state of the transition labeled witha ∈ Σ\\nleaving state u, it suﬃces to determine, using the partition induced byV the access\\nstring u\\n′ that belongs to the same equivalence class asua.T h eﬁ n a l i t yo fe a c hs t a t e\\ncan be determined in a similar way.\\nBoth sets U and V are maintained by the algorithm via a binary decision treeT\\nsimilar to those presented in chapter 8. Figure 13.2a shows an example. T deﬁnes\\nthe partition of all strings induced by the distinguishing stringsV .T h el e a v e so fT\\nare each labeled with a distinct u ∈ U and its internal nodes with a string v ∈ V .\\nT h ed e c i s i o nt r e eq u e s t i o nd e ﬁ n e db yv ∈ V , given a string x ∈ Σ∗,i sw h e t h e rxv\\nis accepted by A, which is determined via a membership query. If accepted, x is\\nassigned to right sub-tree, otherwise to the left sub-tree, and the same is applied\\nrecursively with the sub-trees until a leaf is reached. We denote byT(x)t h el a b e lo f\\nthe leaf reached. For example, for the treeT of ﬁgure 13.2a and target automaton A\\nof ﬁgure 13.2c, T(baa)= b since baa is not accepted by A (root question) and baaa\\nis (question at node a). At its initialization step, the algorithm ensures that the\\nroot node is labeled with ϵ, which is convenient to check the ﬁnality of the strings.\\nThe tentative hypothesis DFA ˆA can be constructed fromT as follows. We denote\\nby ConstructAutomaton() the corresponding function. A distinct state ˆA(u)i s\\ncreated for each leaf u ∈ V . The ﬁnality of a state ˆA(u) is determined based on\\nthe sub-tree of the root node that u belongs to: ˆA(u)i sm a d eﬁ n a li ﬀu belongs300 Learning Automata and Languages\\nQueryLearnAutomata()\\n1 t ← MembershipQuery(ϵ)\\n2 T ← T0\\n3 ˆA ← A0\\n4 while (EquivalenceQuery( ˆA) ̸= true) do\\n5 x ← CounterExample()\\n6 if (T = T0) then\\n7 T ← T1 ⊿ nil replaced with x.\\n8 else j ← argmink A(x[k]) ̸≡T ˆA(x[k])\\n9 Split( ˆA(x[j − 1]))\\n10 ˆA ← ConstructAutomaton(T)\\n11 return ˆA\\nFigure 13.3 Algorithm for learning automata with membership and equivalence\\nqueries. A0 is a single-state automaton with self-loops labeled with all a ∈ Σ.T h a t\\nstate is initial. It is ﬁnal iﬀ t = true. T0 is a tree with root node labeled with ϵ and\\ntwo leaves, one labeled with ϵ, the other with nil. the right leaf is labeled with ϵ\\nlabels iﬀ t = true. T1 i st h et r e eo b t a i n e df r o mT0 by replacing nil with x.\\nto the right sub-tree that is iﬀ u = ϵu is accepted by A. The destination of the\\ntransition labeled with a ∈ Σ leaving state ˆA(u) is the state ˆA(v)w h e r ev = T(ua).\\nFigure 13.2b shows the DFA ˆA constructed from the decision tree of ﬁgure 13.2a.\\nFor convenience, for anyx ∈ Σ∗, we denote byU( ˆA(x)) the access string identifying\\nstate ˆA(x).\\nFigure 13.3 shows the pseudocode of the algorithm. The initialization steps at\\nlines 1–3 construct a tree T with a single internal node labeled with ϵ and one leaf\\nstring labeled with ϵ, the other left undetermined and labeled with nil.T h e ya l s o\\ndeﬁne a tentative DFA ˆA with a single state with self-loops labeled with all elements\\nof the alphabet. That single state is an initial state. It is made a ﬁnal state only if\\nϵ is accepted by the target DFA A, which is determined via the membership query\\nof line 1.\\nAt each iteration of the loop of lines 4–11, an equivalence query is used. IfˆA is not\\nequivalent to A, then a counter-example stringx is received (line 5). IfT is the tree\\nconstructed in the initialization step, then the leaf labeled withnil is replaced with\\nx (lines 6–7). Otherwise, since x is a counter-example, states A(x)a n d ˆA(x)h a v ea\\ndiﬀerent ﬁnality; thus, the string x deﬁning A(x) and the access string U( ˆA(x)) are13.3 Eﬃcient exact learning 301\\nv′\\nT(x[j − 1])\\nu′\\nx[j − 1]\\nxjv\\nv′\\nT(x[j − 1]) u′\\nFigure 13.4 Illustration of the splitting procedure Split( bA(x[j − 1])).\\nassigned to diﬀerent equivalence classes by T. Thus, there exists a smallest j such\\nthat A(x[j]) and ˆA(x[j]) are not equivalent, that is, such that the preﬁx x[j]o f x\\nand the access stringU( ˆA(x[j])) are assigned to diﬀerent leaves byT. j cannot be 0\\nsince the initialization ensures that ˆA(ϵ) is an initial state and has the same ﬁnality\\nas the initial state A(ϵ)o f A. The equivalence of A(x[j]) and ˆA(x[j]) is tested by\\nchecking the equality of T(x[j]) and T(U( ˆA(x[j]))), which can be both determined\\nusing the tree T and membership queries (line 8).\\nNow, by deﬁnition, A(x[j − 1]) and ˆA(x[j − 1]) are equivalent, that is T assigns\\nx[j −1] to the leaf labeled withU( ˆA(x[j −1])). But, x[j −1] and U( ˆA(x[j −1])) must\\nbe distinguished since A(x[j − 1]) and ˆA(x[j − 1]) admit transitions labeled with\\nt h es a m el a b e lxj to two non-equivalent states. Let v be a distinguishing string for\\nA(x[j]) and ˆA(x[j]). v can be obtained as the least common ancestor of the leaves\\nlabeled with x[j]a n d U( ˆA(x[j])). To distinguish x[j − 1] and U( ˆA(x[j − 1])), it\\nsuﬃces to split the leaf of T labeled with T(x[j − 1]) to create an internal node xjv\\ndominating a leaf labeled with x[j − 1] and another one labeled with T(x[j − 1])\\n(line 9). Figure 13.4 illustrates this construction. Thus, this provides a new access\\nstring x[j − 1] which, by construction, is distinguished from U( ˆA(x[j − 1])) and all\\nother access strings.\\nThus, the number of access strings (or states of ˆA) increases by one at each\\ni t e r a t i o no ft h el o o p .W h e ni tr e a c h e st h en u m b e ro fs t a t e so fA, all states of A\\nare of the form A(u) for a distinct u ∈ U. A and ˆA have then the same number\\nof states and in fact A = ˆA. Indeed, let (A(u),a ,A(u′)) be a transition in A,t h e n\\nby deﬁnition the equality A(ua)= A(u′) holds. The tree T deﬁnes a partition\\nof all strings in terms of their distinguishing strings in A. Since in A, ua and u′\\nlead to the same state, they are assigned to the same leaf by T, that is, the leaf\\nlabeled with u′. The destination of the transition from ˆA(u)w i t hl a b e la is found\\nby ConstructAutomaton() by determining the leaf in T assigned to ua,t h a t\\nis, u′. Thus, by construction, the same transition ( ˆA(u),a , ˆA(u′)) is created in ˆA.\\nAlso, a state A(u)o f A is ﬁnal iﬀ u accepted by A that is iﬀ u is assigned to the\\nright sub-tree of the root node by T, which is the criterion determining the ﬁnality\\nof ˆA(u). Thus, the automata A and ˆA coincide.302 Learning Automata and Languages\\nA\\n0\\na\\n1b\\nb\\n2a\\nb\\n3a\\nb\\na\\nT ˆA counter-example x\\nε\\nε NIL\\nε\\na\\nb\\nx = b\\nε\\nε b\\nε\\na\\nb\\nb\\na\\nb x = baa\\nε\\na b\\nε ba\\nε\\na\\nbb\\nb\\nbaa\\na\\nb\\nx = baaa\\nε\\na a\\nε ba b baa\\nε\\na\\nbb\\nb\\nbaa\\nb\\nbaaa\\nb\\na\\nFigure 13.5 Illustration of the execution of Algorithm QueryLearnAutomata()\\nfor the target automaton A. Each line shows the current decision tree T and the\\ntentative DFA bA constructed using T.W h e nbA is not equivalent to A, the learner\\nreceives a counter-example x i n d i c a t e di nt h et h i r dc o l u m n .\\nThe following is the analysis of the running-time complexity of the algorithm. At\\neach iteration, one new distinguished access string is found associated to a distinct\\nstate of A, thus, at most|A| states are created. For each counter-examplex, at most\\n|x| tree operations are performed. ConstructingˆA requires O(|Σ||A|) tree operations.\\nThe cost of a tree operation is O(|A|) since it consists of at most |A| membership\\nqueries. Thus, the overall complexity of the algorithm is inO(|Σ||A|2 +n|A|), where\\nn is the maximum length of a counter-example. Note that this analysis assumes\\nthat equivalence and membership queries are made in constant time.\\nOur analysis shows the following result.13.4 Identiﬁcation in the limit 303\\nTheorem 13.4 Learning DFAs with queries\\nThe class of all DFAs is eﬃciently exactly learnable using membership and equiva-\\nlence queries.\\nFigure 13.5 illustrates a full execution of the algorithm in a speciﬁc case.\\nIn the next section, we examine a diﬀerent learning scenario for automata.\\n13.4 Identiﬁcation in the limit\\nIn the identiﬁcation in the limit framework , the problem consists of identifying a\\ntarget concept c exactly after receiving a ﬁnite set of examples. A class of languages\\nis said to be identiﬁable in the limit if there exists an algorithm that identiﬁes\\nany language L in that class after examining a ﬁnite number of examples and its\\nhypothesis remains unchanged thereafter.\\nThis framework is perhaps less realistic from a computational point of view since\\nit requires no upper bound on the number of instances or the eﬃciency of the\\nalgorithm. Nevertheless, it has been argued by some to be similar to the scenario\\nof humans learning languages. In this framework as well, negative results hold for\\nthe general problem of learning DFAs.\\nTheorem 13.5\\nDeterministic automata are not identiﬁable in the limit from positive examples.\\nSome sub-classes of ﬁnite automata can however be successfully identiﬁed in the\\nlimit. Most algorithms for inference of automata are based on a state-partitioning\\nparadigm. They start with an initial DFA, typically a tree accepting the ﬁnite set\\nof sample strings available and the trivial partition: each block is reduced to one\\nstate of the tree. At each iteration, they merge partition blocks while preserving\\nsome congruence property. The iteration ends when no other merging is possible.\\nThe ﬁnal partition deﬁnes the automaton inferred as follows. Thus, the choice of\\nthe congruence fully determines the algorithm and a variety of diﬀerent algorithms\\ncan be deﬁned by varying that choice. A state-splitting paradigm can be similarly\\ndeﬁned starting from the single-state automaton accepting Σ\\n∗. In this section, we\\npresent an algorithm for learning reversible automata, which is a special instance\\nof the general state-partitioning algorithmic paradigm just described.\\nLet A =( Σ,Q ,I,F,E )b eaD F Aa n dl e tπ be a partition of Q.T h eD F Ad e ﬁ n e d\\nby the partition π is called the automaton quotient of A and π. It is denoted by304 Learning Automata and Languages\\nA/π and deﬁned as follows: A/π =( Σ,π,I π,Fπ,E π)w i t h\\nIπ = {B ∈ π: I ∩ B ̸= ∅}\\nFπ = {B ∈ π: F ∩ B ̸= ∅}\\nEπ = {(B,a,B ′): ∃(q,a,q ′) ∈ E | q ∈ B,q ′ ∈ B′,B ∈ π,B ′ ∈ π}.\\nLet S be a ﬁnite set of strings and let Pref(S) denote the set of preﬁxes of all strings\\nof S.A preﬁx-tree automaton accepting exactly the set of strings S is a particular\\nDFA denoted by PT (S)=( Σ , Pref(S), {ϵ},S ,ES)w h e r eΣi st h es e to fa l p h a b e t\\nsymbols used in S and ES deﬁned as follows:\\nES = {(x, a, xa): x ∈ Pref(S),x a ∈ Pref(S)}.\\nFigure 13.7a shows the preﬁx-tree automaton of a particular set of strings S.\\n13.4.1 Learning reversible automata\\nIn this section, we show that the sub-class of reversible automata or reversible\\nlanguages can be identiﬁed in the limit.\\nGiven a DFA A,w ed e ﬁ n ei t sreverse A\\nR as the automaton derived from A by\\nmaking the initial state ﬁnal, the ﬁnal states initial, and by reversing the direction of\\nevery transition. The language accepted by the reverse ofA is precisely the language\\nof the reverse (or mirror image) of the strings accepted by A.\\nDeﬁnition 13.2 Reversible automata\\nA ﬁnite automaton A is said to be reversible iﬀ bothA and A\\nR are deterministic. A\\nlanguage L is said to be reversible if it is the language accepted by some reversible\\nautomaton.\\nSome direct consequences of this deﬁnition are that a reversible automaton A has\\na unique ﬁnal state and that its reverse AR is also reversible. Note also that a trim\\nreversible automaton A is minimal. Indeed, if states q and q′ in A are equivalent,\\nthen, they admit a common string x leading both from q and from q′ to a ﬁnal\\nstate. But, by the reverse determinism of A, reading the reverse of x from the ﬁnal\\nstate must lead to a unique state, which implies that q = q′.\\nFor any u ∈ Σ∗ and any language L ⊆ Σ∗,l e tS u ﬀL(u) denote the set of all\\npossible suﬃxes in L for u:\\nSuﬀL(u)= {v ∈ Σ∗: uv ∈ L}. (13.1)\\nSuﬀL(u) is also often denoted by u−1L.O b s e r v et h a ti fL is a reversible language13.4 Identiﬁcation in the limit 305\\nL, then the following implication holds for any two strings u, u′ ∈ Σ∗:\\nSuﬀL(u) ∩ SuﬀL(u′) ̸= ∅ =⇒ SuﬀL(u)=S u ﬀL(u′). (13.2)\\nIndeed, let A be a reversible automaton accepting L.L e t q be the state of A\\nreached from the initial state when reading u and q′ the one reached reading u′.I f\\nv ∈ SuﬀL(u) ∩ SuﬀL(u′), then v can be read both from q and q′ to reach the ﬁnal\\nstate. Since AR is deterministic, reading back the reverse of v from the ﬁnal state\\nmust lead to a unique state, therefore q = q′,t h a ti sS u ﬀL(u)=S u ﬀL(u′).\\nLet A =( Σ ,Q ,{i0}, {f0},E ) be a reversible automaton accepting a reversible\\nlanguage L. We deﬁne a set of strings SL as follows:\\nSL = {d[q]f[q]: q ∈ Q}∪{ d[q],a ,f[q′]: q,q ′ ∈ Q, a ∈ Σ} ,\\nwhere d[q] is a string of minimum length fromi0 to q,a n df[q] a string of minimum\\nlength from q to f0. As shown by the following proposition, SL characterizes the\\nlanguage L in the sense that any reversible language containingSL must contain L.\\nProposition 13.1\\nLet L be a reversible language. Then,L is the smallest reversible language containing\\nSL.\\nProof Let L′ be a reversible language containing SL and let x = x1 ··· xn be\\nas t r i n ga c c e p t e db yL,w i t hxk ∈ Σf o r k ∈ [1,n ]a n d n ≥ 1. For convenience,\\nwe also deﬁne x0 as ϵ.L e t( q0,x1,q1) ··· (qn−1,xn,q n) be the accepting path in\\nA labeled with x. We show by recurrence that Suﬀ L′ (x0 ··· xk)=S u ﬀ L′ (d[qk])\\nfor all k ∈ [0,n ]. Since d[q0]= d[i0]= ϵ, this clearly holds for k =0 .N o w\\nassume that Suﬀ L′ (x0 ··· xk)=S u ﬀ L′ (d[qk]) for some k ∈ [0,n − 1]. This im-\\nplies immediately that Suﬀ L′ (x0 ··· xkxk+1)=S u ﬀ L′ (d[qk]xk+1). By deﬁnition,\\nSL contains both d[qk+1]f[qk+1]a n d d[qk]xk+1f[qk+1]. Since L′ includes SL,t h e\\nsame holds for L′.T h u s ,f[qk+1] belongs to SuffL′ (d[qk+1) ∩ SuffL′ (d[qk]xk+1).\\nIn view of (13.2), this implies that Suﬀ L′ (d[qk]xk+1)=S u ﬀL′ (d[qk+1]). Thus, we\\nhave SuﬀL′ (x0 ··· xkxk+1)=S u ﬀ L′ (d[qk+1]). This shows that Suﬀ L′ (x0 ··· xk)=\\nSuﬀL′ (d[qk]) holds for all k ∈ [0,n ], in particular, for k = n. Note that since\\nqn = f0,w eh a v ef[qn]= ϵ,t h e r e f o r ed[qn]= d[qn]f[qn]i si n S ⊆ L′,w h i c h\\nimplies that SuﬀL′ (d[qn]) contains ϵ and thus that SuﬀL′ (x0 ··· xk) contains ϵ.T h i s\\nis equivalent to x = x0 ··· xk ∈ L′.\\nFigure 13.6 shows the pseudocode of an algorithm for inferring a reversible\\nautomaton from a sample S of m strings x1,...,x m. The algorithm starts by\\ncreating a preﬁx-tree automaton A for S (line 1) and then iteratively deﬁnes a\\npartition π of the states of A, starting with the trivial partition π0 with one block\\nper state (line 2). The automaton returned is the quotient ofA and the ﬁnal partition306 Learning Automata and Languages\\nLearnReversibleAutomata(S =( x1,...,x m))\\n1 A =( Σ,Q ,{i0},F,E ) ← PT (S)\\n2 π ← π0 ⊿ trivial partition.\\n3 list ←{ (f,f ′): f ′ ∈ F } ⊿f arbitrarily chosen in F.\\n4 while list ̸= ∅do\\n5 Remove(list, (q1,q2))\\n6 if B(q1,π) ̸= B(q2,π) then\\n7 B1 ← B(q1,π)\\n8 B2 ← B(q2,π)\\n9 for all a ∈ Σ do\\n10 if (succ(B1,a) ̸= ∅) ∧ (succ(B2,a) ̸= ∅) then\\n11 Add(list,(succ(B1,a),s u c c(B2,a)))\\n12 if (pred(B1,a) ̸= ∅∧ (pred(B1,a) ̸= ∅) then\\n13 Add(list,(pred(B1,a),p re d(B2,a)))\\n14 Update(succ, pred, B1,B2)\\n15 π ← Merge(π,B1,B2)\\n16 return A/π\\nFigure 13.6 Algorithm for learning reversible automata from a set of positive\\nstrings S.\\nπ deﬁned.\\nThe algorithm maintains a list list of pairs of states whose corresponding blocks\\nare to be merged, starting with all pairs of ﬁnal states ( f,f ′) for an arbitrarily\\nchosen ﬁnal state f ∈ F (line 3). We denote byB(q,π) the block containingq based\\non the partition π.\\nFor each block B and alphabet symbol a ∈ Σ, the algorithm also maintains a\\nsuccessor succ(B,a ), that is, a state that can be reached by reading a from a state\\nof B; succ(B,a )= ∅if no such state exists. It maintains similarly the predecessor\\npred(B,a ), which is a state that admits a transition labeled with a leading to a\\nstate in B; pred(B,a )= ∅if no such state exists.\\nThen, while list is not empty, a pair is removed from list and processed as\\nfollows. If the pair ( q1,q ′\\n1) has not been already merged, the pairs formed by the\\nsuccessors and predecessors of B1 = B(q1,π)a n dB2 = B(q2,π) are added to list\\n(lines 10–13). Before merging blocks B1 and B2 i n t oan e wb l o c kB′ that deﬁnes13.4 Identiﬁcation in the limit 307\\n0 1a\\n10\\nb\\n2a\\n5\\nb\\n3a 4a\\n6a\\n8\\nb\\n7b\\n9a\\n11a\\n14\\nb\\n12b 13a\\n{5, 11}\\n{1, 3, 8, 12}\\nb\\n{6, 10}\\na\\n{0, 2, 4, 7, \\n9, 13, 14}\\na b\\nb\\na\\na\\nb\\n(a) (b)\\nFigure 13.7 Example of inference of a reversible automaton. (a) Preﬁx-treePT (S)\\nrepresenting S =( ϵ, aa, bb, aaaa, abab, abba, baba). (b) Automaton bA returned by\\nLearnReversibleAutomata() for the input S. A double-direction arrow represents\\ntwo transitions with the same label with opposite directions. The language accepted\\nby\\nbA i st h a to fs t r i n g sw i t ha ne v e nn u m b e ro fasa n dbs.\\na new partition π (line 15), the successor and predecessor values for the new block\\nB′ are deﬁned as follows (line 14). For each symbol a ∈ Σ, succ(B′,a)= ∅ if\\nsucc(B1,a)= succ(B2,a)= ∅, otherwise succ(B′,a)i ss e tt oo n eo fsucc(B1,a)i fi t\\nis non-empty,succ(B2,a) otherwise. The predecessor values are deﬁned in a similar\\nway. Figure 13.7 illustrates the application of the algorithm in the case of a sample\\nwith m =7s t r i n g s .\\nProposition 13.2\\nLet S be a ﬁnite set of strings and let A = PT (S) be the preﬁx-tree automaton de-\\nﬁned from S. Then, the ﬁnal partition deﬁned by LearnReversibleAutomata()\\nused with input S is the ﬁnest partition π for which A/π is reversible.\\nProof Let T be the number of iterations of the algorithm for the input sampleS.\\nWe denote byπt the partition deﬁned by the algorithm after t ≥ 1 iterations of the\\nloop, with πT the ﬁnal partition.\\nA/πT is a reversible automaton since all ﬁnal states are guaranteed to be merged\\ninto the same block as a consequence of the initialization step of line 3 and, for\\nany block B, by deﬁnition of the algorithm, states reachable by a ∈ Σf r o mB are\\ncontained in the same block, and similarly for those admitting a transition labeled\\nwith a to a state of B.\\nLet π\\n′ be a partition of the states of A for which A/π′ is reversible. We show\\nby recurrence that πT reﬁnes π′. Clearly, the trivial partitionπ0 reﬁnes π′.A s s u m e\\nthat πs reﬁnes π′ for all s ≤ t. πt+1 is obtained from π by merging two blocks\\nB(q1,πt)a n d B(q2,πt). Since πt reﬁnes π′,w em u s th a v eB(q1,πt) ⊆ B(q1,π ′)\\nand B(q2,πt) ⊆ B(q2,π ′). To show that πt+1 reﬁnes π′,i ts u ﬃ c e st op r o v et h a t308 Learning Automata and Languages\\nB(q1,π ′)= B(q2,π ′).\\nA reversible automaton has only one ﬁnal state, therefore, for the partition π′,\\na l lﬁ n a ls t a t e so fA must be placed in the same block. Thus, if the pair ( q1,q2)\\nprocessed at the ( t + 1)th iteration is a pair of ﬁnal states placed in list at the\\ninitialization step (line 3), then we must have B(q1,π ′)= B(q2,π ′). Otherwise,\\n(q1,q2) was placed in list as a pair of successor or predecessor states of two states\\nq′\\n1 and q′\\n2 merged at a previous iteration s ≤ t.S i n c eπs reﬁnes π′, q′\\n1 and q′\\n2 are in\\nt h es a m eb l o c ko fπ′ and since A/π′ is reversible, q1 and q2 must also be in the same\\nblock as successors or predecessors of the same block for the same labela ∈ Σ, thus\\nB(q1,π ′)= B(q2,π ′).\\nTheorem 13.6\\nLet S be a ﬁnite set of strings and let A be the automaton returned by\\nLearnReversibleAutomata() when used with inputS.T h e n ,L(A) is the small-\\nest reversible language containing S.\\nProof Let L be a reversible language containing S,a n dl e tA′ be a reversible\\nautomaton with L(A′)= L. Since every string of S is accepted by A′,a n y\\nu ∈ Pref(S) can be read from the initial state of A′ to reach some state q(u)\\nof A′. Consider the automaton A′′ derived from A′ by keeping only states of the\\nform q(u) and transitions between such states. A′′ has the unique ﬁnal state of A′\\nsince q(u)i sﬁ n a lf o ru ∈ S, and it has the initial state of A′,s i n c eϵ is a preﬁx\\nof strings of S.F u r t h e r m o r e ,A′′ directly inherits from A′ the property of being\\ndeterministic and reverse deterministic. Thus, A′′ is reversible.\\nThe states of A′′ deﬁne a partition of Pref( S): u, v ∈ Pref(S) are in the same\\nblock iﬀ q(u)= q(v). Since by deﬁnition of the preﬁx-tree PT (S), its states\\ncan be identiﬁed with Pref( S), the states of A′′ also deﬁne a partition π′ of the\\nstates of PT (S)a n dt h u sA′′ = PT (S)/π′. By proposition 13.2, the partition\\nπ deﬁned by algorithm LearnReversibleAutomata() run with input S is the\\nﬁnest such that PT (S)/π is reversible. Therefore, we must have L(PT (S)/π) ⊆\\nL(PT (S)/π′)= L(A′′). Since A′′ is a sub-automaton of A′, L contains L(A′′)a n d\\ntherefore L(PT (S)/π)= L(A), which concludes the proof.\\nFor the following theorem, a positive presentation of a language L is an inﬁnite\\nsequence (xn)n∈N such that {xn : n ∈ N} = L. Thus, in particular, for any x ∈ L\\nthere exists n ∈ N such that x = xn. An algorithm identiﬁes L in the limit from a\\npositive presentation if there exists N ∈ N such that for n ≥ N t h eh y p o t h e s i si t\\nreturns is L.\\nTheorem 13.7 Identiﬁcation in the limit of reversible languages\\nLet L be a reversible language, then algorithm LearnReversibleAutomata()\\nidentiﬁes L in the limit from a positive presentation.13.5 Chapter notes 309\\nProof Let L be a reversible language. By proposition 13.1, L admits a ﬁnite\\ncharacteristic sample SL.L e t( xn)n∈N be a positive presentation of L and let\\nXn denote the union of the ﬁrst n elements of the sequence. Since SL is ﬁnite,\\nthere exists N ≥ 1 such that SL ⊆ XN . By theorem 13.6, for any n ≥ N,\\nLearnReversibleAutomata( )r u no nt h eﬁ n i t es a m p l eXn returns the smallest\\nreversible languageL′ containing Xn a fortioriSL,w h i c h ,b yd e ﬁ n i t i o no fSL,i m p l i e s\\nthat L′ = L.\\nThe main operations needed for the implementation of the algorithm for learning\\nreversible automata are the standard find and union to determine the block a\\nstate belongs to and to merge two blocks into a single one. Using a disjoint-set\\ndata structure for these operations, the time complexity of the algorithm can be\\nshown to be in O(nα(n)), where n denotes the sum of the lengths of all strings\\nin the input sample S and α(n) the inverse of the Ackermann function, which is\\nessentially constant (α(n) ≤ 4f o rn ≤ 1080).\\n13.5 Chapter notes\\nFor an overview of ﬁnite automata and some related recent results, see Hopcroft\\nand Ullman [1979] or the more recent Handbook chapter by Perrin [1990], as well\\nas the series of books by M. Lothaire [Lothaire, 1982, 1990, 2005].\\nTheorem 13.1, stating that the problem of ﬁnding a minimum consistent DFA is\\nNP-hard, is due to Gold [1978]. This result was later extended by Angluin [1978].\\nPitt and Warmuth [1993] further strengthened these results by showing that even an\\napproximation within a polynomial function of the size of the smallest automaton\\nis NP-hard (theorem 13.2). Their hardness results apply also to the case where\\nprediction is made using NFAs. Kearns and Valiant [1994] presented hardness results\\nof a diﬀerent nature relying on cryptographic assumptions. Their results imply that\\nno polynomial-time algorithm can learn consistent NFAs polynomial in the size of\\nthe smallest DFA from a ﬁnite sample of accepted and rejected strings if any of\\nthe generally accepted cryptographic assumptions holds: if factoring Blum integers\\nis hard; or if the RSA public key cryptosystem is secure; or if deciding quadratic\\nresiduosity is hard.\\nOn the positive side, Trakhtenbrot and Barzdin [1973] showed that the smallest\\nﬁnite automaton consistent with the input data can be learned exactly from a\\nuniform complete sample, whose size is exponential in the size of the automaton.\\nThe worst-case complexity of their algorithm is exponential, but a better average-\\ncase complexity can be obtained assuming that the topology and the labeling are\\nselected randomly [Trakhtenbrot and Barzdin, 1973] or even that the topology is\\nselected adversarially [Freund et al., 1993].310 Learning Automata and Languages\\nCortes, Kontorovich, and Mohri [2007a] study an approach to the problem of\\nlearning automata based on linear separation in some appropriate high-dimensional\\nfeature space; see also Kontorovich et al. [2006, 2008]. The mapping of strings to\\nthat feature space can be deﬁned implicitly using the rational kernels presented in\\nchapter 5, which are themselves deﬁned via weighted automata and transducers.\\nThe model of learning with queries was introduced by Angluin [1978], who also\\nproved that ﬁnite automata can be learned in time polynomial in the size of\\nthe minimal automaton and that of the longest counter-example. Bergadano and\\nVarricchio [1995] further extended this result to the problem of learning weighted\\nautomata deﬁned over any ﬁeld. Using the relationship between the size of a minimal\\nweighted automaton over a ﬁeld and the rank of the corresponding Hankel matrix,\\nthe learnability of many other concepts classes such as disjoint DNF can be shown\\n[Beimel et al., 2000]. Our description of an eﬃcient implementation of the algorithm\\nof Angluin [1982] using decision trees is adapted from Kearns and Vazirani [1994].\\nThe model of identiﬁcation in the limit of automata was introduced and analyzed\\nby Gold [1967]. Deterministic ﬁnite automata were shown not to be identiﬁable in\\nthe limit from positive examples [Gold, 1967]. But, positive results were given for\\nthe identiﬁcation in the limit of a number of sub-classes, such as the family of k-\\nreversible languages Angluin [1982] considered in this chapter. Positive results also\\nhold for learning subsequential transducers Oncina et al. [1993]. Some restricted\\nclasses of probabilistic automata such as acyclic probabilistic automata were also\\nshown by Ron et al. [1995] to be eﬃciently learnable.\\nThere is a vast literature dealing with the problem of learning automata. In\\nparticular, positive results have been shown for a variety of sub-families of ﬁnite\\nautomata in the scenario of learning with queries and learning scenarios of diﬀerent\\nkinds have been introduced and analyzed for this problem. The results presented in\\nthis chapter should therefore be viewed only as an introduction to that material.\\n13.6 Exercises\\n13.1 Minimal DFA. Show that a minimal DFA A also has the minimal number\\nof transitions among all other DFAs equivalent to A. Prove that a language L is\\nregular iﬀ Q = {SuﬀL(u): u ∈ Σ∗} is ﬁnite. Show that the number of states of a\\nminimal DFA A with L(A)= L is precisely the cardinality of Q.\\n13.2 VC-dimension of ﬁnite automata.\\n(a) What is the VC-dimension of the family of all ﬁnite automata? What does\\nthat imply for PAC-learning of ﬁnite automata? Does this result change if we13.6 Exercises 311\\nrestrict ourselves to learning acyclic automata (automata with no cycles)?\\n(b) Show that the VC-dimension of the family of DFAs with at most n states\\nis bounded by O(|Σ|n log n).\\n13.3 PAC learning with membership queries. Give an example of a concept classC\\nthat is eﬃciently PAC-learnable with membership queries but that is not eﬃciently\\nexactly learnable.\\n13.4 Learning monotone DNF formulae with queries. Show that the class of mono-\\ntone DNF formulae overn variables is eﬃciently exactly learnable using membership\\nand equivalence queries. (Hint:a prime implicant t of a formula f is a product of\\nliterals such that t implies f but no proper sub-term of t implies f. Use the fact\\nthat for monotone DNF, the number of prime implicants is at the most the number\\nof terms of the formula.)\\n13.5 Learning with unreliable query responses. Consider the problem where the\\nlearner must ﬁnd an integer x selected by the oracle within [1 ,n ], where n ≥ 1i s\\ngiven. To do so, the learner can ask questions of the form ( x ≤ m?) or (x>m ?)\\nfor m ∈ [1,n ] .T h eo r a c l er e s p o n d st ot h e s eq u e s t i o n sb u tm a yg i v ea ni n c o r r e c t\\nresponse to k questions. How many questions should the learner ask to determine\\nx?( Hint: observe that the learner can repeat each question 2 k +1t i m e sa n du s e\\nthe majority vote.)\\n13.6 Algorithm for learning reversible languages. What is the DFA A returned\\nby the algorithm for learning reversible languages when applied to the sample\\nS = {ab, aaabb, aabbb, aabbbb}?S u p p o s ew ea d dan e ws t r i n gt ot h es a m p l e ,s a y\\nx = abab.H o ws h o u l dA be updated to compute the result of the algorithm for\\nS ∪{x}? More generally, describe a method for updating the result of the algorithm\\nincrementally.\\n13.7 k-reversible languages. A ﬁnite automaton A\\n′ is said to be k-deterministic if\\nit is deterministic modulo a lookahead k: if two distinct states p and q are both\\ninitial, or are both reached from another state r by reading a ∈ Σ, then no string\\nu of length k can be read in A′ both from p and q. A ﬁnite automaton A is said to\\nbe k-reversible if it is deterministic and if AR is k-deterministic. A language L is\\nk-reversible if it is accepted by some k-reversible automaton.\\n(a) Prove that L is k-reversible iﬀ for any strings u, u′,v ∈ Σ∗ with |v| = k,\\nSuﬀL(uv) ∩ SuﬀL(u′v) ̸= ∅ =⇒ SuﬀL(uv)=S u ﬀL(u′v).312 Learning Automata and Languages\\n(b) Show that a k-reversible language admits a characteristic language.\\n(c) Show that the following deﬁnes an algorithm for learning k-reversible\\nautomata. Proceed as in the algorithm for learning reversible automata but\\nwith the following merging rule instead: merge blocks B\\n1 and B2 if they can\\nbe reached by the same string u of length k from some other block and if B1\\nand B2 are both ﬁnal or have a common successor.14 Reinforcement Learning\\nThis chapter presents an introduction to reinforcement learning, a rich area of\\nmachine learning with connections to control theory, optimization, and cognitive\\nsciences. Reinforcement learning is the study of planing and learning in a scenario\\nwhere a learner actively interacts with the environment to achieve a certain goal.\\nThis active interaction justiﬁes the terminology ofagent used to refer to the learner.\\nThe achievement of the agent’s goal is typically measured by the reward he receives\\nfrom the environment and which he seeks to maximize.\\nWe ﬁrst introduce the general scenario of reinforcement learning and then intro-\\nduce the model of Markov decision processes (MDPs), which is widely adopted in\\nthis area, as well as essential concepts such as that ofpolicy or policy value related\\nto this model. The rest of the chapter presents several algorithms for the planning\\nproblem, which corresponds to the case where the environment model is known to\\nthe agent, and then a series of learning algorithms for the more general case of an\\nunknown model.\\n14.1 Learning scenario\\nThe general scenario of reinforcement learning is illustrated by ﬁgure 14.1. Unlike\\nthe supervised learning scenario considered in previous chapters, here, the learner\\ndoes not passively receive a labeled data set. Instead, he collects information\\nthrough a course of actions by interacting with the environment.I nr e s p o n s et o\\nan action, the learner or agent, receives two types of information: his current state\\nin the environment, and a real-valued reward, which is speciﬁc to the task and its\\ncorresponding goal.\\nThere are several diﬀerences between the learning scenario of reinforcement\\nlearning and that of supervised learning examined in most of the previous chapters.\\nUnlike the supervised learning scenario, in reinforcement learning there is no ﬁxed\\ndistribution according to which instances are drawn; the choice of a policy deﬁnes\\nt h ed i s t r i b u t i o n .I nf a c t ,s l i g h tc h a n g e st ot h ep o l i c ym a yh a v ed r a m a t i ce ﬀ e c t so n\\nthe rewards received. Furthermore, in general, the environment may not be ﬁxed314 Reinforcement Learning\\nEnvironment\\nAgent\\naction\\nstate\\nreward\\nFigure 14.1 Representation of the general scenario of reinforcement learning.\\nand could vary as a result of the actions selected by the agent. This may be a more\\nrealistic model for some learning problems than the standard supervised learning.\\nThe objective of the agent is to maximize his reward and thus to determine\\nthe best course of actions, or policy, to achieve that objective. However, the\\ninformation he receives from the environment is only the immediate reward related\\nto the action just taken. No future or long-term reward feedback is provided by\\nthe environment. An important aspect of reinforcement learning is to take into\\nconsideration delayed rewards or penalties. The agent is faced with the dilemma\\nbetween exploring unknown states and actions to gain more information about the\\nenvironment and the rewards, and exploiting the information already collected to\\noptimize his reward. This is known as the exploration versus exploitation trade-\\noﬀ inherent in reinforcement learning. Note that within this scenario, training and\\ntesting phases are intermixed.\\nTwo main settings can be distinguished here: the case where the environment\\nmodel is known to the agent, in which case his objective of maximizing the reward\\nreceived is reduced to a planning problem, and the case where the environment\\nmodel is unknown, in which case he faces a learning problem. In the latter case,\\nthe agent must learn from the state and reward information gathered to both\\ngain information about the environment and determine the best action policy. This\\nchapter presents algorithmic solutions for both of these settings.\\n14.2 Markov decision process model\\nWe ﬁrst introduce the model of Markov decision processes (MDPs), a model of the\\nenvironment and interactions with the environment widely adopted in reinforcement\\nlearning. An MDP is a Markovian process deﬁned as follows.\\nDeﬁnition 14.1 MDPs\\nA Markov decision process (MDP) is deﬁned by:14.3 Policy 315\\nas e to f states S, possibly inﬁnite.\\na start state or initial state s0 ∈ S.\\nas e to f actions A, possibly inﬁnite.\\na transition probability Pr[s′|s, a]: distribution over destination statess′ = δ(s, a).\\na reward probability Pr[r′|s, a]: distribution over rewards returnedr′ = r(s, a).\\nThe model is Markovian because the transition and reward probabilities depend\\nonly on the current state s and not the entire history of states and actions taken.\\nThis deﬁnition of MDP can be further generalized to the case of non-discrete state\\na n da c t i o ns e t s .\\nIn a discrete-time model, actions are taken at a set ofdecision epochs {0,...,T },\\nand this is the model we will adopt in what follows. This model can also be\\nstraightforwardly generalized to a continuous-time one where actions are taken at\\narbitrary points in time.\\nWhen T is ﬁnite, the MDP is said to have a ﬁnite horizon . Independently of the\\nﬁniteness of the time horizon, an MDP is said to be ﬁnite when both S and A are\\nﬁnite sets. Here, we are considering the general case where the reward r(s, a)a t\\nstate s when taking action a is a random variable. However, in many cases, the\\nreward is assumed to be a deterministic function of the pair of the state and action\\npair (s, a).\\nFigure 14.2 illustrates the model corresponding to an MDP. At time t ∈ [0,T ]\\nthe state observed by the agent iss\\nt and he takes action at ∈ A. The state reached\\nis st+1 (with probability Pr[ st+1|at,st]) and the reward received rt+1 ∈ R (with\\nprobability Pr[rt+1|at,st]).\\nMany real-world tasks can be represented by MDPs. Figure 14.3 gives the example\\nof a simple MDP for a robot picking up balls on a tennis court.\\n14.3 Policy\\nThe main problem for an agent in an MDP environment is to determine the action\\nto take at each state, that is, an action policy.\\n14.3.1 Deﬁnition\\nDeﬁnition 14.2 Policy\\nA policy is a mapping π: S → A.\\nMore precisely, this is the deﬁnition of a stationary policy since the choice of the\\naction does not depend on the time. More generally, we could deﬁne anon-stationary316 Reinforcement Learning\\nst st+1 st+2\\nat/rt+1 at+1/rt+2\\nFigure 14.2 Illustration of the states and transitions of an MDP at diﬀerent times.\\npolicy as a sequence of mappingsπt : S → A indexed by t. In particular, in the ﬁnite\\nhorizon case, typically a non-stationary policy is necessary.\\nThe agent’s objective is to ﬁnd a policy that maximizes his expected (reward)\\nreturn. The return he receives following a policyπ along a speciﬁc sequence of states\\nst,...,s T is deﬁned as follows:\\nﬁnite horizon ( T< ∞ ): ∑T −t\\nτ=0 r(st+τ,π(st+τ)).\\ninﬁnite horizon ( T = ∞ ): ∑T −t\\nτ=0 γτr(st+τ,π(st+τ)), where γ ∈ [0, 1) is a constant\\nfactor less than one used to discount future rewards.\\nNote that the return is a single scalar summarizing a possibly inﬁnite sequence\\nof immediate rewards. In the discounted case, early rewards are viewed as more\\nvaluable than later ones.\\nThis leads to the following deﬁnition of the value of a policy at each state.\\n14.3.2 Policy value\\nDeﬁnition 14.3 Policy value\\nThe value V\\nπ(s) of a policy π at state s ∈ S is deﬁned as the expected reward\\nreturned when starting at s and following policy π:\\nﬁnite horizon: Vπ(s)=E\\n[∑T −t\\nτ=0 r(st+τ,π(st+τ))|st = s\\n]\\n;\\ninﬁnite discounted horizon: Vπ(s)=E\\n[∑T −t\\nτ=0 γτr(st+τ,π(st+τ))|st = s\\n]\\n;\\nwhere the expectations are over the random selection of the statesst and the reward\\nvalues rt+1. An inﬁnite undiscounted horizon is also often considered based on the\\nlimit of the average reward, when it exists.\\nAs we shall see later, there exists a policy that is optimal for any start state. In view\\nof the deﬁnition of the policy values, seeking the optimal policy can be equivalently\\nformulated as determining a policy with maximum value at all states.\\n14.3.3 Policy evaluation\\nThe value of a policy at state s can be expressed in terms of its values at other\\nstates, forming a system of linear equations.14.3 Policy 317\\nstart  search/[.1, R1]\\nother\\nsearch/[.9, R1]  carry/[.5, R3]\\ncarry/[.5, -1] pickup/[1, R2]\\nFigure 14.3 Example of a simple MDP for a robot picking up balls on a tennis\\ncourt. The set of actions is A = {search,carry,pickup } and the set of states reduced\\nto S = {start, other}. Each transition is labeled with the action followed by the\\nprobability of the transition probability and the reward received after taking that\\naction. R1, R2,a n dR3 are real numbers indicating the reward associated to each\\ntransition (case of deterministic reward).\\nProposition 14.1 Bellman equation\\nThe values Vπ(s) of policy π at states s ∈ S for an inﬁnite horizon MDP obey the\\nfollowing system of linear equations:\\n∀s ∈ S, V π(s)=E [ r(s, π(s)] +γ\\n∑\\ns′\\nPr[s′|s, π(s)]Vπ(s′). (14.1)\\nProof We can decompose the expression of the policy value as a sum of the ﬁrst\\nterm and the rest of the terms:\\nVπ(s)=E\\n[T −t∑\\nτ=0\\nγτr(st+τ,π(st+τ)) | st = s\\n]\\n=E [r(s, π(s)] +γE\\n[T −t∑\\nτ=0\\nγτr(st+1+τ,π(st+1+τ)) | st = s\\n]\\n=E [r(s, π(s)] +γE[Vπ(δ(s, π(s)))],\\nsince we can recognize the expression of Vπ(δ(s, π(s))) in the expectation of the\\nsecond line.\\nThe Bellman equations can be rewritten as\\nV = R + γPV, (14.2)\\nusing the following notation: P denotes the transition probability matrix deﬁned\\nby Ps,s′ =P r [s′|s, π(s)] for all s, s′ ∈ S; V is the value column matrix whose sth\\ncomponent is Vs = Vπ(s); and R the reward column matrix whose sth component\\nis Rs =E [r(s, π(s)]. V is typically the unknown variable in the Bellman equations\\nand is determined by solving for it. The following theorem shows that for a ﬁnite318 Reinforcement Learning\\nMDP this system of linear equations admits a unique solution.\\nTheorem 14.1\\nFor a ﬁnite MDP, Bellman’s equation admits a unique solution given by\\nV0 =( I − γP)−1R. (14.3)\\nProof The Bellman equation (14.2) can be equivalently written as\\n(I − γP)V = R.\\nThus, to prove the theorem it suﬃces to show that (I − γP) is invertible. To do so,\\nnote that the norm inﬁnity of P can be computed using its stochasticity properties:\\n∥P∥∞ =m a x\\ns\\n∑\\ns′\\n|Pss′ | =m a x\\ns\\n∑\\ns′\\nPr[s′|s, π(s)] = 1.\\nThis implies that ∥γP∥∞ = γ< 1. The eigenvalues of P are thus all less than one,\\nand (I − γP) is invertible.\\nThus, for a ﬁnite MDP, when the transition probability matrix P and the reward\\nexpectations R are known, the value of policy π at all states can be determined by\\ninverting a matrix.\\n14.3.4 Optimal policy\\nThe objective of the agent can be reformulated as that of seeking the optimal policy\\ndeﬁned as follows.\\nDeﬁnition 14.4 Optimal policy\\nAp o l i c yπ\\n∗ is optimal if it has maximal value for all states s ∈ S.\\nThus, by deﬁnition, for any s ∈ S, Vπ∗(s)=m a xπ Vπ(s). We will use the shorter\\nnotation V ∗ instead of Vπ∗. V ∗(s) is the maximal cumulative reward the agent can\\nexpect to receive when starting at state s.\\nDeﬁnition 14.5 State-action value function\\nThe optimal state-action value function Q∗ is deﬁned for all (s, a) ∈ S × A as the\\nexpected return for taking actiona ∈ A at state s ∈ S and then following the optimal\\npolicy:\\nQ∗(s, a)=E [ r(s, a)] +γ\\n∑\\ns′ ∈S\\nPr[s′ | s, a]V ∗(s′). (14.4)14.4 Planning algorithms 319\\nIt is not hard to see then that the optimal policy values are related to Q∗ via\\n∀s ∈ S, V ∗(s)=m a x\\na∈A\\nQ∗(s, a). (14.5)\\nIndeed, by deﬁnition, V ∗(s) ≤ maxa∈A Q∗(s, a) for all s ∈ S.I ff o rs o m es we had\\nV ∗(s) < maxa∈A Q∗(s, a), then then maximizing action would deﬁne a better policy.\\nObserve also that, by deﬁnition of the optimal policy, we have\\n∀s ∈ S, π ∗(s) = argmax\\na∈A\\nQ∗(s, a). (14.6)\\nThus, the knowledge of the state-value function Q∗ is suﬃcient for the agent\\nto determine the optimal policy, without any direct knowledge of the reward or\\ntransition probabilities. Replacing Q∗ by its deﬁnition in (14.5) gives the following\\nsystem of equations for the optimal policy values V ∗(s):\\nV ∗(s)=m a x\\na∈A\\n{\\nE[r(s, a)] +γ\\n∑\\ns′ ∈S\\nPr[s′|s, a]V ∗(s′)\\n}\\n, (14.7)\\nalso known as Bellman equations. Note that this new system of equations is not\\nlinear due to the presence of the max operator. It is distinct from the previous linear\\nsystem we deﬁned under the same name in (14.1) and (14.2).\\n14.4 Planning algorithms\\nIn this section, we assume that the environment model is known. That is, the\\ntransition probability Pr[s\\n′|s, a]a n dt h ee x p e c t e dr e w a r dE [r(s, a)] for all s, s′ ∈ S\\nand a ∈ A are assumed to be given. The problem of ﬁnding the optimal policy then\\ndoes not require learning the parameters of the environment model or estimating\\nother quantities helpful in determining the best course of actions, it is purely a\\nplanning problem.\\nThis section discusses three algorithms for this planning problem: the value\\niteration algorithm, the policy iteration algorithm, and a linear programming\\nf o r m u l a t i o no ft h ep r o b l e m .\\n14.4.1 Value iteration\\nThe value iteration algorithm seeks to determine the optimal policy values V\\n∗(s)\\nat each state s ∈ S, and thereby the optimal policy. The algorithm is based on\\nthe Bellman equations (14.7). As already indicated, these equations do not form\\na system of linear equations and require a diﬀerent technique to determine the\\nsolution. The main idea behind the design of the algorithm is to use an iterative320 Reinforcement Learning\\nV alueIteration(V0)\\n1 V ← V0 ⊿ V0 arbitrary value\\n2 while ∥V − Φ(V)∥≥ (1−γ)ϵ\\nγ do\\n3 V ← Φ(V)\\n4 return Φ(V)\\nFigure 14.4 Value iteration algorithm.\\nmethod to solve them: the new values of V (s) are determined using the Bellman\\ne q u a t i o n sa n dt h ec u r r e n tv a l u e s .T h i sp r o c e s si sr e p e a t e du n t i lac o n v e r g e n c e\\ncondition is met.\\nFor a vector V in R\\n|S|,w ed e n o t eb yV (s)i t ssth coordinate, for any s ∈ S.L e t\\nΦ : R|S| → R|S| be the mapping deﬁned based on Bellman’s equations (14.7):\\n∀s ∈ S, [Φ(V)](s)=m a x\\na∈A\\n{\\nE[r(s, a)] +γ\\n∑\\ns′∈S\\nPr[s′|s, a]V (s′)\\n}\\n. (14.8)\\nThe maximizing actions a ∈ A in these equations deﬁne an action to take at each\\nstate s ∈ S, that is a policy π. We can thus rewrite these equations in matrix terms\\nas follows:\\nΦ(V)=m a x\\nπ\\n{Rπ + γPπV}, (14.9)\\nwhere Pπ is the transition probability matrix deﬁned by ( Pπ)ss′ =P r [s′|s, π(s)]\\nfor all s, s′ ∈ S,a n dRπ the reward vector deﬁned by ( Rπ)s =E [r(s, π(s)], for all\\ns ∈ S.\\nThe algorithm is directly based on (14.9). The pseudocode is given above. Starting\\nfrom an arbitrary policy value vector V0 ∈ R|S|, the algorithm iteratively applies\\nΦ to the current V to obtain a new policy value vector until ∥V − Φ(V)∥ <\\n(1−γ)ϵ\\nγ ,w h e r eϵ> 0 is a desired approximation. The following theorem proves the\\nconvergence of the algorithm to the optimal policy values.\\nTheorem 14.2\\nFor any initial value V0, the sequence deﬁned by Vn+1 = Φ(Vn) converges to V∗.\\nProof We ﬁrst show that Φ is γ-Lipschitz for the ∥·∥ ∞ .1 For any s ∈ S and\\n1. A β-Lipschitz function with β< 1i sa l s oc a l l e dβ-contracting.I na complete metric\\nspace, that is a metric space where any Cauchy sequence converges to a point of that14.4 Planning algorithms 321\\nV ∈ R|S|,l e ta∗(s) be the maximizing action deﬁning Φ(V)(s) in (14.8). Then, for\\nany s ∈ S and any U ∈ R|S|,\\nΦ(V)(s) − Φ(U)(s) ≤ Φ(V)(s) −\\n(\\nE[r(s, a∗(s))] +γ\\n∑\\ns′ ∈S\\nPr[s′ | s, a∗(s)]U(s′)\\n⎡\\n= γ\\n∑\\ns′∈S\\nPr[s′|s, a∗(s)][V(s′) − U(s′)]\\n≤ γ\\n∑\\ns′∈S\\nPr[s′|s, a∗(s)]∥V − U∥∞ = γ∥V − U∥∞ .\\nProceeding similarly with Φ(U)(s) − Φ(V)(s), we obtain Φ(U)(s) − Φ(V)(s) ≤\\nγ∥V − U∥∞ .T h u s ,|Φ(V)(s) − Φ(U)(s)|≤ γ∥V − U∥∞ for all s, which implies\\n∥Φ(V) − Φ(U)∥∞ ≤ γ∥V − U∥∞ ,\\nthat is the γ-Lipschitz property of Φ. Now, by Bellman equations (14.7), V∗ =\\nΦ(V∗), thus for any n ∈ N,\\n∥V∗ − Vn+1∥∞ = ∥Φ(V∗) − Φ(Vn)∥∞ ≤ γ∥V∗ − Vn∥∞ ≤ γn+1∥V∗ − V0∥∞ ,\\nwhich proves the convergence of the sequence to V∗ since γ ∈ (0, 1).\\nThe ϵ-optimality of the value returned by the algorithm can be shown as follows.\\nBy the triangle inequality and the γ-Lipschitz property of Φ, for any n ∈ N,\\n∥V∗ − Vn+1∥∞ ≤∥ V∗ − Φ(Vn+1)∥∞ + ∥Φ(Vn+1) − Vn+1∥∞\\n= ∥Φ(V∗) − Φ(Vn+1)∥∞ + ∥Φ(Vn+1) − Φ(Vn)∥∞\\n≤ γ∥V∗ − Vn+1∥∞ + γ∥Vn+1 − Vn∥∞ .\\nThus, if Vn+1 is the policy value returned by the algorithm, we have\\n∥V∗ − Vn+1∥∞ ≤ γ\\n1 − γ∥Vn+1 − Vn∥∞ ≤ ϵ.\\nThe convergence of the algorithm is inO(log 1\\nϵ ) number of iterations. Indeed, observe\\nthat\\n∥Vn+1 −Vn∥∞ = ∥Φ(Vn)−Φ(Vn−1)∥∞ ≤ γ∥Vn −Vn−1∥∞ ≤ γn∥Φ(V0)−V0∥∞ .\\nThus, if n is the largest integer such that (1−γ)ϵ\\nγ ≤∥ Vn+1 − Vn∥∞ ,i tm u s tv e r i f y\\nspace, a β-contracting function f admits a ﬁxed point:a n ys e q u e n c e(f(xn))n∈N converges\\nto some x with f(x)= x. RN , N ≥ 1, or, more generally, any ﬁnite-dimensional vector\\nspace, is a complete metric space.322 Reinforcement Learning\\n1\\na/[3/4, 2]\\n2\\na/[1/4, 2]\\nb/[1, 2]\\nd/[1, 3]\\nc/[1, 2]\\nFigure 14.5 Example of MDP with two states. The state set is reduced to\\nS = {1, 2} and the action set to A = {a, b, c, d}. Only transitions with non-zero\\nprobabilities are represented. Each transition is labeled with the action taken\\nfollowed by a pair [p, r] after a slash separator, where p is the probability of the\\ntransition and r the expected reward for taking that transition.\\n(1−γ)ϵ\\nγ ≤ γn∥Φ(V0) − V0∥∞ and therefore n ≤ O\\n(\\nlog 1\\nϵ\\n⎡\\n.2\\nFigure 14.5 shows a simple example of MDP with two states. The iterated values\\nof these states calculated by the algorithm for that MDP are given by\\nVn+1(1) = max\\n{\\n2+ γ\\n(3\\n4Vn(1) + 1\\n4Vn(2)\\n⎡\\n, 2+ γVn(2)\\n}\\nVn+1(2) = max\\n{\\n3+ γVn(1),2+ γVn(2)\\n}\\n.\\nFor V0(1) = −1, V0(2) = 1, and γ =1 /2, we obtain V1(1) = V1(2) = 5 /2.\\nThus, both states seem to have the same policy value initially. However, by the ﬁfth\\niteration, V5(1) = 4.53125, V5(2) = 5.15625 and the algorithm quickly converges\\nto the optimal values V∗(1) = 14/3a n dV∗(2) = 16/3 showing that state 2 has a\\nhigher optimal value.\\n14.4.2 Policy iteration\\nAn alternative algorithm for determining the best policy consists of using policy\\nevaluations, which can be achieved via a matrix inversion, as shown by theorem 14.1.\\nThe pseudocode of the algorithm known as policy iteration algorithm is given in\\nﬁgure 14.6. Starting with an arbitrary action policy π0, the algorithm repeatedly\\ncomputes the value of the current policy π via that matrix inversion and greedily\\nselects the new policy as the one maximizing the right-hand side of the Bellman\\nequations (14.9).\\nThe following theorem proves the convergence of the policy iteration algorithm.\\nTheorem 14.3\\n2. Here, the O-notation hides the dependency on the discount factor γ. As a function of\\nγ, the running time is not polynomial.14.4 Planning algorithms 323\\nPolicyIteration(π0)\\n1 π ← π0 ⊿π0 arbitrary policy\\n2 π′ ← nil\\n3 while (π ̸= π′) do\\n4 V ← Vπ ⊿ policy evaluation: solve (I − γPπ)V = Rπ.\\n5 π′ ← π\\n6 π ← argmaxπ {Rπ + γPπV} ⊿ greedy policy improvement.\\n7 return π\\nFigure 14.6 Policy iteration algorithm.\\nLet (Vn)n∈N be the sequence of policy values computed by the algorithm, then, for\\nany n ∈ N, the following inequalities hold:\\nVn ≤ Vn+1 ≤ V∗. (14.10)\\nProof Let πn+1 be the policy improvement at the nth iteration of the algorithm.\\nWe ﬁrst show that ( I − γPπn+1 )−1 preserves ordering, that is, for any column\\nmatrices X and Y in R|S|,i f( Y − X) ≥ 0,t h e n(I − γPπn+1 )−1(Y − X) ≥ 0.\\nAs shown in the proof of theorem 14.1, ∥γP∥∞ = γ< 1. Since the radius of\\nconvergence of the power series (1− x)−1 is one, we can use its expansion and write\\n(I − γPπn+1 )−1 =\\n∞∑\\nk=0\\n(γPπn+1 )k.\\nThus, if Z =( Y − X) ≥ 0,t h e n(I − γPπn+1 )−1Z = ∑∞\\nk=0(γPπn+1 )kZ ≥ 0,s i n c e\\nthe entries of matrix Pπn+1 and its powers are all non-negative as well as those of\\nZ.\\nNow, by deﬁnition of πn+1,w eh a v e\\nRπn+1 + γPπn+1 Vn ≥ Rπn + γPπn Vn = Vn,\\nwhich shows thatRπn+1 ≥ (I−γPπn+1 )Vn.S i n c e(I−γPπn+1 )−1 preserves ordering,\\nthis implies that Vn+1 =( I − γPπn+1 )−1Rπn+1 ≥ Vn, which concludes the proof\\nof the theorem.\\nNote that two consecutive policy values can be equal only at the last iteration of\\nthe algorithm. The total number of possible policies is |A||S|,t h u st h i sc o n s t i t u t e s\\na straightforward upper bound on the maximal number of iterations. Better upper324 Reinforcement Learning\\nbounds of the form O\\n(|A||S|\\n|S|\\n⎡\\nare known for this algorithm.\\nFor the simple MDP shown by ﬁgure 14.5, let the initial policy π0 be deﬁned by\\nπ0(1) = b, π0(2) = c. Then, the system of linear equations for evaluating this policy\\nis\\n{\\nVπ0 (1) = 1 +γVπ0 (2)\\nVπ0 (2) = 2 +γVπ0 (2),\\nwhich gives Vπ0 (1) = 1+γ\\n1−γ and Vπ0 (2) = 2\\n1−γ .\\nTheorem 14.4\\nLet (Un)n∈N be the sequence of policy values generated by the value iteration\\nalgorithm, and (Vn)n∈N the one generated by the policy iteration algorithm. If\\nU0 = V0,t h e n ,\\n∀n ∈ N, Un ≤ Vn ≤ V∗. (14.11)\\nProof We ﬁrst show that the function Φ previously introduced is monotonic. Let\\nU and V be such thatU ≤ V and letπ be the policy such thatΦ(U)= Rπ+γPπU.\\nThen,\\nΦ(U) ≤ Rπ + γPπV ≤ max\\nπ′\\n{Rπ′ + γPπ′ V} = Φ(V).\\nThe proof is by induction on n. Assume that Un ≤ Vn,t h e nb yt h em o n o t o n i c i t y\\nof Φ,w eh a v e\\nUn+1 = Φ(Un) ≤ Φ(Vn)=m a x\\nπ\\n{Rπ + γPπVn}.\\nLet πn+1 be the maximizing policy, that is,πn+1 =a r g m a xπ {Rπ +γPπVn}. Then,\\nΦ(Vn)= Rπn+1 + γPπn+1 Vn ≤ Rπn+1 + γPπn+1 Vn+1 = Vn+1,\\nand thus Un+1 ≤ Vn+1.\\nThe theorem shows that the policy iteration algorithm converges in a smaller\\nnumber of iterations than the value iteration algorithm due to the optimal policy.\\nBut, each iteration of the policy iteration algorithm requires computing a policy\\nvalue, that is, solving a system of linear equations, which is more expensive to\\ncompute that an iteration of the value iteration algorithm.\\n14.4.3 Linear programming\\nAn alternative formulation of the optimization problem deﬁned by the Bellman\\nequations (14.7) is via linear programming (LP), that is an optimization prob-14.5 Learning algorithms 325\\nlem with a linear objective function and linear constraints. LPs admit (weakly)\\npolynomial-time algorithmic solutions. There exist a variety of diﬀerent methods\\nfor solving relative large LPs in practice, using the simplex method, interior-point\\nmethods, or a variety of special-purpose solutions. All of these methods could be\\napplied in this context.\\nBy deﬁnition, the equations (14.7) are each based on a maximization. These\\nmaximizations are equivalent to seeking to minimize all elements of {V (s): s ∈ S}\\nunder the constraints V (s) ≥ E[r(s, a)] + γ∑\\ns′∈S Pr[s′|s, a]V (s′), (s ∈ S). Thus,\\nthis can be written as the following LP for any set of ﬁxed positive weightsα(s) > 0,\\n(s ∈ S):\\nmin\\nV\\n∑\\ns∈S\\nα(s)V (s) (14.12)\\nsubject to ∀s ∈ S, ∀a ∈ A, V(s) ≥ E[r(s, a)] +γ\\n∑\\ns′ ∈S\\nPr[s′|s, a]V (s′),\\nwhere α > 0 is the vector with the sth component equal to α(s).3 To make each\\ncoeﬃcient α(s) interpretable as a probability, we can further add the constraints that∑\\ns∈S α(s)=1 .T h en u m b e ro fr o w so ft h i sL Pi s|S||A| and its number of columns\\n|S|. The complexity of the solution techniques for LPs is typically more favorable in\\nterms of the number of rows than the number of columns. This motivates a solution\\nbased on the equivalent dual formulation of this LP which can be written as\\nmax\\nx\\n∑\\ns∈S,a∈A\\nE[r(s, a)]x(s, a) (14.13)\\nsubject to ∀s ∈ S,\\n∑\\na∈A\\nx(s′,a)= α(s′)+ γ\\n∑\\ns∈S,a∈A\\nPr[s′|s, a] x(s′,a)\\n∀s ∈ S, ∀a ∈ A, x(s, a) ≥ 0,\\nand for which the number of rows is only |S| and the number of columns |S||A|.\\nHere x(s, a) can be interpreted as the probability of being in state s and taking\\naction a.\\n14.5 Learning algorithms\\nThis section considers the more general scenario where the environment model of\\nan MDP, that is the transition and reward probabilities , is unknown. This matches\\n3. Let us emphasize that the LP is only in terms of the variables V (s), as indicated by\\nthe subscript of the minimization operator, and not in terms of V (s)a n dα(s).326 Reinforcement Learning\\nmany realistic applications of reinforcement learning where, for example, a robot is\\nplaced in an environment that it needs to explore in order to reach a speciﬁc goal.\\nHow can an agent determine the best policy in this context? Since the environment\\nmodels are not known, he may seek to learn them by estimating transition or reward\\nprobabilities. To do so, as in the standard case of supervised learning, the agent\\nneeds some amount of training information. In the context of reinforcement learning\\nwith MDPs, the training information is the sequence of immediate rewards the agent\\nreceives based on the actions he has taken.\\nThere are two main learning approaches that can be adopted. One known as the\\nmodel-free approach consists of learning an action policy directly. Another one, a\\nmodel-based approach, consists of ﬁrst learning the environment model, and then\\nuse that to learn a policy. The Q-learning algorithm we present for this problem is\\nwidely adopted in reinforcement learning and belongs to the family of model-free\\napproaches.\\nThe estimation and algorithmic methods adopted for learning in reinforcement\\nlearning are closely related to the concepts and techniques in stochastic approxi-\\nmation. Thus, we start by introducing several useful results of this ﬁeld that will\\nbe needed for the proofs of convergence of the reinforcement learning algorithms\\npresented.\\n14.5.1 Stochastic approximation\\nStochastic approximation methods are iterative algorithms for solving optimization\\nproblems whose objective function is deﬁned as the expectation of some random\\nvariable, or to ﬁnd the ﬁxed point of a function H that is accessible only through\\nnoisy observations. These are precisely the type of optimization problems found in\\nreinforcement learning. For example, for the Q-learning algorithm we will describe,\\nthe optimal state-action value function Q\\n∗ is the ﬁxed point of some function H\\nthat is deﬁned as an expectation and thus not directly accessible.\\nWe start with a basic result whose proof and related algorithm show the ﬂavor\\nof more complex ones found in stochastic approximation. The theorem is a gener-\\nalization of a result known as the strong law of large numbers. It shows that under\\nsome conditions on the coeﬃcients, an iterative sequence of estimatesμm converges\\nalmost surely (a.s.) to the mean of a bounded random variable.\\nTheorem 14.5 Mean estimation\\nLet X be a random variable taking values in[0, 1] and let x0,...,x m be i.i.d. values\\nof X. Deﬁne the sequence (μm)m∈N by\\nμm+1 =( 1 − αm)μm + αmxm, (14.14)14.5 Learning algorithms 327\\nwith μ0 = x0, αm ∈ [0,1], ∑\\nm≥0 αm =+ ∞ and ∑\\nm≥0 α2\\nm < +∞ .T h e n ,\\nμm\\na.s\\n−−→ E[X]. (14.15)\\nProof We give the proof of the L2 convergence. The a.s. convergence is shown\\nlater for a more general theorem. By the independence assumption, for m ≥ 0,\\nVar[μm+1]=( 1 − αm)2 Var[μm]+ α2\\nm Var[xm] ≤ (1 − αm) Var[μm]+ α2\\nm. (14.16)\\nLet ϵ> 0 and suppose that there existsN ∈ N such that for allm ≥ N, Var[μm] ≥ ϵ.\\nThen, for m ≥ N,\\nVar[μm+1] ≤ Var[μm] − αm Var[μm]+ α2\\nm ≤ Var[μm] − αmϵ + α2\\nm,\\nwhich implies, by reapplying this inequality, that\\nVar[μm+N ] ≤ Var[μN ] − ϵ\\nm+N∑\\nn=N\\nαn +\\nm+N∑\\nn=N\\nα2\\nn\\n\\ued19 \\ued18\\ued17 \\ued1a\\n→−∞ when m→∞\\n,\\ncontradicting Var[μm+N ] ≥ 0. Thus, this contradicts the existence of such an integer\\nN. Therefore, for all N ∈ N,t h e r ee x i s t sm0 ≥ N such that Var[μm0 ] ≤ ϵ.\\nChoose N large enough so that for all m ≥ N, the inequality αm ≤ ϵ holds. This\\nis possible since the sequence (α2\\nm)m∈N and thus (αm)m∈N converges to zero in view\\nof ∑\\nm≥0 α2\\nm < +∞ . We will show by induction that for anym ≥ m0, Var[μm] ≤ ϵ,\\nwhich implies the statement of the theorem.\\nAssume that Var[ μm] ≤ ϵ for some m ≥ m0. Then, using this assumption,\\ninequality 14.16, and the fact that αm ≤ ϵ, the following inequality holds:\\nVar[μm+1] ≤ (1 − αm)ϵ + ϵαm = ϵ.\\nThus, this proves that limm→ +∞ Var[μm] = 0, that is the L2 convergence of μm to\\nE[X].\\nNote that the hypotheses of the theorem related to the sequence (αm)m∈N hold in\\nparticular when αm = 1\\nm . The special case of the theorem with this choice of αm\\ncoincides with the strong law of large numbers. This result has tight connections\\nwith the general problem of stochastic optimization.\\nStochastic optimization is the general problem of ﬁnding the solution to the\\nequation\\nx = H(x),\\nwhere x ∈ RN ,w h e n328 Reinforcement Learning\\nH(x) cannot be computed, for example, because H is not accessible or because\\nthe cost of its computation is prohibitive;\\nbut an i.i.d. sample of m noisy observations H(xi)+ wi are available, i ∈ [1,m],\\nwhere the noise random variable w has expectation zero: E[w]= 0.\\nThis problem arises in a variety of diﬀerent contexts and applications. As we shall\\nsee, it is directly related to the learning problem for MDPs.\\nOne general idea for solving this problem is to use an iterative method and deﬁne\\na sequence (xt)t∈N in a way similar to what is suggested by theorem 14.5:\\nxt+1 =( 1 − αt)xt + αt[H(xt)+ wt] (14.17)\\n= xt + αt[H(xt)+ wt − xt], (14.18)\\nwhere (αt)t∈N follow conditions similar to those assumed in theorem 14.5. More\\ngenerally, we consider sequences deﬁned via\\nxt+1 = xt + αtD(xt,wt), (14.19)\\nwhere D is a function mapping RN × RN to RN . There are many diﬀerent theorems\\nguaranteeing the convergence of this sequence under various assumptions. We will\\npresent one of the most general forms of such theorems, which relies on the following\\ngeneral result.\\nTheorem 14.6 Supermartingale convergence\\nLet (X\\nt)t∈N, (Yt)t∈N,a n d (Zt)t∈N be sequences of non-negative random variables\\nsuch that ∑∞\\nt=0 Yt < ∞ .L e t Ft denote all the information for t′ ≤ t: Ft =\\n{(Xt′ )t′ ≤t, (Yt′ )t′≤t, (Zt′ )t′ ≤t}.T h e n ,i fE\\n[\\nXt+1\\n⏐⏐Ft\\n]\\n≤ Xt + Yt − Zt, the following\\nholds:\\nXt converges to a limit (with probability one).\\n∑∞\\nt=0 Zt < ∞ .\\nThe following is one of the most general forms of such theorems.\\nTheorem 14.7\\nLet D be a function mapping RN × RN to RN, (xt)t∈N and (wt)t∈N two sequences\\nin RN,a n d(αt)t∈N a sequence of real numbers with xt+1 = xt + αtD(xt,wt).L e t\\nFt denote the entire history for t′ ≤ t, that is: Ft = {(xt′ )t′ ≤t, (wt′ )t′ ≤t, (αt′ )t′ ≤t}.\\nLet Ψ denote x → 1\\n2 ∥x − x∗∥2\\n2 for some x∗ ∈ RN and assume that D and (α)t∈N\\nverify the following conditions:\\n∃K1,K2 ∈ R:E\\n[\\n∥D(xt,wt)∥2\\n2\\n⏐⏐ Ft\\n]\\n≤ K1 + K2 Ψ(xt);\\n∃c ≥ 0: ∇Ψ(xt)⊤ E\\n[\\nD(xt,wt)\\n⏐⏐ Ft\\n]\\n≤− cΨ(xt);14.5 Learning algorithms 329\\nαt > 0, ∑∞\\nt=0 αt = ∞ , ∑∞\\nt=0 α2\\nt < ∞ .\\nThen, the sequence xt converges almost surely to x∗:\\nxt\\na.s\\n−−→ x∗. (14.20)\\nProof Since function Ψ is quadratic, a Taylor expansion gives\\nΨ(xt+1)=Ψ ( xt)+ ∇Ψ(xt)⊤(xt+1 − xt)+ 1\\n2(xt+1 − xt)⊤∇2Ψ(xt)(xt+1 − xt).\\nThus,\\nE\\n[\\nΨ(xt+1)\\n⏐⏐Ft\\n]\\n=Ψ (xt)+ αt∇Ψ(xt)⊤ E\\n[\\nD(xt,wt)\\n⏐⏐Ft\\n]\\n+ α2\\nt\\n2 E\\n[\\n∥D(xt,wt)∥2⏐⏐Ft\\n]\\n≤ Ψ(xt) − αtcΨ(xt)+ α2\\nt\\n2 (K1 + K2Ψ(xt))\\n=Ψ (xt)+ α2\\nt K1\\n2 −\\n(\\nαtc − α2\\nt K2\\n2\\n⎡\\nΨ(xt).\\nSince by assumption the series∑∞\\nt=0 α2\\nt is convergent, (α2\\nt )t and thus (αt)t converges\\nto zero. Therefore, for t suﬃciently large, the term\\n(\\nαtc − α2\\nt K2\\n2\\n⎡\\nΨ(xt) has the\\nsign of αtcΨ(xt) and is non-negative, since αt > 0, Ψ( xt) ≥ 0, and c> 0.\\nThus, by the supermartingale convergence theorem 14.6, Ψ( xt) converges and∑∞\\nt=0\\n(\\nαtc − α2\\nt K2\\n2\\n⎡\\nΨ(xt) < ∞ . Since Ψ(xt) converges and ∑∞\\nt=0 α2\\nt < ∞ ,w eh a v e∑∞\\nt=0\\nα2\\nt K2\\n2 Ψ(xt) < ∞ .B u t ,s i n c e∑∞\\nt=0 αt = ∞ , if the limit of Ψ(xt) were non-zero,\\nwe would have ∑∞\\nt=0 αtcΨ(xt)= ∞ . This implies that the limit of Ψ( xt) is zero,\\nthat is limt→∞ ∥xt − x∗∥2 → 0, which implies xt\\na.s\\n−−→ x∗.\\nThe following is another related result for which we do not present the full proof.\\nTheorem 14.8\\nLet H be a function mapping RN to RN,a n d(xt)t∈N, (wt)t∈N,a n d(αt)t∈N be three\\nsequences in RN with\\n∀s ∈ [1,N ], xt+1(s)= xt(s)+ αt(s)\\n[\\nH(xt)(s) − xt(s)+ wt(s)\\n]\\n.\\nLet Ft denote the entire history fort′ ≤ t,t h a ti s :Ft = {(xt′ )t′ ≤t,(wt′ )t′≤t, (αt′ )t′ ≤t}\\nand assume that the following conditions are met:\\n∃K1,K2 ∈ R:E\\n[\\nw2\\nt (s)\\n⏐⏐ Ft\\n]\\n≤ K1 + K2 ∥xt∥2 for some norm ∥·∥ ;\\nE\\n[\\nwt\\n⏐⏐ Ft\\n]\\n=0 ;\\n∀s ∈ [1,N ], ∑∞\\nt=0 αt = ∞ , ∑∞\\nt=0 α2\\nt < ∞ ;a n d\\nH is a ∥·∥ ∞ -contraction with ﬁxed point x∗.330 Reinforcement Learning\\nThen, the sequence xt converges almost surely to x∗:\\nxt\\na.s\\n−−→ x∗. (14.21)\\nThe next sections present several learning algorithms for MDPs with an unknown\\nmodel.\\n14.5.2 TD(0) algorithm\\nThis section presents an algorithm, TD(0) algorithm, for evaluating a policy in the\\ncase where the environment model is unknown. The algorithm is based on Bellman’s\\nlinear equations giving the value of a policy π (see proposition 14.1):\\nV\\nπ(s)=E [ r(s, π(s)] +γ\\n∑\\ns′\\nPr[s′|s, π(s)]Vπ(s′)\\n=E\\ns′\\n[\\nr(s, π(s)) +γVπ(s′)|s\\n]\\n.\\nHowever, here the probability distribution according to which this last expectation\\nis deﬁned is not known. Instead, the TD(0) algorithm consists of\\nsampling a new state s′;a n d\\nupdating the policy values according to the following, which justiﬁes the name of\\nthe algorithm:\\nV (s) ← (1 − α)V (s)+ α[r(s, π(s)) +γV(s′)]\\n= V (s)+ α[r(s, π(s)) +γV(s′) − V (s)\\ued19 \\ued18\\ued17 \\ued1a\\ntemporal diﬀerence of V values\\n]. (14.22)\\nHere, the parameter α is a function of the number of visits to the state s.\\nThe pseudocode of the algorithm is given above. The algorithm starts with an\\narbitrary policy value vector V0. An initial state is returned by SelectState at\\nthe beginning of each epoch. Within each epoch, the iteration continues until a\\nﬁnal state is found. Within each iteration, action π(s) is taken from the current\\nstate s following policy π. The new state s\\n′ reached and the reward r′ received are\\nobserved. The policy value of state s is then updated according to the rule (14.22)\\nand current state set to be s′.\\nThe convergence of the algorithm can be proven using theorem 14.8. We will give\\ninstead the full proof of the convergence of the Q-learning algorithm, for which that\\nof TD(0) can be viewed as a special case.14.5 Learning algorithms 331\\nTD(0)()\\n1 V ← V0 ⊿ initialization.\\n2 for t ← 0 to T do\\n3 s ← SelectState()\\n4 for each step of epoch t do\\n5 r′ ← Reward(s, π(s))\\n6 s′ ← NextState(π,s)\\n7 V (s) ← (1 − α)V (s)+ α[r′ + γV(s′)]\\n8 s ← s′\\n9 return V\\n14.5.3 Q-learning algorithm\\nThis section presents an algorithm for estimating the optimal state-action value\\nfunction Q∗ in the case of an unknown model. Note that the optimal policy or policy\\nvalue can be straightforwardly derived from Q∗ via: π∗(s) = argmax a∈A Q∗(s, a)\\nand V ∗(s)=m a x a∈A Q∗(s, a). To simplify the presentation, we will assume a\\ndeterministic reward function.\\nThe Q-learning algorithm is based on the equations giving the optimal state-\\naction value function Q∗ (14.4):\\nQ∗(s, a)=E [ r(s, a)] +γ\\n∑\\ns′∈S\\nPr[s′ | s, a]V ∗(s′)\\n=E\\ns′\\n[r(s, a)+ γmax\\na∈A\\nQ∗(s, a)].\\nAs for the policy values in the previous section, the distribution model is not known.\\nThus, the Q-learning algorithm consists of the following main steps:\\nsampling a new state s′;a n d\\nupdating the policy values according to the following:\\nQ(s, a) ← αQ(s, a)+( 1 − α)[r(s, a)+ γmax\\na′ ∈A\\nQ(s′,a ′)]. (14.23)\\nwhere the parameter α is a function of the number of visits to the state s.\\nThe algorithm can be viewed as a stochastic formulation of the value iteration\\nalgorithm presented in the previous section. The pseudocode is given above. Within332 Reinforcement Learning\\nQ-Learning(π)\\n1 Q ← Q0 ⊿ initialization, e.g., Q0 =0 .\\n2 for t ← 0 to T do\\n3 s ← SelectState()\\n4 for each step of epoch t do\\n5 a ← SelectAction(π,s) ⊿ policy π derived from Q,e . g . ,ϵ-greedy.\\n6 r′ ← Reward(s, a)\\n7 s′ ← NextState(s, a)\\n8 Q(s, a) ← Q(s, a)+ α\\n[\\nr′ + γmaxa′ Q(s′,a ′) − Q(s, a)\\n]\\n9 s ← s′\\n10 return Q\\neach epoch, an action is selected from the current state s using a policy π derived\\nfrom Q. The choice of the policy π is arbitrary so long as it guarantees that every\\npair (s, a) is visited inﬁnitely many times. The reward received and the state s′\\nobserved are then used to update Q following (14.23).\\nTheorem 14.9\\nConsider a ﬁnite MDP. Assume that for all s ∈ S and a ∈ A, ∑∞\\nt=0 αt(s, a)= ∞ ,\\nand ∑∞\\nt=0 α2\\nt (s, a) < ∞ with αt(s, a) ∈ [0, 1]. Then, the Q-learning algorithm\\nconverges to the optimal value Q∗ (with probability one).\\nNote that the conditions on αt(s, a) impose that each state-action pair is visited\\ninﬁnitely many times.\\nProof Let (Qt(s, a))t≥0 denote the sequence of state-action value functions at\\n(s, a) ∈ S × A generated by the algorithm. By deﬁnition of the Q-learning updates,\\nQt+1(st,at)= Qt(st,at)+ α\\n[\\nr(st,at)+ γmax\\na′\\nQt(st+1,a ′) − Qt(st,at)\\n]\\n.\\nThis can be rewritten as the following for all s ∈ S and a ∈ A:\\nQt+1(s, a)= Qt(s, a)+ αt(s, a)\\n[\\nr(s, a)+ γ E\\ns′ ∼Pr[·|s,a]\\n[\\nmax\\na′\\nQt(s′,a ′)\\n]\\n− Qt(s, a)\\n]\\n+ γαt(s, a)\\n[\\nmax\\na′\\nQt(s′,a ′) − E\\ns′ ∼Pr[·|s,a]\\n[\\nmax\\na′\\nQt(s′,a ′)\\n]]\\n, (14.24)\\nif we deﬁne αt(s, a)a s0i f( s, a) ̸=( st,at)a n d αt(st,at) otherwise. Now, let Qt14.5 Learning algorithms 333\\ndenote the vector with components Qt(s, a), wt the vector whose s′th is\\nwt(s′)=m a x\\na′\\nQt(s′,a ′) − E\\ns′ ∼Pr[·|s,a]\\n[\\nmax\\na′\\nQt(s′,a ′)\\n]\\n,\\nand H(Qt) the vector with components H(Qt)(x, a)d e ﬁ n e db y\\nH(Qt)(x, a)= r(s, a)+ γ E\\ns′ ∼Pr[·|s,a]\\n[\\nmax\\na′\\nQt(s′,a ′)\\n]\\n.\\nThen, in view of (14.24),\\n∀(s, a) ∈ S ×A, Qt+1(s, a)= Qt(s, a)+ αt(s, a)\\n[\\nH(Qt)(s, a) − Qt(s, a)+ γwt(s)\\n]\\n.\\nWe now show that the hypotheses of theorem 14.8 hold for Qt and wt, which will\\nimply the convergence of Qt to Q∗. The conditions on αt hold by assumption. By\\ndeﬁnition of wt,E [wt\\n⏐⏐ Ft] = 0. Also, for any s′ ∈ S,\\n|wt(s′)|≤ max\\na′\\n|Qt(s′,a ′)| +\\n⏐⏐⏐⏐ E\\ns′ ∼Pr[·|s,a]\\n[\\nmax\\na′\\nQt(s′,a ′)\\n]⏐⏐⏐⏐\\n≤ 2m a x\\ns′\\n| max\\na′\\nQt(s′,a ′)| =2 ∥Qt∥∞ .\\nThus, E\\n[\\nw2\\nt (s)\\n⏐⏐ Ft\\n]\\n≤ 4∥Qt∥2\\n∞ . Finally, H is a γ-contraction for ∥·∥ ∞ since for\\nany Q′\\n1,Q′′\\n2 ∈ R|S|×|A|,a n d(s, a) ∈ S × A, we can write\\n|H(Q2)(x, a) − H(Q′\\n1)(x, a)| =\\n⏐⏐\\n⏐\\n⏐γ E\\ns′∼Pr[·|s,a]\\n[\\nmax\\na′\\nQ2(s′,a ′) − max\\na′\\nQ1(s′,a ′)\\n]⏐⏐\\n⏐\\n⏐\\n≤ γ E\\ns′ ∼Pr[·|s,a]\\n[⏐⏐⏐max\\na′\\nQ2(s′,a ′) − max\\na′\\nQ1(s′,a ′)\\n⏐⏐⏐\\n]\\n≤ γ E\\ns′ ∼Pr[·|s,a]\\nmax\\na′\\n[|Q2(s′,a ′) − Q1(s′,a ′)|]\\n≤ γmax\\ns′\\nmax\\na′\\n[|Q2(s′,a ′) − Q1(s′,a ′)|]\\n= γ∥Q′′\\n2 − Q′\\n1∥∞ .\\nSince H is a contraction, it admits a ﬁxed point Q∗: H(Q∗)= Q∗.\\nT h ec h o i c eo ft h ep o l i c yπ according to which an action a is selected (line 5) is not\\nspeciﬁed by the algorithm and, as already indicated, the theorem guarantees the\\nconvergence of the algorithm for an arbitrary policy so long as it ensures that every\\npair (s, a) is visited inﬁnitely many times. In practice, several natural choices are\\nconsidered for π. One possible choice is the policy determined by the state-action\\nvalue at timet, Qt. Thus, the action selected from states is argmaxa∈A Qt(s, a). But\\nthis choice typically does not guarantee that all actions are taken or that all states\\nare visited. Instead, a standard choice in reinforcement learning is the so-called ϵ-\\ngreedy policy, which consists of selecting with probability (1 − ϵ) the greedy action334 Reinforcement Learning\\nfrom state s, that is, argmaxa∈A Qt(s, a), and with probability ϵ a random action\\nfrom s,f o rs o m eϵ ∈ (0, 1). Another possible choice is the so-called Boltzmann\\nexploration, which, given the current state-action value Q,e p o c ht ∈ [0,T ], and\\ncurrent state s, consists of selecting action a with the following probability:\\npt(a|s, Q)= e\\nQ(s,a)\\nτt\\n∑\\na′∈A e\\nQ(s,a′ )\\nτt\\n,\\nwhere τt is the temperature. τt must be deﬁned so that τt → 0a s t →∞ ,w h i c h\\nensures that for large values of t, the greedy action based on Q is selected. This is\\nnatural, since as t increases, we can expect Q to be close to the optimal function.\\nOn the other hand, τt must be chosen so that it does not tend to 0 too fast to\\nensure that all actions are visited inﬁnitely often. It can be chosen, for instance, as\\n1/ log(n\\nt(s)), where nt(s) is the number of times s has been visited up to epoch t.\\nReinforcement learning algorithms include two components: a learning policy,\\nwhich determines the action to take, and an update rule, which deﬁnes the new\\nestimate of the optimal value function. For an oﬀ-policy algorithm, the update\\nrule does not necessarily depend on the learning policy. Q-learning is an oﬀ-policy\\nalgorithm since its update rule (line 8 of the pseudocode) is based on the max\\noperator and the comparison of all possible actions a\\n′,t h u si td o e sn o td e p e n do n\\nthe policy π. In contrast, the algorithm presented in the next section, SARSA, is\\nan on-policy algorithm.\\n14.5.4 SARSA\\nSARSA is also an algorithm for estimating the optimal state-value function in the\\ncase of an unknown model. The pseudocode is given in ﬁgure 14.7. The algorithm\\nis in fact very similar to Q-learning, except that its update rule (line 9 of the\\npseudocode) is based on the actiona\\n′ selected by the learning policy. Thus, SARSA\\nis an on-policy algorithm, and its convergence therefore crucially depends on the\\nlearning policy. In particular, the convergence of the algorithm requires, in addition\\nto all actions being selected inﬁnitely often, that the learning policy becomes greedy\\nin the limit. The proof of the convergence of the algorithm is nevertheless close to\\nthat of Q-learning.\\nThe name of the algorithm derives from the sequence of instructions deﬁning\\nsuccessively s, a, r\\n′, s′,a n d a′, and the fact that the update to the function Q\\ndepends on the quintuple (s, a, r′,s ′,a).14.5 Learning algorithms 335\\nSARSA(π)\\n1 Q ← Q0 ⊿ initialization, e.g., Q0 =0 .\\n2 for t ← 0 to T do\\n3 s ← SelectState()\\n4 a ← SelectAction(π(Q),s) ⊿ policy π derived from Q,e . g . ,ϵ-greedy.\\n5 for each step of epoch t do\\n6 r′ ← Reward(s, a)\\n7 s′ ← NextState(s, a)\\n8 a′ ← SelectAction(π(Q),s ′) ⊿ policy π derived from Q,e . g . ,ϵ-greedy.\\n9 Q(s, a) ← Q(s, a)+ αt(s, a)\\n[\\nr′ + γQ(s′,a ′) − Q(s, a)\\n]\\n10 s ← s′\\n11 a ← a′\\n12 return Q\\nFigure 14.7 The SARSA algorithm.\\n14.5.5 TD( λ) algorithm\\nBoth TD(0) and Q-learning algorithms are only based on immediate rewards. The\\nidea of TD(λ) consists instead of using multiple steps ahead. Thus, forn> 1s t e p s ,\\nwe would have the update\\nV (s) ← V (s)+ α(Rn\\nt − V (s)),\\nwhere Rn\\nt is deﬁned by\\nRn\\nt = rt+1 + γrt+2 + ... + γn−1rt+n + γnV (st+n).\\nHow should n be chosen? Instead of selecting a speciﬁc n,T D (λ) is based on a\\ngeometric distribution over all rewardsRn\\nt , that is, it usesRλ\\nt =( 1− λ) ∑∞\\nn=0 λnRn\\nt\\ninstead of Rn\\nt where λ ∈ [0, 1]. Thus, the main update becomes\\nV (s) ← V (s)+ α(Rλ\\nt − V (s)).\\nThe pseudocode of the algorithm is given above. Forλ = 0, the algorithm coincides\\nwith TD(0). λ = 1 corresponds to the total future reward.\\nIn the previous sections, we presented learning algorithms for an agent navigating336 Reinforcement Learning\\nTD(λ)()\\n1 V ← V0 ⊿ initialization.\\n2 e ← 0\\n3 for t ← 0 to T do\\n4 s ← SelectState()\\n5 for each step of epoch t do\\n6 s′ ← NextState(π,s)\\n7 δ← r(s, π(s)) +λV(s′) − V (s)\\n8 e(s) ← λe(s)+1\\n9 for u ∈ S do\\n10 if u ̸= s then\\n11 e(u) ← γλe(u)\\n12 V (u) ← V (u)+ αδe(u)\\n13 s ← s′\\n14 return V\\nin an unknown environment. The scenario faced in many practical applications is\\nmore challenging; often, the information the agent receives about the environment\\nis uncertain or unreliable. Such problems can be modeled as partially observable\\nMarkov decision processes (POMDPs). POMDPs are deﬁned by augmenting the\\ndeﬁnition of MDPs with an observation probability distribution depending on the\\naction taken, the state reached, and the observation. The presentation of their model\\nand solution techniques are beyond the scope of this material.\\n14.5.6 Large state space\\nIn some cases in practice, the number of states or actions to consider for the\\nenvironment may be very large. For example, the number of states in the game\\nof backgammon is estimated to be over 10\\n20.T h u s ,t h ea l g o r i t h m sp r e s e n t e di n\\nthe previous section can become computationally impractical for such applications.\\nMore importantly, generalization becomes extremely diﬃcult.\\nS u p p o s ew ew i s ht oe s t i m a t et h ep o l i c yv a l u eV\\nπ(s)a te a c hs t a t es using\\nexperience obtained using policy π. To cope with the case of large state spaces,\\nwe can map each state of the environment to RN v i aam a p p i n gΦ : S → RN ,w i t h14.6 Chapter notes 337\\nN relatively small ( N ≈ 200 has been used for backgammon) and approximate\\nVπ(s) by a function fw(s) parameterized by some vector w. For example, fw could\\nbe a linear function deﬁned by fw(s)= w ·Φ(s) for alls ∈ S, or some more complex\\nnon-linear function of w. The problem then consists of approximating Vπ with fw\\nand can be formulated as a regression problem. Note, however, that the empirical\\ndata available is not i.i.d.\\nSuppose that at each time stept the agent receives the exact policy valueVπ(st).\\nThen, if the family of functions fw is diﬀerentiable, a gradient descent method\\napplied to the empirical squared loss can be used to sequentially update the weight\\nvector w via:\\nwt+1 = wt − α∇wt\\n1\\n2[Vπ(st) − fwt (st)]2 = wt + α[Vπ(st) − fwt (st)]∇wt fwt (st).\\nIt is worth mentioning, however, that for large action spaces, there are simple cases\\nwhere the methods used do not converge and instead cycle.\\n14.6 Chapter notes\\nReinforcement learning is an important area of machine learning with a large body\\nof literature. This chapter presents only a brief introduction to this area. For a\\nmore detailed study, the reader could consult the book of Sutton and Barto [1998],\\nwhose mathematical content is short, or those of Puterman [1994] and Bertsekas\\n[1987], which discuss in more depth several aspects, as well as the more recent book\\nof Szepesv´ari [2010]. The Ph.D. theses of Singh [1993] and Littman [1996] are also\\nexcellent sources.\\nSome foundational work on MDPs and the introduction of the temporal diﬀerence\\n(TD) methods are due to Sutton [1984]. Q-learning was introduced and analyzed\\nby Watkins [1989], though it can be viewed as a special instance of TD methods.\\nThe ﬁrst proof of the convergence of Q-learning was given by Watkins and Dayan\\n[1992].\\nMany of the techniques used in reinforcement learning are closely related to those\\nof stochastic approximation which originated with the work of Robbins and Monro\\n[1951], followed by a series of results including Dvoretzky [1956], Schmetterer [1960],\\nKiefer and Wolfowitz [1952], and Kushner and Clark [1978]. For a recent survey of\\nstochastic approximation, including a discussion of powerful proof techniques based\\non ODE (ordinary diﬀerential equations), see Kushner [2010] and the references\\ntherein. The connection with stochastic approximation was emphasized by Tsitsiklis\\n[1994] and Jaakkola et al. [1994], who gave a related proof of the convergence of\\nQ-learning. For the convergence rate of Q-learning, consult Even-Dar and Mansour\\n[2003]. For recent results on the convergence of the policy iteration algorithm, see Ye338 Reinforcement Learning\\n[2011], which shows that the algorithm is strongly polynomial for a ﬁxed discount\\nfactor.\\nReinforcement learning has been successfully applied to a variety of problems\\nincluding robot control, board games such as backgammon in which Tesauro’s TD-\\nGammon reached the level of a strong master [Tesauro, 1995] (see also chapter\\n11 of Sutton and Barto [1998]), chess, elevator scheduling problems [Crites and\\nBarto, 1996], telecommunications, inventory management, dynamic radio channel\\nassignment [Singh and Bertsekas, 1997], and a number of other problems (see\\nchapter 1 of Puterman [1994]).Conclusion\\nWe described a large variety of machine learning algorithms and techniques and\\ndiscussed their theoretical foundations as well as their use and applications. While\\nthis is not a fully comprehensive presentation, it should nevertheless oﬀer the reader\\nsome idea of the breadth of the ﬁeld and its multiple connections with a variety of\\nother domains, including statistics, information theory, optimization, game theory,\\nand automata and formal language theory.\\nThe fundamental concepts, algorithms, and proof techniques we presented should\\nsupply the reader with the necessary tools for analyzing other learning algorithms,\\nincluding variants of the algorithms analyzed in this book. They are also likely to\\nbe helpful for devising new algorithms or for studying new learning schemes. We\\nstrongly encourage the reader to explore both and more generally to seek enhanced\\nsolutions for all theoretical, algorithmic, and applied learning problems.\\nThe exercises included at the end of each chapter, as well as the full solutions we\\nprovide separately, should help the reader become more familiar with the techniques\\nand concepts described. Some of them could also serve as a starting point for\\nresearch work and the investigation of new questions.\\nMany of the algorithms we presented as well as their variants can be directly\\nused in applications to derive eﬀective solutions to real-world learning problems.\\nOur detailed description of the algorithms and discussion should help with their\\nimplementation or their adaptation to other learning scenarios.\\nMachine learning is a relatively recent ﬁeld and yet probably one of the most\\nactive ones in computer science. Given the wide accessibility of digitized data and\\nits many applications, we can expect it to continue to grow at a very fast pace\\nover the next few decades. Learning problems of diﬀerent nature, some arising\\ndue to the substantial increase of the scale of the data, which already requires\\nprocessing billions of records in some applications, others related to the introduction\\nof completely new learning frameworks, are likely to pose new research challenges\\nand require novel algorithmic solutions. In all cases, learning theory, algorithms,\\nand applications form an exciting area of computer science and mathematics, which\\nwe hope this book could at least partly communicate.Appendix A Linear Algebra Review\\nIn this appendix, we introduce some basic notions of linear algebra relevant to the\\nmaterial presented in this book. This appendix does not represent an exhaustive\\ntutorial, and it is assumed that the reader has some prior knowledge of the subject.\\nA.1 Vectors and norms\\nWe will denote by H a vector space whose dimension may be inﬁnite.\\nA.1.1 Norms\\nDeﬁnition A.1\\nA mapping Φ: H → R\\n+ is said to deﬁne a norm on H if it veriﬁes the following\\naxioms:\\ndeﬁniteness: ∀x ∈ H, Φ(x)=0 ⇔ x = 0;\\nhomogeneity: ∀x ∈ H, ∀α ∈ R, Φ(αx)= |α|Φ(x);\\ntriangle inequality: ∀x,y ∈ H,Φ(x + y) ≤ Φ(x)+Φ (y).\\nA norm is typically denoted by ∥·∥ . Examples of vector norms are the absolute\\nvalue on R and the Euclidean (or L2) norm on RN . More generally, for any p ≥ 1\\nthe Lp norm is deﬁned on RN as\\n∀x ∈ RN , ∥x∥p =\\n( N∑\\nj=1\\n|xj |p\\n⎡1/p\\n. (A.1)\\nThe L1, L2,a n dL∞ norms are the some of the most commonly used norms, where\\n∥x∥∞ =m a xj∈[1,N] xj.T w on o r m s∥·∥ and ∥·∥ ′ are said to be equivalent iﬀ there\\nexists α, β >0 such that for all x ∈ H,\\nα∥x∥≤∥ x∥′ ≤ β∥x∥. (A.2)342 Linear Algebra Review\\nThe following general inequalities relating these norms can be proven straightfor-\\nwardly:\\n∥x∥2 ≤∥ x∥1 ≤\\n√\\nN ∥x∥2 (A.3)\\n∥x∥∞ ≤∥ x∥2 ≤\\n√\\nN ∥x∥∞ (A.4)\\n∥x∥∞ ≤∥ x∥1 ≤ N ∥x∥∞ . (A.5)\\nThe second inequality of the ﬁrst line can be shown using the Cauchy-Schwarz\\ninequality presented later while the other inequalities are clear. These inequalities\\nshow the equivalence of these three norms. More generally, all norms on a ﬁnite-\\ndimensional space are equivalent. The following additional properties hold for the\\nL\\n∞ norm: for all x ∈ H,\\n∀p ≥ 1, ∥x∥∞ ≤∥ x∥p ≤ N1/p∥x∥∞ (A.6)\\nlim\\np→ +∞\\n∥x∥p = ∥x∥∞ . (A.7)\\nThe inequalities of the ﬁrst line are straightforward and imply the limit property of\\nthe second line.\\nWe will often consider a Hilbert space, that is a vector space equipped with an\\ninner product ⟨·, ·⟩ and that is complete (all Cauchy sequences are convergent). The\\ninner product induces a norm deﬁned as follows:\\n∀x ∈ H, ∥x∥H =\\n√\\n⟨x,x⟩. (A.8)\\nA.1.2 Dual norms\\nDeﬁnition A.2\\nLet ∥·∥ be a norm on R\\nN. Then, the dual norm ∥·∥ ∗ associated to ∥·∥ is the norm\\ndeﬁned by\\n∀y ∈ H, ∥y∥∗ =s u p\\n∥x∥=1\\n|⟨y,x⟩| . (A.9)\\nFor any p, q ≥ 1t h a ta r econjugate that is such that 1\\np + 1\\nq =1 ,t h e Lp and Lq\\nnorms are dual norms of each other. In particular, the dual norm of L2 is the L2\\nnorm, and the dual norm of the L1 norm is the L∞ norm.\\nProposition A.1 H¨older’s inequality\\nLet p, q ≥ 1 be conjugate: 1\\np + 1\\nq =1 .T h e n ,f o ra l lx, y ∈ RN,\\n|⟨x,y⟩| ≤ ∥ x∥p∥y∥q, (A.10)\\nwith equality when |yi| = |xi|p−1 for all i ∈ [1,N ].A.1 Vectors and norms 343\\nProof The statement holds trivially for x = 0 or y = 0; thus, we can assume\\nx ̸= 0 and y ̸= 0.L e ta, b >0. By the concavity of log (see deﬁnition B.5), we can\\nwrite\\nlog\\n(1\\npap + 1\\nqbq\\n⎡\\n≥ 1\\np log(ap)+ 1\\nq log(bq)=l o g (a)+l o g (b)=l o g (ab).\\nTaking the exponential of the left- and right-hand sides gives\\n1\\npap + 1\\nqbq ≥ ab,\\nwhich is known asYoung’s inequality. Using this inequality witha = |xj |/∥x∥p and\\nb = |yj |/∥y∥q for j ∈ [1,N ] and summing up gives\\n∑N\\nj=1 |xjyj |\\n∥x∥p∥y∥q\\n≤ 1\\np\\n∥x∥p\\n∥x∥p + 1\\nq\\n∥y∥q\\n∥y∥q = 1\\np + 1\\nq =1 .\\nSince |⟨x,y⟩| ≤ ∑N\\nj=1 |xjyj |, the inequality claim follows. The equality case can be\\nveriﬁed straightforwardly.\\nTaking p = q = 2 immediately yields the following result known as the Cauchy-\\nSchwarz inequality .\\nCorollary A.1 Cauchy-Schwarz inequality\\nFor all x,y ∈ RN,\\n|⟨x,y⟩| ≤ ∥ x∥2∥y∥2, (A.11)\\nwith equality iﬀ x and y are collinear.\\nLet H be the hyperplane in RN whose equation is given by\\nw · x + b =0 ,\\nfor some normal vector w ∈ RN and oﬀset b ∈ R.L e tdp(x, H) denote the distance\\nof x to the hyperplane H,t h a ti s ,\\ndp(x, H)= i n f\\nx′ ∈H\\n∥x′ − x∥p. (A.12)\\nThen, the following identity holds for all p ≥ 1:\\ndp(x, H)= |w · x + b|\\n∥w∥q\\n, (A.13)\\nwhere q is the conjugate of p: 1\\np + 1\\nq = 1. (A.13) can be shown by a straightforward\\napplication of the results of appendix B to the constrained optimization problem\\n(A.12).344 Linear Algebra Review\\nA.2 Matrices\\nFor a matrix M ∈ Rm×n with m rows and n columns, we denote by Mij its ijth\\nentry, for all i ∈ [1,m]a n d j ∈ [1,n ]. For any m ≥ 1, we denote by Im the m-\\ndimensional identity matrix, and refer to it as I when the dimension is clear from\\nthe context.\\nThe transpose of M is denoted byM⊤ and deﬁned by (M⊤)ij = Mji for all (i, j).\\nFor any two matrices M ∈ Rm×n and N ∈ Rn×p,( MN)⊤ = N⊤M⊤. M is said to\\nbe symmetric iﬀ Mij = Mji for all (i, j), that is, iﬀ M = M⊤.\\nThe trace of a square matrix M is denoted by Tr[M] and deﬁned as Tr[ M]=∑N\\ni=1 Mii. For any two matricesM ∈ Rm×n and N ∈ Rn×m, the following identity\\nholds: Tr[MN]=T r [NM]. More generally, the following cyclic property holds with\\nthe appropriate dimensions for the matrices M, N,a n dP:\\nTr[MNP]=T r [PMN]=T r [NPM]. (A.14)\\nThe inverse of a square matrixM, which exists whenM has full rank, is denoted\\nby M−1 and is the unique matrix satisfying MM−1 = M−1M = I.\\nA.2.1 Matrix norms\\nA matrix norm is a norm deﬁned over R\\nm×n where m and n are the dimensions\\nof the matrices considered. Many matrix norms, including those discussed below,\\nsatisfy the following submultiplicative property:\\n∥MN∥≤∥ M∥∥N∥. (A.15)\\nThe matrix norm induced by the vector norm ∥·∥\\np or the operator norm induced\\nby that norm is also denoted by ∥·∥ p and deﬁned by\\n∥M∥p =s u p\\n∥x∥p≤1\\n∥Mx∥p . (A.16)\\nThe norm induced forp =2i sk n o w na st h espectral norm, which equals the largest\\nsingular value of M (see section A.2.2), or the square-root of the largest eigenvalue\\nof M⊤M:\\n∥M∥2 = σ1(M)=\\n√\\nλmax(M⊤M). (A.17)A.2 Matrices 345\\nNot all matrix norms are induced by vector norms. The Frobenius norm denoted\\nby ∥·∥ F is the most notable of such norms and is deﬁned by:\\n∥M∥F =\\n( m∑\\ni=1\\nn∑\\nj=1\\nM2\\nij\\n⎡1/2\\n.\\nThe Frobenius norm can be interpreted as the L2 norm of a vector when treating\\nM as a vector of size mn. It also coincides with the norm induced by theFrobenius\\nproduct, which is the inner product deﬁned over for all M,N ∈ Rm×n by\\n⟨M,N⟩F =T r [M⊤N]. (A.18)\\nThis relates the Frobenius norm to the singular values of M:\\n∥M∥2\\nF =T r [M⊤M]=\\nr∑\\ni=1\\nσi(M)2 ,\\nwhere r =r a n k (M). The second equality follows from properties of SPSD matrices\\n(see section A.2.3).\\nFor anyj ∈ [1,n ], let Mj denote the jth column of M,t h a ti sM =[ M1 ··· Mn].\\nThen, for any p, r ≥ 1, the Lp,r group norm of M is deﬁned by\\n∥M∥p,r =\\n( n∑\\nj=1\\n∥Mi∥r\\np\\n⎡1/r\\n.\\nO n eo ft h em o s tc o m m o n l yu s e dg r o u pn o r m si st h eL2,1 norm deﬁned by\\n∥M∥2,1 =\\nn∑\\ni=1\\n∥Mi∥2 .\\nA.2.2 Singular value decomposition\\nThe compact singular value decomposition (SVD) of M,w i t hr =r a n k (M) ≤\\nmin(m, n), can be written as follows:\\nM = UMΣMV⊤\\nM .\\nThe r × r matrix ΣM =d i a g (σ1,...,σ r) is diagonal and contains the non-zero\\nsingular values of M sorted in decreasing order, that is σ1 ≥ ... ≥ σr > 0.\\nUM ∈ Rm×r and VM ∈ Rn×r have orthonormal columns that contain the left and\\nright singular vectors of M corresponding to the sorted singular values.Uk ∈ Rm×k\\nare the top k ≤ r left singular vectors of M.\\nThe orthogonal projection onto the span of Uk c a nb ew r i t t e na sPUk = UkU⊤\\nk ,\\nwhere PUk is SPSD and idempotent, i.e.,P2\\nUk = PUk . Moreover, the orthogonal pro-346 Linear Algebra Review\\njection onto the subspace orthogonal toUk is deﬁned as PUk,⊥ . Similar deﬁnitions,\\ni.e., Vk,PVk ,PVk,⊥ , hold for the right singular vectors.\\nThe generalized inverse,o r Moore-Penrose pseudo-inverse of a matrix M is\\ndenoted by M† and deﬁned by\\nM† = UMΣ†\\nMV⊤\\nM , (A.19)\\nwhere Σ†\\nM =d i a g (σ−1\\n1 ,...,σ −1\\nr ). For any square m × m matrix M with full rank,\\ni.e., r = m, the pseudo-inverse coincides with the matrix inverse: M† = M−1.\\nA.2.3 Symmetric positive semideﬁnite (SPSD) matrices\\nDeﬁnition A.3\\nA symmetric matrix M ∈ R\\nm×m is said to be positive semideﬁnite iﬀ\\nx⊤Mx ≥ 0 (A.20)\\nfor all x ∈ Rm. M is said to be positive deﬁnite if the inequality is strict.\\nKernel matrices (see chapter 5) and orthogonal projection matrices are two examples\\nof SPSD matrices. It is straightforward to show that a matrix M is SPSD iﬀ its\\neigenvalues are all non-negative. Furthermore, the following properties hold for any\\nSPSD matrix M:\\nM admits a decomposition M = X⊤X for some matrix X and the Cholesky\\ndecomposition p r o v i d e so n es u c hd e c o m p o s i t i o ni nw h i c hX is an upper triangular\\nmatrix.\\nThe left and right singular vectors of M are the same and the SVD of M is also\\nits eigenvalue decomposition.\\nThe SVD of an arbitrary matrix X = UXΣXV⊤\\nX deﬁnes the SVD of two related\\nSPSD matrices: the left singular vectors (UX) are the left singular vectors ofXX⊤,\\nthe right singular vectors (VX) are the right singular vectors ofX⊤X and the non-\\nzero singular values of X are the square roots of the non-zero singular values of\\nXX⊤ and X⊤X.\\nThe trace ofM is the sum of its singular values, i.e., Tr[M]= ∑r\\ni=1 σi(M), where\\nrank(M)= r.\\nThe top singular vector of M, u1, maximizes the Rayleigh quotient ,w h i c hi s\\ndeﬁned as\\nr(x,M)= x⊤Mx\\nx⊤x .\\nIn other words, u1 =a r g m a xx r(x,M)a n dr(u,M)= σ1(M). Similarly, if M′ =A.2 Matrices 347\\nPUi,⊥M, that is, the projection of M onto the subspace orthogonal to Ui,t h e n\\nui+1 =a r g m a xx r(x,M′), where ui+1 is the (i + 1)st singular vector of M.Appendix B Convex Optimization\\nIn this appendix, we introduce the main deﬁnitions and results of convex optimiza-\\ntion needed for the analysis of the learning algorithms presented in this book.\\nB.1 Diﬀerentiation and unconstrained optimization\\nWe start with some basic deﬁnitions for diﬀerentiation needed to present Fermat’s\\ntheorem and to describe some properties of convex functions.\\nDeﬁnition B.1 Gradient\\nLet f : X⊆ R\\nN → R be a diﬀerentiable function. Then, thegradient of f at x ∈X\\nis the vector in RN denoted by ∇f(x) a n dd e ﬁ n e db y\\n∇f(x)=\\n⎡\\n⎢⎢⎣\\n∂f\\n∂x1\\n(x)\\n..\\n.\\n∂f\\n∂xN\\n(x)\\n⎤\\n⎥⎥⎦.\\nDeﬁnition B.2 Hessian\\nLet f : X⊆ RN → R be a twice diﬀerentiable function. Then, the Hessian of f at\\nx ∈X is the matrix in RN ×N denoted by ∇2f(x) a n dd e ﬁ n e db y\\n∇2f(x)=\\n[ ∂2f\\n∂xi,xj\\n(x)\\n]\\n1≤i,j≤N\\n.\\nNext, we present a classic result for unconstrained optimization.\\nTheorem B.1 Fermat’s theorem\\nLet f : X⊆ RN → R be a diﬀerentiable function. If f admits a local extremum at\\nx∗ ∈X ,t h e n∇f(x∗)=0 , that is, x∗ is a stationary point.350 Convex Optimization\\nFigure B.1 Examples of a convex (left) and a concave (right) functions. Note that\\nany line segment drawn between two points on the convex function lies entirely\\nabove the graph of the function while any line segment drawn between two points\\non the concave function lies entirely below the graph of the function.\\nB.2 Convexity\\nThis section introduces the notions of convex sets and convex functions. Convex\\nfunctions play an important role in the design and analysis of learning algorithms,\\nin part because a local minimum of a convex function is necessarily also a global\\nminimum. Thus, the properties of a learning hypothesis that is a local minimum\\nof a convex optimization are often well understood, while for some non-convex\\noptimization problems, there may be a very large number of local minima for which\\nno clear characterization can be given.\\nDeﬁnition B.3 Convex set\\nAs e tX⊆ R\\nN is said to be convex if for any two pointsx,y ∈X the segment [x,y]\\nlies in X, that is\\n{αx +( 1 − α)y:0 ≤ α ≤ 1}⊆X .\\nDeﬁnition B.4 Convex hull\\nThe convex hull conv( X) of a set of points X⊆ RN is the minimal convex set\\ncontaining X and can be equivalently deﬁned as follows:\\nconv(X)=\\n{ m∑\\ni=1\\nαixi : m ≥ 1, ∀i ∈ [1,m],xi ∈X ,αi ≥ 0,\\nm∑\\ni=1\\nαi =1\\n}\\n. (B.1)\\nLet Epif denote the epigraph of function f : X→ R, that is the set of points lying\\nabove its graph: {(x, y): x ∈X ,y ≥ f(x)}.B.2 Convexity 351\\nf(y)\\n(x, f(x))\\nf(x)+ ∇f(x)·(y − x)\\nFigure B.2 Illustration of the ﬁrst-order property satisﬁed by all convex functions.\\nDeﬁnition B.5 Convex function\\nLet X be a convex set. A function f : X→ R is said to be convex iﬀ Epif is a\\nconvex set, or, equivalently, if for all x,y ∈X and α ∈ [0, 1],\\nf(αx +( 1 − α)y) ≤ αf(x)+( 1 − α)f(y) . (B.2)\\nf is said to be strictly convex if inequality (B.2) is strict for all x,y ∈X where\\nx ̸= y and α ∈ (0, 1). f is said to be (strictly) concave when −f is (strictly)\\nconvex. Figure B.1 shows simple examples of a convex and concave functions.\\nConvex functions can also be characterized in terms of their ﬁrst- or second-order\\ndiﬀerential.\\nTheorem B.2\\nLet f be a diﬀerentiable function, then f is convex if and only if dom(f) is convex\\nand the following inequalities hold:\\n∀x,y ∈ dom(f),f (y) − f(x) ≥∇ f(x) · (y − x) . (B.3)\\nThe property (B.3) is illustrated by ﬁgure B.2: for a convex function, the hyperplane\\ntangent at x is always below the graph.\\nTheorem B.3\\nLet f be a twice diﬀerentiable function, then f is convex iﬀ dom(f) is convex and\\nits Hessian is positive semideﬁnite:\\n∀x ∈ dom(f), ∇2f(x) ⪰ 0 .\\nRecall that a symmetric matrix is positive semideﬁnite if all of its eigenvalues are\\nnon-negative. Further, note that when f is scalar, this theorem states that f is\\nconvex if and only if its second derivative is always non-negative, that is, for all\\nx ∈ dom(f),f\\n′′(x) ≥ 0.\\nExample B.1 Linear functions352 Convex Optimization\\nAny linear function f is both convex and concave, since equation (B.2) holds with\\nequality for both f and −f by the deﬁnition of linearity.\\nExample B.2 Quadratic function\\nThe function f : x ↦→ x2 deﬁned over R is convex since it is twice diﬀerentiable and\\nfor all x ∈ R, f ′′(x)=2 > 0.\\nExample B.3 Norms\\nAny norm ∥·∥ deﬁned over a convex set X is convex since by the triangle inequality\\nand homogeneity property of the norm, for all α ∈ [0, 1],x,y ∈X , we can write\\n∥αx +( 1 − α)y∥≤∥ αx∥ + ∥(1 − α)y∥ = α∥x∥ +( 1 − α)∥y∥ .\\nExample B.4 Maximum function\\nThe max function deﬁned for all x ∈ RN ,b y x ↦→ maxj∈[1,N] xj is convex. For all\\nα ∈ [0,1],x,y ∈ RN ,b yt h es u b a d d i t i v i t yo fm a x ,w ec a nw r i t e\\nmax\\nj\\n(αxj +(1 −α)yj) ≤ max\\nj\\n(αxj)+max\\nj\\n((1−α)yj)= αmax\\nj\\n(xj)+(1 −α)m a x\\nj\\n(yj) .\\nOne useful approach for proving convexity or concavity of functions is to make\\nuse of composition rules. For simplicity of presentation, we will assume twice\\ndiﬀerentiability, although the results can also be proven without this assumption.\\nLemma B.1 Composition of convex/concave functions\\nAssume h : R → R and g : R\\nN → R are twice diﬀerentiable functions and for all\\nx ∈ RN, deﬁne f(x)= h(g(x)). Then the following implications are valid:\\nh is convex and non-decreasing, andg is convex =⇒ f is convex.\\nh is convex and non-increasing, and g is concave =⇒ f is convex.\\nh is concave and non-decreasing, andg is concave =⇒ f is concave.\\nh is concave and non-increasing, andg is convex =⇒ f is concave.\\nProof We restrict ourselves ton = 1, since it suﬃces to prove convexity (concav-\\nity) along all arbitrary lines that intersect the domain. Now, consider the second\\nderivative of f:\\nf\\n′′(x)= h′′(g(x))g′(x)2 + h′(g(x))g′′(x) . (B.4)\\nNote that if h is convex and non-decreasing, we have h′′ ≥ 0a n d h′ ≥ 0.\\nFurthermore, if g is convex we also have g′′ ≥ 0, and it follows that f ′′(x) ≥ 0,\\nwhich proves the ﬁrst statement. The remainder of the statements are proven in a\\nsimilar manner.\\nExample B.5 Composition of functionsB.3 Constrained optimization 353\\nT h ep r e v i o u sl e m m ac a nb eu s e dt oi m m e d i a t e l yp r o v et h ec o n v e x i t yo rc o n c a v i t y\\nof the following composed functions:\\nIf f : RN → R is convex, then exp(f)i sc o n v e x .\\nAny squared norm ∥·∥ 2 is convex.\\nFor all x ∈ RN the function x ↦→ log(∑N\\nj=1 xj) is concave.\\nThe following is a useful inequality applied in a variety of contexts. It is in fact a\\nquasi-direct consequence of the deﬁnition of convexity.\\nTheorem B.4 Jensen’s inequality\\nLet X be a random variable taking values in a non-empty convex setC ⊆ RN with a\\nﬁnite expectation E[X],a n df a measurable convex function deﬁned over C.T h e n ,\\nE[X] is in C, E[f(X)] is ﬁnite, and the following inequality holds:\\nf(E[X]) ≤ E[f(X)].\\nProof We give a sketch of the proof, which essentially follows from the deﬁnition\\nof convexity. Note that for any ﬁnite set of elementsx1,...,x n in C and any positive\\nreals α1,...,α n such that ∑n\\ni=1 αi =1 ,w eh a v e\\nf\\n( n∑\\ni=1\\nαixi\\n⎡\\n≤\\nn∑\\ni=1\\nαif(xi) .\\nThis follows straightforwardly by induction from the deﬁnition of convexity. Since\\nthe αis can be interpreted as probabilities, this immediately proves the inequality\\nfor any distribution with a ﬁnite support deﬁned by α =( α1,...,α n):\\nf(E\\nα\\n[X]) ≤ E\\nα\\n[f(X)].\\nExtending this to arbitrary distributions can be shown via the continuity of f on\\nany open set, which is guaranteed by the convexity of f, and the weak density of\\ndistributions with ﬁnite support in the family of all probability measures.\\nB.3 Constrained optimization\\nWe now deﬁne a general constrained optimization problem and the speciﬁc proper-\\nties associated to convex constrained optimization problems.354 Convex Optimization\\nDeﬁnition B.6 Constrained optimization problem\\nLet X⊆ RN and f,g i : X→ R,f o ra l li ∈ [1,m].T h e n ,aconstrained optimization\\nproblem has the form:\\nmin\\nx∈X\\nf(x)\\nsubject to: gi(x) ≤ 0, ∀i ∈{ 1,...,m }.\\nThis general formulation does not make any convexity assumptions and can be\\naugmented with equality constraints. It is referred to as the primal problem in\\ncontrast with a related problem introduced later. We will denote byp∗ the optimal\\nvalue of the objective.\\nFor any x ∈X ,w ew i l ld e n o t eb yg(x) the vector (g1(x),...,g m(x))⊤.T h u s ,t h e\\nconstraints can be written as g(x) ≤ 0. To any constrained optimization problem,\\nwe can associate a Lagrange function that plays an important in the analysis of the\\nproblem and its relationship with another related optimization problem.\\nDeﬁnition B.7 Lagrangian\\nThe Lagrange function or the Lagrangian associated to the general constrained\\noptimization problem deﬁned in (B.6) is the function deﬁned over X× R+ by:\\n∀x ∈X , ∀α ≥ 0, L(x, α)= f(x)+\\nm∑\\ni=1\\nαigi(x) ,\\nwhere the variables αi are known as the Lagrange or dual variables with α =\\n(α1,...,α m)⊤.\\nAny equality constraint of the form g(x)=0f o raf u n c t i o ng can be equivalently\\nexpressed by two inequalities: −g(x) ≤ 0a n d+ g(x) ≤ 0. Let α− ≥ 0b et h e\\nLagrange variable associated to the ﬁrst constraint and α+ ≥ 0 the one associated\\nto the second constraint. The sum of the terms corresponding to these constraints\\nin the deﬁnition of the Lagrange function can therefore be written as αg(x)w i t h\\nα =( α+ −α− ). Thus, in general, for an equality constraintg(x) = 0 the Lagrangian\\nis augmented with a termαg(x)b u tw i t hα ∈ R not constrained to be non-negative.\\nNote that in the case of a convex optimization problem , equality constraints g(x)\\na r er e q u i r e dt ob ea ﬃ n es i n c eb o t hg(x)a n d−g(x) are required to be convex.\\nDeﬁnition B.8 Dual function\\nThe (Lagrange) dual function associated to the constrained optimization problem is\\ndeﬁned by\\n∀α ≥ 0,F (α)= i n f\\nx∈X\\nL(x, α)= i n f\\nx∈X\\n(\\nf(x)+\\nm∑\\ni=1\\nαigi(x)\\n⎡\\n. (B.5)B.3 Constrained optimization 355\\nNote that F is always concave, since the Lagrangian is linear with respect toα and\\nsince the inﬁmum preserves concavity. We further observe that\\n∀α ≥ 0,F (α) ≤ p∗, (B.6)\\nsince for any feasible x, f(x)+ ∑m\\ni=1 αigi(x) ≤ f(x). The dual function naturally\\nleads to the following optimization problem.\\nDeﬁnition B.9 Dual problem\\nThe dual (optimization) problemassociated to the constrained optimization problem\\nis\\nmax\\nα\\nF(α)\\nsubject to: α ≥ 0 .\\nThe dual problem is always a convex optimization problem (as a maximization of a\\nconcave problem). Let d∗ denote optimal value. By (B.6), the following inequality\\nalways holds:\\nd∗ ≤ p∗ (weak duality).\\nThe diﬀerence (p∗ − d∗) is known as the duality gap.T h ee q u a l i t yc a s e\\nd∗ = p∗ (strong duality)\\ndoes not hold in general. However, strong duality does hold when convex problems\\nsatisfy a constraint qualiﬁcation. We will denote by int(X) the interior of the set\\nX.\\nDeﬁnition B.10 Strong constraint qualiﬁcation\\nAssume that int(X) ̸= ∅. Then, the strong constraint qualiﬁcation or Slater’s\\ncondition is deﬁned as\\n∃x ∈ int(X): g(x) < 0. (B.7)\\nAf u n c t i o nh: X→ R is said to be aﬃne if it can be deﬁned for all x ∈X by\\nh(x)= w · x + b,f o rs o m ew ∈ RN and b ∈ R.\\nDeﬁnition B.11 Weak constraint qualiﬁcation\\nAssume that int(X) ̸= ∅. Then, the weak constraint qualiﬁcation or weak Slater’s\\ncondition is deﬁned as\\n∃x ∈ int(X): ∀i ∈ [1,m],\\n(\\ngi(x) < 0\\n⎡\\n∨\\n(\\ngi(x)=0 ∧ gi aﬃne\\n⎡\\n. (B.8)356 Convex Optimization\\nWe next present suﬃcient and necessary conditions for solutions to constrained\\noptimization problems, based on the saddle point of the Lagrangian and Slater’s\\ncondition.\\nTheorem B.5 Saddle point — suﬃcient condition\\nLet P be a constrained optimization problem over X = R\\nN.I f (x∗,α∗) is a saddle\\npoint of the associated Lagrangian, that is,\\n∀x ∈ RN , ∀α ≥ 0, L(x∗, α) ≤L (x∗, α∗) ≤L (x, α∗), (B.9)\\nthen (x∗, α∗) is a solution of the problem P.\\nProof By the ﬁrst inequality, the following holds:\\n∀α ≥ 0, L(x∗,α) ≤L (x∗,α∗) ⇒∀ α ≥ 0, α · g(x∗) ≤ α∗ · g(x∗)\\n⇒ g(x∗) ≤ 0 ∧ α∗ · g(x∗)=0 , (B.10)\\nwhere g(x∗) ≤ 0 in (B.10) follows by letting α → +∞ and α∗ · g(x∗) = 0 follows\\nby letting α → 0. In view of (B.10), the second inequality in (B.9) gives,\\n∀x, L(x∗,α∗) ≤L (x, α∗) ⇒∀ x,f (x∗) ≤ f(x)+ α∗ · g(x).\\nThus, for all x satisfying the constraints, that is g(x) ≤ 0, we have\\nf(x∗) ≤ f(x),\\nwhich completes the proof.\\nTheorem B.6 Saddle point — necessary condition\\nAssume that f and gi, i ∈ [1,m],a r econvex functions and that Slater’s condition\\nholds. Then, if x is a solution of the constrained optimization problem, then there\\nexists α ≥ 0 such that (x, α) is a saddle point of the Lagrangian.\\nTheorem B.7 Saddle point — necessary condition\\nAssume that f and gi, i ∈ [1,m],a r econvex diﬀerentiable functions and that the\\nweak Slater’s condition holds. If x is a solution of the constrained optimization\\nproblem, then there existsα ≥ 0 such that (x, α) is a saddle point of the Lagrangian.\\nWe conclude with a theorem providing necessary and suﬃcient optimality con-\\nditions when the problem is convex, the objective function diﬀerentiable, and the\\nconstraints qualiﬁed.\\nTheorem B.8 Karush-Kuhn-Tucker’s theorem\\nAssume that f,g\\ni : X→ R, ∀i ∈{ 1,...,m } are convex and diﬀerentiable and that\\nthe constraints are qualiﬁed. Then x is a solution of the constrained program if andB.4 Chapter notes 357\\nif only there exists α ≥ 0 such that,\\n∇xL(x,α)= ∇xf(x)+ α ·∇ xg(x) = 0 (B.11)\\n∇α L(x,α)= g(x) ≤ 0 (B.12)\\nα · g(x)=\\nm∑\\ni=1\\nαig(xi)=0 . (B.13)\\nThe conditions B.11–B.13 are known as theKKT conditions. Note that the last two\\nKKT conditions are equivalent to\\ng(x) ≤ 0 ∧ (∀i ∈{ 1,...,m }, ¯αigi(x)=0 ) . (B.14)\\nThese equalities are known as complementarity conditions.\\nProof For the forward direction, since the constraints are qualiﬁed, if x is a\\nsolution, then there existsα such that the (x,α) is a saddle point of the Lagrangian\\nand all three conditions are satisﬁed (the ﬁrst condition follows by deﬁnition of a\\nsaddle point, and the second two conditions follow from (B.10)).\\nIn the opposite direction, if the conditions are met, then for any x such that\\ng(x) ≤ 0, we can write\\nf(x) − f(\\nx) ≥∇ xf(x) · (x − x) (convexity of f)\\n≥−\\nm∑\\ni=1\\nαi∇xgi(x) · (x − x) (ﬁrst condition)\\n≥−\\nm∑\\ni=1\\nαi[gi(x) − gi(x)] (convexity of gis)\\n≥−\\nm∑\\ni=1\\nαigi(x) ≥ 0, (third and second condition)\\nwhich shows that f(x) is the minimum of f over the set of points satisfying the\\nconstraints.\\nB.4 Chapter notes\\nThe results presented in this appendix are based on three main theorems: theo-\\nrem B.1 due to Fermat (1629); theorem B.5 due to Lagrange (1797), and theo-\\nrem B.8 due to Karush [1939] and Kuhn and Tucker [1951].\\nFor a more extensive material on convex optimization, we strongly recommend\\nthe book of Boyd and Vandenberghe [2004].Appendix C Probability Review\\nIn this appendix, we give a brief review of some basic notions of probability and\\nwill also deﬁne the notation that is used throughout the textbook.\\nC.1 Probability\\nA probability spaceis a model based on three components: asample space,a n events\\nset,a n da probability distribution:\\nsample space Ω: Ω is the set of all elementary events or outcomes possible in a\\ntrial, for example, each of the six outcomes in {1,..., 6} when casting a die.\\nevents set F: F is a σ-algebra, that is a set of subsets of Ω containing Ω that\\nis closed under complementation and countable union (therefore also countable\\ni n t e r s e c t i o n ) .A ne x a m p l eo fa ne v e n tm a yb e“ t h ed i el a n d so na no d dn u m b e r ” .\\nprobability distribution: Pr is a mapping from the set of all eventsF to [0, 1] such\\nthat Pr[Ω] = 1 and, for all mutually exclusive eventsA1,...,A n,\\nPr[A1 ∪ ... ∪ An]=\\nn∑\\ni=1\\nPr[Ai].\\nThe discrete probability distribution associated with a fair die can be deﬁned by\\nPr[Ai]=1 /6f o ri ∈{ 1 ... 6},w h e r eAi is the event that the die lands on value i.\\nC.2 Random variables\\nDeﬁnition C.1 Random variables\\nA random variable X is a function X :Ω → R that is measurable, that is such that\\nfor any interval I, the subset of the sample space {ω ∈ Ω: X(ω) ∈ I} is an event.\\nThe probability mass function of a discrete random variable X is deﬁned as the\\nfunction x ↦→ Pr[X = x]. The joint probability mass function of discrete random360 Probability Review\\n0 10 20 300\\n0.05\\n0.1\\n0.15\\nFigure C.1 Approximation of the binomial distribution (in red) by a normal\\ndistribution (in blue).\\nvariables X and Y is deﬁned as the function ( x, y) ↦→ Pr[X = x ∧ Y = y].\\nA probability distribution is said to be absolutely continuous when it admits a\\nprobability density function, that is a functionf associated to a real-valued random\\nvariable X that satisﬁes for all a, b ∈ R\\nPr[a ≤ X ≤ b]=\\n∫ b\\na\\nf(x)dx . (C.1)\\nDeﬁnition C.2 Binomial distribution\\nA random variable X is said to follow a binomial distribution B(n, p) with n ∈ N\\nand p ∈ [0, 1] if for any k ∈{ 0,1,...,n },\\nPr[X = k]=\\n(n\\nk\\n⎡\\npk(1 − p)n−k .\\nDeﬁnition C.3 Normal distribution\\nA random variableX is said to follow anormal (or Gaussian) distribution N(μ, σ2)\\nwith μ ∈ R and σ> 0 if its probability density function is given by,\\nf(x)= 1√\\n2πσ2 exp\\n(\\n− (x − μ)2\\n2σ2\\n⎡\\n.\\nThe standard normal distribution N(0, 1) is the normal distribution with zero mean\\nand unit variance.\\nThe normal distribution is often used to approximate a binomial distribution.\\nFigure C.1 illustrates that approximation.\\nDeﬁnition C.4 Laplace distributionC.3 Conditional probability and independence 361\\nA random variableX is said to follow aLaplace distributionwith location parameter\\nμ ∈ R and scale parameter b> 0 if its probability density function is given by,\\nf(x)= 1\\n2b exp\\n(\\n− |x − μ|\\nb\\n⎡\\n.\\nDeﬁnition C.5 Poisson distribution\\nA random variable X is said to follow a Poisson distribution with λ> 0 if for any\\nk ∈ N,\\nPr[X = k]= λke−λ\\nk! .\\nThe deﬁnition of the following family of distributions uses the notion of indepen-\\ndence of random variables deﬁned in the next section.\\nDeﬁnition C.6 χ2-squared distribution\\nThe χ2-distribution (or chi-squared distribution)w i t hk degrees of freedom is the\\ndistribution of the sum of the squares of k independent random variables, each\\nfollowing a standard normal distribution.\\nC.3 Conditional probability and independence\\nDeﬁnition C.7 Conditional probability\\nThe conditional probability of event A given event B is deﬁned by\\nPr[A | B]= Pr[A ∩ B]\\nPr[B] , (C.2)\\nwhen Pr[B] ̸=0 .\\nDeﬁnition C.8 Independence\\nTwo events A and B are said to be independent if\\nPr[A ∩ B]=P r [A]P r [B]. (C.3)\\nEquivalently, A and B are independent iﬀPr[A | B]=P r [A] when Pr[B] ̸=0 .\\nA sequence of random variables is said to beindependently and identically distributed\\n(i.i.d.) when the random variables are mutually independent and follow the same\\ndistribution.\\nThe following are basic probability formulae related to the notion of conditional\\nprobability. They hold for any events A, B,a n d A1,...,A n, with the additional362 Probability Review\\nconstraint Pr[B] ̸= 0 needed for the Bayes formula to be well deﬁned:\\nPr[A ∪ B]=P r [A]+P r [B] − Pr[A ∩ B]( sum rule)( C . 4 )\\nPr[\\nn⋃\\ni=1\\nAi] ≤\\nn∑\\ni=1\\nPr[Ai]( union bound)( C . 5 )\\nPr[A | B]= Pr[B | A]P r [A]\\nPr[B] (Bayes formula)( C . 6 )\\nPr[\\nn⋂\\ni=1\\nAi]=P r [A1]P r [A2 | A1] ··· Pr[An |\\nn−1⋂\\ni=1\\nAi]( chain rule). (C.7)\\nThe sum rule follows immediately from the decomposition ofA ∪B as the union of\\nthe disjoint sets A and (B − A ∩B). The union bound is a direct consequence of the\\nsum rule. The Bayes formula follows immediately from the deﬁnition of conditional\\nprobability and the observation that: Pr[A|B]P r [B]=P r [B|A]P r [A]=P r [A ∩ B].\\nSimilarly, the chain rule follows the observation that Pr[A1]P r [A2|A1]=P r [A1 ∩A2];\\nusing the same argument shows recursively that the product of the ﬁrst k terms of\\nt h er i g h t - h a n ds i d ee q u a l sP r [⋂k\\ni=1 Ai].\\nFinally, assume that Ω = A1 ∪ A2 ∪ ... ∪ An with Ai ∩ Aj = ∅for i ̸= j, i.e., the\\nAis are mutually disjoint. Then, the following formula is valid for any eventB:\\nPr[B]=\\nn∑\\ni=1\\nPr[B | Ai]P r [Ai]( theorem of total probability). (C.8)\\nThis follows the observation that Pr[B | Ai]P r [Ai]=P r [B ∩Ai] by deﬁnition of the\\nconditional probability and the fact that the events B ∩ Ai are mutually disjoint.\\nExample C.1 Application of the Bayes formula\\nLet H be a set of hypotheses. The maximum a posteriori (MAP) principle consists\\nof selecting the hypothesis ˆh ∈ H that is the most probable given the observation\\nO. Thus, by the Bayes formula, it is given by\\nˆh =a r g m a x\\nh∈H\\nPr[h|O] = argmax\\nh∈H\\nPr[O|h]P r [h]\\nPr[O] =a r g m a x\\nh∈H\\nPr[O|h]P r [h]. (C.9)\\nNow, suppose we need to determine if a patient has a rare disease, given a laboratory\\ntest of that patient. The hypothesis set is reduced to the two outcomes:d (disease)\\nand nd (no disease), thus H = {d, nd}. The laboratory test is either pos (positive)\\nor neg (negative), thus O = {pos, neg}.\\nSuppose that the disease is rare, say Pr[ d]= .005 and that the laboratory is\\nrelatively accurate: Pr[ pos|d]= .98, and Pr[ neg|nd]= .95. Then, if the test is\\npositive, what should be the diagnosis? We can compute the right-hand side ofC.4 Expectation, Markov’s inequality , and Moment-Generating function 363\\n(C.9) for both hypotheses to determine ˆh:\\nPr[pos|d]P r [d]= .98 × .005 = .0049\\nPr[pos|nd]P r [nd]=( 1 − .95) × .(1 − .005) = .04975 >. 0049.\\nThus, in this case, the MAP prediction is ˆh = nd: with the values indicated, a\\npatient with a positive test result is nonetheless more likely not to have the disease!\\nC.4 Expectation, Markov’s inequality, and oment- enerating\\nfunction\\nDeﬁnition C.9 Expectation\\nThe expectation or mean of a random variable X is denoted by E[X] and deﬁned\\nby\\nE[X]=\\n∑\\nx\\nxPr[X = x]. (C.10)\\nWhen X follows a probability distribution D,w ew i l la l s ow r i t eEx∼D[x] instead of\\nE[X] to explicitly indicate the distribution. A fundamental property of expectation,\\nwhich is straightforward to verify using its deﬁnition, is that it is linear, that is, for\\nany two random variables X and Y and any a, b ∈ R, the following holds:\\nE[aX + bY ]= aE[X]+ b E[Y ]. (C.11)\\nFurthermore, when X and Y are independent random variables, then the following\\nidentity holds:\\nE[XY ]=E [ X]E [Y ]. (C.12)\\nIndeed, by deﬁnition of expectation and of independence, we can write\\nE[XY ]=\\n∑\\nx,y\\nxy Pr[X = x ∧ Y = y]=\\n∑\\nx,y\\nxy Pr[X = x]P r [Y = y]\\n=\\n(∑\\nx\\nxPr[X = x]\\n⎡(∑\\ny\\ny Pr[Y = y]\\n⎡\\n,\\nwhere in the last step we used Fubini’s theorem . The following provides a simple\\nbound for a non-negative random variable in terms of its expectation, known as\\nMarkov’s inequality.\\nmg364 Probability Review\\nTheorem C.1 Markov’s inequality\\nLet X be a non-negative random variable with E[X] < ∞ . Then for all t> 0,\\nPr\\n[\\nX ≥ t E[X]\\n]\\n≤ 1\\nt . (C.13)\\nProof The proof steps are as follows:\\nPr[X ≥ t E[X]] =\\n∑\\nx≥t E[X]\\nPr[X = x] (by deﬁnition)\\n≤\\n∑\\nx≥t E[X]\\nPr[X = x] x\\nt E[X]\\n(\\nusing x\\nt E[X] ≥ 1\\n⎡\\n≤\\n∑\\nx\\nPr[X = x] x\\nt E[X] (extending non-negative sum)\\n=E\\n[ X\\nt E[X]\\n]\\n= 1\\nt (linearity of expectation).\\nThis concludes the proof.\\nThe following function based on the notion of expectation is often useful in the\\nanalysis of the properties of a distribution.\\nDeﬁnition C.10 Moment-generating function\\nThe moment-generating function of a random variableX is the functiont ↦→ E[etX]\\ndeﬁned over the set of t ∈ R for which the expectation is ﬁnite.\\nWe will present in the next chapter a general bound on the moment-generating\\nfunction of a zero-mean bounded random variable (Lemma D.1). Here, we illustrate\\nits computation in the case of a χ\\n2-distribution.\\nExample C.2 Moment-generating function of χ2-distribution\\nLet X be a random variable following a χ2-squared distribution with k degrees of\\nfreedom. We can write X = ∑k\\ni=1 X2\\ni where the Xis are independent and follow a\\nstandard normal distribution.\\nLet t< 1/2. By the i.i.d. assumption about the variables Xi, we can write\\nE[etX]=E\\n[ k∏\\ni=1\\netX2\\ni\\n]\\n=\\nk∏\\ni=1\\nE\\n[\\netX2\\ni\\n]\\n=E\\n[\\netX2\\n1\\n]k\\n.\\nBy deﬁnition of the standard normal distribution, we have\\nE[etX2\\n1 ]= 1√\\n2π\\n∫ +∞\\n−∞\\netx2\\ne\\n− x2\\n2 dx = 1√\\n2π\\n∫ +∞\\n−∞\\ne(1−2t) − x2\\n2 dx\\n= 1√\\n2π\\n∫ +∞\\n−∞\\ne\\n− u2\\n2\\n√1 − 2tdu =( 1 − 2t)\\n1\\n2 ,C.5 Variance and Chebyshev’s inequality 365\\nwhere we used the change of variable u = √1 − 2tx. In view of that, the moment-\\ngenerating function of the χ2-distribution is given by\\n∀t< 1/2, E[etX]=( 1 − 2t)\\nk\\n2 . (C.14)\\nC.5 Variance and Chebyshev’s inequality\\nDeﬁnition C.11 Variance — Standard deviation\\nThe variance of a random variable X is denoted by Var[X] a n dd e ﬁ n e db y\\nVar[X]=E [ (X − E[X])2]. (C.15)\\nThe standard deviation of a random variable X is denoted by σX a n dd e ﬁ n e db y\\nσX =\\n√\\nVar[X]. (C.16)\\nFor any random variable X and any a ∈ R, the following basic properties hold for\\nthe variance, which can be proven straightforwardly:\\nVar[X]=E [ X2] − E[X]2 (C.17)\\nVar[aX]= a2 Var[X]. (C.18)\\nFurthermore, when X and Y are independent , then\\nVar[X + Y ]=V a r [X]+V a r [Y ]. (C.19)\\nIndeed, using the linearity of expectation and the identity E[X]E [Y ] − E[XY ]=0\\nwhich holds by the independence of X and Y , we can write\\nVar[X + Y ]=E [ (X + Y )2] − E[X + Y ]2\\n=E [X2 + Y 2 +2 XY ] − (E[X]2 +E [Y ]2 +2E [XY ])\\n=( E [X2] − E[X]2)+( E [Y 2] − E[Y ]2)+2 ( E [X]E [Y ] − E[XY ])\\n= Var[X]+V a r [Y ].\\nThe following inequality known as Chebyshev’s inequality bounds the deviation\\nof a random variable from its expectation in terms of its standard deviation.366 Probability Review\\nTheorem C.2 Chebyshev’s inequality\\nLet X be a random variable with Var[X] < +∞ .T h e n ,f o ra l lt> 0, the following\\ninequality holds:\\nPr\\n[\\n|X − E[X]|≥ tσX\\n]\\n≤ 1\\nt2 . (C.20)\\nProof Observe that:\\nPr\\n[\\n|X − E[X]|≥ tσX\\n]\\n=P r [ (X − E[X])2 ≥ t2σ2\\nX].\\nThe result follows by application of Markov’s inequality to (X − E[X])2.\\nWe will use Chebyshev’s inequality to prove the following theorem.\\nTheorem C.3 Weak law of large numbers\\nLet (Xn)n∈N be a sequence of independent random variables with the same meanμ\\nand variance σ2 < ∞ .L e tXn = 1\\nn\\n∑n\\ni=1 Xi, then, for any ϵ> 0,\\nlim\\nn→∞\\nPr[|Xn − μ|≥ ϵ]=0 . (C.21)\\nProof Since the variables are independent, we can write\\nVar[Xn]=\\nn∑\\ni=1\\nVar\\n[Xi\\nn\\n]\\n= nσ2\\nn2 = σ2\\nn .\\nThus, by Chebyshev’s inequality (witht = ϵ/(Var[Xn])1/2), the following holds:\\nPr[|Xn − μ|≥ ϵ] ≤ σ2\\nnϵ2 ,\\nwhich implies (C.21).\\nExample C.3 Applying Chebyshev’s inequality\\nSuppose we roll a pair of fair dicen times. Can we give a good estimate of the total\\nvalue of the n rolls? If we compute the mean and variance, we ﬁnd μ =7 n and\\nσ2 =3 5/6n (we leave it to the reader to verify these expressions). Thus, applying\\nChebyshev’s inequality, we see that the ﬁnal sum will lie within 7 n ± 10\\n√\\n35\\n6 n in\\nat least 99 percent of all experiments. Therefore, the odds are better than 99 to 1\\nthat the sum will be between 6.975M and 7.025M after 1M rolls.\\nDeﬁnition C.12 Covariance\\nThe covariance of two random variables X and Y is denoted by Cov(X,Y ) and\\ndeﬁned by\\nCov(X,Y )=E\\n[\\n(X − E[X])(Y − E[Y ])\\n]\\n. (C.22)C.5 Variance and Chebyshev’s inequality 367\\nIt is straightforward to see that two random variables X and Y are independent\\niﬀ Cov(X,Y ) = 0. The covariance deﬁnes a positive semideﬁnite and symmetric\\nbilinear form:\\nsymmetry: Cov(X,Y )=C o v (Y,X ) for any two random variables X and Y ;\\nbilinearity: Cov( X + X′,Y )=C o v ( X,Y )+C o v (X′,Y )a n dC o v (aX, Y)=\\naCov(X,Y ) for any random variables X, X′,a n dY and a ∈ R;\\npositive semideﬁniteness: Cov( X,X )=V a r [X] ≥ 0 for any random variable X.\\nThe following Cauchy-Schwarz inequality holds for random variablesX and Y with\\nVar[X] < +∞ and Var[Y ] < +∞ :\\n| Cov(X,Y )|≤\\n√\\nVar[X] Var[Y ]. (C.23)\\nThe following deﬁnition\\nDeﬁnition C.13\\nThe covariance matrix of a vector of random variables X =( X1,...,X N ) is the\\nmatrix in RN ×N denoted by C(X) a n dd e ﬁ n e db y\\nC(X)=E\\n[\\n(X − E[X])(X − E[X])⊤]\\n. (C.24)\\nThus, C(X)=( C o v (Xi,X j))ij. It is straightforward to show that\\nC(X)=E [ XX⊤] − E[X]E [X]⊤. (C.25)\\nWe close this appendix with the following well-known theorem of probability.\\nTheorem C.4 Central limit theorem\\nLet X1,...,X n be a sequence of i.i.d. random variables with mean μ and standard\\ndeviation σ.L e tXn = 1\\nn\\n∑n\\ni=1 Xi and σ2\\nn = σ2/n.T h e n ,(Xn − μ)/σn converges\\nto the N(0, 1) in distribution, that is for any t ∈ R,\\nlim\\nn→∞\\nPr[(Xn − μ)/σn ≤ t]=\\n∫ t\\n−∞\\n1√\\n2πe− x2\\n2 dx .Appendix D Concentration inequalities\\nIn this appendix, we present several concentration inequalities used in the proofs\\ngiven in this book. Concentration inequalities give probability bounds for a random\\nvariable to be concentrated around its mean, or for it to deviate from its mean or\\nsome other value.\\nD.1 Hoeﬀding’s inequality\\nWe ﬁrst present Hoeﬀding’s inequality , whose proof makes use of the general\\nChernoﬀ bounding technique. Given a random variableX and ϵ> 0, this technique\\nconsists of proceeding as follows to bound Pr[X ≥ ϵ]. For any t> 0, ﬁrst Markov’s\\ninequality is used to bound Pr[X ≥ ϵ]:\\nPr[X ≥ ϵ]=P r [e\\ntX ≥ etϵ] ≤ e−tϵ E[etX] . (D.1)\\nThen, an upper boundg(t) is found for E[etX]a n dt is selected to minimizee−tϵg(t).\\nFor Hoeﬀding’s inequality, the following lemma provides an upper bound on E[etX].\\nLemma D.1 Hoeﬀding’s lemma\\nLet X be a random variable with E[X]=0 and a ≤ X ≤ b with b>a .T h e n ,f o r\\nany t> 0, the following inequality holds:\\nE[etX] ≤ e\\nt2(b− a)2\\n8 . (D.2)\\nProof By the convexity of x ↦→ ex, for all x ∈ [a, b], the following holds:\\netx ≤ b − x\\nb − aeta + x − a\\nb − a etb .\\nThus, using E[X]=0 ,\\nE[etX] ≤ E\\n[b − X\\nb − a eta + X − a\\nb − a etb\\n]\\n= b\\nb − aeta + −a\\nb − aetb = eφ(t) ,370 Concentration inequalities\\nwhere,\\nφ(t)=l o g\\n( b\\nb − aeta + −a\\nb − aetb\\n⎡\\n= ta +l o g\\n( b\\nb − a + −a\\nb − aet(b−a)\\n⎡\\n.\\nFor any t> 0, the ﬁrst and second derivative of φ are given below:\\nφ′(t)= a − aet(b−a)\\nb\\nb−a − a\\nb−a et(b−a) = a − a\\nb\\nb−a e−t(b−a) − a\\nb−a\\n,\\nφ′′(t)= −abe−t(b−a)\\n[ b\\nb−a e−t(b−a) − a\\nb−a ]2\\n= α(1 − α)e−t(b−a)(b − a)2\\n[(1 − α)e−t(b−a) + α]2\\n= α\\n[(1 − α)e−t(b−a) + α]\\n(1 − α)e−t(b−a)\\n[(1 − α)e−t(b−a) + α](b − a)2 .\\nwhere α denotes −a\\nb−a . Note that φ(0) = φ′(0) = 0 and thatφ′′(t)= u(1 − u)(b − a)2\\nwhere u = α\\n[(1−α)e− t(b− a)+α] .S i n c eu is in [0, 1], u(1 − u) is upper bounded by 1 /4\\nand φ′(t) ≤ (b−a)2\\n4 . Thus, by the second order expansion of function φ, there exists\\nθ ∈ [0,t] such that:\\nφ(t)= φ(0) +tφ′(0) + t2\\n2 φ′′(θ) ≤ t2 (b − a)2\\n8 , (D.3)\\nwhich completes the proof.\\nThe lemma can be used to prove the following result known asHoeﬀding’s inequality.\\nTheorem D.1 Hoeﬀding’s inequality\\nLet X1,...,X m be independent random variables withXi taking values in [ai,b i] for\\nall i ∈ [1,m ]. Then for anyϵ> 0, the following inequalities hold forSm = ∑m\\ni=1 Xi:\\nPr[Sm − E[Sm] ≥ ϵ] ≤ e−2ϵ2/ Pm\\ni=1(bi−ai)2\\n(D.4)\\nPr[Sm − E[Sm] ≤− ϵ] ≤ e−2ϵ2/ Pm\\ni=1(bi−ai)2\\n. (D.5)\\nProof Using the Chernoﬀ bounding technique and lemma D.1, we can write:\\nPr[Sm − E[Sm] ≥ ϵ] ≤ e−tϵ E[et(Sm−E[Sm])]\\n=Π m\\ni=1e−tϵ E[et(Xi−E[Xi])] (independence of Xis)\\n≤ Πm\\ni=1e−tϵet2(bi−ai)2/8 (lemma D.1)\\n= e−tϵet2 Pm\\ni=1(bi−ai)2/8\\n≤ e−2ϵ2/ Pm\\ni=1(bi−ai)2\\n,D.2 McDiarmid’s inequality 371\\nw h e r ew ec h o s et =4 ϵ/ ∑m\\ni=1(bi − ai)2 to minimize the upper bound. This proves\\nthe ﬁrst statement of the theorem, and the second statement is shown in a similar\\nway.\\nWhen the variance σ2\\nXi of each random variable Xi is known and the σ2\\nXi sa r e\\nrelatively small, better concentration bounds can be derived (see Bennett’s and\\nBernstein’s inequalities proven in exercise D.4).\\nD.2 McDiarmid’s inequality\\nThis section presents a concentration inequality that is more general than Hoeﬀd-\\ning’s inequality. Its proof makes use of a Hoeﬀding’s inequality for martingale dif-\\nferences.\\nDeﬁnition D.1 Martingale Diﬀerence\\nA sequence of random variables V1,V2,... is a martingale diﬀerence sequence with\\nrespect to X1,X2,... if for all i> 0, Vi is a function of X1,...,X i and\\nE[Vi+1|X1,...,X i]=0 . (D.6)\\nThe following result is similar to Hoeﬀding’s lemma.\\nLemma D.2\\nLet V and Z be random variables satisfying E[V |Z]=0 and, for some function f\\nand constant c ≥ 0, the inequalities:\\nf(Z) ≤ V ≤ f(Z)+ c. (D.7)\\nThen, for all t> 0, the following upper bound holds:\\nE[esV |Z] ≤ et2c2/8 . (D.8)\\nProof The proof follows using the same steps as in that of lemma D.1 with\\nconditional expectations used instead of expectations: conditioned on Z, V takes\\nvalues in [a, b]w i t ha = f(Z)a n db = f(Z)+ c and its expectation vanishes.\\nThe lemma is used to prove the following theorem, which is one of the main results\\nof this section.\\nTheorem D.2 Azuma’s inequality\\nLet V1,V2,... be a martingale diﬀerence sequence with respect to the random vari-\\nables X1,X2,... , and assume that for all i> 0 there is a constant ci ≥ 0 and372 Concentration inequalities\\nrandom variable Zi, which is a function of X1,...,X i−1, that satisfy\\nZi ≤ Vi ≤ Zi + ci . (D.9)\\nThen, for all ϵ> 0 and m, the following inequalities hold:\\nPr\\n[ m∑\\ni=1\\nVi ≥ ϵ\\n]\\n≤ exp\\n( −2ϵ2\\n∑m\\ni=1 c2\\ni\\n⎡\\n(D.10)\\nPr\\n[ m∑\\ni=1\\nVi ≤− ϵ\\n]\\n≤ exp\\n( −2ϵ2\\n∑m\\ni=1 c2\\ni\\n⎡\\n. (D.11)\\nProof For any k ∈ [1,m], let Sk = ∑k\\ni=1 Vk. Then, using Chernoﬀ’s bounding\\ntechnique, for any t> 0, we can write\\nPr\\n[\\nSm ≥ ϵ\\n]\\n≤ e−tϵ E[etSm ]\\n= e−tϵ E\\n[\\netSm− 1 E[etVm |X1,...,X m−1]\\n]\\n≤ e−tϵ E[etSm− 1 ]et2c2\\nm/8 (lemma D.2)\\n≤ e−tϵet2 Pm\\ni=1 c2\\ni /8 (iterating previous argument)\\n= e−2ϵ2/ Pm\\ni=1 c2\\ni ,\\nw h e r ew ec h o s et =4 ϵ/ ∑m\\ni=1 c2\\ni to minimize the upper bound. This proves the ﬁrst\\nstatement of the theorem, and the second statement is shown in a similar way.\\nThe following is the second main result of this section. Its proof makes use of\\nAzuma’s inequality.\\nTheorem D.3 McDiarmid’s inequality\\nLet X1,...,X m ∈X m be a set of m ≥ 1 independent random variables and\\nassume that there exist c1,...,c m > 0 such that f : Xm → R satisﬁes the following\\nconditions:\\n⏐⏐f(x1,...,x i,...,x m) − f(x1,...,x ′\\ni,...x m)\\n⏐⏐ ≤ ci , (D.12)\\nfor all i ∈ [1,m] and any pointsx1,...,x m,x ′\\ni ∈X .L e tf(S) denote f(X1,...,X m),\\nthen, for all ϵ> 0, the following inequalities hold:\\nPr[f(S) − E[f(S)] ≥ ϵ] ≤ exp\\n( −2ϵ2\\n∑m\\ni=1 c2\\ni\\n⎡\\n(D.13)\\nPr[f(S) − E[f(S)] ≤− ϵ] ≤ exp\\n( −2ϵ2\\n∑m\\ni=1 c2\\ni\\n⎡\\n. (D.14)\\nProof Deﬁne a sequence of random variables Vk, k ∈ [1,m], as follows: V =D.3 Other inequalities 373\\nf(S) − E[f(S)], V1 =E [V |X1] − E[V ], and for k> 1,\\nVk =E [V |X1,...,X k] − E[V |X1,...,X k−1] .\\nNote that V = ∑m\\nk=1 Vk. Furthermore, the random variable E[ V |X1,...,X k]i sa\\nfunction of X1,...,X k. Conditioning on X1,...,X k−1 and taking its expectation is\\ntherefore:\\nE\\n[\\nE[V |X1,...,X k]|X1,...,X k−1\\n]\\n=E [V |X1,...,X k−1],\\nwhich implies E[Vk|X1,...,X k−1] = 0. Thus, the sequence (Vk)k∈[1,m] is a martin-\\ngale diﬀerence sequence. Next, observe that, since E[ f(S)] is a scalar, Vk can be\\nexpressed as follows:\\nVk =E [f(S)|X1,...,X k] − E[f(S)|X1,...,X k−1] .\\nThus, we can deﬁne an upper bound Wk and lower bound Uk for Vk by:\\nWk =s u p\\nx\\nE[f(S)|X1,...,X k−1,x] − E[f(S)|X1,...,X k−1]\\nUk =i n f\\nx\\nE[f(S)|X1,...,X k−1,x] − E[f(S)|X1,...,X k−1].\\nNow, by (D.12), for any k ∈ [1,m], the following holds:\\nWk − Uk =s u p\\nx,x′\\nE[f(S)|X1,...,X k−1,x] − E[f(S)|X1,...,X k−1,x ′] ≤ ck , (D.15)\\nthus, Uk ≤ Vk ≤ Uk + ck. In view of these inequalities, we can apply Azuma’s\\ninequality to V = ∑m\\nk=1 Vk, which yields exactly (D.13) and (D.14).\\nMcDiarmid’s inequality is used in several of the proofs in this book. It can be\\nunderstood in terms of stability: if changing any of its argument aﬀectsf only in a\\nlimited way, then, its deviations from its mean can be exponentially bounded. Note\\nalso that Hoeﬀding’s inequality (theorem D.1) is a special instance of McDiarmid’s\\ninequality where f is deﬁned by f :( x\\n1,...,x m) ↦→ 1\\nm\\n∑m\\ni=1 xi.\\nD.3 Other inequalities\\nThis section presents several other inequalities useful in the proofs of various results\\npresented in this book.374 Concentration inequalities\\nD.3.1 Binomial distribution: Slud’s inequality\\nLet B(m, p) be a binomial random variable and k an integer such that p ≤ 1\\n4 and\\nk ≥ mp or p ≤ 1\\n2 and mp ≤ k ≤ m(1 − p). Then, the following inequality holds:\\nPr[B ≥ k] ≥ Pr\\n[\\nN ≥ k − mp√\\nmp(1 − p)\\n]\\n, (D.16)\\nwhere N is in standard normal form.\\nD.3.2 Normal distribution: tail bound\\nIf N is a random variable following the standard normal distribution, then foru> 0,\\nPr[N ≥ u] ≥ 1\\n2\\n(\\n1 −\\n√\\n1 − e−u2\\n⎡\\n. (D.17)\\nD.3.3 Khintchine-Kahane inequality\\nThe following inequality is useful in a variety of diﬀerent contexts, including in the\\nproof of a lower bound for the empirical Rademacher complexity of linear hypotheses\\n(chapter 5).\\nTheorem D.4 Khintchine-Kahane inequality\\nLet (H, ∥·∥ ) be a normed vector space and let x\\n1,..., xm be m ≥ 1 elements of\\nH.L e tσ =( σ1,...,σ m)⊤ with σis independent uniform random variables taking\\nvalues in {−1, +1} (Rademacher variables). Then, the following inequalities hold:\\n1\\n2 E\\nσ\\n[\\ued79\\ued79\\ued79\\nm∑\\ni=1\\nσixi\\n\\ued79\\ued79\\ued79\\n2]\\n≤\\n(\\nE\\nσ\\n[\\ued79\\ued79\\ued79\\nm∑\\ni=1\\nσixi\\n\\ued79\\ued79\\ued79\\n]⎡2\\n≤ E\\nσ\\n[\\ued79\\ued79\\ued79\\nm∑\\ni=1\\nσixi\\n\\ued79\\ued79\\ued79\\n2]\\n. (D.18)\\nProof The second inequality is a direct consequence of the convexity of x ↦→ x2\\nand Jensen’s inequality (theorem B.4).\\nTo prove the left-hand side inequality, ﬁrst note that for any β1,...,β m ∈ R,\\nexpanding the product ∏m\\ni=1(1 + βi) leads exactly to the sum of all monomi-\\nals βδ1\\n1 ··· βδm\\nm ,w i t he x p o n e n t sδ1,...,δ m in {0,1}. We will use the notation\\nβδ1\\n1 ··· βδm\\nm = βδ and |δ| = ∑m\\ni=1 δm for any δ =( δ1,...,δ m) ∈{ 0, 1}m.I nv i e wo f\\nthat, for any (α1,...,α m) ∈ Rm and t> 0, the following equality holds:\\nt2\\nm∏\\ni=1\\n(1 +αi/t)= t2 ∑\\nδ∈{0,1}m\\nαδ/t|δ| =\\n∑\\nδ∈{0,1}m\\nt2−|δ|αδ.D.3 Other inequalities 375\\nDiﬀerentiating both sides with respect to t and setting t =1y i e l d s\\n2\\nm∏\\ni=1\\n(1 +αi) −\\nm∑\\nj=1\\nαj\\n∏\\ni̸=j\\n(1 +αi)=\\n∑\\nδ∈{0,1}m\\n(2 −| δ|)αδ . (D.19)\\nFor any σ ∈{ − 1, +1}m,l e tSσ be deﬁned by Sσ = ∥sσ ∥ with sσ = ∑m\\ni=1 σixi.\\nThen, setting αi = σiσ′\\ni, multiplying both sides of (D.19) bySσ Sσ ′ , and taking the\\nsum over all σ,σ′ ∈{ −1, +1}m yields\\n∑\\nσ,σ ′∈{−1,+1}m\\n(\\n2\\nm∏\\ni=1\\n(1 +σiσ′\\ni) −\\nm∑\\nj=1\\nσjσ′\\nj\\n∏\\ni̸=j\\n(1 +σiσ′\\ni)\\n⎡\\nSσ Sσ ′\\n=\\n∑\\nσ,σ ′∈{−1,+1}m\\n∑\\nδ∈{0,1}m\\n(2 −| δ|)σδσ′δSσ Sσ ′\\n=\\n∑\\nδ∈{0,1}m\\n(2 −| δ|)\\n∑\\nσ,σ ′∈{−1,+1}m\\nσδσ′δSσ Sσ ′\\n=\\n∑\\nδ∈{0,1}m\\n(2 −| δ|)\\n[ ∑\\nσ ∈{−1,+1}m\\nσδSσ\\n]2\\n.\\n(D.20)\\nNote that the terms of the right-hand sum with|δ|≥ 2 are non-positive. The terms\\nwith |δ| = 1 are null: sinceSσ = S−σ ,w eh a v e∑\\nσ ∈{−1,+1}m σδSσ = 0 in that case.\\nThus, the right-hand side can be upper bounded by the term with δ =0 ,t h a ti s ,\\n2\\n(∑\\nσ ∈{−1,+1}m Sσ\\n⎡2\\n. The left-hand side of (D.20) can be rewritten as follows:\\n∑\\nσ ∈{−1,+1}m\\n(2m+1 − m2m−1)S2\\nσ +2 m−1 ∑\\nσ ∈{−1,+1}m\\nσ ′∈B(σ,1)\\nSσ Sσ ′\\n=2 m ∑\\nσ ∈{−1,+1}m\\nS2\\nσ +2 m−1 ∑\\nσ ∈{−1,+1}m\\nSσ\\n( ∑\\nσ ′ ∈B(σ,1)\\nSσ ′ − (m − 2)Sσ\\n⎡\\n,\\n(D.21)\\nwhere B(σ, 1) denotes the set of σ′ that diﬀer from σ in exactly one coordinate\\nj ∈ [1,m], that is the set ofσ′ with Hamming distance one fromσ. Note that for any\\nsuch σ′, sσ − sσ ′ =2 σjxj for one coordinate j ∈ [1,m], thus, ∑\\nσ ′ ∈B(σ,1) sσ − sσ ′ =\\n2sσ . In light of that and using the triangle inequality, we can write\\n(m − 2)Sσ = ∥msσ ∥−∥ 2sσ ∥ =\\n\\ued79\\ued79\\ued79\\n∑\\nσ ′ ∈B(σ,1)\\nsσ\\n\\ued79\\ued79\\ued79 −\\n\\ued79\\ued79\\ued79\\n∑\\nσ ′∈B(σ,1)\\nsσ − sσ ′\\n\\ued79\\ued79\\ued79\\n≤\\n\\ued79\\ued79\\ued79\\n∑\\nσ ′ ∈B(σ,1)\\nsσ ′\\n\\ued79\\ued79\\ued79 ≤\\n∑\\nσ ′ ∈B(σ,1)\\nSσ ′ .\\nThus, the second sum of (D.21) is non-negative and the left-hand side of (D.20) can376 Concentration inequalities\\nbe lower bounded by the ﬁrst sum 2 m ∑\\nσ ∈{−1,+1}m S2\\nσ . Combining this with the\\nupper bound found for (D.20) gives\\n2m ∑\\nσ ∈{−1,+1}m\\nS2\\nσ ≤ 2\\n[ ∑\\nσ ∈{−1,+1}m\\nSσ\\n]2\\n.\\nDividing both sides by 22m and using Pr[σ]=1 /2m gives Eσ [S2\\nσ ] ≤ 2(Eσ [Sσ ])2 and\\ncompletes the proof.\\nThe constant 1/2 appearing in the ﬁrst inequality of (D.18) is optimal. To see this,\\nconsider the case where m = 2 and x1 = x2 = x for some non-zero vector x ∈ H.\\nThen, the left-hand side of the ﬁrst inequality is 1\\n2\\n∑m\\ni=1 ∥xi∥2 = ∥x∥2 and the\\nright-hand side\\n(\\nEσ\\n[\\n∥(σ1 + σ2)x∥\\n]⎡2\\n= ∥x∥2(Eσ [|σ1 + σ2|])2 = ∥x∥2.\\nNote that when the norm ∥·∥ corresponds to an inner product, as in the case of\\na Hilbert space H, we can write\\nE\\nσ\\n[\\ued79\\ued79\\ued79\\nm∑\\ni=1\\nσixi\\n\\ued79\\ued79\\ued79\\n2]\\n=\\nm∑\\ni,j=1\\nE\\nσ\\n[\\nσiσj(xi · xj)\\n]\\n=\\nm∑\\ni,j=1\\nE\\nσ\\n[σiσj](xi · xj)=\\nm∑\\ni=1\\n∥xi∥2,\\nsince by the independence of the random variables σi,f o r i ̸= j,E σ [σiσj]=\\nEσ [σi]Eσ [σj] = 0. Thus, (D.18) can then be rewritten as follows:\\n1\\n2\\nm∑\\ni=1\\n∥xi∥2 ≤\\n(\\nE\\nσ\\n[\\ued79\\ued79\\n\\ued79\\nm∑\\ni=1\\nσixi\\n\\ued79\\ued79\\n\\ued79\\n]⎡\\n2\\n≤\\nm∑\\ni=1\\n∥xi∥2 . (D.22)\\nD.4 Chapter notes\\nThe improved version of Azuma’s inequality [Hoeﬀding, 1963, Azuma, 1967] pre-\\nsented in this chapter is due to McDiarmid [1989]. The improvement is a reduction\\nof the exponent by a factor of 4. This also appears in McDiarmid’s inequality, which\\nis derived from the inequality for bounded martingale sequences. The inequalities\\npresented in exercise D.4 are due to Bernstein [1927] and Bennett [1962]; the exercise\\nis from Devroye and Lugosi [1995].\\nThe binomial inequality of section D.3.1 is due to Slud [1977]. The tail bound\\nof section D.3.2 is due to Tate [1953] (see also Anthony and Bartlett [1999]). The\\nKhintchine-Kahane inequality was ﬁrst studied in the case of real-valued variables\\nx\\n1,...,x m by Khintchine [1923], with better constants and simpler proofs later\\nprovided by Szarek [1976], Haagerup [1982], and Tomaszewski [1982]. The inequality\\nwas extended to normed vector spaces by Kahane [1964]. The proof presented here\\nis due to Lata/suppressla and Oleszkiewicz [1994] and provides the best possible constants.D.5 Exercises 377\\nD.5 Exercises\\nD.1 Twins paradox. Professor Mamoru teaches at a university whose computer\\nscience and math building has F =3 0ﬂ o o r s .\\n(1) Assume that the ﬂoors are independent and that they have the same\\nprobability to be selected by someone taking the elevator. How many people\\nshould take the elevator in order to make it likely (probability more than half)\\nt h a tt w oo ft h e mg ot ot h es a m eﬂ o o r ?(Hint: use the Taylor series expansion of\\ne\\n−x =1 − x + ... and give an approximate general expression of the solution.)\\n(2) Professor Mamoru is popular, and his ﬂoor is in fact more likely to be\\nselected than others. Assuming that all other ﬂoors are equiprobable, derive\\nthe general expression of the probability that two persons go to the same ﬂoor,\\nusing the same approximation as before. How many people should take the\\nelevator in order to make it likely that two of them go to the same ﬂoor when\\nthe probability of Professor Mamoru’s ﬂoor is .25, .35, or .5? When q = .5,\\nwould the answer change if the number of ﬂoors were insteadF =1 ,000?\\n(3) The probability models assumed in (1) and (2) are both naive. If you had\\naccess to the data collected by the elevator guard, how would you deﬁne a more\\nfaithful model?\\nD.2 Concentration bounds. Let X be a non-negative random variable satisfying\\nPr[X>t ] ≤ ce\\n−2mt2\\nfor all t> 0a n ds o m ec> 0. Show that E[X2] ≤ log(ce)\\n2m (Hint:\\nto do that, use the identity E[X2]=\\n∫ ∞\\n0 Pr[X2 >t ]dt,w r i t e\\n∫ ∞\\n0 =\\n∫ u\\n0 +\\n∫ ∞\\nu ,b o u n d\\nthe ﬁrst term by u and ﬁnd the best u to minimize the upper bound).\\nD.3 Comparison of Hoeﬀding’s and Chebyshev’s inequalities. Let X1,...,X m be\\na sequence of random variables taking values in [0 ,1] with the same mean μ and\\nvariance σ2 < ∞ and let X = 1\\nm\\n∑m\\ni=1 Xi.\\n(a) For any ϵ> 0, give a bound on Pr[|X−μ| >ϵ ] using Chebyshev’s inequality,\\nthen Hoeﬀding’s inequality. For what values of σ is Chebyshev’s inequality\\ntighter?\\n(b) Assume that the random variables Xi take values in {0, 1}. Show that\\nσ2 ≤ 1\\n4 . Use this to simplify Chebyshev’s inequality. Choose ϵ = .05 and\\nplot Chebyshev’s inequality thereby modiﬁed and Hoeﬀding’s inequality as a\\nfunction of m (you can use your preferred program for generating the plots).\\nD.4 Bennett’s and Bernstein’s inequalities. The objective of this problem is to prove378 Concentration inequalities\\nthese two inequalities.\\n(a) Show that for any t> 0, and any random variable X with E[X]=0 ,\\nE[X2]= σ2,a n dX ≤ c,\\nE[etX] ≤ ef(σ2/c2), (D.23)\\nwhere\\nf(x)=l o g\\n( 1\\n1+ xe−ctx + x\\n1+ xect\\n⎡\\n.\\n(b) Show that f ′′(x) ≤ 0f o rx ≥ 0.\\n(c) Using Chernoﬀ’s bounding technique, show that\\nPr\\n[ 1\\nm\\nm∑\\ni=1\\nXi ≥ ϵ\\n]\\n≤ e−tmϵ+Pm\\ni=1 f(σ2\\nXi /c2),\\nwhere (σ2\\nXi i st h ev a r i a n c eo fXi.\\n(d) Show that f(x) ≤ f(0) +xf ′(0) = (ect − 1 − ct)x.\\n(e) Using the bound derived in (4), ﬁnd the optimal value of t.\\n(f) Bennett’s inequality.L e tX1,...,X m be independent real-valued random\\nvariables with zero mean such that for i =1 ,...,m , Xi ≤ c.L e t σ2 =\\n1\\nm\\n∑m\\ni=1 σ2\\nXi . Show that\\nPr\\n[ 1\\nm\\nm∑\\ni=1\\nXi >ϵ\\n]\\n≤ exp\\n(\\n− mσ2\\nc2 θ\\n( ϵc\\nσ2\\n⎡⎡\\n, (D.24)\\nwhere θ(x)=( 1+ x) log(1 +x) − x.\\n(g) Bernstein’s inequality. Show that under the same conditions as Bennett’s\\ninequality\\nPr\\n[ 1\\nm\\nm∑\\ni=1\\nXi >ϵ\\n]\\n≤ exp\\n(\\n− mϵ2\\n2σ2 +2 cϵ/3\\n⎡\\n. (D.25)\\n(Hint: show that for all x ≥ 0, θ(x) ≥ h(x)= 3\\n2\\nx2\\nx+3 .)\\n(h) Write Hoeﬀding’s inequality assuming the same conditions. For what values\\nof σ is Bernstein’s inequality better than Hoeﬀding’s inequality?Appendix E Notation\\nTa b l e E . 1 Summary of notation.\\nR Set of real numbers\\nR+ Set of non-negative real numbers\\nRn Set of n-dimensional real-valued vectors\\nRn×m Set of n × m real-valued matrices\\n[a, b] Closed interval between a and b\\n(a, b) Open interval between a and b\\n{a, b, c} Set containing elements a, b and c\\nN Set of natural numbers, i.e., {0, 1,... }\\nlog Logarithm with base e\\nloga Logarithm with base a\\nS An arbitrary set\\n|S| Number of elements in S\\ns ∈S An element in set S\\nX Input space\\nY Target space\\nH Feature space\\n⟨·, ·⟩ Inner product in feature space\\nv An arbitrary vector\\n1 Vector of all ones\\nvi ith component of v\\n∥v∥ L2 norm of v\\n∥v∥p Lp norm of v\\nu ◦v Hadamard or entry-wise product of vectors u and v380 Notation\\nf ◦g Composition of functions f and g\\nT1 ◦T2 C o m p o s i t i o no fw e i g h t e dt r a n s d u c e r sT1 and T2\\nM An arbitrary matrix\\n∥M∥2 Spectral norm of M\\n∥M∥F Frobenius norm of M\\nM⊤ Transpose of M\\nM† Pseudo-inverse of M\\nTr[M]T r a c e o f M\\nI Identity matrix\\nK : X× X→ R Kernel function over X\\nK Kernel matrix\\n1A Indicator function indicating membership in subset A\\nR(·) Generalization error or risk\\nˆR(·) E m p i r i c a le r r o ro rr i s k\\nRm(·) Rademacher complexity over all samples of size m\\nˆRS(·) Empirical Rademacher complexity with respect to sample S\\nN(0, 1) Standard normal distribution\\nE\\nx∼D\\n[·] Expectation over x drawn from distribution D\\nΣ∗ Kleene closure over a set of characters ΣReferences\\nShivani Agarwal, Thore Graepel, Ralf Herbrich, Sariel Har-Peled, and Dan Roth.\\nGeneralization bounds for the area under the ROC curve. Journal of Machine\\nLearning, 6:393–425, 2005.\\nShivani Agarwal and Partha Niyogi. Stability and generalization of bipartite ranking\\nalgorithms. In Conference on Learning Theory, pages 32–47, 2005.\\nNir Ailon and Mehryar Mohri. An eﬃcient reduction of ranking to classiﬁcation.\\nIn Conference on Learning Theory, pages 87–98, 2008.\\nMark A. Aizerman, E. M. Braverman, and Lev I. Rozono`er. Theoretical foundations\\nof the potential function method in pattern recognition learning.Automation and\\nRemote Control, 25:821–837, 1964.\\nCyril Allauzen, Corinna Cortes, and Mehryar Mohri. Large-scale training of SVMs\\nwith automata kernels. In International Conference on Implementation and\\nApplication of Automata, pages 17–27, 2010.\\nCyril Allauzen and Mehryar Mohri. N-way composition of weighted ﬁnite-state\\ntransducers. International Journal of Foundations of Computer Science , 20(4):\\n613–627, 2009.\\nErin L. Allwein, Robert E. Schapire, and Yoram Singer. Reducing multiclass to\\nbinary: A unifying approach for margin classiﬁers. Journal of Machine Learning,\\n1:113–141, 2000.\\nNoga Alon, Shai Ben-David, Nicol` o Cesa-Bianchi, and David Haussler. Scale-\\nsensitive dimensions, uniform convergence, and learnability. Journal of ACM ,\\n44:615–631, July 1997.\\nNoga Alon and Joel Spencer. The Probabilistic Method. John Wiley, 1992.\\nDana Angluin. On the complexity of minimum inference of regular sets.Information\\nand Control, 39(3):337–350, 1978.\\nDana Angluin. Inference of reversible languages. Journal of the ACM, 29(3):741–\\n765, 1982.\\nMartin Anthony and Peter L. Bartlett. Neural Network Learning: Theoretical\\nFoundations. Cambridge University Press, 1999.382 REFERENCES\\nNachman Aronszajn. Theory of reproducing kernels. Transactions of the American\\nMathematical Society, 68(3):337–404, 1950.\\nPatrick Assouad. Densit´e et dimension.Annales de l’institut Fourier, 33(3):233–282,\\n1983.\\nKazuoki Azuma. Weighted sums of certain dependent random variables. Tohoku\\nMathematical Journal, 19(3):357–367, 1967.\\nMaria-Florina Balcan, Nikhil Bansal, Alina Beygelzimer, Don Coppersmith, John\\nLangford, and Gregory B. Sorkin. Robust reductions from ranking to classiﬁca-\\ntion. Machine Learning, 72(1-2):139–153, 2008.\\nPeter L. Bartlett, St´ephane Boucheron, and G´abor Lugosi. Model selection and\\nerror estimation. Machine Learning, 48:85–113, September 2002a.\\nPeter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. Localized Rademacher\\ncomplexities. In Conference on Computational Learning Theory, volume 2375,\\npages 79–97. Springer-Verlag, 2002b.\\nPeter L. Bartlett and Shahar Mendelson. Rademacher and Gaussian complexities:\\nRisk bounds and structural results. Journal of Machine Learning, 3, 2002.\\nAmos Beimel, Francesco Bergadano, Nader H. Bshouty, Eyal Kushilevitz, and\\nStefano Varricchio. Learning functions represented as multiplicity automata.\\nJournal of the ACM, 47:2000, 2000.\\nMikhail Belkin and Partha Niyogi. Laplacian eigenmaps and spectral techniques\\nfor embedding and clustering. In Advances in Neural Information Processing\\nSystems, 2001.\\nGeorge Bennett. Probability inequalities for the sum of independent random\\nvariables. Journal of the American Statistical Association, 57:33–45, 1962.\\nChristian Berg, Jens P.R. Christensen, and Paul Ressel. Harmonic Analysis on\\nSemigroups: Theory of Positive Deﬁnite and Related Functions , volume 100.\\nSpringer, 1984.\\nFrancesco Bergadano and Stefano Varricchio. Learning behaviors of automata from\\nshortest counterexamples. In Conference on Computational Learning Theory ,\\npages 380–391, 1995.\\nSergei Natanovich Bernstein. Sur l’extension du th´ eor`eme limite du calcul des\\nprobabilit´es aux sommes de quantit´es d´ependantes. Mathematische Annalen, 97:\\n1–59, 1927.\\nDimitri P. Bertsekas.Dynamic Programming: Deterministic and Stochastic Models.\\nPrentice-Hall, 1987.\\nAvrim Blum and Yishay Mansour. From external to internal regret. InConference\\non Learning Theory, pages 621–636, 2005.REFERENCES 383\\nAvrim Blum and Yishay Mansour. Learning, regret minimization, and equilibria.\\nIn Noam Nisan, Tim Roughgarden, ´Eva Tardos, and Vijay Vazirani, editors,\\nAlgorithmic Game Theory, chapter 4, pages 4–30. Cambridge University Press,\\n2007.\\nAnselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth.\\nLearnability and the Vapnik-Chervonenkis dimension. Journal of the ACM,3 6\\n(4):929–965, 1989.\\nBernhard E. Boser, Isabelle M. Guyon, and Vladimir N. Vapnik. A training algo-\\nrithm for optimal margin classiﬁers. In Conference on Computational Learning\\nTheory, pages 144–152, 1992.\\nOlivier Bousquet and Andr´e Elisseeﬀ. Stability and generalization. Journal of\\nMachine Learning, 2:499–526, 2002.\\nStephen P. Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge\\nUniversity Press, 2004.\\nLeo Breiman. Prediction games and arcing algorithms. Neural Computation, 11:\\n1493–1517, October 1999.\\nLeo Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classiﬁcation and\\nRegression Trees. Wadsworth, 1984.\\nNicol`o Cesa-Bianchi. Analysis of two gradient-based algorithms for on-line regres-\\nsion. Journal of Computer System Sciences, 59(3):392–411, 1999.\\nNicol`o Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization abil-\\nity of on-line learning algorithms. In Advances in Neural Information Processing\\nSystems, pages 359–366, 2001.\\nNicol`o Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization\\nability of on-line learning algorithms.IEEE Transactions on Information Theory,\\n50(9):2050–2057, 2004.\\nNicol`o Cesa-Bianchi, Yoav Freund, David Haussler, David P. Helmbold, Robert E.\\nSchapire, and Manfred K. Warmuth. How to use expert advice. Journal of the\\nACM, 44(3):427–485, 1997.\\nNicol`o Cesa-Bianchi and G´abor Lugosi. Potential-based algorithms in online pre-\\ndiction and game theory. In Conference on Learning Theory, pages 48–64, 2001.\\nNicol`o Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games. Cam-\\nbridge University Press, 2006.\\nNicol`o Cesa-Bianchi, Yishay Mansour, and Gilles Stoltz. Improved second-order\\nbounds for prediction with expert advice. In Conference on Learning Theory,\\npages 217–232, 2005.\\nBernard Chazelle. The Discrepancy Method: Randomness and Complexity. Cam-384 REFERENCES\\nbridge University Press, New York, NY, USA, 2000.\\nMichael Collins, Robert E. Schapire, and Yoram Singer. Logistic regression, Ad-\\naboost and Bregman distances. Machine Learning, 48:253–285, September 2002.\\nCorinna Cortes, Patrick Haﬀner, and Mehryar Mohri. Rational kernels: Theory and\\nalgorithms. Journal of Machine Learning, 5:1035–1062, 2004.\\nCorinna Cortes, Leonid Kontorovich, and Mehryar Mohri. Learning languages with\\nrational kernels. InConference on Learning Theory, volume 4539 ofLecture Notes\\nin Computer Science, pages 349–364. Springer, Heidelberg, Germany, June 2007a.\\nCorinna Cortes, Yishay Mansour, and Mehryar Mohri. Learning bounds for impor-\\ntance weighting. In Advances in Neural Information Processing Systems, Van-\\ncouver, Canada, 2010a. MIT Press.\\nCorinna Cortes and Mehryar Mohri. AUC optimization vs. error rate minimization.\\nIn Advances in Neural Information Processing Systems, 2003.\\nCorinna Cortes and Mehryar Mohri. Conﬁdence intervals for the area under the\\nROC curve. In Advances in Neural Information Processing Systems, volume 17,\\nVancouver, Canada, 2005. MIT Press.\\nCorinna Cortes, Mehryar Mohri, Dmitry Pechyony, and Ashish Rastogi. Stability\\nof transductive regression algorithms. In International Conference on Machine\\nLearning, Helsinki, Finland, July 2008a.\\nCorinna Cortes, Mehryar Mohri, and Ashish Rastogi. An alternative ranking\\nproblem for search engines. In Workshop on Experimental Algorithms , pages\\n1–22, 2007b.\\nCorinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh. Learning sequence\\nkernels. In Proceedings of IEEE International Workshop on Machine Learning\\nfor Signal Processing,C a n c ´un, Mexico, October 2008b.\\nCorinna Cortes, Mehryar Mohri, and Ameet Talwalkar. On the impact of kernel\\napproximation on learning accuracy. In Conference on Artiﬁcial Intelligence and\\nStatistics, 2010b.\\nCorinna Cortes, Mehryar Mohri, and Jason Weston. A general regression framework\\nfor learning string-to-string mappings. In Predicted Structured Data. MIT Press,\\n2007c.\\nCorinna Cortes and Vladimir Vapnik. Support-vector networks.Machine Learning,\\n20(3):273–297, 1995.\\nDavid Cossock and Tong Zhang. Statistical analysis of Bayes optimal subset\\nranking. IEEE Transactions on Information Theory, 54(11):5140–5154, 2008.\\nT r e v o rF .C o xa n dM i c h a e lA .A .C o x .Multidimensional Scaling. Chapman &\\nHall/CRC, 2nd edition, 2000.REFERENCES 385\\nKoby Crammer and Yoram Singer. Improved output coding for classiﬁcation using\\ncontinuous relaxation. In Advances in Neural Information Processing Systems,\\n2001.\\nKoby Crammer and Yoram Singer. On the algorithmic implementation of multiclass\\nkernel-based vector machines. Journal of Machine Learning, 2, 2002.\\nRobert Crites and Andrew Barto. Improving elevator performance using reinforce-\\nment learning. In Advances in Neural Information Processing Systems , pages\\n1017–1023. MIT Press, 1996.\\nFelipe Cucker and Steve Smale. On the mathematical foundations of learning.\\nBulletin of the American Mathematical Society, 39(1):1–49, 2001.\\nSanjoy Dasgupta and Anupam Gupta. An elementary proof of a theorem of Johnson\\nand Lindenstrauss. Random Structures and Algorithms, 22(1):60–65, 2003.\\nLuc Devroye and G´abor Lugosi. Lower bounds in pattern recognition and learning.\\nPattern Recognition, 28(7):1011–1018, 1995.\\nLuc Devroye and T. J. Wagner. Distribution-free inequalities for the deleted and\\nholdout error estimates. IEEE Transactions on Information Theory, 25(2):202–\\n207, 1979a.\\nLuc Devroye and T. J. Wagner. Distribution-free performance bounds for potential\\nfunction rules. IEEE Transactions on Information Theory, 25(5):601–604, 1979b.\\nThomas G. Dietterich. An experimental comparison of three methods for construct-\\ning ensembles of decision trees: Bagging, boosting, and randomization. Machine\\nLearning, 40(2):139–157, 2000.\\nThomas G. Dietterich and Ghulum Bakiri. Solving multiclass learning problems\\nvia error-correcting output codes. Journal of Artiﬁcial Intelligence Research,2 :\\n263–286, 1995.\\nHarris Drucker and Corinna Cortes. Boosting decision trees. InAdvances in Neural\\nInformation Processing Systems, pages 479–485, 1995.\\nHarris Drucker, Robert E. Schapire, and Patrice Simard. Boosting performance\\nin neural networks. International Journal of Pattern Recognition and Artiﬁcial\\nIntelligence, 7(4):705–719, 1993.\\nRichard M. Dudley. The sizes of compact subsets of Hilbert space and continuity\\nof Gaussian processes. Journal of Functional Analysis, 1(3):290–330, 1967.\\nRichard M. Dudley. A course on empirical processes.Lecture Notes in Mathematics,\\n1097:2 – 142, 1984.\\nRichard M. Dudley. Universal Donsker classes and metric entropy. Annals of\\nProbability, 14(4):1306–1326, 1987.\\nRichard M. Dudley.Uniform Central Limit Theorems. Cambridge University Press,386 REFERENCES\\n1999.\\nNigel Duﬀy and David P. Helmbold. Potential boosters? In Advances in Neural\\nInformation Processing Systems, pages 258–264, 1999.\\nAryeh Dvoretzky. On stochastic approximation. In Proceedings of the Third\\nBerkeley Symposium on Mathematical Statistics and Probability , pages 39–55,\\n1956.\\nCynthia Dwork, Ravi Kumar, Moni Naor, and D. Sivakumar. Rank aggregation\\nmethods for the web. In International World Wide Web Conference, pages 613–\\n622, 2001.\\nBradley Efron, Trevor Hastie, Iain Johnstone, and Robert Tibshirani. Least angle\\nregression. Annals of Statistics, 32(2):407–499, 2004.\\nJames P. Egan.Signal Detection Theory and ROC Analysis. Academic Press, 1975.\\nAndrzej Ehrenfeucht, David Haussler, Michael J. Kearns, and Leslie G. Valiant. A\\ngeneral lower bound on the number of examples needed for learning. InConference\\non Learning Theory, pages 139–154, 1988.\\nEyal Even-Dar and Yishay Mansour. Learning rates for q-learning. Machine\\nLearning, 5:1–25, 2003.\\nDean P. Foster and Rakesh V. Vohra. Calibrated learning and correlated equilib-\\nrium. Games and Economic Behavior, 21:40–55, 1997.\\nDean P. Foster and Rakesh V. Vohra. Asymptotic calibration. Biometrika, pages\\n379–390, 1998.\\nDean P. Foster and Rakesh V. Vohra. Regret in the on-line decision problem.Games\\nand Economic Behavior, 29(1-2):7–35, 1999.\\nYoav Freund. Boosting a weak learning algorithm by majority. In Conference on\\nComputational Learning Theory, pages 202–216. Morgan Kaufmann Publishers\\nInc., 1990.\\nYoav Freund. Boosting a weak learning algorithm by majority. Information and\\nComputation, 121:256–285, September 1995.\\nYoav Freund, Raj D. Iyer, Robert E. Schapire, and Yoram Singer. An eﬃcient\\nboosting algorithm for combining preferences. Journal of Machine Learning,4 ,\\n2003.\\nYoav Freund, Michael J. Kearns, Dana Ron, Ronitt Rubinfeld, Robert E. Schapire,\\nand Linda Sellie. Eﬃcient learning of typical ﬁnite automata from random walks.\\nIn Proceedings the ACM Symposium on Theory of Computing , pages 315–324,\\n1993.\\nYoav Freund and Robert E. Schapire. Game theory, on-line prediction and boosting.\\nIn Conference on Learning Theory, pages 325–332, 1996.REFERENCES 387\\nYoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line\\nlearning and an application to boosting. Journal of Computer System Sciences,\\n55(1):119–139, 1997.\\nYoav Freund and Robert E. Schapire. Large margin classiﬁcation using the percep-\\ntron algorithm. Machine Learning, 37:277–296, 1999a.\\nYoav Freund and Robert E. Schapire. Adaptive game playing using multiplicative\\nweights. Games and Economic Behavior, 29(1-2):79–103, October 1999b.\\nJerome H. Friedman. Greedy function approximation: A gradient boosting machine.\\nAnnals of Statistics, 29:1189–1232, 2000.\\nJerome H. Friedman, Trevor Hastie, and Robert Tibshirani. Additive logistic\\nregression: A statistical view of boosting. Annals of Statistics, 38(2), 2000.\\nE. Mark Gold. Language identiﬁcation in the limit. Information and Control,1 0\\n(5):447–474, 1967.\\nE. Mark Gold. Complexity of automaton identiﬁcation from given data.Information\\nand Control, 37(3):302–320, 1978.\\nDavid M. Green and John A Swets. Signal Detection Theory and Psychophysics.\\nWiley, 1966.\\nAdam J. Grove and Dale Schuurmans. Boosting in the limit: Maximizing the margin\\nof learned ensembles. In Proceedings of the Fifteenth National Conference on\\nArtiﬁcial Intelligence , pages 692–699, 1998.\\nUﬀe Haagerup. The best constants in the Khintchine inequality. Studia Math,7 0\\n(3):231–283, 1982.\\nJihun Ham, Daniel D. Lee, Sebastian Mika, and Bernhard Sch¨ olkopf. A kernel\\nview of the dimensionality reduction of manifolds. In International Conference\\non Machine Learning, 2004.\\nJames A. Hanley and Barbara J. McNeil. The meaning and use of the area under\\na receiver operating characteristic (ROC) curve. Radiology, 143:29–36, 1982.\\nJames Hannan. Approximation to Bayes risk in repeated plays. Contributions to\\nthe Theory of Games, 3:97–139, 1957.\\nSergiu Hart and Andreu M. Mas-Colell. A simple adaptive procedure leading to\\ncorrelated equilibrium. Econometrica, 68(5):1127–1150, 2000.\\nDavid Haussler. Decision theoretic generalizations of the PAC model for neural net\\nand other learning applications. Information and Computation, 100(1):78–150,\\n1992.\\nDavid Haussler. Sphere packing numbers for subsets of the boolean n-cube with\\nbounded Vapnik-Chervonenkis dimension. Journal of Combinatorial Theory,\\nSeries A, 69(2):217 – 232, 1995.388 REFERENCES\\nDavid Haussler. Convolution Kernels on Discrete Structures. Technical Report\\nUCSC-CRL-99-10, University of California at Santa Cruz, 1999.\\nDavid Haussler, Nick Littlestone, and Manfred K. Warmuth. Predicting {0,1}-\\nfunctions on randomly drawn points (extended abstract). In Foundations of\\nComputer Science, pages 100–109, 1988.\\nRalf Herbrich, Thore Graepel, and Klaus Obermayer. Large margin rank boundaries\\nfor ordinal regression. In Advances in Large Margin Classiﬁers, pages 115–132.\\nMIT Press, Cambridge, MA, 2000.\\nWassily Hoeﬀding. Probability inequalities for sums of bounded random variables.\\nJournal of the American Statistical Association, 58(301):13–30, 1963.\\nArthur E. Hoerl and Robert W. Kennard. Ridge regression: Biased estimation for\\nnonorthogonal problems. Technometrics, 12(1):55–67, 1970.\\nKlaus-Uwe H¨oﬀgen, Hans-Ulrich Simon, and Kevin S. Van Horn. Robust trainability\\nof single neurons. Journal of Computer and Systems Sciences , 50(1):114–125,\\n1995.\\nJohn E. Hopcroft and Jeﬀrey D. Ullman. Introduction to Automata Theory,\\nLanguages and Computation. Addison-Wesley, 1979.\\nCho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, S. Sathiya Keerthi, and S. Sundarara-\\njan. A dual coordinate descent method for large-scale linear SVM. In Interna-\\ntional Conference on Machine Learning, pages 408–415, 2008.\\nTommi Jaakkola, Michael I. Jordan, and Satinder P. Singh. Convergence of\\nstochastic iterative dynamic programming algorithms. Neural Computation,6 :\\n1185–1201, 1994.\\nKalervo J¨arvelin and Jaana Kek¨al¨ainen. IR evaluation methods for retrieving highly\\nrelevant documents. In ACM Special Interest Group on Information Retrieval,\\npages 41–48, 2000.\\nThorsten Joachims. Optimizing search engines using clickthrough data. In Knowl-\\nedge and Discovery and Data Mining, pages 133–142, 2002.\\nWilliam B. Johnson and Joram Lindenstrauss. Extensions of Lipschitz mappings\\ninto a Hilbert space. Contemporary Mathematics, 26:189–206, 1984.\\nJean-Pierre Kahane. Sur les sommes vectorielles ∑ ±un. Comptes Rendus Hebdo-\\nmadaires des S’eances de l’Acad´emie des Sciences, Paris, 259:2577–2580, 1964.\\nAdam Kalai and Santosh Vempala. Eﬃcient algorithms for online decision problems.\\nIn Conference on Learning Theory, pages 26–40, 2003.\\nWilliam Karush. Minima of Functions of Several Variables with Inequalities as Side\\nConstraints. Master’s thesis, Department of Mathematics, University of Chicago,\\n1939.REFERENCES 389\\nMichael J. Kearns and Yishay Mansour. A fast, bottom-up decision tree pruning\\nalgorithm with near-optimal generalization. In International Conference on\\nMachine Learning, pages 269–277, 1998.\\nMichael J. Kearns and Yishay Mansour. On the boosting ability of top-down\\ndecision tree learning algorithms. Journal of Computer and System Sciences ,\\n58(1):109–128, 1999.\\nMichael J. Kearns and Dana Ron. Algorithmic stability and sanity-check bounds\\nfor leave-one-out cross-validation. Neural Computation, 11(6):1427–1453, 1999.\\nMichael J. Kearns and Robert E. Schapire. Eﬃcient distribution-free learning of\\nprobabilistic concepts (extended abstract). In Foundations of Computer Science,\\npages 382–391, 1990.\\nMichael J. Kearns and Leslie G. Valiant. Cryptographic limitations on learning\\nboolean formulae and ﬁnite automata. Technical Report 14, Harvard University,\\n1988.\\nMichael J. Kearns and Leslie G. Valiant. Cryptographic limitations on learning\\nboolean formulae and ﬁnite automata. Journal of ACM, 41(1):67–95, 1994.\\nMichael J. Kearns and Umesh V. Vazirani. An Introduction to Computational\\nLearning Theory. MIT Press, 1994.\\nAleksandr Khintchine. ¨Uber dyadische br¨uche. Mathematische Zeitschrift, 18(1):\\n109–116, 1923.\\nJack Kiefer and Jacob Wolfowitz. Stochastic estimation of the maximum of a\\nregression function. Annals of Mathematical Statistics, 23(1):462–466, 1952.\\nGeorge Kimeldorf and Grace Wahba. Some results on tchebycheﬃan spline func-\\ntions. Journal of Mathematical Analysis and Applications, 33(1):82–95, 1971.\\nJyrki Kivinen and Manfred K. Warmuth. Boosting as entropy projection. In\\nConference on Learning Theory, pages 134–144, 1999.\\nVladimir Koltchinskii. Rademacher penalties and structural risk minimization.\\nIEEE Transactions on Information Theory, 47(5):1902–1914, 2001.\\nVladimir Koltchinskii and Dmitry Panchenko. Rademacher processes and bounding\\nthe risk of function learning. In High Dimensional Probability II, pages 443–459.\\nBirkh¨auser, 2000.\\nVladmir Koltchinskii and Dmitry Panchenko. Empirical margin distributions and\\nbounding the generalization error of combined classiﬁers. Annals of Statistics,\\n30, 2002.\\nLeonid Kontorovich, Corinna Cortes, and Mehryar Mohri. Learning linearly sepa-\\nrable languages. In Algorithmic Learning Theory, pages 288–303, 2006.\\nLeonid Kontorovich, Corinna Cortes, and Mehryar Mohri. Kernel methods for390 REFERENCES\\nlearning languages. Theoretical Computer Science, 405:223–236, 2008.\\nHarold W. Kuhn and Albert W. Tucker. Nonlinear programming. In 2nd Berkeley\\nSymposium, pages 481–492, Berkeley, 1951. University of California Press.\\nHarold J. Kushner and D. S. Clark. Stochastic Approximation Methods for Con-\\nstrained and Unconstrained Systems, volume 26 ofApplied Mathematical Sciences.\\nSpringer-Verlag, 1978.\\nHarold Kushner. Stochastic approximation: a survey. Wiley Interdisciplinary\\nReviews Computational Statistics, 2(1):87–96, 2010.\\nJohn D. Laﬀerty, Andrew McCallum, and Fernando C. N. Pereira. Conditional\\nrandom ﬁelds: Probabilistic models for segmenting and labeling sequence data.\\nIn International Conference on Machine Learning, pages 282–289, 2001.\\nJohn Laﬀerty. Additive models, boosting, and inference for generalized divergences.\\nIn Conference on Learning Theory, pages 125–133, 1999.\\nRafa/suppressl Lata/suppressla and Krzysztof Oleszkiewicz. On the best constant in the khintchine-\\nkahane inequality. Studia Math, 109(1):101–104, 1994.\\nGuy Lebanon and John D. Laﬀerty. Boosting and maximum likelihood for expo-\\nnential models. In Advances in Neural Information Processing Systems, pages\\n447–454, 2001.\\nMichel Ledoux and Michel Talagrand. Probability in Banach Spaces: Isoperimetry\\nand Processes. Springer, New York, 1991.\\nEhud Lehrer. A wide range no-regret theorem. Games and Economic Behavior,4 2\\n(1):101–115, 2003.\\nNick Littlestone. Learning quickly when irrelevant attributes abound: A new linear-\\nthreshold algorithm. Machine Learning, 2(4):285–318, 1987.\\nNick Littlestone. From on-line to batch learning. InConference on Learning Theory,\\npages 269–284, 1989.\\nNick Littlestone and Manfred K. Warmuth. The weighted majority algorithm. In\\nFoundations of Computer Science, pages 256–261, 1989.\\nNick Littlestone and Manfred K. Warmuth. The weighted majority algorithm.\\nInformation and Computation, 108(2):212–261, 1994.\\nMichael L. Littman.Algorithms for Sequential Decision Making. PhD thesis, Brown\\nUniversity, 1996.\\nPhilip M. Long and Rocco A. Servedio. Random classiﬁcation noise defeats all\\nconvex potential boosters. Machine Learning, 78:287–304, March 2010.\\nM. Lothaire. Combinatorics on Words. Cambridge University Press, 1982.\\nM. Lothaire. Mots.H e r m `es, 1990.REFERENCES 391\\nM. Lothaire. Applied Combinatorics on Words. Cambridge University Press, 2005.\\nYishay Mansour and David A. McAllester. Boosting with multi-way branching\\nin decision trees. In Advances in Neural Information Processing Systems, pages\\n300–306, 1999.\\nYishay Mansour and David A. McAllester. Generalization bounds for decision trees.\\nIn Conference on Learning Theory, pages 69–74, 2000.\\nLlew Mason, Jonathan Baxter, Peter L. Bartlett, and Marcus R. Frean. Boosting\\nalgorithms as gradient descent. In Advances in Neural Information Processing\\nSystems, pages 512–518, 1999.\\nPascal Massart. Some applications of concentration inequalities to statistics. An-\\nnales de la Facult´e des Sciences de Toulouse, IX:245–303, 2000.\\nPeter McCullagh. Regression models for ordinal data. Journal of the Royal\\nStatistical Society B, 42(2), 1980.\\nPeter McCullagh and John A. Nelder. Generalized Linear Models. Chapman &\\nHall, 1983.\\nColin McDiarmid. On the method of bounded diﬀerences. Surveys in Combina-\\ntorics, 141(1):148–188, 1989.\\nRon Meir and Gunnar R¨atsch. Advanced lectures on machine learning, machine\\nlearning summer school, canberra, australia. In Machine Learning Summer\\nSchool, pages 118–183, 2002.\\nRon Meir and Gunnar R¨atsch. An Introduction to Boosting and Leveraging, pages\\n118–183. Springer, 2003.\\nJames Mercer. Functions of positive and negative type, and their connection with\\nthe theory of integral equations. Philosophical Transactions of the Royal Society\\nof London. Series A, Containing Papers of a Mathematical or Physical Character,\\n209(441-458):415, 1909.\\nSebastian Mika, Bernhard Scholkopf, Alex J. Smola, Klaus-Robert Muller, Matthias\\nScholz, and Gunnar Ratsch. Kernel PCA and de-noising in feature spaces. In\\nAdvances in Neural Information Processing Systems, pages 536–542, 1999.\\nMarvin Minsky and Seymour Papert. Perceptrons: An Introduction to Computa-\\ntional Geometry. MIT Press, 1969.\\nMehryar Mohri. Semiring frameworks and algorithms for shortest-distance prob-\\nlems. Journal of Automata, Languages and Combinatorics, 7(3):321–350, 2002.\\nMehryar Mohri. Weighted automata algorithms. In Manfred Droste, Werner\\nKuich, and Heiko Vogler, editors, Handbook of Weighted Automata, pages 213–\\n254. Springer, 2009.\\nMehryar Mohri, Fernando Pereira, and Michael D. Riley. Weighted automata in text392 REFERENCES\\nand speech processing. European Conference on Artiﬁcial Intelligence, Workshop\\non Extended Finite State Models of Language, 2005.\\nMehryar Mohri and Afshin Rostamizadeh. Stability bounds for stationaryϕ-mixing\\nand β-mixing processes. Journal of Machine Learning, 11:789–814, 2010.\\nJorge Nocedal. Updating quasi-newton matrices with limited storage. Mathematics\\nof Computation, 35(151):773–782, 1980.\\nAlbert B.J. Novikoﬀ. On convergence proofs on perceptrons. In Proceedings of the\\nSymposium on the Mathematical Theory of Automata, volume 12, pages 615–622,\\n1962.\\nJos´e Oncina, Pedro Garc´ıa, and Enrique Vidal. Learning subsequential transducers\\nfor pattern recognition interpretation tasks. IEEE Transactions on Pattern\\nAnalysis and Machine Intelligence, 15(5):448–458, 1993.\\nKarl Pearson. On lines and planes of closest ﬁt to systems of points in space.\\nPhilosophical Magazine, 2(6):559–572, 1901.\\nFernando C. N. Pereira and Michael D. Riley. Speech recognition by composition\\nof weighted ﬁnite automata. In Finite-State Language Processing, pages 431–453.\\nMIT Press, 1997.\\nDominique Perrin. Finite automata. In J. Van Leuwen, editor, Handbook of\\nTheoretical Computer Science, Volume B: Formal Models and Semantics, pages\\n1–57. Elsevier, 1990.\\nLeonard Pitt and Manfred K. Warmuth. The minimum consistent DFA problem\\ncannot be approximated within any polynomial. Journal of the ACM , 40(1):\\n95–142, 1993.\\nJohn C. Platt. Fast training of support vector machines using sequential minimal\\noptimization. In Advances in Kernel Methods, pages 185–208. MIT Press, 1999.\\nDavid Pollard. Convergence of Stochastic Processess. Springer, 1984.\\nDavid Pollard. Asymptotics via empirical processes. Statistical Science, 4(4):341 –\\n366, 1989.\\nMartin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic\\nProgramming. John Wiley & Sons, Inc., 1994.\\nJ. Ross Quinlan. Induction of decision trees. Machine Learning, 1(1):81–106, 1986.\\nJ. Ross Quinlan. C4.5: Programs for Machine Learning. Morgan Kaufmann, 1993.\\nGunnar R¨atsch, Takashi Onoda, and Klaus-Robert M¨uller. Soft margins for Ad-\\naBoost. Machine Learning, 42:287–320, March 2001.\\nGunnar R¨atsch and Manfred K. Warmuth. Maximizing the margin with boosting.\\nIn Conference on Learning Theory, pages 334–350, 2002.\\nRyan M. Rifkin. Everything Old Is New Again: A Fresh Look at Historical Ap-REFERENCES 393\\nproaches in Machine Learning. PhD thesis, Massachusetts Institute of Technol-\\nogy, 2002.\\nRyan Rifkin and Aldebaro Klautau. In defense of one-vs-all classiﬁcation. Journal\\nof Machine Learning, 5:101–141, 2004.\\nH. Robbins and S. Monro. A stochastic approximation method. Annals of Mathe-\\nmatical Statistics, 22(3):400–407, 1951.\\nW.H. Rogers and T. J. Wagner. A ﬁnite sample distribution-free performance bound\\nfor local discrimination rules. Annals of Statistics, 6(3):506–514, 1978.\\nDana Ron, Yoram Singer, and Naftali Tishby. On the learnability and usage of\\nacyclic probabilistic ﬁnite automata. In Conference on Computational Learning\\nTheory, pages 31–40, 1995.\\nFrank Rosenblatt. The perceptron: A probabilistic model for information storage\\nand organization in the brain. Psychological Review, 65(6):386, 1958.\\nSam T. Roweis and Lawrence K. Saul. Nonlinear dimensionality reduction by locally\\nlinear embedding. Science, 290(5500):2323, 2000.\\nCynthia Rudin, Corinna Cortes, Mehryar Mohri, and Robert E. Schapire. Margin-\\nbased ranking meets boosting in the middle. In Conference on Learning Theory,\\n2005.\\nCynthia Rudin, Ingrid Daubechies, and Robert E. Schapire. The dynamics of\\nAdaBoost: Cyclic behavior and convergence of margins. Journal of Machine\\nLearning, 5:1557–1595, 2004.\\nNorbert Sauer. On the density of families of sets.Journal of Combinatorial Theory,\\nSeries A, 13(1):145–147, 1972.\\nCraig Saunders, Alexander Gammerman, and Volodya Vovk. Ridge regression\\nlearning algorithm in dual variables. In International Conference on Machine\\nLearning, volume 521, 1998.\\nRobert E. Schapire. The strength of weak learnability. Machine Learning, 5:197–\\n227, July 1990.\\nRobert E. Schapire. The boosting approach to machine learning: An overview. In\\nNonlinear Estimation and Classiﬁcation, pages 149–172. Springer, 2003.\\nRobert E. Schapire, Yoav Freund, Peter Bartlett, and Wee Sun Lee. Boosting\\nthe margin: A new explanation for the eﬀectiveness of voting methods. In\\nInternational Conference on Machine Learning, pages 322–330, 1997.\\nRobert E. Schapire and Yoram Singer. Improved boosting algorithms using\\nconﬁdence-rated predictions. Machine Learning, 37(3):297–336, 1999.\\nRobert E. Schapire and Yoram Singer. Boostexter: A boosting-based system for\\ntext categorization. Machine Learning, 39(2-3):135–168, 2000.394 REFERENCES\\nLeopold Schmetterer. Stochastic approximation. In Proceedings of the Fourth\\nBerkeley Symposium on Mathematical Statistics and Probability, pages 587–609,\\n1960.\\nIsaac J. Schoenberg. Metric spaces and positive deﬁnite functions. Transactions of\\nthe American Mathematical Society, 44(3):522–536, 1938.\\nBernhard Sch¨olkopf, Ralf Herbrich, Alex J. Smola, and Robert Williamson. A\\ngeneralized representer theorem. Technical Report 2000-81, Neuro-COLT, 2000.\\nBernhard Sch¨olkopf and Alex Smola. Learning with Kernels. MIT Press, 2002.\\nShai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learn-\\nability and stability in the general learning setting. In Conference on Learning\\nTheory, 2009.\\nJohn Shawe-Taylor, Peter L. Bartlett, Robert C. Williamson, and Martin Anthony.\\nStructural risk minimization over data-dependent hierarchies.IEEE Transactions\\non Information Theory, 44(5):1926–1940, 1998.\\nJohn Shawe-Taylor and Nello Cristianini. Kernel Methods for Pattern Analysis .\\nCambridge University Press, 2004.\\nSaharon Shelah. A combinatorial problem; stability and order for models and\\ntheories in inﬁnitary languages. Paciﬁc Journal of Mathematics , 41(1), 1972.\\nSatinder P. Singh. Learning to Solve Markovian Decision Processes.P h D t h e s i s ,\\nUniversity of Massachusetts, 1993.\\nSatinder P. Singh and Dimitri Bertsekas. Reinforcement learning for dynamic chan-\\nnel allocation in cellular telephone systems. In Advances in Neural Information\\nProcessing Systems, pages 974–980. MIT Press, 1997.\\nMaurice Sion. On general minimax theorems. Paciﬁc Journal of Mathematics ,8\\n(1):171–176, 1958.\\nEric V. Slud. Distribution inequalities for the binomial law. Annals of Probability,\\n5(3):404–412, 1977.\\nGilles Stoltz and G´abor Lugosi. Internal regret in on-line portfolio selection. In\\nConference on Learning Theory, pages 403–417, 2003.\\nRich Sutton. Temporal Credit Assignment in Reinforcement Learning.P h Dt h e s i s ,\\nUniversity of Massachusetts, 1984.\\nRichard S. Sutton and Andrew G. Barto.Reinforcement Learning : An Introduction.\\nMIT Press, 1998.\\nS.J. Szarek. On the best constants in the Khintchin inequality.Studia Math, 58(2):\\n197–208, 1976.\\nCsaba Szepesv´ari. Algorithms for Reinforcement Learning. Synthesis Lectures on\\nArtiﬁcial Intelligence and Machine Learning. Morgan & Claypool, 2010.REFERENCES 395\\nEiji Takimoto and Manfred K. Warmuth. Path kernels and multiplicative updates.\\nIn Conference on Learning Theory, pages 74–89, 2002.\\nBenjamin Taskar, Carlos Guestrin, and Daphne Koller. Max-margin Markov net-\\nworks. In Advances in Neural Information Processing Systems, 2003.\\nRobert F. Tate. On a double inequality of the normal distribution. The Annals of\\nMathematical Statistics, 1:132–134, 1953.\\nJoshua Tenenbaum, Vin de Silva, and John C. Langford. A global geometric\\nframework for nonlinear dimensionality reduction.Science, 290(5500):2319–2323,\\n2000.\\nGerald Tesauro. Temporal diﬀerence learning and TD-gammon. Communications\\nof the ACM, 38:58–68, March 1995.\\nRobert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the\\nRoyal Statistical Society. Series B, 58(1):267–288, 1996.\\nB. Tomaszewski. Two remarks on the Khintchine-Kahane inequality. InColloquium\\nMathematicum, volume 46, 1982.\\nBoris Trakhtenbrot and Janis M. Barzdin. Finite Automata: Behavior and Synthe-\\nsis. North-Holland, 1973.\\nJohn N. Tsitsiklis. Asynchronous stochastic approximation and q-learning. In\\nMachine Learning, volume 16, pages 185–202, 1994.\\nIoannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin Al-\\ntun. Large margin methods for structured and interdependent output variables.\\nJournal of Machine Learning, 6:1453–1484, 2005.\\nLeslie G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):\\n1134–1142, 1984.\\nVladimir N. Vapnik. Statistical Learning Theory. Wiley-Interscience, 1998.\\nVladimir N. Vapnik. The Nature of Statistical Learning Theory. Springer-Verlag,\\n2000.\\nVladimir N. Vapnik.Estimation of Dependences Based on Empirical Data.S p r i n g e r -\\nVerlag, 2006.\\nVladimir N. Vapnik and Alexey Chervonenkis. A note on one class of perceptrons.\\nAutomation and Remote Control, 25, 1964.\\nVladimir N. Vapnik and Alexey Chervonenkis. On the uniform convergence of\\nrelative frequencies of events to their probabilities. Theory of Probability and Its\\nApplications, 16:264, 1971.\\nVladimir N. Vapnik and Alexey Chervonenkis. Theory of Pattern Recognition .\\nNauka, 1974.\\nSantosh S. Vempala. The random projection method. In DIMACS Series in396 REFERENCES\\nDiscrete Mathematics and Theoretical Computer Science, volume 65. American\\nMathematical Society, 2004.\\nMathukumalli Vidyasagar. A Theory of Learning and Generalization: With Appli-\\ncations to Neural Networks and Control Systems. Springer-Verlag, 1997.\\nSethu Vijayakumar and Si Wu. Sequential support vector classiﬁers and regression.\\nInternational Conference on Soft Computing, 1999.\\nJohn von Neumann. Zur Theorie der Gesellschaftsspiele. Mathematische Annalen,\\n100(1):295–320, 1928.\\nVladimir G. Vovk. Aggregating strategies. InConference on Learning Theory, pages\\n371–386, 1990.\\nGrace Wahba. Spline Models for Observational Data , volume 59 of CBMS-NSF\\nRegional Conference Series in Applied Mathematics. Society for Industrial and\\nApplied Mathematics, 1990.\\nC h r i s t o p h e rJ .C .H .W a t k i n s .Learning from Delayed Rewards .P h D t h e s i s ,\\nCambridge University, 1989.\\nChristopher J. C. H. Watkins. Dynamic alignment kernels. Technical Report CSD-\\nTR-98-11, Royal Holloway, University of London, 1999.\\nChristopher J. C. H. Watkins and Peter Dayan. Q-learning. Machine Learning,8\\n(3-4):279–292, 1992.\\nKilian Q. Weinberger and Lawrence K. Saul. An introduction to nonlinear dimen-\\nsionality reduction by maximum variance unfolding. In Conference on Artiﬁcial\\nIntelligence, 2006.\\nJason Weston and Chris Watkins. Support vector machines for multi-class pattern\\nrecognition. European Symposium on Artiﬁcial Neural Networks, 4(6), 1999.\\nBernard Widrow and Marcian E. Hoﬀ. Adaptive switching circuits. Neurocomput-\\ning: Foundations of Research, 1988.\\nHuan Xu, Shie Mannor, and Constantine Caramanis. Sparse algorithms are not\\nstable: A no-free-lunch theorem. In Conference on Communication, Control, and\\nComputing, pages 1299–1303, 2008.\\nYinyu Ye. The simplex and policy-iteration methods are strongly polynomial for the\\nmarkov decision problem with a ﬁxed discount rate. Mathematics of Operations\\nResearch, 36(4):593–603, 2011.\\nMartin Zinkevich. Online convex programming and generalized inﬁnitesimal gra-\\ndient ascent. In International Conference on Machine Learning, pages 928–936,\\n2003.Index\\nβ-stable, see stable\\nϵ-transition, 111, 295\\nγ-fat-dimension, see fat-shattering di-\\nmension\\nγ-shattered, see fat-shattered\\nσ-admissible, 271\\nσ-algebra, 359\\nk-CNF formula, 20, 21\\nk-term DNF formula, 20, 21\\naccess string, 298–302\\naccuracy, 13, 19, 29, 52, 124–126, 130,\\n140, 142, 214, 254\\npairwise ranking, 215, 225\\naction, 8, 138, 153, 154, 175, 313–315,\\n317–322, 325, 326, 330, 332–\\n334, 336\\ncolumn, 138, 139\\ngreedy, 333, 334\\npolicy, see policy\\nrandom, 334\\nrow, 138, 139\\nspace, 337\\nAdaBoost, 121–132, 134–146, 169, 192,\\n193, 206, 209, 214, 218, 220,\\n222–224, 233, 234\\nAdaBoost.MH, 192, 193, 206, 207\\nAdaBoost.MR, 192, 206, 208\\nadaptive boosting, see AdaBoost\\nadversarial, 7, 152, 153, 174, 309\\nargument, 150\\nassumption, 148\\nchoice, 229\\nscenario, 147\\nalgebraic transduction, 111, see also\\ncontext-free grammar\\nalgorithm, 1, 4\\naggregated, 183, 190\\ndeterministic, 147, 152, 153, 156,\\n209, 227–230, 233\\nlearning, 1–5, 21, 28, 29, 48, 49, 51,\\n52, 58, 59\\noﬀ-policy, 334\\non-policy, 334\\nrandomized, 147, 153, 154, 156, 179,\\n209, 227, 228, 230, 234\\nrobust, 2\\nuncombined, 183, 190, 199, 206\\nalgorithmic stability, see stability\\narea under the curve, see AUC\\narea under the ROC curve, see AUC\\nassumption\\nstochastic, 295, 296\\nAUC, 209, 224–226, 232, 233, 235\\nautomaton\\nk-deterministic, 311\\nk-reversible, 311, 312\\nacyclic, 295, 311\\nprobabilistic, 310\\ndeterministic, 294–296, 304, 305,\\n308, 311, see also DFA\\nﬁnite, 294\\nlearning with queries, 298, 300, 303,\\nsee also QueryLearnAutomata\\nalgorithm\\nminimization, 295398 INDEX\\nnon-deterministic, 295,see alsoNFA\\npreﬁx-tree, 304, 305, 307, 308\\nquotient, 303\\nreverse, 304, 305\\nreverse determinism, 304\\nreverse deterministic, 308\\nreversible, 304, 305, 307–309\\nlearning, 306, 312,see also Learn-\\nReversibleAutomata algorithm\\nAzuma’s inequality, 172, 371–373, 376\\nbase\\nclassiﬁer, see classiﬁer\\nrankers, 214, 216, 218\\nBayes\\nclassiﬁer, 25, 52, 118\\nerror, 25, 26, 229\\nformula, 362\\nhypothesis, 25\\nBellman equations, 317–322, 324, 330\\nBennett’s inequality, 371, 377, 378\\nBernstein’s inequality, 378\\nb i a s ,6 ,5 2\\nbigram, 113\\ngappy, 113, 114\\nkernel, 113\\ngappy, 113\\nBipartiteRankBoost, 223, see also Rank-\\nBoost\\nboosting, 8, 121, 122, 132, 136, 138, 140–\\n143, 191, 192, 194, 206, 220\\nby ﬁltering, 141\\nby majority, 141\\nmulti-class, 8, 183, 191, 192, 207,\\nsee also AdaBoost.MH, see also\\nAdaBoost.MR\\nranking, 209, 214, see also Rank-\\nBoost\\nround, 122–124, 130, 131, 134, 140,\\n141, 143–145, 215, 216, 220\\nstump, 129, 130, 144, 193, 207\\ntrees, 263\\nBregman divergence, 142, 271, 272\\ngeneralized, 272, 273\\ncalibration problem, 199, 200, 206\\nCauchy-Schwarz inequality, 77, 96, 102,\\n162, 180, 190, 273, 275, 342,\\n343, 367\\ncentral limit theorem, 367\\nchain rule, 362\\nChebyshev’s inequality, 365, 366, 377\\nChernoﬀ\\nbound, 50\\nbounding technique, 369, 370, 372,\\n378\\nCholesky decomposition, 99, 346\\npartial, 251, 256\\nclassiﬁcation, 2, 124, 229\\nbinary, 11, 38\\ndocument, 1\\nimage, 118\\nlinear, 63\\non-line, 159\\nmulti-class, 8, 183\\non-line, 147\\nstability, 9, 276\\ntext, 2\\ntwo-group, 87\\nXOR, 93\\nclassiﬁer, see also classiﬁcation\\naccuracy, 126\\nbase, 121, 122, 124–126, 128, 129,\\n131, 132, 134, 136, 138, 140,\\n143, 192, 193, 216\\nBayes, 25\\nbinary, 63\\nedge, 125, 136–140, 216\\nerror, 6\\nhyperplane, 42\\nlinear, 63, 159\\nmargin, 131INDEX 399\\nmulti-class, 183\\nclique, 204\\nclustering, 2, 101, 194\\nalgorithm, 2\\nco-accessible, 109\\ncode\\nbinary, 202\\ncontinuous, 202\\ndiscrete, 202\\nerror-correction, 201, 202\\nmatrix, 202\\nternary, 202\\nword, 201, 202\\ncomplementarity conditions, 67, 73, 74,\\n253, 357\\nconcave, 351\\nfunction, 68, 74, 102, 176, 249, 350–\\n353, 355\\nproblem, 355\\nconcavity, 343, 352\\nconcentration inequalities, 369–372\\nconcept, 11\\nclass, 1, 3, 11, 13, 14, 16, 18–21, 29–\\n31, 33, 57, 59, 121, 149, 295, 297\\nuniversal, 19\\ncondition\\nKKT, 66, 73, 191, 249, 253, 255, 357\\nMercer’s, 91, 120\\nSlater’s, 355, 356, see also con-\\nstraint qualiﬁcation\\nweak, 355\\nConditional Random Fields, see CRFs\\nconﬁdence, 13, 19, 32, 59, 78, 124, 132,\\n185, 211\\ninterval, 233\\nscore, 199, 201, 202\\nconjugate, 132, 171, 342, 343\\nconsistent, 3, 5, 11, 296\\nalgorithm, 17–19, 32, 58\\ncase, 17, 23\\nDFA, see DFA learning minimum\\nconsistent\\nexpert, 149\\nhypothesis, 17–19, 59, 144\\nlearner, 5\\nNFA, 309\\npairwise, 227\\nconstraint\\nL\\n1-, 259\\naﬃne, 66, 68, 72, 73, 191, 253, 260,\\n354\\ndiﬀerentiable, 66, 73, 191, 248\\nequality, 354\\nqualiﬁcation\\nstrong, 355\\nweak, 355\\ncontext-free\\ngrammar, 293, 297\\nlanguage, 111, 293\\nconvex, 72, 83, 126, 161, 218, 257, 352,\\n353\\nd-gon, 44, 45\\ncombination, 132–134, 192\\nconstraint, 68, 72, 73\\ndomain, 351\\nfunction, 51, 66, 72, 73, 126, 128,\\n143, 144, 157, 159, 172, 179,\\n191, 192, 196, 205, 207, 208,\\n218, 219, 224, 246, 248, 256,\\n271–273, 349–354, 356\\nhull, 42–44, 132, 220, 350\\nintersection, 57\\nloss, 128, 147, 153, 156, 157, 159,\\n172, 175, 181, 219, 256, 271–\\n273, 277\\noptimization, 9, 65, 66, 68, 72, 84,\\n94, 191, 248, 257, 349, 350, 353–\\n357\\npolygon, 45\\npotential, 141, 142\\nQP, 66, 74, 253, 255400 INDEX\\nregion, 195\\nset, 350–353\\nstrictly, 66, 351\\nupper bound, 72, 73, 126, 128, 218\\nconvexity, 36, 53, 72, 91, 158, 161, 173,\\n180, 181, 207, 218, 248, 352–\\n354, 357, 369, 374\\ncovariance, 366, 367\\nmatrix, 282, 283, 287, 290, 367\\ncovering, 61\\nnumbers, 55, 61, 233\\nCRFs, 205, 207\\ncross-validation, 140, 256\\nn-fold, 5, 6, 28, 72, 87, 198\\nerror, 5, 6, 86\\nleave-one-out, 6\\ndata\\nset, 2\\ntest, 4\\ntraining, 3\\nunseen, 3\\nvalidation, 3\\nDCG, 233, 234\\nnormalized, 233\\ndecision epoch, 315, 330, 332, 334\\ndecision stump, 130, 140, 141\\ndecision trees, 129, 130, 141, 183, 191,\\n194, 195, 197, 198, 206, 208,\\n263, 299, 300, 302, 310\\nbinary, 150, 194\\nbinary space partition trees, 195\\nclassiﬁcation, 299\\nlearning, 195, 197, 206, see also\\nGreedyDecisionTrees algorithm\\nnode, 194\\nquestion, 194–196, 299\\ncategorical, 194\\nnumerical, 194\\nsphere trees, 195\\nstump, see also boosting stump, see\\ndecision stump\\nDFA, 295, 296, 298–300, 302–304, 309–\\n311\\nacyclic, 295\\nconsistent, 296\\nequivalent, 295\\nlearning, 303, 309\\nminimum consistent, 296\\nlearning with queries, 298, 303\\nminimal, 295, 296, 298, 310\\nminimization, 296\\nreverse, 304\\nVC-dimension, 311\\ndichotomy, 41–46\\ndiﬀerentiable\\nfunction, 349, 351, 352, 356\\nupper bound, 126, 128\\ndimensionality reduction, 2, 7, 101, 281,\\n285, 288, 290\\ndiscounted cumulative gain, see DCG\\ndistribution, 359, 360\\nχ\\n2-squared, 288, 289, 361\\nabsolutely continuous, 360\\nbinomial, 360\\nchi-squared, 361\\ndensity function, 360\\nGaussian, 360\\nLaplace, 361\\nnormal, 360\\nPoisson, 361\\nprobability, 359\\ndistribution-free model, 13\\nDNF formula, 20, 311\\ndisjoint, 310\\ndoubling trick, 155, 158, 174, 175\\ndual, 251\\nfunction, 354\\nnorm, 342\\noptimization, 66–68, 74, 75, 83, 84,\\n100, 191, 207, 249, 255, 264, 355INDEX 401\\nproblem, 355\\nSVM, 164\\nSVR, 262\\nvariables, 67, 70, 74, 264, 354\\nduality\\ngap, 355\\nstrong, 68, 355\\nweak, 355\\nDualPerceptron, 167, 168\\nearly stopping, 141\\nedge, see classiﬁer edge\\nemphasis function, 231, 232, 235\\nempirical kernel map, see kernel\\nempirical risk minimization, 26, 27, 38\\nensemble\\nalgorithms, 121\\nhypotheses, 133, 220\\nmargin bound, 133\\nmethods, 121, 122, 220\\nranking, 220\\nenvelope, 262\\nenvironment, 1, 8, 313, 314, 326, 336\\nMDP, 315\\nmodel, 313, 314, 319, 325, 326, 330\\nunknown, 336\\nErd¨os, 48\\nERM, see empirical risk minimization\\nerror, 12, see also risk\\napproximation, 26\\nBayes, 25\\ncross-validation, 5\\nempirical, 8, 12, 184, 380\\nestimation, 26\\ngeneralization, 8, 12, 380\\nleave-one-out, 69\\nmean squared, 238\\nreconstruction, 282\\ntest, 5\\ntraining, 5\\ntrue, 12\\nevent, 30, 118, 119, 359, 361, 362\\nelementary, 359\\nindependent, 361\\nindicator, 12\\nmutually disjoint, 362\\nmutually exclusive, 359\\nset, 359\\nexamples, 3, 11\\ni.i.d., 12\\nincorrectly labeled, 141\\nlabeled, 4\\nmisclassiﬁed, 144\\nnegative, 29\\npositive, 19, 303\\nunlabeled, 7\\nexpectation, 363\\nlinearity, 363\\nexperience, 1, 336\\nexpert, 32, 148–154, 156, 157, 168, 169,\\n171, 174, 175, 179\\nactive, 149\\nadvice, 32, 147, 148\\nalgorithm, 175\\nbest, 148, 151, 152, 175\\nexploitation, 8, 314\\nexploration, 8, 314\\nBoltzmann, 334\\nexploitation dilemma, 8, 314\\nExponential-Weighted-Average algorithm,\\n8, 156, 157, 173, 174\\nfalse negative, 14\\nfalse positive, 14\\nerror, 87\\nrate, 225, 226\\nfat-shattered, 244\\nfat-shattering, 262\\ndimension, 244, 245\\nfeature, 3\\nextraction, 281402 INDEX\\nmapping, 96–98, 102, 117, 167, 189,\\n190, 214, 247, 252, 254, 255,\\n281, 284\\nmissing, 198\\npoor, 96\\nrelevant, 3, 4, 118, 204\\nspace, 76, 82, 83, 90, 91, 96, 117,\\n118, 140, 194, 213, 246, 247,\\n251, 310, 379\\nuncorrelated, 96\\nvector, 4\\nFermat’s theorem, 349\\nﬁnal\\nstate, 107–109, 294, 295, 299–301,\\n304–308, 312, 330\\nweight, 107, 108, 110, 114\\nﬁxed point, 199, 321, 326, 327, 329, 333\\nFrobenius\\nnorm, 283, 345, 380\\nproduct, 345\\nFubini’s theorem, 49, 363\\nfunction\\naﬃne, 66, 246, 355\\nconcave, see concave function\\ncontinuous, 91, 96, 120\\ncontracting, 320, 321\\nconvex, see convex function\\ndiﬀerentiable, 192, 349, 351, 352,\\n356\\nﬁnal weight, 107\\nkernel, 120\\nLipschitz, 78, 80, 96, 186, 188, 212,\\n240, 254, 255, 271, 274, 276,\\n320, 321\\nmaximum, 352\\nmeasurable, see measurable func-\\ntion\\nmoment-generating, 288, 364, 365,\\n370\\nquasi-concave, 176\\nsemi-continuous, 176\\nstate-action value, 318, 326, 331,\\n332\\nsupremum, 36\\nsymmetric, 91\\ngame, 138\\ntheory, 121, 137, 139, 142, 147, 176,\\n339\\nvalue, 139\\nzero-sum, 138, 139, 174\\ngap penalty, 113\\ngeneralization, 5\\nbound, 16, 17, 22, 23, 26, 33, 35, 37,\\n38, 40, 48, 54, 55, 59–61, 75,\\n77–80, 103, 132–134, 183, 185,\\n187, 190, 197, 206, 208, 211,\\n213, 237, 239–242, 244, 247,\\n251, 254, 255, 259, 262, 264,\\n267, 276–278, see also margin\\nbound, see alsostability bound,\\nsee also VC-dimension bound\\nerror, 8, 12, 13, 18, 21, 22, 24–26,\\n29, 48, 61, 63, 69, 70, 82, 118,\\n131, 136, 144, 148, 172, 174,\\n184, 187, 200, 208, 210, 212,\\n213, 221, 238, 268, 270, 276\\ngradient, 66, 73, 224, 349\\ndescent, 337,see also stochastic gra-\\ndient descent\\nGram matrix, 68, 92, 116,see also kernel\\nmatrix\\ngraph, 204, 287\\nacyclic, 111\\nLaplacian, 286, 291\\nneighborhood, 287\\nstructure, 205\\nGreedyDecisionTrees algorithm, 195\\ngrowth function, 33, 38–41, 45, 47, 56\\ngeneralization bound, 40\\nlower bound, 56\\nH¨older’s inequality, 180, 259, 342INDEX 403\\nHalving algorithm, 148–150, 152\\nHamming distance, 184, 201, 202, 204,\\n375\\nHessian, 66, 68, 180, 349, 351\\nHilbert space, 89, 91, 94–97, 103, 105,\\n116, 117, 119, 342, 376\\npre-, 96\\nreproducing kernel, 95, 96, 115, 270\\nhinge loss, 72, 73, 82, 83, 177, 276\\nquadratic, 72, 73, 278\\nHoeﬀding’s inequality, 21, 39, 61, 158,\\n170, 173, 235, 238, 239, 369–\\n371, 373, 377, 378\\nhorizon, 158, 315\\nﬁnite, 315, 316\\ninﬁnite, 316, 317\\ndiscounted, 316\\nundiscounted, 316\\nhyperplane, 42, 63\\ncanonical, 65\\nVC-dimension, 76\\nequation, 64\\nmarginal, 65\\nmaximum-margin, 64\\nminimal error, 84\\noptimal, 83\\npseudo-dimension, 242\\nsoft-margin, 84\\ntangent, 271\\nVC-dimension, 42\\nhypothesis, 4\\nBayes, 25\\nbest-in-class, 26\\nconsistent, 17\\nlinear, 63\\nset, 4, 12\\nﬁnite, 8, 11\\ninﬁnite, 8, 33\\nsingle, 22\\ni.i.d., 361\\nidentiﬁcation in the limit, see language\\nidentiﬁcation in the limit\\nimpurity, 196, 197\\nentropy, 196\\nGini index, 196\\nmean squared error, 198\\nmisclassiﬁcation, 196\\ninconsistent, 11\\ncase, 21, 239\\nhypothesis, 21\\nindependence, see random variable inde-\\npendence\\npairwise on irrelevant alternatives,\\n228\\ninequality\\nAzuma’s, 172, 371–373, 376\\nBennett’s, 371, 377, 378\\nBernstein’s, 371, 377, 378\\nCauchy-Schwarz, 77, 94, 96, 102,\\n162, 180, 190, 273, 275, 342,\\n343, 367\\nChebyshev’s, 365, 366, 377\\nconcentration, see concentration in-\\nequalities\\nH¨older’s, 180, 259, 342\\nHoeﬀding’s, 21, 39, 61, 158, 170,\\n173, 235, 238, 239, 369–371,\\n373, 377, 378\\nJensen’s, 36, 39, 53, 76, 77, 102, 158,\\n190, 353, 374\\nKhintchine-Kahane, 103, 156, 374,\\n376\\nMarkov’s, 288, 363, 366, 369\\nMcDiarmid’s, 33, 35, 36, 117, 269,\\n371–373, 376\\nPinsker’s, 279\\nYoung’s, 343\\ninference\\nautomata, 303, 307\\ntransductive, 7\\ninput space, 11404 INDEX\\ninstances, 3, 11\\nsparse, 177\\nweighted, 143\\ninteraction, 1, 313, 314\\nIsomap, 285, 286, 290\\nJensen’s inequality, 36, 39, 53, 76, 77,\\n102, 158, 190, 353, 374\\nJohnson-Lindenstrauss lemma, 288–290\\nKarush-Kuhn-Tucker conditions\\nsee KKT conditions, 356\\nkernel, 89, 90\\nbigram, 113\\ngappy, 113\\ncontinuous, 115\\nconvolution, 115\\ndiﬀerence, 116\\nempirical map, 96–98, 260\\nfunctions, 89, 90\\nGaussian, 94\\nmatrix, 92\\nmethods, 89, 90\\nn-gram, 120\\nnegative deﬁnite symmetric, 89, 103\\nnormalized, 97\\npolynomial, 92, 117\\npositive deﬁnite symmetric, 8, 89,\\n91, 92\\nclosure properties, 99\\npositive semideﬁnite, 92\\nrational, 8, 83, 89, 106, 111, 113,\\n115, 119, 310\\nPDS, 112–115\\nridge regression, see KRR\\nsequence, 106, 112, see also kernel\\nrational\\nsigmoid, 94\\nstring, 115\\ntensor product, 99\\nKernelPerceptron, see Perceptron algo-\\nrithm kernel\\nKhintchine-Kahane inequality, 103, 156,\\n374, 376\\nKKT conditions, 66, 73, 191, 249, 253,\\n255, 356, 357\\nKPCA, see PCA kernel\\nKullback-Leibler divergence, 279\\nlabels, 3, 8, 11, 25, 31, 42\\ncategories, 3\\nreal-valued, 3\\ntarget, 96\\ntrue, 5\\nvalues, 3\\nLagrange, 357\\nfunction, 354, see also Lagrangian\\nmultipliers, 85, 86\\nvariables, 66, 73, 74, 354\\nLagrangian, 66, 67, 73, 74, 191, 248, 253,\\n255, 354–357\\nlanguage\\nk-reversible, 310–312\\naccepted, 295, 296, 304, 307\\ncomplement, 110\\ncontext-free, see context-free lan-\\nguage\\nformal, 339\\nidentiﬁcation in the limit, 294, 303,\\n308, 310\\nlearning, 9, 293, 294, 303\\nlinearly separable, 115\\npositive presentation, 308\\nregular, 293, 295, 310\\nreverse, 304\\nreversible, 304, 305, 308–310\\nlearning, 311\\nLaplacian eigenmaps, 285–288, 290, 291\\nLasso, 9, 237, 245, 257–260, 266, 277\\ngroup, 261\\non-line, see OnLineLasso algorithm\\nlaw of large numbers\\nstrong, 326, 327INDEX 405\\nweak, 366\\nlearner, 7\\nactive, 296, 313\\nbase, 123, 127, 130, 136, 139, 143,\\n144, 191\\nconsistent, 5\\npassive, 313\\nstrong, 122\\nweak, 121, 129, 130, 136, 141, 143,\\n194, 206, 214\\nlearning, 115, 313\\nactive, 8\\nexact, 294, 295\\non-line, 7\\npolicy, 334\\nproblem, 314\\nrandomized, 153\\nreinforcement, 8\\nsemi-supervised, 7\\nsupervised, 7\\ntransductive, 7\\nunsupervised, 7\\nwith queries, 297\\nlearning bound,see generalization bound\\nconsistent case, 17\\nﬁnite hypothesis set, 17, 23\\ninconsistent case, 23\\nLearnReversibleAutomata algorithm, 303,\\n304, 306–310\\nlemma\\ncontraction, see Talagrand’s lemma\\nHoeﬀding’s, 369\\nJohnson-Lindenstrauss, 288–290\\nMassart’s, 39, 40, 54, 56, 258\\nSauer’s, 45–48, 55, 56, 58\\nTalagrand’s, 56, 78, 186, 240, 254\\nlinearly separable, 70, 71, 77, 83, 90,\\n93, 115, 118, 140, 162–164, 166,\\n167, 224,see also realizable set-\\nting\\nLipschitz\\nfunction, see function Lipschitz\\nproperty, 79, 321\\nLLE, 287, 288, 290, 292\\nlocally linear embedding, see LLE\\nlogistic regression, 128, 129, 141, 142\\nloss\\nϵ-insensitive, 252\\nquadratic, 255\\nσ-admissible, 271\\naverage, 172\\nbinary, see loss, zero-one\\nbounded, 171\\nconvex, 128\\nconvex upper bound, 126, 128\\ncumulative, 148\\nexpected, 139\\nexponential, 126\\nfunction, 4, 34, 238\\nHamming, 204\\nhinge, see hinge loss\\nHuber, 256\\nlogistic, 128\\nmargin, 77, 185\\nempirical, 78\\nmatrix, 138\\nmisclassiﬁcation, 4\\nmulti-label, 192\\nnon-convex, 181\\nnon-diﬀerentiable, 277\\npairwise ranking, 213\\nexponential, 218\\nranking\\ndisagreement, 227\\ntop k, 232\\nsquared, 4, 148, 238\\nunbounded, 238\\nzero-one, 4, 37, 148, 154\\npairwise misranking, 218\\nM\\n3N, 205, 207406 INDEX\\nmanifold learning, 2, 281, 284, 285, 290,\\nsee also dimensionality reduc-\\ntion\\nmargin, 63, 64, 75, 162, 185\\nL1-, 131, 132\\nbound, 8, 80\\ngeometric, 75\\nhard, 71\\nloss, 77, 78, 185\\nempirical, 78\\nmaximum-, 64, 65, 136, 137, 140,\\n177, 233\\nmulti-class, 185\\npairwise ranking, 211\\nsoft, 71, 84, 141, 142\\ntheory, 8, 64, 75, 83, 121, 137\\nmargin bound\\nbinary classiﬁcation, 80\\ncovering numbers, 233\\nensemble\\nRademacher complexity, 133\\nranking, 220\\nVC-Dimension, 133\\nkernel-based hypotheses, 103\\nmulti-class classiﬁcation, 187, 190\\nranking, 212, 234\\nkernel-based hypotheses, 213\\nMarginPerceptron, 177, 178\\nMarkov decision process, see MDP\\nMarkov’s inequality, 288, 363, 366, 369\\nmartingale diﬀerences, 371, 373, 376\\nMassart’s lemma, 39, 40, 54, 56, 258\\nmatrix, 344\\nGram, 68\\nidentity, 66\\nkernel, 92\\nloss, 138\\nmultiplication, 108\\nnorm\\ninduced, 344\\npositive semideﬁnite, 346\\ntrace, 103, 344, 346\\ntranspose, 344\\nupper triangular, 346\\nmaximum likelihood, 129\\nMaximum-Margin Markov Networks,see\\nM\\n3N\\nMcDiarmid’s inequality, 33, 35, 36, 117,\\n269, 371–373, 376\\nMDP, 313, 314\\nenvironment, 315\\nﬁnite, 315\\npartially observable, 336\\nmean, 363, 366, 367, 369, 373, 377\\nestimation, 326\\nzero-, 360, 364, 378\\nmeasurable, 12, 34, 359\\nfunction, 25, 118, 243, 353\\nsubset, 237\\nMercer’s\\ncondition\\nsee condition Mercer’s, 396\\ntheorem, 91\\nmetric space, 320\\ncomplete, 320, 321\\nmirror image, 304\\nmistake, 149–152, 171, 177\\nbound, 8, 149–151, 161, 166, 169,\\n171, 176\\ncumulative, 153\\nmodel, 148, 171\\nrate, 150\\nmodel\\nbased approach, 326\\ncontinuous-time, 315\\ndiscrete-time, 315\\ndistribution-free, 13\\nfree approach, 326\\nselection, 5, 6, 27\\nmoment-generating function, 288, 364,\\n365, 370\\nmono-label case, 183–185, 207INDEX 407\\nmulti-label\\ncase, 183, 184, 192, 207\\nerror, 207\\nloss, 192\\nn-way composition, 113, 115\\nNDCG, see DCG normalized\\nNDS kernel, see kernel negative-deﬁnite\\nsymmetric\\nNFA, 295, 309\\nconsistent, 309\\nnode impurity, see impurity\\nnoise, 25, 26, 30, 54, 140–142, 144\\nassumption, 26\\naverage, 25, 26\\nlearning in presence of, 30\\nmodel, 31\\nrandom, 34, 141, 142, 328\\nrate, 30, 31\\nsource, 198\\nnon-convex\\nloss, 181\\nnon-diﬀerentiable loss, 271, 277\\nnon-realizable case, 11, 33, 50, 51, 54, 55,\\n150\\nnorm, 341\\nequivalent, 341\\nFrobenius, 345\\ngroup, 189, 261, 345\\nmatrix, see matrix norm\\nspectral, 344\\nvector, see vector norm\\nOccam’s razor principle, 24, 29, 48, 63,\\n239, 296\\non-line learning, 147\\non-line to batch conversion, 147, 171,\\n176, 181\\nOn-line-SVM algorithm, 177\\none-versus-all, 8, 198–202, 206\\none-versus-one, 8, 199–202, 208\\none-versus-rest, see one-versus-all\\nOnLineDualSVR algorithm, 262\\nOnLineLasso algorithm, 262, 265, 266\\noperator norm, 344\\noptimization\\nconstrained, 354\\ndual, 355\\nprimal, 354\\noutlier, 71, 72, 74, 141\\nOVA, see one-versus-all\\nOVO, see one-versus-one\\nPAC-learning, 8, 11, 13, 14, 16, 18–21,\\n26, 28–33, 54, 59, 121, 147\\nagnostic, 24, 25, 50\\nalgorithm, 13, 14, 18, 32, 58\\neﬃciently, 13\\nmodel, 11, 13, 14, 20, 24, 28, 29\\nweakly, 121\\nwith membership queries, 297\\npacking numbers, 55\\npairwise consistent, 227\\nparadigm\\nstate-partitioning, 303\\nstate-splitting, 303\\nparse tree, 106\\npartially observable Markov decision\\nprocess, see POMDP\\npath, 107–111, 114, 115, 161, 175, 294,\\n295\\nϵ-, 109, 110, 115\\naccepting, 107, 108, 111, 112, 114,\\n294, 295, 305\\nlabel, 107\\nmatching, 109\\nredundant, 109\\nshortest- problem\\non-line, 175\\nsuccessful, see accepting\\nPCA, 9, 281\\nkernel, 9, 281, 283–288, 290, 292408 INDEX\\nPDS kernel, see kernel positive-deﬁnite\\nsymmetric\\nPerceptron algorithm, 8, 84, 147, 159–\\n163, 166–169, 171, 176–178, 234\\ndual, 167, 168\\nkernel, 168, 176, 181\\nmargin, see MarginPerceptron\\nranking, see RankPerceptron\\nupdate, 177\\nvoted, 163, 168\\nPinsker’s inequality, 279\\npivot, 230\\nplanning, 9\\nalgorithm, 319\\nproblem, 313, 314, 319\\npolicy, 313–315, 322, 326\\nϵ-greedy, 333\\niteration, 319, 322–324, 337,see also\\nPolicyIteration algorithm\\nlearning, 334\\nnon-stationary, 316\\nstationary, 315\\nvalue, 313, 316\\nPolicyIteration algorithm, 323\\nPolynomial-Weighted-Average algorithm,\\n179\\nPOMDP, 336\\npositive semideﬁnite, 92, 346\\npotential function, 151, 152, 154, 157,\\n170, 179, 180\\nprecision, 232\\naverage, 232\\npreference\\n-based\\nranking, 9\\nsetting, 209, 210, 226, 227, 233\\nfunction, 210, 211, 226–230\\npreﬁx, 114, 294, 301, 304, 308\\nprincipal component analysis, see PCA\\nprior knowledge, 4, 96, 98\\nprobabilistic method, 48, 55, 288\\nprobability, 359\\nconditional, 361\\ndistribution, 359\\njoint mass function, 359\\nmass function, 359\\ntheorem of total, 362\\nprobably approximately correct,see PAC\\npseudo-dimension, 237, 239, 242–245,\\n262\\npseudo-inverse, 98, 246, 287, 346\\nQ-learning\\nalgorithm, 326, 330–332, 334, 335,\\n337\\nupdate, 332\\nQP, 66, 68, 83, 85, 192, 200, 205, 253,\\n255, 259, 260\\nconvex, 66, 74\\nquadratic programming, see QP\\nquery\\nequivalence, 297, 298, 300, 303, 311\\nmembership, 297–303, 311\\nsubset, 226, 227\\nQueryLearnAutomata algorithm, 298,\\n300\\nQuickSort algorithm, 230\\nrandomized, 230, 231, 234\\nRademacher complexity, 8, 33–40, 54, 56,\\n63, 78, 84, 133, 134, 183, 189,\\n190, 209, 211, 213, 220, 233,\\n237, 239, 241, 245, 267, 380\\nL\\np loss functions, 240\\nbinary classiﬁcation bound, 37\\nbound, 48, 240, 254, 259\\nconvex combinations, 132, 133\\nempirical, 34, 37, 38, 55, 77, 102,\\n103, 186, 380\\ngeneralization bounds, 103\\nkernel-based hypotheses, 102, 247\\nlinear hypotheses, 77INDEX 409\\nlinear hypotheses with bounded L1\\nnorm, 257, 258\\nlocal, 54\\nmargin bound\\nbinary classiﬁcation, 80\\nensembles, 133\\nmulti-class classiﬁcation, 187\\nranking, 212\\nmulti-class kernel-based hypotheses,\\n189, 206\\nregression bound, 239, 240, 262\\nRademacher variables, 34\\nradial basis function, 94\\nRadon’s theorem, 43, 44\\nrandom variable, 359\\nindependence, 39, 76, 289, 327, 361,\\n363, 365, 370, 376\\nindependent, 363, 365, 367\\nmeasurable, 359\\nmoment-generating function, 364\\nRandomized-Weighted-Majority algorithm,\\n147, 153–155, 175, 179\\nrank aggregation, 233\\nRankBoost, 8, 206–209, 214–220, 222–\\n224, 233–235\\nranking, 2, 7, 209, 229\\nbipartite, 221, 234\\nmultipartite, 235\\nRankBoost, 214\\nwith SVMs, 213\\nRankPerceptron, 234\\nrate\\nfalse positive, 225, 226\\ntrue positive, 225, 226, 232\\nrational kernel, 8, 83, 89, 106, 111, 113,\\n115, 119, 310\\nPDS, 112–115\\nRayleigh quotient, 283, 346\\nRBF, see radial basis function\\nrealizable case, 11, 49, 55, 59, 149–152,\\n162, 163\\nrecall, 232\\nregression, 2, 237\\nboosting trees, 263\\ndecision trees, 263\\ngroup norm, 260\\nKRR, 245, 247\\nLasso, 245, 257\\nlinear, 237, 245\\nneural networks, 263\\non-line, 261\\nordinal, 234\\nridge, see KRR\\nSVR, 245, 252\\nunbounded, 238, 262\\nregret, 148, 152, 154–157, 159, 172, 173,\\n175, 179–181, 228, 229\\naverage, 155\\nbound, 157–159, 174, 175, 179, 180,\\n209, 229\\nsecond-order, 179\\ncumulative, 179\\nexternal, 148, 175, 176\\ninstantaneous, 179, 180\\ninternal, 175, 176\\nlower bound, 155\\nminimization, 173–175, 179\\nper round, 155\\npreference function, 228, 229\\nranking, 228\\nswap, 175, 176\\nweak, 228\\nregular\\nexpression, 114, 295\\nlanguage, 295\\nregularization, 28, 142, 246\\nL\\n1-, 141, 142\\n-based algorithm, 28\\nparameter, 28, 181, 197\\npath, 259\\nterm, 28, 248, 250, 257, 271\\nregularizer, 28410 INDEX\\nrelative entropy, 142, 170, 171, 279\\nrepresenter theorem, 101, 115\\nreproducing\\nkernel Hilbert space, see Hilbert\\nspace\\nproperty, 95\\nreward, 8, 313–316, 330, 332, 335\\ncumulative, 318\\ndelayed, 314\\ndeterministic, 315, 317, 331\\nexpected, 316, 319, 322\\nfuture, 316, 335\\nimmediate, 8, 314, 316, 326, 335\\nlong-term, 8\\nprobability, 315, 319, 325, 326\\nvector, 320\\nrisk, 12, 380, see also error\\nempirical, 12, 380\\nminimization, see ERM\\nempirical minimization, 27\\npenalized empirical, 181\\nstructural\\nminimization, see SRM\\nRKHS, see Hilbert space\\nROC curve, 209, 224–226, 233, see also\\nAUC\\nRWM algorithm, see Randomized-\\nWeighted-Majority algorithm\\nsaddle point, 356, 357\\nnecessary condition, 356\\nsuﬃcient condition, 356\\nsample\\ncomplexity, 1, 11, 14, 16–18, 29, 30,\\n33, 52, 58\\ntest, 4\\ntraining, 3\\nvalidation, 3\\nsample space, 359\\nSARSA algorithm, 334, 335\\nSauer’s lemma, 45–48, 55, 56, 58\\nscenario\\ndeterministic, 25, 184, 210, 237\\nrandomized, 153\\nstochastic, 24, 25, 147, 184, 210, 227,\\n237\\nscore-based setting, 209, 211, 214, 221,\\n226, 227, 233\\nscores, 4\\nscoring function, 185, 189, 199, 202, 203,\\n210, 211, 235\\nsequence, 90, 106, 110, 111\\nkernel, 89, 106, 108, 111, 112\\nbigram, 113\\nmapping, 111\\nprotein, 106\\nsimilarity, 106\\nstochastic, 155\\nsequential minimal optimization algo-\\nrithm, see SMO algorithm\\nsetting\\ndeterministic, 25\\nstochastic, 24, 25, 171\\nshattering, 41, 241\\ncoeﬃcient, 55\\nwitness, 241\\nshortest-distance algorithm, 108, 111,\\n115\\nall-pairs, 286\\nsingular\\nvalue, 283–288, 344–346\\nvalue decomposition, see SVD\\nvector, 282–288, 291, 346, 347\\nslack variable, 71, 84, 191, 206, 214, 222,\\n248, 252\\nSMO algorithm, 68, 83, 85, 86\\nsort-by-degree algorithm, 229\\nSPSD, see symmetric positive semideﬁ-\\nnite\\nSRM, 27–29\\nstability, 233, 251, 256, 267–270, 277,\\n278, 372, 373INDEX 411\\nbound, 268, 277\\nKRR, 275, 278\\nranking, 277\\nregression, 278\\nSVM, 276, 278\\nSVR, 274\\ncoeﬃcient, 267, 268, 270–276\\nkernel, 263, 278\\nstable, 268, 273\\nstandard deviation, 6, 86, 365, 367\\nstandard normal\\ndistribution, 289, 290, 292, 360, 361,\\n364, 374\\nform, 374\\nrandom variable, 289\\nstate, 107, 313, 315\\ndestination, 294\\nﬁnal, 107\\ninitial, 107, 315\\norigin, 294\\nstart, 315\\nstate-action\\npair, 332\\nvalue, 333, 334\\nvalue function, see function\\nstationary point, 349\\nstochastic\\napproximation, 326\\ngradient descent, 161, 177, 261, 263,\\n266\\noptimization, 327, 337\\nstochasticity, 318\\nstrategy, 139\\ngrow-then-prune, 197\\nmixed, 138, 139\\npure, 138, 139\\nstring, 107, 108, 112, 113, 119, 294, 295,\\n298–300, 303–305, 307–312\\naccepted, 295, 296, 304, 305\\naccess, 299\\ncounter-example, 300\\ndistinguishing, 299, 301\\nempty, 106, 294, 295\\nﬁnality, 299\\nkernel, 106\\nleaf, 300\\nnegative, 296\\npartition, 299, 301\\npositive, 296, 306\\nrejected, 296, 309\\nstructural risk minimization, see SRM\\nstructure, 203\\nstructured\\noutput, 203, 204\\nprediction, 2, 183, 184, 203–205, 207\\nsubgradient, 272, 273\\nsubsequence, 119\\nsubsequences, 106\\nsubstring, 106\\nsum rule, 362\\nsupermartingale convergence, 328, 329\\nsupport vector, 67, 74, 162\\nmachine, see SVM\\nnetworks, 83\\nregression, see SVR\\nSVD, 98, 99, 345\\nSVM, 8, 63–75, 82–87, 89–91, 94, 100–\\n102, 106, 115, 118, 119, 131,\\n137, 142, 143, 162–164, 166–\\n168, 176, 177, 191, 192, 200,\\n201, 205, 209, 213, 214, 222,\\n233, 252, 253, 255, 256, 267,\\n271, 276, 278\\nmulti-class, 8, 183, 191, 203, 204,\\n206\\nranking with, 8, 213, 214, 233, 234\\nregression, see SVR\\nSVMStruct, 205\\nSVR, 237, 245, 252, 255–257, 260, 261,\\n263, 267, 271, 274, 275\\ndual, 262, 264412 INDEX\\non-line, see OnLineDualSVR al-\\ngorithm\\nHuber loss, 264\\non-line, 263\\nquadratic, 255, 256, 264\\non-line, 266\\nstability, 274\\ntarget\\nconcept, 12\\nvalues, 11\\nTD(λ) algorithm, 335, 336\\nTD(0) algorithm, 330, 331, 335\\ntheorem\\ncentral limit, 367\\nFermat’s, 349\\nFubini’s, 49, 363\\nMercer’s, 91\\nRadon’s, 43, 44\\nrepresenter, 101\\nvon Neumann’s minimax, 139, 174\\ntransducer\\nacyclic, 108\\ncomposition, 108, 109, 115, 380\\ncounting, 113, 114\\ninverse, 112\\nweighted, 106–109, 111–113\\ntransition, 107–112, 114, 294, 295, 299–\\n301, 304, 306–308, 310, 315–\\n317, 322, 326\\nlabel, 107\\nprobability, 315, 317–320, 325, 326\\ntrigrams, 90\\ntrue positive rate, 225, 226, 232\\nuniform convergence bound, 17, 23\\nuniform stability, see stability\\nuniformly β-stable, see stable\\nunion bound, 15, 362\\nupdate rule, 85, 169, 334\\nadditive, 169\\nmultiplicative, 169, 176\\nvalue iteration, 319, 324, see also Val-\\nueIteration algorithm\\nValueIteration algorithm, 320\\nvariance, 6, 54, 70, 166, 282–284, 289,\\n290, 365, 366, 371, 377, 378\\nunit, 287, 288, 360\\nVC-dimension, 8, 33, 41\\nensemble margin bound, 133\\ngeneralization bound, 48\\nlower bounds, 48, 49, 51\\nvector, 341\\nnorm, 341, 344, 345\\nsingular\\nleft, 345, 346\\nright, 345, 346\\nspace, 341, 342\\nnormed, 374\\nvon Neumann’s minimax theorem, 139,\\n174\\nweight function, 231, 235\\nWeighted-Majority algorithm, 147, 150–\\n152, 154, 156, 169, 175,see also\\nRandomized-Weighted-Majority\\nalgorithm\\nWidrow-Hoﬀ algorithm, 261\\non-line, 263\\nWinnow\\nalgorithm, 8, 147, 159, 168–171, 176\\nupdate, 169\\nWM algorithm, see Weighted-Majority\\nalgorithm\\nYoung’s inequality, 343Adaptive Computation and Machine Learning Thomas Dietterich, Editor\\nChristopher Bishop, David Heckerman, Michael Jordan, and Michael Kearns, As-\\nsociate Editors\\nBioinformatics: The Machine Learning Approach, Pierre Baldi and Søren Brunak\\nReinforcement Learning: An Introduction, Richard S. Sutton and Andrew G. Barto\\nGraphical Models for Machine Learning and Digital Communication, Brendan J.\\nFrey\\nLearning in Graphical Models, Michael I. Jordan\\nCausation, Prediction, and Search, second edition, Peter Spirtes, Clark Glymour,\\nand Richard Scheines\\nPrinciples of Data Mining, David Hand, Heikki Mannila, and Padhraic Smyth\\nBioinformatics: The Machine Learning Approach, second edition, Pierre Baldi and\\nSoren Brunak\\nLearning Kernel Classiﬁers: Theory and Algorithms, Ralf Herbrich\\nLearning with Kernels: Support Vector Machines, Regularization, Optimization,\\nand Beyond, Bernhard Sch¨olkopf and Alexander J. Smola\\nIntroduction to Machine Learning, Ethem Alpaydin\\nGaussian Processes for Machine Learning, Carl Edward Rasmussen and Christo-\\npher K.I. Williams\\nSemi-Supervised Learning, Olivier Chapelle, Bernhard Sch¨ olkopf, and Alexander\\nZien, Eds.\\nThe Minimum Description Length Principle, Peter D. Gr¨unwald\\nIntroduction to Statistical Relational Learning, Lise Getoor and Ben Taskar, Eds.\\nProbabilistic Graphical Models: Principles and Techniques, Daphne Koller and\\nNir FriedmanIntroduction to Machine Learning, second edition, Ethem Alpaydin\\nBoosting: Foundations and Algorithms, Robert E. Schapire and Yoav Freund\\nMachine Learning: A Probabilistic Perspective, Kevin P. Murphy\\nFoundations of Machine Learning, Mehryar Mohri, Afshin Rostamizadeh, and\\nAmeet Talwalkar'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["#chunk paragraps to sentences using nltk\n","import nltk\n","nltk.download('punkt_tab')\n","from nltk.tokenize import sent_tokenize\n","\n","sentences = sent_tokenize(pdf_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kFI-3PPfwyYd","executionInfo":{"status":"ok","timestamp":1771672417344,"user_tz":-330,"elapsed":6512,"user":{"displayName":"Avanti Buche","userId":"17362902153167164591"}},"outputId":"b742ee2e-ab25-489c-8b0c-4fddd8dd0e3c"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]}]},{"cell_type":"code","source":["# Convert text to vectors / Generate embedding\n","# Load embedding model\n","from sentence_transformers import SentenceTransformer\n","model = SentenceTransformer('all-MiniLM-L6-v2')\n","\n","embeddings = model.encode(sentences)\n","print(embeddings.shape)   # (4(rows), 384)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":713,"referenced_widgets":["b20ca1cf618d4b849c46d13452503996","baec4060975045108ef04e38dca25dd7","439b66d25e6548e698e143ed4f8e16c7","7176d3a6db6a43cca948f93598dd480c","ea0645d9c4334441b5ccca882314e791","bdc44a3b17a349b89c518480c1101153","fbb544b5bdf24796b29487600debca03","6d3cc8dec6c84e0396d013196d84fb17","4ddf4e006e194a2fad2352ade0eaf1b0","357c757f430b496fadbbe91f648343c0","5030db9b2f0f4b98b4500a934ac0ffc0","c9e53eb55b7d42aeb0d146e177042126","d1911460d22843328d34ef230dc10b5e","2ee789fcbf854633a14675c6e84ef843","4f249c79562949498be9c00e68f3aa56","453ad2d9bf9d4ef985c5cd8b9772da12","a9ccda1e024541c49691e3a2083fc5fe","2f1764ac61f74024a1406eaf70d87a1c","8a7a6a412aac4fea9bff48f564c4c4f9","e0c11e6feaf94707aa04185c86b135af","06a7041dd8bc4302a86087caeb672438","e08299523a544b43b10f9584f9534503","ebf981adb1984d32881748dcf2b8df69","58fde1ca389b4da0b90170fb489429d8","7a10c2b3a61c430a943cb7329be09cc8","673d63fa0eb847fb8c8d3b886635fdef","d0e0bc9d69b34d02b8b2fd2a149c088e","65eb81bc0cd54314887d65f8d6c25fb3","2c91252501d44994ad22957fd200044a","85405b2ade25423893b1868090abe554","7431982e081d4629a657c1eef50b8f5f","2dfb5fc67e3a4b4899cdf71e710f04ff","9af6800aa36d48339a95859187d6a6de","c44b11ecc30b4e37a61db8ba9e38a65c","1feb2ddde91f4dc78f06a68fd5ad5fe3","2559106ae98746a8b056e1c7ba5d266b","a4265518daee48feb9c212b9e9c0a30b","4afd0455163d4219b91a10496f4babf9","f33dda0ce05f4879971ee41c0799f98c","ff6db71b1c1048e2bef8e8ea562ca66a","143d4c2998094bfc8d60ebf3a5c56e08","4c0476e97c54441b893badb536b79591","5e19a9e205bc45c98fa10910dd10c15c","be639ec6a0924ecf83193a379a92d288","c502f87705e34a26a9bc0910822f273a","5aaa75f74b06478fb887e934cb66b589","f508bcc24d7f4569a6680acf42b16993","4cfcfb4f1ccc4796ac9196d441aad245","9e8553c251ac4b68bb7284782da8c0f7","cd1f60e79bea47d4ac7616b7a2338c48","4453e205da2146fd9ab91570d51eb11d","68db6624565f4d22830c84908d853662","e670515a26064b9c986b7c75667cbc51","8c955861da35467885156d30a7a6c052","ba72286ea83c4a2393992aaf384e40b7","e201a10d5a0b420c9e02181277eb920e","fc3659470e3648b1a76c849048ffa0fa","4f0ed3d077a241b49bcad1d1ae3d38ad","db0dfe33dcdc4403a5ccf6f9a1de27f2","a0ae516d522845fd95171d0bbd544a6f","3f21943c24f14a549e1a04d2c1ecdfbf","34695a3522804e239b44a02845a28603","269f0fc174e942fd9ba40fe975d9c7f2","f692b9a536ae4c0f9da3bd1543f24cb7","a611552b86e448dfa3daff4a0c861a51","8f634d03f33f4b0fb3f8a6f2a410af6b","7b03617ed98f439fbb0a95e225e24a61","86f554a6ed6d4cf9a696e8789a8ea83c","f346c358460b4c13b8088141e90efcd2","c20af6f69c744a79b1b918278860740b","97fe510b7e2043dcbf8ecc65f8e3e0f7","e7e48d30f8864258b255b8d8e94ec307","883341fe1f1a49e58585b88550518b59","b55f01da6e1646de8c8be5b00739d3c9","c537d3d6877c49d0b1461d78ecfde4ea","286bcb22854a4db19f5f06e8f854cd66","40eedc37cb6c4ffcbe43a5a910d2ac8d","3c5c75f0c35c49a2b6a69a6ad2078686","4cfed9ccc3b74f4fa8dbac2dda7ac7cf","3408b2c4ecc04651bdee6c3f9f02c10d","3407a2df8c4d4341be9a8c5d31261aef","618d5dbbd5e041f9908a58da02c77e07","af13f686f06d43fc8608cd55a7b1c141","a1d9ac8ad3b3465499b320699a8b99fa","39e5093dfbc44a1d9787fb1cc711795e","292ca77c18e54280b91fe8cc5dd727d8","8374aa02bdb946218f9522f2421de271","2bcae1d2f0734d9a88c054254e723e15","67c3e2bca9014ec4b5f5eb2cc1a2d121","b63f6515b05e4255abf1084789ae18a4","9aa8031cca36474680073061990d505c","614b968caced42cc811c0708a8fc8098","e04ec99b0d234d318eed6987762e759e","6c7268b169ec4e77857c9a50cb125a5c","b769630311614cada2bc64de1f0b1c7e","6bf0c66c4dbc4b79880b4835df7191a9","6910898877404007919e0f462f054386","5b9c5d4f34da4d04bba44a32fbcee73f","93ff7796c11f455e8901f02e8740d70c","1108d09ac7d7482780f7b66411dc74f5","8838033bd87d401bbc72d9eec4ba96b2","02e9835911ba4c2ca14d2e5aead2e10a","b62b546a08564c3e8f757aa8ea43572d","006a3398cb424f64b383351264e8b936","9b502c2ee6a94cad851d89c0c4f2a097","755f712d937440cb820b8b4f5e11687b","7da23274f60d44a98608fee0b1452a34","ae0295f541f94ab7a095e948b05c9de6","c609ef8e65734d7a85f027ed6cdcc12b","73c19384315843538edbad0661cf468b","d555e86160dd472aa780393493492aff","9098a07cfe2e458fa7da27ee1b2fcb4b","be711bc1f0c0414bb35f2bca37438046","fbddb950b5564ea483b02833b64814ab","c3201c8edf924a6ba530bfa5d3815ee7","15a1d4659407431897d9adb4a76a2d07","9603b9227be045bc945611312ce120f5","dad76eeb09434780aa84b766e55301e8","c483475716bf475f8923f1ca70414995","620a8a6960064bf8893c07b383e22e79","5baf8653980f4dbf827be500a307a248","1d0b87eaf7b44d58b39d713b34e89cb4","407058ee2060424caa16d50eec97d4c3","0eac74743c864bcc9524b20b51317779","6b9d70afe20e48ba8976b5daec15556e","7888acf64f884912a577c482eab85e83","74bcd9e02bcb4cf8bd1b3dad03225acb","11f7b481ec6b4432bcbe1a70c1f5e397","7a639ca6d7ed441cb3b49ecd2043d595","41e4d3b79c1b4013a1ff3fd476010e33","b7f6f4c72c6d48dd856593a919b90ea5","549a6aa5753e47df8860a70d265876b3"]},"id":"qTFBwSVjvL3E","executionInfo":{"status":"ok","timestamp":1771672662342,"user_tz":-330,"elapsed":242670,"user":{"displayName":"Avanti Buche","userId":"17362902153167164591"}},"outputId":"e356b61c-f660-4002-84ea-e910b208162b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b20ca1cf618d4b849c46d13452503996"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9e53eb55b7d42aeb0d146e177042126"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["README.md: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebf981adb1984d32881748dcf2b8df69"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n","WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"]},{"output_type":"display_data","data":{"text/plain":["sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c44b11ecc30b4e37a61db8ba9e38a65c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c502f87705e34a26a9bc0910822f273a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e201a10d5a0b420c9e02181277eb920e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b03617ed98f439fbb0a95e225e24a61"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n","Key                     | Status     |  | \n","------------------------+------------+--+-\n","embeddings.position_ids | UNEXPECTED |  | \n","\n","Notes:\n","- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c5c75f0c35c49a2b6a69a6ad2078686"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67c3e2bca9014ec4b5f5eb2cc1a2d121"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1108d09ac7d7482780f7b66411dc74f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d555e86160dd472aa780393493492aff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d0b87eaf7b44d58b39d713b34e89cb4"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["(5815, 384)\n"]}]},{"cell_type":"code","source":["#create vector database using faiss\n","import faiss\n","\n","dimension = embeddings.shape[1]\n","# Create index\n","index = faiss.IndexFlatL2(dimension)\n","\n","# Add vectors\n","index.add(np.array(embeddings))\n","\n","print(\"Total vectors stored:\", index.ntotal)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dP2gj0A1v3vZ","executionInfo":{"status":"ok","timestamp":1771672662356,"user_tz":-330,"elapsed":9,"user":{"displayName":"Avanti Buche","userId":"17362902153167164591"}},"outputId":"c980430a-4777-4c26-96e8-91dc3d1b4b5f"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Total vectors stored: 5815\n"]}]},{"cell_type":"code","source":["# Perform similar search\n","\n","query = \"What is a machine learning?\"\n","query_vector = model.encode([query])\n","\n","# Search top 10 similar results\n","k = 10\n","distances, indices = index.search(np.array(query_vector), k)\n","\n","print(\"Most similar documents:\")\n","for idx in indices[0]:\n","    print(sentences[idx])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VkEI0FEYwEZd","executionInfo":{"status":"ok","timestamp":1771672753112,"user_tz":-330,"elapsed":23,"user":{"displayName":"Avanti Buche","userId":"17362902153167164591"}},"outputId":"7dbcb185-db74-4a97-a915-eb86aa356a87"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Most similar documents:\n","Machine learning is a relatively recent ﬁeld and yet probably one of the most\n","active ones in computer science.\n","Since the success of a learning algorithm depends on the data used, machine\n","learning is inherently related to data analysis and statistics.\n","It covers fundamental modern topics in\n","machine learning while providing the theoretical basis and conceptual tools needed\n","for the discussion and justiﬁcation of algorithms.\n","The main practical objectives of machine learning consist of generating accurate\n","predictions for unseen items and of designing eﬃcient and robust algorithms to\n","produce these predictions, even for large-scale problems.\n","Finally, we thank the MIT Press publication team for their help and support in\n","the development of this text.1 Introduction\n","Machine learning can be broadly deﬁned as computational methods using experience\n","to improve performance or to make accurate predictions.\n","Machine learning consists of designing eﬃcient and accurate prediction algo-\n","rithms.\n","Machine learning.\n","The analysis of the conditions under which this can indeed\n","be realized is the topic of much modern theoretical and applied machine learning\n","research.\n","Supervised learning: The learner receives a set of labeled examples as training\n","data and makes predictions for all unseen points.\n","More generally, learning\n","techniques are data-driven methods combining fundamental concepts in computer\n","science with ideas from statistics, probability and optimization.\n"]}]},{"cell_type":"code","source":["query2 = \"What is analysis\"\n","query_vector2 = model.encode([query2])\n","\n","# Search top 10 similar results\n","k = 10\n","distances, indices = index.search(np.array(query_vector2), k)\n","\n","print(\"Most similar documents:\")\n","for idx in indices[0]:\n","    print(sentences[idx])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IVnHAZWFy8Dq","executionInfo":{"status":"ok","timestamp":1771672755374,"user_tz":-330,"elapsed":133,"user":{"displayName":"Avanti Buche","userId":"17362902153167164591"}},"outputId":"23ade15c-7195-4036-c18a-e8a0e3564bb6"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Most similar documents:\n","Such an algorithm-dependent analysis could have the beneﬁt\n","of a more informative guarantee.\n","Visualization: to visualize the data for exploratory analysis by mapping the input\n","data into two- or three-dimensional spaces.\n","Here,experience refers to\n","the past information available to the learner, which typically takes the form of\n","electronic data collected and made available for analysis.\n","A ﬁner analysis can be used in fact to eliminate that factor.\n","Since the success of a learning algorithm depends on the data used, machine\n","learning is inherently related to data analysis and statistics.\n","Clustering is often per-\n","formed to analyze very large data sets.\n","For example, in the context of social net-\n","work analysis, clustering algorithms attempt to identify “communities” within large\n","groups of people.\n","More generally, learning\n","techniques are data-driven methods combining fundamental concepts in computer\n","science with ideas from statistics, probability and optimization.\n","Regression is a common task in machine learning with a\n","variety of applications, which justiﬁes the speciﬁc chapter we reserve to its analysis.\n","One may ask if an analysis of the properties of a speciﬁc algorithm could lead\n","to ﬁner guarantees.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"9ZDZY6eT0Agg"},"execution_count":null,"outputs":[]}]}